{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#librerias\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import re \n",
    "import random\n",
    "import json \n",
    "import time\n",
    "import plotly.express as px\n",
    "from dataclasses import dataclass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_line(line):\n",
    "    line_split=line.split()\n",
    "    try:\n",
    "        int(line_split[0])\n",
    "        #si no dio error, okey entra\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "#vamos a hacer una función para cargar todos los datos de una   \n",
    "@dataclass\n",
    "class Data:\n",
    "    folder: str\n",
    "    load_stage: int\n",
    "    base_dir:str = \"/home/arturo/Documents/programacion_stuff/DeepElasticity/data/001-LinearElasticity\"\n",
    "    E: str = None\n",
    "    initialPosition_data_pd: pd.DataFrame =None\n",
    "    stress_data_pd: pd.DataFrame =None\n",
    "    displacement_data_pd: pd.DataFrame=None\n",
    "    restricted_data_pd: pd.DataFrame =None\n",
    "    force_data_pd: pd.DataFrame =None \n",
    "    final_data_pd: pd.DataFrame =None\n",
    "    collocation_data_np:pd.DataFrame =None\n",
    "    Pos_min: np.ndarray = None\n",
    "    Pos_max: np.ndarray = None\n",
    "    adjancet_matrix: np.ndarray=None\n",
    "\n",
    "    @classmethod\n",
    "    def keep_line(line):\n",
    "        line_split=line.split()\n",
    "        try:\n",
    "            int(line_split[0])\n",
    "            #si no dio error, okey entra\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def load_data(self,load_stage=10,normalize_pos=False):\n",
    "        # carga las posiciones de los nodos    \n",
    "        initialPosition_data_path = f\"{self.base_dir}/ARTURO_TEST_1/NODES.txt\"\n",
    "\n",
    "        with open(initialPosition_data_path,\"r\") as f:\n",
    "            aux_list=f.readlines()\n",
    "\n",
    "        initialPosition_data=[i for i in aux_list if keep_line(i)]\n",
    "        initialPosition_data=[i.strip(\"\\n\") for i in initialPosition_data]\n",
    "        initialPosition_data=[i.split() for i in initialPosition_data]\n",
    "        initialPosition_data_np=np.array(initialPosition_data).astype(float)[:,[0,1,2,3]]\n",
    "        self.initialPosition_data_pd=pd.DataFrame(initialPosition_data_np,columns=[\"Node\",\"X\",\"Y\",\"Z\"]).set_index(\"Node\")\n",
    "        self.Pos_min=self.initialPosition_data_pd.min().to_numpy()\n",
    "        self.Pos_max=self.initialPosition_data_pd.max().to_numpy()\n",
    "\n",
    "        #noramlizamos las posiciones \n",
    "        if normalize_pos:\n",
    "            self.initialPosition_data_pd=(self.initialPosition_data_pd-self.initialPosition_data_pd.min())/(self.initialPosition_data_pd.max()-self.initialPosition_data_pd.min())\n",
    "\n",
    "        #carga el streess\n",
    "        if self.E is not None:\n",
    "            stress_data_path = f\"{self.base_dir}/{self.folder}/PSOL_{load_stage}_NODAL_STRESSES_E{self.E}.txt\"\n",
    "        else: \n",
    "            stress_data_path = f\"{self.base_dir}/{self.folder}/PSOL_{load_stage}_NODAL_STRESSES.txt\"\n",
    "        with open(stress_data_path,\"r\") as f:\n",
    "            aux_list=f.readlines()\n",
    "\n",
    "\n",
    "        stress_data=[i for i in aux_list if keep_line(i)]\n",
    "        stress_data=[i.strip(\"\\n\").strip() for i in stress_data]\n",
    "        patron = \"[-.\\d]+E-*\\d{3}|^\\d{0,4}|0\\.0000\"\n",
    "\n",
    "        stress_data=[re.findall(patron,i) for i in stress_data]\n",
    "        stress_data=np.array(stress_data,dtype=float)\n",
    "        self.stress_data_pd=pd.DataFrame(stress_data,columns=[\"Node\",\"SX\",\"SY\",\"SZ\",\"SXY\",\"SYZ\",\"SXZ\"]).set_index(\"Node\")[[\"SX\",\"SY\",\"SZ\",\"SYZ\",\"SXZ\",\"SXY\"]] # lo queremos asi : s11,s22,s33,s23,s13,s12\n",
    "\n",
    "        # cargar datos de desplazamiento  \n",
    "        if self.E is not None:\n",
    "            displacement_data_path = f\"{self.base_dir}/{self.folder}/PSOL_{load_stage}_NODAL_DISP_E{self.E}.txt\"\n",
    "        else:\n",
    "            displacement_data_path = f\"{self.base_dir}/{self.folder}/PSOL_{load_stage}_NODAL_DISP.txt\"\n",
    "\n",
    "        with open(displacement_data_path,\"r\") as f:\n",
    "            aux_list=f.readlines()\n",
    "\n",
    "\n",
    "        displacement_data=[i for i in aux_list if keep_line(i)]\n",
    "        displacement_data=[i.strip(\"\\n\").strip() for i in displacement_data]\n",
    "        patron = r\"[-.\\d]+E-*\\d{3}|^\\d{0,4}|-?\\d+\\.\\d+|0\\.0000\"\n",
    "\n",
    "        displacement_data=[re.findall(patron,i) for i in displacement_data]\n",
    "        displacement_data=np.array(displacement_data,dtype=float)\n",
    "        self.displacement_data_pd=pd.DataFrame(displacement_data,columns=[\"Node\",\"UX\",\"UY\",\"UZ\",\"USUM\"]).set_index(\"Node\")\n",
    "\n",
    "        # cargar datos de boundaries en el movimiento\n",
    "        #cargar los nodos fijos\n",
    "        with open(f\"{self.base_dir}/ARTURO_TEST_1/RESTRINGED_NODES.txt\",\"r\") as f:\n",
    "            restricted_data=f.readlines()\n",
    "\n",
    "        restricted_data=[i for i in restricted_data if keep_line(i)]\n",
    "        restricted_data=[i.strip(\"\\n\") for i in restricted_data]\n",
    "        restricted_data=[i.split() for i in restricted_data]\n",
    "        restricted_data_np=np.array(restricted_data)[:,[0,1]]\n",
    "        self.restricted_data_pd=pd.DataFrame(restricted_data_np,columns=[\"Node\",\"Direccion\"])\n",
    "        self.restricted_data_pd=self.restricted_data_pd.groupby(\"Node\")[\"Direccion\"].apply(lambda x : list(x)).to_frame().sort_index()\n",
    "        self.restricted_data_pd.index=self.restricted_data_pd.index.astype(int)\n",
    "        self.restricted_data_pd=self.restricted_data_pd.sort_index()\n",
    "        self.restricted_data_pd=self.restricted_data_pd.rename(columns={\"Direccion\":\"Restricciones\"})\n",
    "\n",
    "        with open(f\"{self.base_dir}/ARTURO_TEST_1/FORCE_ON_NODES.txt\",\"r\") as f:\n",
    "            force_data=f.readlines()\n",
    "\n",
    "\n",
    "        force_data=[i for i in force_data if keep_line(i)]\n",
    "        force_data=[i.strip(\"\\n\") for i in force_data]\n",
    "        force_data=[i.split() for i in force_data]\n",
    "        force_data_np=np.array(force_data)[:,[0,1,2]]\n",
    "        self.force_data_pd=pd.DataFrame(force_data_np,columns=[\"Node\",\"Direccion_Fuerza\",\"Fuerza\"])\n",
    "        self.force_data_pd[\"Fuerza\"]=self.force_data_pd[\"Fuerza\"].astype(float)\n",
    "        self.force_data_pd=self.force_data_pd.set_index(\"Node\")\n",
    "        self.force_data_pd.index=self.force_data_pd.index.astype(int)\n",
    "\n",
    "\n",
    "        #ponemos todos los datos en común usando los nodos como clave   \n",
    "        self.final_data_pd=self.initialPosition_data_pd.merge(self.stress_data_pd,left_index=True,right_index=True,how=\"left\").merge(self.force_data_pd,left_index=True,right_index=True,how=\"left\").merge(self.displacement_data_pd,left_index=True,right_index=True,how=\"left\").merge(self.restricted_data_pd,left_index=True,right_index=True,how=\"left\")\n",
    "\n",
    "        self.final_data_pd[\"Final_X\"]=self.final_data_pd[\"X\"]-self.final_data_pd[\"UX\"]\n",
    "        self.final_data_pd[\"Final_Y\"]=self.final_data_pd[\"Y\"]-self.final_data_pd[\"UY\"]\n",
    "        self.final_data_pd[\"Final_Z\"]=self.final_data_pd[\"Z\"]-self.final_data_pd[\"UZ\"]\n",
    "\n",
    "    def create_colloc_points(self,n_colloc=70000,colloc_type=\"random_2\"):\n",
    "\n",
    "        if colloc_type==\"random_1\":\n",
    "\n",
    "            X_coloc = np.random.uniform(self.Pos_min[0], self.Pos_max[0], (n_colloc, 1))\n",
    "            Y_coloc = np.random.uniform(self.Pos_min[1], self.Pos_max[1], (n_colloc, 1))\n",
    "            Z_coloc = np.random.uniform(self.Pos_min[2], self.Pos_max[2], (n_colloc, 1))\n",
    "            self.collocation_data_np = np.array(np.meshgrid(X_coloc, Y_coloc,Z_coloc)).T.reshape(-1, 3)           # Combine the two arrays\n",
    "            \n",
    "\n",
    "        \n",
    "        elif colloc_type==\"random_2\":\n",
    "            #este tipo de random es una distribucion por todo el espacio para cada punto\n",
    "            points=[]\n",
    "            for _ in range(n_colloc):\n",
    "                X_coloc_aux = np.random.uniform(self.Pos_min[0], self.Pos_max[0], 1)\n",
    "                Y_coloc_aux = np.random.uniform(self.Pos_min[1], self.Pos_max[1], 1)\n",
    "                Z_coloc_aux = np.random.uniform(self.Pos_min[2], self.Pos_max[2], 1)\n",
    "                points.append(np.array([X_coloc_aux,Y_coloc_aux,Z_coloc_aux]))\n",
    "\n",
    "            self.collocation_data_np = np.array(points).reshape(-1, 3)\n",
    "\n",
    "\n",
    "    def prepare_pytorch_data(self,n_colloc=70000,colloc_type=\"random_2\",percentage_stress_data=0.8,train_percent=0.8):\n",
    "        #para el entrenamiento necesitamos diferentes conjuntos de datos\n",
    "\n",
    "        # 1. los datos experimentales que saldrán de los datos sin limitaciones de movimiento\n",
    "        # pero no haremos ninguna diferenciación más, los que tengan una fuerza aplicada nos da igual\n",
    "        # lo tomamos también\n",
    "\n",
    "        # 1.1. Otro set de datos que son los que tienen limitaciones totales del movimiento\n",
    "        # 1.2. Otro que será los que tengan limitaciones direccionales del movimiento\n",
    "\n",
    "        # estos dos anteriores, simplemente tienen desplamiento nulo en las direcciones que correspondan\n",
    "        # por eso lo vamos a meter con los datos normales, pero si quisieramos darles mayor importancia\n",
    "        # podríamos tenerlos en un término a parte den la funcion de perdida.   \n",
    "\n",
    "        # 2. Los collocation points, esto son x,y,z repartidas por todo el dominio que nos interese  \n",
    "\n",
    "\n",
    "        # 3. Las BC. Aquí entran las NBC, y en el futuro la DBC de antes. Antes imponíamos solo donde estaba\n",
    "        # aplicada la fuerza, pero esto no tiene porque ser así, es más, estamos muy limitados si así es. Si solo quisiéramos \n",
    "        # darle información de la Fuerza, habría que ver otra manera creo yo. Por ahora, le voy a dar todos o un subconjunto de los \n",
    "        # puntos y valores de sigma en la superficie, para que lo tenga como referencia para aprender la fisica y hacer bien los\n",
    "        # desplazamientos.\n",
    "\n",
    "        # Los datos de test será sacados del conjunto de datos no restringidos de desplazamientos\n",
    "\n",
    "        # El tema de la normalizacion: ----\n",
    "\n",
    "\n",
    "        # colloc points\n",
    "        if self.collocation_data_np is None:\n",
    "            self.create_colloc_points(n_colloc=n_colloc,colloc_type=colloc_type)\n",
    "        return_colloc_points=torch.tensor(self.collocation_data_np,requires_grad=True)\n",
    "\n",
    "        # desp_data, tanto los limitados como el resto  \n",
    "\n",
    "        # no limitados, separamos en 80 y 20\n",
    "        indx_non_restricted=[int(i) for i in self.displacement_data_pd.index if i not in self.restricted_data_pd.index ]\n",
    "        random.shuffle(indx_non_restricted)\n",
    "        self.indx_train_non_restricted,self.indx_test_non_restricted = indx_non_restricted[:int(len(indx_non_restricted)//(1/train_percent))], indx_non_restricted[int(len(indx_non_restricted)//(1/train_percent)):]\n",
    "        \n",
    "        train_init_pos_non_restricted, train_disp_non_restricted = torch.tensor(self.initialPosition_data_pd.loc[self.indx_train_non_restricted,[\"X\",\"Y\",\"Z\"]].to_numpy(),requires_grad=True) , torch.tensor(self.displacement_data_pd.loc[self.indx_train_non_restricted,[\"UX\",\"UY\",\"UZ\"]].to_numpy(),requires_grad=True)\n",
    "        test_init_pos_non_restricted, test_disp_non_restricted = torch.tensor(self.initialPosition_data_pd.loc[self.indx_test_non_restricted,[\"X\",\"Y\",\"Z\"]].to_numpy(),requires_grad=True) , torch.tensor(self.displacement_data_pd.loc[self.indx_test_non_restricted,[\"UX\",\"UY\",\"UZ\"]].to_numpy(),requires_grad=True)\n",
    "\n",
    "        # limitados\n",
    "        initpos_restricted_data=torch.tensor(self.initialPosition_data_pd.loc[self.restricted_data_pd.index,[\"X\",\"Y\",\"Z\"]].to_numpy(),requires_grad=True)\n",
    "\n",
    "        disp_restricted_data=torch.tensor(self.displacement_data_pd.loc[self.restricted_data_pd.index,[\"UX\",\"UY\",\"UZ\"]].to_numpy(),requires_grad=True)\n",
    "\n",
    "        train_init_pos_main,train_disp_main=torch.concat([train_init_pos_non_restricted,initpos_restricted_data]),torch.concat([train_disp_non_restricted,disp_restricted_data])\n",
    "        test_init_pos_main,test_disp_main=test_init_pos_non_restricted, test_disp_non_restricted\n",
    "\n",
    "        #los indices los tenemos trazados:   \n",
    "\n",
    "        self.index_train=list(self.indx_train_non_restricted) + self.restricted_data_pd.index.to_list() \n",
    "        self.index_test=list(self.indx_test_non_restricted)\n",
    "        # data del NCB\n",
    "        # voy a escoder un pocentaje de datos de estos de los que dar.\n",
    "        selected_stresses=self.stress_data_pd.sample(frac=percentage_stress_data,axis=0)\n",
    "        self.index_stress=selected_stresses.index.to_list()\n",
    "        position_selected_stresses=torch.tensor(self.initialPosition_data_pd.loc[selected_stresses.index].to_numpy(),requires_grad=True)\n",
    "        return_stress=torch.tensor(selected_stresses.to_numpy(),requires_grad=True)\n",
    "        # si quisieramos hacer una separacion\n",
    "        #from sklearn... import train_test_split\n",
    "        #train_test_split(self.stress_data_pd.to_numpy())\n",
    "\n",
    "        return train_init_pos_main.float(),train_disp_main.float(),test_init_pos_main.float(),test_disp_main.float(),position_selected_stresses.float(),return_stress.float(),return_colloc_points.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Data(\"MULTIPLE_E_VALUES_NEW\",10,E=\"0.005\")\n",
    "data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = f\"{data.base_dir}/ARTURO_TEST_1/ELEMENT.txt\"\n",
    "with open(aux,\"r\") as f:\n",
    "    aux_list=f.readlines()\n",
    "aux2=[i for i in aux_list if keep_line(i)]\n",
    "aux3=[i.strip(\"\\n\") for i in aux2]\n",
    "aux4=[i.split() for i in aux3]\n",
    "\n",
    "aux5=np.array(aux4).astype(int)[:,-8:]-1\n",
    "AM=np.zeros((aux5.max()+1,aux5.max()+1))\n",
    "print(AM.shape)\n",
    "\n",
    "for i in aux5:\n",
    "    for j in i:\n",
    "        AM[i,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_symmetric(AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0f0fd18790>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_pos_main,train_disp_main,test_init_pos_main,test_disp_main,position_selected_stresses,return_stress,return_colloc_points=data.prepare_pytorch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29.9780, 29.6480, 54.3700],\n",
       "        [36.7280, 52.0220, 25.7490],\n",
       "        [53.8730, 30.0980, 42.9400],\n",
       "        ...,\n",
       "        [54.9290, 45.5750, 33.1110],\n",
       "        [25.8770, 45.5720, 26.5590],\n",
       "        [35.8910, 26.4700, 50.9610]], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_init_pos_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_nodos_train=[i-1 for i in data.index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3452, 3452)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AM[indice_nodos_train][:,indice_nodos_train].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AM[indice_nodos_train,indice_nodos_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_nodos_test=[i-1 for i in data.index_test]\n",
    "\n",
    "# Convertir la matriz de adyacencia en un grafo NetworkX\n",
    "G_test = nx.from_numpy_array(AM[indice_nodos_test][:,indice_nodos_test])\n",
    "\n",
    "# Convertir el grafo NetworkX en un Data object de PyG\n",
    "edge_index_test = from_networkx(G_test).edge_index\n",
    "\n",
    "# Separar las características (XYZ) y las etiquetas (UVW)\n",
    "node_features_test = train_init_pos_main\n",
    "node_labels_test = train_disp_main\n",
    "\n",
    "# Crear el objeto Data\n",
    "data_graph_test = torch_geometric.data.Data(x=node_features_test, edge_index=edge_index_test,y=node_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convertir la matriz de adyacencia en un grafo NetworkX\n",
    "G = nx.from_numpy_array(AM[indice_nodos_train][:,indice_nodos_train])\n",
    "\n",
    "# Convertir el grafo NetworkX en un Data object de PyG\n",
    "edge_index = from_networkx(G).edge_index\n",
    "\n",
    "# Separar las características (XYZ) y las etiquetas (UVW)\n",
    "node_features = train_init_pos_main\n",
    "node_labels = train_disp_main\n",
    "\n",
    "# Crear el objeto Data\n",
    "data_graph = torch_geometric.data.Data(x=node_features, edge_index=edge_index,y=node_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GraphNetwork, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 256)\n",
    "        self.conv3 = GCNConv(256, 128)\n",
    "        self.conv4 = GCNConv(128, 3)  # 3 para UVW\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch_geometric.loader.DataLoader([data_graph], batch_size=32, shuffle=True)\n",
    "test_loader = torch_geometric.loader.DataLoader([data_graph_test], batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.00954055786133\n",
      "1780.94140625\n",
      "124.82212829589844\n",
      "10.888711929321289\n",
      "16.032329559326172\n",
      "16.299205780029297\n",
      "12.954318046569824\n",
      "11.254220008850098\n",
      "11.39570140838623\n",
      "10.816266059875488\n",
      "9.932208061218262\n",
      "10.282572746276855\n",
      "9.942652702331543\n",
      "8.860779762268066\n",
      "8.689820289611816\n",
      "8.793295860290527\n",
      "8.481279373168945\n",
      "8.109197616577148\n",
      "8.106876373291016\n",
      "7.874996662139893\n",
      "7.699046611785889\n",
      "7.59025239944458\n",
      "7.246985912322998\n",
      "7.012524127960205\n",
      "6.896516799926758\n",
      "6.727396488189697\n",
      "6.580513954162598\n",
      "6.737776279449463\n",
      "6.560667991638184\n",
      "6.290341854095459\n",
      "6.321619033813477\n",
      "6.193453311920166\n",
      "6.614050388336182\n",
      "6.264137268066406\n",
      "5.991918563842773\n",
      "5.862837314605713\n",
      "5.985054016113281\n",
      "5.978955268859863\n",
      "5.868234634399414\n",
      "5.875114440917969\n",
      "5.647683620452881\n",
      "5.592000961303711\n",
      "5.395564556121826\n",
      "5.5066962242126465\n",
      "5.397740364074707\n",
      "5.290587425231934\n",
      "5.42905330657959\n",
      "5.259007930755615\n",
      "5.1197285652160645\n",
      "5.1576151847839355\n",
      "5.1289496421813965\n",
      "5.203034400939941\n",
      "5.181918144226074\n",
      "5.038269519805908\n",
      "4.972908973693848\n",
      "4.994227409362793\n",
      "4.735247611999512\n",
      "5.020314693450928\n",
      "4.837803840637207\n",
      "4.488053321838379\n",
      "4.725003719329834\n",
      "4.619722366333008\n",
      "4.457147121429443\n",
      "4.867295742034912\n",
      "4.402186393737793\n",
      "4.350686550140381\n",
      "4.0902323722839355\n",
      "4.335694313049316\n",
      "4.4489898681640625\n",
      "4.497030258178711\n",
      "4.128516674041748\n",
      "4.1191840171813965\n",
      "4.282498836517334\n",
      "4.063050746917725\n",
      "3.872951030731201\n",
      "4.105218410491943\n",
      "4.187803268432617\n",
      "3.8903439044952393\n",
      "4.133499622344971\n",
      "4.024093151092529\n",
      "3.9221441745758057\n",
      "3.6368517875671387\n",
      "3.790344476699829\n",
      "3.443835973739624\n",
      "4.029586315155029\n",
      "3.673731565475464\n",
      "3.8970108032226562\n",
      "3.8130860328674316\n",
      "3.7870442867279053\n",
      "3.6370832920074463\n",
      "4.086495876312256\n",
      "3.724323034286499\n",
      "3.5074846744537354\n",
      "3.599107265472412\n",
      "4.014111518859863\n",
      "3.533820152282715\n",
      "3.58644700050354\n",
      "3.647144317626953\n",
      "3.481987237930298\n",
      "3.457515001296997\n",
      "3.418217658996582\n",
      "3.3958067893981934\n",
      "3.3279061317443848\n",
      "3.4237685203552246\n",
      "3.6306941509246826\n",
      "3.538785219192505\n",
      "3.3153481483459473\n",
      "3.901075601577759\n",
      "3.5853192806243896\n",
      "3.2005064487457275\n",
      "3.9652109146118164\n",
      "4.228705406188965\n",
      "3.2871923446655273\n",
      "4.032564163208008\n",
      "3.406324625015259\n",
      "4.001715183258057\n",
      "3.3285787105560303\n",
      "3.625286102294922\n",
      "3.557056188583374\n",
      "3.766326904296875\n",
      "3.58298921585083\n",
      "3.4401233196258545\n",
      "3.236631393432617\n",
      "3.3715150356292725\n",
      "3.352144718170166\n",
      "3.650818347930908\n",
      "3.4890148639678955\n",
      "3.7425076961517334\n",
      "3.1079392433166504\n",
      "3.42094087600708\n",
      "3.2473089694976807\n",
      "3.6834397315979004\n",
      "3.2931995391845703\n",
      "3.2929558753967285\n",
      "3.1735918521881104\n",
      "3.1890177726745605\n",
      "3.2358038425445557\n",
      "3.30452823638916\n",
      "3.130401372909546\n",
      "3.273021697998047\n",
      "3.298563241958618\n",
      "3.1451215744018555\n",
      "3.034212827682495\n",
      "3.1580770015716553\n",
      "3.2124853134155273\n",
      "3.160659074783325\n",
      "3.1194558143615723\n",
      "3.220444917678833\n",
      "3.080488443374634\n",
      "3.0497868061065674\n",
      "3.142183542251587\n",
      "3.123824119567871\n",
      "3.0215299129486084\n",
      "3.543990135192871\n",
      "3.2158005237579346\n",
      "3.209148406982422\n",
      "3.1000678539276123\n",
      "3.197021007537842\n",
      "3.1647872924804688\n",
      "3.019665002822876\n",
      "3.318528175354004\n",
      "3.0462090969085693\n",
      "2.987095355987549\n",
      "2.9862687587738037\n",
      "2.809173822402954\n",
      "2.9999969005584717\n",
      "2.69960618019104\n",
      "2.9390876293182373\n",
      "2.9658915996551514\n",
      "2.800144910812378\n",
      "2.979684591293335\n",
      "3.0323476791381836\n",
      "2.9609193801879883\n",
      "3.0562245845794678\n",
      "2.9070959091186523\n",
      "3.121703624725342\n",
      "3.0378708839416504\n",
      "2.9172871112823486\n",
      "2.8541676998138428\n",
      "2.639235019683838\n",
      "2.906661033630371\n",
      "3.08805775642395\n",
      "3.0433597564697266\n",
      "2.9812052249908447\n",
      "2.9650068283081055\n",
      "2.8558976650238037\n",
      "3.100996494293213\n",
      "3.0441925525665283\n",
      "3.052314043045044\n",
      "2.971712112426758\n",
      "2.8381893634796143\n",
      "3.062840461730957\n",
      "2.923466444015503\n",
      "3.0361461639404297\n",
      "2.838766098022461\n",
      "2.671921730041504\n",
      "2.9476184844970703\n",
      "2.9097094535827637\n",
      "2.8245019912719727\n",
      "2.9730849266052246\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_graph=data_graph.to(device)\n",
    "data_graph_test=data_graph_test.to(device)\n",
    "model = GraphNetwork(num_features=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data_graph)\n",
    "    loss = criterion(output, data_graph.y)\n",
    "    print(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 2.6339\n",
      "Epoch: 1, Train Loss: 2.7275\n",
      "Epoch: 2, Train Loss: 2.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 2.5837\n",
      "Epoch: 4, Train Loss: 2.5904\n",
      "Epoch: 5, Train Loss: 2.6400\n",
      "Epoch: 6, Train Loss: 2.8131\n",
      "Epoch: 7, Train Loss: 2.9780\n",
      "Epoch: 8, Train Loss: 2.6071\n",
      "Epoch: 9, Train Loss: 2.7410\n",
      "Epoch: 10, Train Loss: 2.6966\n",
      "Epoch: 11, Train Loss: 2.6755\n",
      "Epoch: 12, Train Loss: 2.7488\n",
      "Epoch: 13, Train Loss: 2.6804\n",
      "Epoch: 14, Train Loss: 2.6357\n",
      "Epoch: 15, Train Loss: 2.8737\n",
      "Epoch: 16, Train Loss: 3.2688\n",
      "Epoch: 17, Train Loss: 3.6444\n",
      "Epoch: 18, Train Loss: 2.7591\n",
      "Epoch: 19, Train Loss: 3.6375\n",
      "Epoch: 20, Train Loss: 4.1303\n",
      "Epoch: 21, Train Loss: 3.0536\n",
      "Epoch: 22, Train Loss: 3.8251\n",
      "Epoch: 23, Train Loss: 2.6613\n",
      "Epoch: 24, Train Loss: 3.3545\n",
      "Epoch: 25, Train Loss: 2.5794\n",
      "Epoch: 26, Train Loss: 3.4652\n",
      "Epoch: 27, Train Loss: 2.6399\n",
      "Epoch: 28, Train Loss: 2.7718\n",
      "Epoch: 29, Train Loss: 2.6585\n",
      "Epoch: 30, Train Loss: 2.9024\n",
      "Epoch: 31, Train Loss: 2.7144\n",
      "Epoch: 32, Train Loss: 2.6193\n",
      "Epoch: 33, Train Loss: 2.6956\n",
      "Epoch: 34, Train Loss: 2.7248\n",
      "Epoch: 35, Train Loss: 2.6790\n",
      "Epoch: 36, Train Loss: 2.5781\n",
      "Epoch: 37, Train Loss: 2.6998\n",
      "Epoch: 38, Train Loss: 2.4896\n",
      "Epoch: 39, Train Loss: 2.4834\n",
      "Epoch: 40, Train Loss: 2.4690\n",
      "Epoch: 41, Train Loss: 2.6013\n",
      "Epoch: 42, Train Loss: 2.5162\n",
      "Epoch: 43, Train Loss: 2.5833\n",
      "Epoch: 44, Train Loss: 2.7401\n",
      "Epoch: 45, Train Loss: 2.5031\n",
      "Epoch: 46, Train Loss: 2.8890\n",
      "Epoch: 47, Train Loss: 2.5297\n",
      "Epoch: 48, Train Loss: 2.6921\n",
      "Epoch: 49, Train Loss: 2.5871\n",
      "Epoch: 50, Train Loss: 2.6473\n",
      "Epoch: 51, Train Loss: 2.6849\n",
      "Epoch: 52, Train Loss: 2.4122\n",
      "Epoch: 53, Train Loss: 2.5931\n",
      "Epoch: 54, Train Loss: 2.3859\n",
      "Epoch: 55, Train Loss: 2.2945\n",
      "Epoch: 56, Train Loss: 2.4708\n",
      "Epoch: 57, Train Loss: 2.4056\n",
      "Epoch: 58, Train Loss: 2.5854\n",
      "Epoch: 59, Train Loss: 2.5485\n",
      "Epoch: 60, Train Loss: 2.5861\n",
      "Epoch: 61, Train Loss: 2.5349\n",
      "Epoch: 62, Train Loss: 2.5466\n",
      "Epoch: 63, Train Loss: 2.3918\n",
      "Epoch: 64, Train Loss: 2.6121\n",
      "Epoch: 65, Train Loss: 2.4359\n",
      "Epoch: 66, Train Loss: 2.3849\n",
      "Epoch: 67, Train Loss: 2.6118\n",
      "Epoch: 68, Train Loss: 2.4807\n",
      "Epoch: 69, Train Loss: 2.6328\n",
      "Epoch: 70, Train Loss: 2.3720\n",
      "Epoch: 71, Train Loss: 2.5225\n",
      "Epoch: 72, Train Loss: 2.6119\n",
      "Epoch: 73, Train Loss: 2.6183\n",
      "Epoch: 74, Train Loss: 2.2045\n",
      "Epoch: 75, Train Loss: 2.4519\n",
      "Epoch: 76, Train Loss: 2.0765\n",
      "Epoch: 77, Train Loss: 2.5101\n",
      "Epoch: 78, Train Loss: 2.7393\n",
      "Epoch: 79, Train Loss: 2.5967\n",
      "Epoch: 80, Train Loss: 2.3420\n",
      "Epoch: 81, Train Loss: 2.6123\n",
      "Epoch: 82, Train Loss: 2.3617\n",
      "Epoch: 83, Train Loss: 2.3257\n",
      "Epoch: 84, Train Loss: 2.7703\n",
      "Epoch: 85, Train Loss: 2.3166\n",
      "Epoch: 86, Train Loss: 2.4224\n",
      "Epoch: 87, Train Loss: 2.6741\n",
      "Epoch: 88, Train Loss: 2.3912\n",
      "Epoch: 89, Train Loss: 2.5017\n",
      "Epoch: 90, Train Loss: 2.1498\n",
      "Epoch: 91, Train Loss: 2.4364\n",
      "Epoch: 92, Train Loss: 2.3851\n",
      "Epoch: 93, Train Loss: 2.2361\n",
      "Epoch: 94, Train Loss: 2.3121\n",
      "Epoch: 95, Train Loss: 2.3902\n",
      "Epoch: 96, Train Loss: 2.2427\n",
      "Epoch: 97, Train Loss: 2.1227\n",
      "Epoch: 98, Train Loss: 2.3972\n",
      "Epoch: 99, Train Loss: 2.1855\n",
      "Epoch: 100, Train Loss: 2.1536\n",
      "Epoch: 101, Train Loss: 2.3661\n",
      "Epoch: 102, Train Loss: 2.2879\n",
      "Epoch: 103, Train Loss: 2.2125\n",
      "Epoch: 104, Train Loss: 2.1221\n",
      "Epoch: 105, Train Loss: 2.3395\n",
      "Epoch: 106, Train Loss: 2.7851\n",
      "Epoch: 107, Train Loss: 2.5574\n",
      "Epoch: 108, Train Loss: 2.1379\n",
      "Epoch: 109, Train Loss: 2.2776\n",
      "Epoch: 110, Train Loss: 2.0222\n",
      "Epoch: 111, Train Loss: 2.3593\n",
      "Epoch: 112, Train Loss: 1.9691\n",
      "Epoch: 113, Train Loss: 2.1131\n",
      "Epoch: 114, Train Loss: 2.0153\n",
      "Epoch: 115, Train Loss: 2.0826\n",
      "Epoch: 116, Train Loss: 2.1772\n",
      "Epoch: 117, Train Loss: 2.4315\n",
      "Epoch: 118, Train Loss: 2.0562\n",
      "Epoch: 119, Train Loss: 2.3571\n",
      "Epoch: 120, Train Loss: 2.0957\n",
      "Epoch: 121, Train Loss: 2.2813\n",
      "Epoch: 122, Train Loss: 2.3099\n",
      "Epoch: 123, Train Loss: 2.1630\n",
      "Epoch: 124, Train Loss: 2.0205\n",
      "Epoch: 125, Train Loss: 2.4029\n",
      "Epoch: 126, Train Loss: 3.6114\n",
      "Epoch: 127, Train Loss: 2.1665\n",
      "Epoch: 128, Train Loss: 2.4648\n",
      "Epoch: 129, Train Loss: 3.3561\n",
      "Epoch: 130, Train Loss: 2.2206\n",
      "Epoch: 131, Train Loss: 3.1184\n",
      "Epoch: 132, Train Loss: 3.1734\n",
      "Epoch: 133, Train Loss: 2.4932\n",
      "Epoch: 134, Train Loss: 3.0858\n",
      "Epoch: 135, Train Loss: 2.2040\n",
      "Epoch: 136, Train Loss: 2.9909\n",
      "Epoch: 137, Train Loss: 2.3985\n",
      "Epoch: 138, Train Loss: 3.1163\n",
      "Epoch: 139, Train Loss: 2.2558\n",
      "Epoch: 140, Train Loss: 2.5565\n",
      "Epoch: 141, Train Loss: 2.3307\n",
      "Epoch: 142, Train Loss: 2.6351\n",
      "Epoch: 143, Train Loss: 2.6146\n",
      "Epoch: 144, Train Loss: 2.2170\n",
      "Epoch: 145, Train Loss: 2.5506\n",
      "Epoch: 146, Train Loss: 2.0588\n",
      "Epoch: 147, Train Loss: 2.0935\n",
      "Epoch: 148, Train Loss: 2.2391\n",
      "Epoch: 149, Train Loss: 2.2879\n",
      "Epoch: 150, Train Loss: 1.8988\n",
      "Epoch: 151, Train Loss: 2.0161\n",
      "Epoch: 152, Train Loss: 2.1103\n",
      "Epoch: 153, Train Loss: 2.0448\n",
      "Epoch: 154, Train Loss: 2.0227\n",
      "Epoch: 155, Train Loss: 2.2369\n",
      "Epoch: 156, Train Loss: 2.1780\n",
      "Epoch: 157, Train Loss: 2.2062\n",
      "Epoch: 158, Train Loss: 2.1974\n",
      "Epoch: 159, Train Loss: 1.9421\n",
      "Epoch: 160, Train Loss: 2.0435\n",
      "Epoch: 161, Train Loss: 2.1129\n",
      "Epoch: 162, Train Loss: 1.9434\n",
      "Epoch: 163, Train Loss: 2.0773\n",
      "Epoch: 164, Train Loss: 2.1142\n",
      "Epoch: 165, Train Loss: 1.8931\n",
      "Epoch: 166, Train Loss: 1.7555\n",
      "Epoch: 167, Train Loss: 1.8710\n",
      "Epoch: 168, Train Loss: 2.1515\n",
      "Epoch: 169, Train Loss: 2.0336\n",
      "Epoch: 170, Train Loss: 1.8233\n",
      "Epoch: 171, Train Loss: 1.7003\n",
      "Epoch: 172, Train Loss: 1.8903\n",
      "Epoch: 173, Train Loss: 2.1521\n",
      "Epoch: 174, Train Loss: 1.8996\n",
      "Epoch: 175, Train Loss: 2.1684\n",
      "Epoch: 176, Train Loss: 1.8007\n",
      "Epoch: 177, Train Loss: 1.9982\n",
      "Epoch: 178, Train Loss: 2.3522\n",
      "Epoch: 179, Train Loss: 1.8716\n",
      "Epoch: 180, Train Loss: 2.3575\n",
      "Epoch: 181, Train Loss: 2.5604\n",
      "Epoch: 182, Train Loss: 1.9723\n",
      "Epoch: 183, Train Loss: 2.3194\n",
      "Epoch: 184, Train Loss: 2.1586\n",
      "Epoch: 185, Train Loss: 2.4155\n",
      "Epoch: 186, Train Loss: 2.3285\n",
      "Epoch: 187, Train Loss: 2.0788\n",
      "Epoch: 188, Train Loss: 1.9644\n",
      "Epoch: 189, Train Loss: 2.0319\n",
      "Epoch: 190, Train Loss: 1.8334\n",
      "Epoch: 191, Train Loss: 2.2641\n",
      "Epoch: 192, Train Loss: 2.3472\n",
      "Epoch: 193, Train Loss: 1.9177\n",
      "Epoch: 194, Train Loss: 1.8358\n",
      "Epoch: 195, Train Loss: 2.2003\n",
      "Epoch: 196, Train Loss: 1.7781\n",
      "Epoch: 197, Train Loss: 1.8953\n",
      "Epoch: 198, Train Loss: 1.8771\n",
      "Epoch: 199, Train Loss: 1.9412\n",
      "Epoch: 200, Train Loss: 2.1680\n",
      "Epoch: 201, Train Loss: 1.9420\n",
      "Epoch: 202, Train Loss: 1.8280\n",
      "Epoch: 203, Train Loss: 1.6253\n",
      "Epoch: 204, Train Loss: 2.0504\n",
      "Epoch: 205, Train Loss: 1.9293\n",
      "Epoch: 206, Train Loss: 1.7667\n",
      "Epoch: 207, Train Loss: 2.0071\n",
      "Epoch: 208, Train Loss: 1.8949\n",
      "Epoch: 209, Train Loss: 1.6722\n",
      "Epoch: 210, Train Loss: 2.1014\n",
      "Epoch: 211, Train Loss: 2.3386\n",
      "Epoch: 212, Train Loss: 2.1529\n",
      "Epoch: 213, Train Loss: 1.7644\n",
      "Epoch: 214, Train Loss: 1.6705\n",
      "Epoch: 215, Train Loss: 1.8941\n",
      "Epoch: 216, Train Loss: 1.7053\n",
      "Epoch: 217, Train Loss: 1.7866\n",
      "Epoch: 218, Train Loss: 1.7193\n",
      "Epoch: 219, Train Loss: 1.7499\n",
      "Epoch: 220, Train Loss: 1.5506\n",
      "Epoch: 221, Train Loss: 1.6224\n",
      "Epoch: 222, Train Loss: 1.6526\n",
      "Epoch: 223, Train Loss: 1.9372\n",
      "Epoch: 224, Train Loss: 2.0874\n",
      "Epoch: 225, Train Loss: 1.9309\n",
      "Epoch: 226, Train Loss: 1.8579\n",
      "Epoch: 227, Train Loss: 2.6541\n",
      "Epoch: 228, Train Loss: 1.6625\n",
      "Epoch: 229, Train Loss: 2.3334\n",
      "Epoch: 230, Train Loss: 3.5593\n",
      "Epoch: 231, Train Loss: 1.9751\n",
      "Epoch: 232, Train Loss: 4.3720\n",
      "Epoch: 233, Train Loss: 3.0053\n",
      "Epoch: 234, Train Loss: 3.3974\n",
      "Epoch: 235, Train Loss: 2.0918\n",
      "Epoch: 236, Train Loss: 3.8186\n",
      "Epoch: 237, Train Loss: 2.9332\n",
      "Epoch: 238, Train Loss: 3.7134\n",
      "Epoch: 239, Train Loss: 1.9823\n",
      "Epoch: 240, Train Loss: 3.8600\n",
      "Epoch: 241, Train Loss: 2.1536\n",
      "Epoch: 242, Train Loss: 2.8768\n",
      "Epoch: 243, Train Loss: 2.9015\n",
      "Epoch: 244, Train Loss: 2.2662\n",
      "Epoch: 245, Train Loss: 2.8086\n",
      "Epoch: 246, Train Loss: 2.3880\n",
      "Epoch: 247, Train Loss: 2.5548\n",
      "Epoch: 248, Train Loss: 2.3819\n",
      "Epoch: 249, Train Loss: 2.2497\n",
      "Epoch: 250, Train Loss: 2.5019\n",
      "Epoch: 251, Train Loss: 2.2065\n",
      "Epoch: 252, Train Loss: 2.5328\n",
      "Epoch: 253, Train Loss: 2.5440\n",
      "Epoch: 254, Train Loss: 1.9595\n",
      "Epoch: 255, Train Loss: 2.4407\n",
      "Epoch: 256, Train Loss: 2.0490\n",
      "Epoch: 257, Train Loss: 1.9778\n",
      "Epoch: 258, Train Loss: 2.5764\n",
      "Epoch: 259, Train Loss: 2.1195\n",
      "Epoch: 260, Train Loss: 1.9514\n",
      "Epoch: 261, Train Loss: 2.0738\n",
      "Epoch: 262, Train Loss: 2.4594\n",
      "Epoch: 263, Train Loss: 1.9414\n",
      "Epoch: 264, Train Loss: 2.1133\n",
      "Epoch: 265, Train Loss: 1.9046\n",
      "Epoch: 266, Train Loss: 1.8887\n",
      "Epoch: 267, Train Loss: 1.8836\n",
      "Epoch: 268, Train Loss: 1.9938\n",
      "Epoch: 269, Train Loss: 1.6097\n",
      "Epoch: 270, Train Loss: 1.8312\n",
      "Epoch: 271, Train Loss: 1.6665\n",
      "Epoch: 272, Train Loss: 1.8874\n",
      "Epoch: 273, Train Loss: 1.8036\n",
      "Epoch: 274, Train Loss: 1.5359\n",
      "Epoch: 275, Train Loss: 1.6205\n",
      "Epoch: 276, Train Loss: 1.6885\n",
      "Epoch: 277, Train Loss: 1.6460\n",
      "Epoch: 278, Train Loss: 1.7559\n",
      "Epoch: 279, Train Loss: 1.5310\n",
      "Epoch: 280, Train Loss: 1.6825\n",
      "Epoch: 281, Train Loss: 1.7761\n",
      "Epoch: 282, Train Loss: 1.7181\n",
      "Epoch: 283, Train Loss: 1.7105\n",
      "Epoch: 284, Train Loss: 1.8427\n",
      "Epoch: 285, Train Loss: 1.5915\n",
      "Epoch: 286, Train Loss: 1.4869\n",
      "Epoch: 287, Train Loss: 1.5537\n",
      "Epoch: 288, Train Loss: 1.4978\n",
      "Epoch: 289, Train Loss: 1.7344\n",
      "Epoch: 290, Train Loss: 1.5243\n",
      "Epoch: 291, Train Loss: 1.7306\n",
      "Epoch: 292, Train Loss: 1.7246\n",
      "Epoch: 293, Train Loss: 1.6652\n",
      "Epoch: 294, Train Loss: 1.7115\n",
      "Epoch: 295, Train Loss: 1.5539\n",
      "Epoch: 296, Train Loss: 1.4749\n",
      "Epoch: 297, Train Loss: 1.5184\n",
      "Epoch: 298, Train Loss: 1.7009\n",
      "Epoch: 299, Train Loss: 1.7232\n",
      "Epoch: 300, Train Loss: 1.9162\n",
      "Epoch: 301, Train Loss: 1.5657\n",
      "Epoch: 302, Train Loss: 1.8520\n",
      "Epoch: 303, Train Loss: 1.6941\n",
      "Epoch: 304, Train Loss: 2.4143\n",
      "Epoch: 305, Train Loss: 2.2696\n",
      "Epoch: 306, Train Loss: 1.9321\n",
      "Epoch: 307, Train Loss: 2.2587\n",
      "Epoch: 308, Train Loss: 1.7842\n",
      "Epoch: 309, Train Loss: 2.6202\n",
      "Epoch: 310, Train Loss: 1.9133\n",
      "Epoch: 311, Train Loss: 2.1544\n",
      "Epoch: 312, Train Loss: 1.9936\n",
      "Epoch: 313, Train Loss: 2.5075\n",
      "Epoch: 314, Train Loss: 2.0420\n",
      "Epoch: 315, Train Loss: 2.2749\n",
      "Epoch: 316, Train Loss: 1.7291\n",
      "Epoch: 317, Train Loss: 2.1310\n",
      "Epoch: 318, Train Loss: 1.8531\n",
      "Epoch: 319, Train Loss: 1.6968\n",
      "Epoch: 320, Train Loss: 1.7820\n",
      "Epoch: 321, Train Loss: 1.6483\n",
      "Epoch: 322, Train Loss: 1.9480\n",
      "Epoch: 323, Train Loss: 1.7613\n",
      "Epoch: 324, Train Loss: 1.8244\n",
      "Epoch: 325, Train Loss: 1.6539\n",
      "Epoch: 326, Train Loss: 1.8818\n",
      "Epoch: 327, Train Loss: 1.5164\n",
      "Epoch: 328, Train Loss: 1.5297\n",
      "Epoch: 329, Train Loss: 1.6310\n",
      "Epoch: 330, Train Loss: 1.6462\n",
      "Epoch: 331, Train Loss: 1.5924\n",
      "Epoch: 332, Train Loss: 1.6532\n",
      "Epoch: 333, Train Loss: 1.6436\n",
      "Epoch: 334, Train Loss: 1.5588\n",
      "Epoch: 335, Train Loss: 1.5303\n",
      "Epoch: 336, Train Loss: 1.5683\n",
      "Epoch: 337, Train Loss: 1.5496\n",
      "Epoch: 338, Train Loss: 1.8489\n",
      "Epoch: 339, Train Loss: 1.4419\n",
      "Epoch: 340, Train Loss: 1.6962\n",
      "Epoch: 341, Train Loss: 1.3732\n",
      "Epoch: 342, Train Loss: 1.4486\n",
      "Epoch: 343, Train Loss: 1.4221\n",
      "Epoch: 344, Train Loss: 1.4081\n",
      "Epoch: 345, Train Loss: 1.5542\n",
      "Epoch: 346, Train Loss: 1.5440\n",
      "Epoch: 347, Train Loss: 1.5710\n",
      "Epoch: 348, Train Loss: 1.4682\n",
      "Epoch: 349, Train Loss: 1.4929\n",
      "Epoch: 350, Train Loss: 1.5671\n",
      "Epoch: 351, Train Loss: 1.2578\n",
      "Epoch: 352, Train Loss: 1.3939\n",
      "Epoch: 353, Train Loss: 1.5036\n",
      "Epoch: 354, Train Loss: 1.4585\n",
      "Epoch: 355, Train Loss: 1.3707\n",
      "Epoch: 356, Train Loss: 1.5141\n",
      "Epoch: 357, Train Loss: 1.3585\n",
      "Epoch: 358, Train Loss: 1.5088\n",
      "Epoch: 359, Train Loss: 1.6942\n",
      "Epoch: 360, Train Loss: 1.4414\n",
      "Epoch: 361, Train Loss: 1.4283\n",
      "Epoch: 362, Train Loss: 1.7303\n",
      "Epoch: 363, Train Loss: 1.5496\n",
      "Epoch: 364, Train Loss: 1.5608\n",
      "Epoch: 365, Train Loss: 2.0942\n",
      "Epoch: 366, Train Loss: 1.6785\n",
      "Epoch: 367, Train Loss: 2.7015\n",
      "Epoch: 368, Train Loss: 2.1813\n",
      "Epoch: 369, Train Loss: 2.2333\n",
      "Epoch: 370, Train Loss: 2.4247\n",
      "Epoch: 371, Train Loss: 1.6068\n",
      "Epoch: 372, Train Loss: 2.3762\n",
      "Epoch: 373, Train Loss: 1.6551\n",
      "Epoch: 374, Train Loss: 2.1494\n",
      "Epoch: 375, Train Loss: 1.6217\n",
      "Epoch: 376, Train Loss: 2.0490\n",
      "Epoch: 377, Train Loss: 1.8914\n",
      "Epoch: 378, Train Loss: 1.8395\n",
      "Epoch: 379, Train Loss: 1.7989\n",
      "Epoch: 380, Train Loss: 2.0732\n",
      "Epoch: 381, Train Loss: 2.0449\n",
      "Epoch: 382, Train Loss: 1.5223\n",
      "Epoch: 383, Train Loss: 1.9216\n",
      "Epoch: 384, Train Loss: 1.6152\n",
      "Epoch: 385, Train Loss: 1.7557\n",
      "Epoch: 386, Train Loss: 1.4262\n",
      "Epoch: 387, Train Loss: 1.6764\n",
      "Epoch: 388, Train Loss: 1.5147\n",
      "Epoch: 389, Train Loss: 1.6322\n",
      "Epoch: 390, Train Loss: 1.5239\n",
      "Epoch: 391, Train Loss: 1.4859\n",
      "Epoch: 392, Train Loss: 1.6010\n",
      "Epoch: 393, Train Loss: 1.8411\n",
      "Epoch: 394, Train Loss: 1.3505\n",
      "Epoch: 395, Train Loss: 1.5570\n",
      "Epoch: 396, Train Loss: 1.6405\n",
      "Epoch: 397, Train Loss: 1.4365\n",
      "Epoch: 398, Train Loss: 1.8178\n",
      "Epoch: 399, Train Loss: 1.6318\n",
      "Epoch: 400, Train Loss: 1.4771\n",
      "Epoch: 401, Train Loss: 1.4659\n",
      "Epoch: 402, Train Loss: 1.6752\n",
      "Epoch: 403, Train Loss: 1.4563\n",
      "Epoch: 404, Train Loss: 1.3102\n",
      "Epoch: 405, Train Loss: 1.2452\n",
      "Epoch: 406, Train Loss: 1.7345\n",
      "Epoch: 407, Train Loss: 1.4713\n",
      "Epoch: 408, Train Loss: 1.6411\n",
      "Epoch: 409, Train Loss: 1.6679\n",
      "Epoch: 410, Train Loss: 1.5883\n",
      "Epoch: 411, Train Loss: 1.5481\n",
      "Epoch: 412, Train Loss: 1.5090\n",
      "Epoch: 413, Train Loss: 1.4256\n",
      "Epoch: 414, Train Loss: 1.7578\n",
      "Epoch: 415, Train Loss: 1.5416\n",
      "Epoch: 416, Train Loss: 1.5131\n",
      "Epoch: 417, Train Loss: 1.7364\n",
      "Epoch: 418, Train Loss: 1.3423\n",
      "Epoch: 419, Train Loss: 1.4503\n",
      "Epoch: 420, Train Loss: 1.4764\n",
      "Epoch: 421, Train Loss: 1.6340\n",
      "Epoch: 422, Train Loss: 1.3178\n",
      "Epoch: 423, Train Loss: 1.4145\n",
      "Epoch: 424, Train Loss: 1.5245\n",
      "Epoch: 425, Train Loss: 1.2335\n",
      "Epoch: 426, Train Loss: 1.4411\n",
      "Epoch: 427, Train Loss: 1.2988\n",
      "Epoch: 428, Train Loss: 1.3880\n",
      "Epoch: 429, Train Loss: 1.4313\n",
      "Epoch: 430, Train Loss: 1.3070\n",
      "Epoch: 431, Train Loss: 1.2818\n",
      "Epoch: 432, Train Loss: 1.3154\n",
      "Epoch: 433, Train Loss: 1.4619\n",
      "Epoch: 434, Train Loss: 1.4466\n",
      "Epoch: 435, Train Loss: 1.4071\n",
      "Epoch: 436, Train Loss: 1.2493\n",
      "Epoch: 437, Train Loss: 1.2276\n",
      "Epoch: 438, Train Loss: 1.3188\n",
      "Epoch: 439, Train Loss: 1.2753\n",
      "Epoch: 440, Train Loss: 1.2155\n",
      "Epoch: 441, Train Loss: 1.2673\n",
      "Epoch: 442, Train Loss: 1.3048\n",
      "Epoch: 443, Train Loss: 1.5084\n",
      "Epoch: 444, Train Loss: 1.2697\n",
      "Epoch: 445, Train Loss: 1.3529\n",
      "Epoch: 446, Train Loss: 1.2344\n",
      "Epoch: 447, Train Loss: 1.3483\n",
      "Epoch: 448, Train Loss: 1.5189\n",
      "Epoch: 449, Train Loss: 1.4676\n",
      "Epoch: 450, Train Loss: 1.3352\n",
      "Epoch: 451, Train Loss: 1.4181\n",
      "Epoch: 452, Train Loss: 1.4838\n",
      "Epoch: 453, Train Loss: 1.2828\n",
      "Epoch: 454, Train Loss: 1.6897\n",
      "Epoch: 455, Train Loss: 1.6408\n",
      "Epoch: 456, Train Loss: 1.3200\n",
      "Epoch: 457, Train Loss: 1.3969\n",
      "Epoch: 458, Train Loss: 1.3399\n",
      "Epoch: 459, Train Loss: 1.7556\n",
      "Epoch: 460, Train Loss: 1.4090\n",
      "Epoch: 461, Train Loss: 1.4563\n",
      "Epoch: 462, Train Loss: 1.5287\n",
      "Epoch: 463, Train Loss: 1.1746\n",
      "Epoch: 464, Train Loss: 1.8284\n",
      "Epoch: 465, Train Loss: 1.1833\n",
      "Epoch: 466, Train Loss: 1.5137\n",
      "Epoch: 467, Train Loss: 1.1312\n",
      "Epoch: 468, Train Loss: 1.4090\n",
      "Epoch: 469, Train Loss: 1.3516\n",
      "Epoch: 470, Train Loss: 1.2052\n",
      "Epoch: 471, Train Loss: 1.4566\n",
      "Epoch: 472, Train Loss: 1.2167\n",
      "Epoch: 473, Train Loss: 1.2067\n",
      "Epoch: 474, Train Loss: 1.2708\n",
      "Epoch: 475, Train Loss: 1.1818\n",
      "Epoch: 476, Train Loss: 1.2988\n",
      "Epoch: 477, Train Loss: 1.3024\n",
      "Epoch: 478, Train Loss: 1.2822\n",
      "Epoch: 479, Train Loss: 1.1248\n",
      "Epoch: 480, Train Loss: 1.2299\n",
      "Epoch: 481, Train Loss: 1.2387\n",
      "Epoch: 482, Train Loss: 1.2563\n",
      "Epoch: 483, Train Loss: 1.3753\n",
      "Epoch: 484, Train Loss: 1.4321\n",
      "Epoch: 485, Train Loss: 1.2994\n",
      "Epoch: 486, Train Loss: 1.1855\n",
      "Epoch: 487, Train Loss: 1.2713\n",
      "Epoch: 488, Train Loss: 1.2724\n",
      "Epoch: 489, Train Loss: 1.1958\n",
      "Epoch: 490, Train Loss: 1.4957\n",
      "Epoch: 491, Train Loss: 1.4205\n",
      "Epoch: 492, Train Loss: 1.3090\n",
      "Epoch: 493, Train Loss: 1.3943\n",
      "Epoch: 494, Train Loss: 1.2633\n",
      "Epoch: 495, Train Loss: 1.2578\n",
      "Epoch: 496, Train Loss: 1.5620\n",
      "Epoch: 497, Train Loss: 1.5240\n",
      "Epoch: 498, Train Loss: 1.1466\n",
      "Epoch: 499, Train Loss: 1.4430\n",
      "Epoch: 500, Train Loss: 1.2878\n",
      "Epoch: 501, Train Loss: 1.2048\n",
      "Epoch: 502, Train Loss: 1.4815\n",
      "Epoch: 503, Train Loss: 1.3402\n",
      "Epoch: 504, Train Loss: 1.2308\n",
      "Epoch: 505, Train Loss: 1.2951\n",
      "Epoch: 506, Train Loss: 1.2200\n",
      "Epoch: 507, Train Loss: 1.7141\n",
      "Epoch: 508, Train Loss: 1.2917\n",
      "Epoch: 509, Train Loss: 1.2412\n",
      "Epoch: 510, Train Loss: 1.4788\n",
      "Epoch: 511, Train Loss: 1.4511\n",
      "Epoch: 512, Train Loss: 1.1621\n",
      "Epoch: 513, Train Loss: 1.3477\n",
      "Epoch: 514, Train Loss: 1.0831\n",
      "Epoch: 515, Train Loss: 1.1989\n",
      "Epoch: 516, Train Loss: 1.2474\n",
      "Epoch: 517, Train Loss: 1.0893\n",
      "Epoch: 518, Train Loss: 1.2318\n",
      "Epoch: 519, Train Loss: 1.3938\n",
      "Epoch: 520, Train Loss: 1.0093\n",
      "Epoch: 521, Train Loss: 1.3342\n",
      "Epoch: 522, Train Loss: 1.5046\n",
      "Epoch: 523, Train Loss: 1.2628\n",
      "Epoch: 524, Train Loss: 1.2470\n",
      "Epoch: 525, Train Loss: 1.1778\n",
      "Epoch: 526, Train Loss: 1.1320\n",
      "Epoch: 527, Train Loss: 1.1240\n",
      "Epoch: 528, Train Loss: 1.2076\n",
      "Epoch: 529, Train Loss: 1.0975\n",
      "Epoch: 530, Train Loss: 1.0353\n",
      "Epoch: 531, Train Loss: 1.0937\n",
      "Epoch: 532, Train Loss: 1.2784\n",
      "Epoch: 533, Train Loss: 1.2303\n",
      "Epoch: 534, Train Loss: 1.2888\n",
      "Epoch: 535, Train Loss: 1.1296\n",
      "Epoch: 536, Train Loss: 1.2508\n",
      "Epoch: 537, Train Loss: 1.5425\n",
      "Epoch: 538, Train Loss: 1.0545\n",
      "Epoch: 539, Train Loss: 1.4972\n",
      "Epoch: 540, Train Loss: 1.2983\n",
      "Epoch: 541, Train Loss: 1.0816\n",
      "Epoch: 542, Train Loss: 1.3122\n",
      "Epoch: 543, Train Loss: 1.2928\n",
      "Epoch: 544, Train Loss: 1.3358\n",
      "Epoch: 545, Train Loss: 1.7206\n",
      "Epoch: 546, Train Loss: 1.4906\n",
      "Epoch: 547, Train Loss: 1.1737\n",
      "Epoch: 548, Train Loss: 1.3794\n",
      "Epoch: 549, Train Loss: 1.1320\n",
      "Epoch: 550, Train Loss: 1.6301\n",
      "Epoch: 551, Train Loss: 1.2844\n",
      "Epoch: 552, Train Loss: 1.2131\n",
      "Epoch: 553, Train Loss: 1.3735\n",
      "Epoch: 554, Train Loss: 1.2150\n",
      "Epoch: 555, Train Loss: 1.1309\n",
      "Epoch: 556, Train Loss: 1.0579\n",
      "Epoch: 557, Train Loss: 1.0603\n",
      "Epoch: 558, Train Loss: 1.0744\n",
      "Epoch: 559, Train Loss: 1.1252\n",
      "Epoch: 560, Train Loss: 1.1325\n",
      "Epoch: 561, Train Loss: 1.1565\n",
      "Epoch: 562, Train Loss: 1.0635\n",
      "Epoch: 563, Train Loss: 0.9933\n",
      "Epoch: 564, Train Loss: 1.1114\n",
      "Epoch: 565, Train Loss: 1.2205\n",
      "Epoch: 566, Train Loss: 1.1257\n",
      "Epoch: 567, Train Loss: 1.0059\n",
      "Epoch: 568, Train Loss: 1.0647\n",
      "Epoch: 569, Train Loss: 1.0783\n",
      "Epoch: 570, Train Loss: 1.1198\n",
      "Epoch: 571, Train Loss: 1.1394\n",
      "Epoch: 572, Train Loss: 1.0526\n",
      "Epoch: 573, Train Loss: 0.9735\n",
      "Epoch: 574, Train Loss: 1.1666\n",
      "Epoch: 575, Train Loss: 1.0532\n",
      "Epoch: 576, Train Loss: 1.1054\n",
      "Epoch: 577, Train Loss: 1.0364\n",
      "Epoch: 578, Train Loss: 1.2552\n",
      "Epoch: 579, Train Loss: 1.3888\n",
      "Epoch: 580, Train Loss: 1.1878\n",
      "Epoch: 581, Train Loss: 0.9754\n",
      "Epoch: 582, Train Loss: 1.2923\n",
      "Epoch: 583, Train Loss: 1.1196\n",
      "Epoch: 584, Train Loss: 1.2459\n",
      "Epoch: 585, Train Loss: 1.2459\n",
      "Epoch: 586, Train Loss: 1.0552\n",
      "Epoch: 587, Train Loss: 1.4502\n",
      "Epoch: 588, Train Loss: 1.5250\n",
      "Epoch: 589, Train Loss: 1.0829\n",
      "Epoch: 590, Train Loss: 1.7740\n",
      "Epoch: 591, Train Loss: 1.7497\n",
      "Epoch: 592, Train Loss: 1.1246\n",
      "Epoch: 593, Train Loss: 1.9450\n",
      "Epoch: 594, Train Loss: 1.2216\n",
      "Epoch: 595, Train Loss: 1.3853\n",
      "Epoch: 596, Train Loss: 1.3906\n",
      "Epoch: 597, Train Loss: 1.1072\n",
      "Epoch: 598, Train Loss: 1.4869\n",
      "Epoch: 599, Train Loss: 1.1765\n",
      "Epoch: 600, Train Loss: 1.6936\n",
      "Epoch: 601, Train Loss: 1.6191\n",
      "Epoch: 602, Train Loss: 1.1303\n",
      "Epoch: 603, Train Loss: 1.8083\n",
      "Epoch: 604, Train Loss: 1.1066\n",
      "Epoch: 605, Train Loss: 1.8060\n",
      "Epoch: 606, Train Loss: 1.1251\n",
      "Epoch: 607, Train Loss: 1.5572\n",
      "Epoch: 608, Train Loss: 1.1040\n",
      "Epoch: 609, Train Loss: 1.8605\n",
      "Epoch: 610, Train Loss: 1.1999\n",
      "Epoch: 611, Train Loss: 1.7444\n",
      "Epoch: 612, Train Loss: 1.1974\n",
      "Epoch: 613, Train Loss: 1.5692\n",
      "Epoch: 614, Train Loss: 1.3802\n",
      "Epoch: 615, Train Loss: 1.4121\n",
      "Epoch: 616, Train Loss: 1.4686\n",
      "Epoch: 617, Train Loss: 1.2425\n",
      "Epoch: 618, Train Loss: 1.5615\n",
      "Epoch: 619, Train Loss: 1.2264\n",
      "Epoch: 620, Train Loss: 1.8614\n",
      "Epoch: 621, Train Loss: 1.1740\n",
      "Epoch: 622, Train Loss: 1.4874\n",
      "Epoch: 623, Train Loss: 1.1033\n",
      "Epoch: 624, Train Loss: 1.3108\n",
      "Epoch: 625, Train Loss: 1.2031\n",
      "Epoch: 626, Train Loss: 1.3152\n",
      "Epoch: 627, Train Loss: 1.0119\n",
      "Epoch: 628, Train Loss: 1.0812\n",
      "Epoch: 629, Train Loss: 1.2951\n",
      "Epoch: 630, Train Loss: 1.4076\n",
      "Epoch: 631, Train Loss: 1.0708\n",
      "Epoch: 632, Train Loss: 1.1317\n",
      "Epoch: 633, Train Loss: 0.9957\n",
      "Epoch: 634, Train Loss: 1.1588\n",
      "Epoch: 635, Train Loss: 1.0539\n",
      "Epoch: 636, Train Loss: 1.0818\n",
      "Epoch: 637, Train Loss: 1.0066\n",
      "Epoch: 638, Train Loss: 1.0156\n",
      "Epoch: 639, Train Loss: 1.1167\n",
      "Epoch: 640, Train Loss: 0.9745\n",
      "Epoch: 641, Train Loss: 1.1194\n",
      "Epoch: 642, Train Loss: 1.0118\n",
      "Epoch: 643, Train Loss: 1.0768\n",
      "Epoch: 644, Train Loss: 1.2055\n",
      "Epoch: 645, Train Loss: 0.9569\n",
      "Epoch: 646, Train Loss: 1.1218\n",
      "Epoch: 647, Train Loss: 1.1429\n",
      "Epoch: 648, Train Loss: 1.1388\n",
      "Epoch: 649, Train Loss: 1.0330\n",
      "Epoch: 650, Train Loss: 1.0460\n",
      "Epoch: 651, Train Loss: 1.0186\n",
      "Epoch: 652, Train Loss: 1.0116\n",
      "Epoch: 653, Train Loss: 0.9495\n",
      "Epoch: 654, Train Loss: 1.0278\n",
      "Epoch: 655, Train Loss: 0.9135\n",
      "Epoch: 656, Train Loss: 1.1645\n",
      "Epoch: 657, Train Loss: 1.0890\n",
      "Epoch: 658, Train Loss: 1.0292\n",
      "Epoch: 659, Train Loss: 1.3302\n",
      "Epoch: 660, Train Loss: 1.1374\n",
      "Epoch: 661, Train Loss: 1.1863\n",
      "Epoch: 662, Train Loss: 1.2342\n",
      "Epoch: 663, Train Loss: 0.9576\n",
      "Epoch: 664, Train Loss: 0.9770\n",
      "Epoch: 665, Train Loss: 1.0467\n",
      "Epoch: 666, Train Loss: 1.2393\n",
      "Epoch: 667, Train Loss: 1.0509\n",
      "Epoch: 668, Train Loss: 1.0016\n",
      "Epoch: 669, Train Loss: 0.8971\n",
      "Epoch: 670, Train Loss: 1.0907\n",
      "Epoch: 671, Train Loss: 1.0148\n",
      "Epoch: 672, Train Loss: 0.9205\n",
      "Epoch: 673, Train Loss: 0.9085\n",
      "Epoch: 674, Train Loss: 1.0424\n",
      "Epoch: 675, Train Loss: 0.9259\n",
      "Epoch: 676, Train Loss: 1.0451\n",
      "Epoch: 677, Train Loss: 1.1870\n",
      "Epoch: 678, Train Loss: 0.9445\n",
      "Epoch: 679, Train Loss: 0.9668\n",
      "Epoch: 680, Train Loss: 1.0315\n",
      "Epoch: 681, Train Loss: 0.9669\n",
      "Epoch: 682, Train Loss: 0.9075\n",
      "Epoch: 683, Train Loss: 1.0061\n",
      "Epoch: 684, Train Loss: 0.9647\n",
      "Epoch: 685, Train Loss: 0.8897\n",
      "Epoch: 686, Train Loss: 0.9437\n",
      "Epoch: 687, Train Loss: 1.0529\n",
      "Epoch: 688, Train Loss: 1.1230\n",
      "Epoch: 689, Train Loss: 0.8930\n",
      "Epoch: 690, Train Loss: 1.1253\n",
      "Epoch: 691, Train Loss: 1.2629\n",
      "Epoch: 692, Train Loss: 1.0361\n",
      "Epoch: 693, Train Loss: 1.2557\n",
      "Epoch: 694, Train Loss: 0.9601\n",
      "Epoch: 695, Train Loss: 1.1432\n",
      "Epoch: 696, Train Loss: 1.3679\n",
      "Epoch: 697, Train Loss: 0.9141\n",
      "Epoch: 698, Train Loss: 1.5090\n",
      "Epoch: 699, Train Loss: 1.4840\n",
      "Epoch: 700, Train Loss: 1.0947\n",
      "Epoch: 701, Train Loss: 1.2022\n",
      "Epoch: 702, Train Loss: 1.1217\n",
      "Epoch: 703, Train Loss: 1.1757\n",
      "Epoch: 704, Train Loss: 0.9806\n",
      "Epoch: 705, Train Loss: 1.3926\n",
      "Epoch: 706, Train Loss: 1.0285\n",
      "Epoch: 707, Train Loss: 1.0548\n",
      "Epoch: 708, Train Loss: 1.0358\n",
      "Epoch: 709, Train Loss: 0.9895\n",
      "Epoch: 710, Train Loss: 1.2745\n",
      "Epoch: 711, Train Loss: 0.9954\n",
      "Epoch: 712, Train Loss: 1.0237\n",
      "Epoch: 713, Train Loss: 1.1305\n",
      "Epoch: 714, Train Loss: 0.8959\n",
      "Epoch: 715, Train Loss: 1.1453\n",
      "Epoch: 716, Train Loss: 0.9792\n",
      "Epoch: 717, Train Loss: 0.9918\n",
      "Epoch: 718, Train Loss: 0.9576\n",
      "Epoch: 719, Train Loss: 0.9869\n",
      "Epoch: 720, Train Loss: 1.2107\n",
      "Epoch: 721, Train Loss: 1.0376\n",
      "Epoch: 722, Train Loss: 0.9906\n",
      "Epoch: 723, Train Loss: 1.5176\n",
      "Epoch: 724, Train Loss: 1.2035\n",
      "Epoch: 725, Train Loss: 1.2067\n",
      "Epoch: 726, Train Loss: 1.5202\n",
      "Epoch: 727, Train Loss: 0.8372\n",
      "Epoch: 728, Train Loss: 1.3362\n",
      "Epoch: 729, Train Loss: 0.9441\n",
      "Epoch: 730, Train Loss: 1.5026\n",
      "Epoch: 731, Train Loss: 1.3943\n",
      "Epoch: 732, Train Loss: 1.0268\n",
      "Epoch: 733, Train Loss: 1.3992\n",
      "Epoch: 734, Train Loss: 0.9465\n",
      "Epoch: 735, Train Loss: 1.1285\n",
      "Epoch: 736, Train Loss: 1.0937\n",
      "Epoch: 737, Train Loss: 0.9888\n",
      "Epoch: 738, Train Loss: 0.9951\n",
      "Epoch: 739, Train Loss: 0.8421\n",
      "Epoch: 740, Train Loss: 1.0450\n",
      "Epoch: 741, Train Loss: 0.9247\n",
      "Epoch: 742, Train Loss: 0.8991\n",
      "Epoch: 743, Train Loss: 0.9384\n",
      "Epoch: 744, Train Loss: 1.0137\n",
      "Epoch: 745, Train Loss: 1.1453\n",
      "Epoch: 746, Train Loss: 1.0990\n",
      "Epoch: 747, Train Loss: 0.9804\n",
      "Epoch: 748, Train Loss: 1.0621\n",
      "Epoch: 749, Train Loss: 0.9429\n",
      "Epoch: 750, Train Loss: 0.9514\n",
      "Epoch: 751, Train Loss: 0.8913\n",
      "Epoch: 752, Train Loss: 0.9456\n",
      "Epoch: 753, Train Loss: 1.0297\n",
      "Epoch: 754, Train Loss: 1.0085\n",
      "Epoch: 755, Train Loss: 0.8436\n",
      "Epoch: 756, Train Loss: 0.8923\n",
      "Epoch: 757, Train Loss: 0.9238\n",
      "Epoch: 758, Train Loss: 0.9467\n",
      "Epoch: 759, Train Loss: 0.8278\n",
      "Epoch: 760, Train Loss: 0.9593\n",
      "Epoch: 761, Train Loss: 0.9441\n",
      "Epoch: 762, Train Loss: 0.8528\n",
      "Epoch: 763, Train Loss: 0.9860\n",
      "Epoch: 764, Train Loss: 0.9521\n",
      "Epoch: 765, Train Loss: 1.0958\n",
      "Epoch: 766, Train Loss: 1.7074\n",
      "Epoch: 767, Train Loss: 0.9534\n",
      "Epoch: 768, Train Loss: 1.1398\n",
      "Epoch: 769, Train Loss: 1.0862\n",
      "Epoch: 770, Train Loss: 1.0100\n",
      "Epoch: 771, Train Loss: 1.1594\n",
      "Epoch: 772, Train Loss: 0.8203\n",
      "Epoch: 773, Train Loss: 0.9890\n",
      "Epoch: 774, Train Loss: 0.9458\n",
      "Epoch: 775, Train Loss: 0.8262\n",
      "Epoch: 776, Train Loss: 0.8394\n",
      "Epoch: 777, Train Loss: 0.8717\n",
      "Epoch: 778, Train Loss: 0.8830\n",
      "Epoch: 779, Train Loss: 0.9293\n",
      "Epoch: 780, Train Loss: 0.8490\n",
      "Epoch: 781, Train Loss: 1.0284\n",
      "Epoch: 782, Train Loss: 0.9517\n",
      "Epoch: 783, Train Loss: 1.1454\n",
      "Epoch: 784, Train Loss: 1.2969\n",
      "Epoch: 785, Train Loss: 0.9646\n",
      "Epoch: 786, Train Loss: 1.1053\n",
      "Epoch: 787, Train Loss: 1.0079\n",
      "Epoch: 788, Train Loss: 1.0216\n",
      "Epoch: 789, Train Loss: 1.2500\n",
      "Epoch: 790, Train Loss: 0.8052\n",
      "Epoch: 791, Train Loss: 1.2171\n",
      "Epoch: 792, Train Loss: 0.9456\n",
      "Epoch: 793, Train Loss: 1.4006\n",
      "Epoch: 794, Train Loss: 1.3010\n",
      "Epoch: 795, Train Loss: 0.8369\n",
      "Epoch: 796, Train Loss: 1.2778\n",
      "Epoch: 797, Train Loss: 0.8143\n",
      "Epoch: 798, Train Loss: 1.0229\n",
      "Epoch: 799, Train Loss: 0.8892\n",
      "Epoch: 800, Train Loss: 0.9323\n",
      "Epoch: 801, Train Loss: 0.8249\n",
      "Epoch: 802, Train Loss: 1.0106\n",
      "Epoch: 803, Train Loss: 1.0052\n",
      "Epoch: 804, Train Loss: 0.9349\n",
      "Epoch: 805, Train Loss: 1.2179\n",
      "Epoch: 806, Train Loss: 0.9445\n",
      "Epoch: 807, Train Loss: 0.9799\n",
      "Epoch: 808, Train Loss: 1.2280\n",
      "Epoch: 809, Train Loss: 0.9019\n",
      "Epoch: 810, Train Loss: 1.2421\n",
      "Epoch: 811, Train Loss: 0.9208\n",
      "Epoch: 812, Train Loss: 1.0175\n",
      "Epoch: 813, Train Loss: 1.0143\n",
      "Epoch: 814, Train Loss: 0.8823\n",
      "Epoch: 815, Train Loss: 0.8314\n",
      "Epoch: 816, Train Loss: 0.8387\n",
      "Epoch: 817, Train Loss: 0.8970\n",
      "Epoch: 818, Train Loss: 0.8807\n",
      "Epoch: 819, Train Loss: 0.9006\n",
      "Epoch: 820, Train Loss: 0.8447\n",
      "Epoch: 821, Train Loss: 0.8438\n",
      "Epoch: 822, Train Loss: 0.9632\n",
      "Epoch: 823, Train Loss: 0.8128\n",
      "Epoch: 824, Train Loss: 0.7675\n",
      "Epoch: 825, Train Loss: 0.9512\n",
      "Epoch: 826, Train Loss: 0.9355\n",
      "Epoch: 827, Train Loss: 0.8719\n",
      "Epoch: 828, Train Loss: 0.7901\n",
      "Epoch: 829, Train Loss: 0.8376\n",
      "Epoch: 830, Train Loss: 0.8298\n",
      "Epoch: 831, Train Loss: 1.0413\n",
      "Epoch: 832, Train Loss: 0.9059\n",
      "Epoch: 833, Train Loss: 0.9263\n",
      "Epoch: 834, Train Loss: 1.1632\n",
      "Epoch: 835, Train Loss: 1.0786\n",
      "Epoch: 836, Train Loss: 1.0101\n",
      "Epoch: 837, Train Loss: 1.3830\n",
      "Epoch: 838, Train Loss: 1.0001\n",
      "Epoch: 839, Train Loss: 1.0089\n",
      "Epoch: 840, Train Loss: 1.3207\n",
      "Epoch: 841, Train Loss: 0.9032\n",
      "Epoch: 842, Train Loss: 1.0853\n",
      "Epoch: 843, Train Loss: 1.0308\n",
      "Epoch: 844, Train Loss: 0.9295\n",
      "Epoch: 845, Train Loss: 1.0332\n",
      "Epoch: 846, Train Loss: 0.8435\n",
      "Epoch: 847, Train Loss: 1.0908\n",
      "Epoch: 848, Train Loss: 0.9121\n",
      "Epoch: 849, Train Loss: 0.9404\n",
      "Epoch: 850, Train Loss: 1.3434\n",
      "Epoch: 851, Train Loss: 0.8913\n",
      "Epoch: 852, Train Loss: 1.3113\n",
      "Epoch: 853, Train Loss: 0.9809\n",
      "Epoch: 854, Train Loss: 1.1633\n",
      "Epoch: 855, Train Loss: 1.0794\n",
      "Epoch: 856, Train Loss: 0.9996\n",
      "Epoch: 857, Train Loss: 1.5824\n",
      "Epoch: 858, Train Loss: 1.1083\n",
      "Epoch: 859, Train Loss: 1.0499\n",
      "Epoch: 860, Train Loss: 1.0077\n",
      "Epoch: 861, Train Loss: 0.9933\n",
      "Epoch: 862, Train Loss: 1.0709\n",
      "Epoch: 863, Train Loss: 0.9341\n",
      "Epoch: 864, Train Loss: 1.1185\n",
      "Epoch: 865, Train Loss: 0.8188\n",
      "Epoch: 866, Train Loss: 1.1104\n",
      "Epoch: 867, Train Loss: 0.9233\n",
      "Epoch: 868, Train Loss: 0.8634\n",
      "Epoch: 869, Train Loss: 0.8596\n",
      "Epoch: 870, Train Loss: 0.9009\n",
      "Epoch: 871, Train Loss: 0.7856\n",
      "Epoch: 872, Train Loss: 0.8400\n",
      "Epoch: 873, Train Loss: 0.9369\n",
      "Epoch: 874, Train Loss: 0.8216\n",
      "Epoch: 875, Train Loss: 0.8402\n",
      "Epoch: 876, Train Loss: 0.8348\n",
      "Epoch: 877, Train Loss: 0.8894\n",
      "Epoch: 878, Train Loss: 0.8470\n",
      "Epoch: 879, Train Loss: 0.9956\n",
      "Epoch: 880, Train Loss: 0.8173\n",
      "Epoch: 881, Train Loss: 0.9803\n",
      "Epoch: 882, Train Loss: 1.1540\n",
      "Epoch: 883, Train Loss: 0.8405\n",
      "Epoch: 884, Train Loss: 0.9148\n",
      "Epoch: 885, Train Loss: 1.0498\n",
      "Epoch: 886, Train Loss: 0.9028\n",
      "Epoch: 887, Train Loss: 0.9292\n",
      "Epoch: 888, Train Loss: 0.8302\n",
      "Epoch: 889, Train Loss: 0.8397\n",
      "Epoch: 890, Train Loss: 0.8742\n",
      "Epoch: 891, Train Loss: 0.9018\n",
      "Epoch: 892, Train Loss: 0.9118\n",
      "Epoch: 893, Train Loss: 0.8275\n",
      "Epoch: 894, Train Loss: 0.9110\n",
      "Epoch: 895, Train Loss: 0.8044\n",
      "Epoch: 896, Train Loss: 0.9351\n",
      "Epoch: 897, Train Loss: 1.0508\n",
      "Epoch: 898, Train Loss: 0.8563\n",
      "Epoch: 899, Train Loss: 1.0016\n",
      "Epoch: 900, Train Loss: 0.9867\n",
      "Epoch: 901, Train Loss: 0.8254\n",
      "Epoch: 902, Train Loss: 0.8886\n",
      "Epoch: 903, Train Loss: 0.9163\n",
      "Epoch: 904, Train Loss: 0.9506\n",
      "Epoch: 905, Train Loss: 0.8865\n",
      "Epoch: 906, Train Loss: 0.8989\n",
      "Epoch: 907, Train Loss: 0.8196\n",
      "Epoch: 908, Train Loss: 0.8712\n",
      "Epoch: 909, Train Loss: 0.8706\n",
      "Epoch: 910, Train Loss: 0.8214\n",
      "Epoch: 911, Train Loss: 0.9154\n",
      "Epoch: 912, Train Loss: 0.8432\n",
      "Epoch: 913, Train Loss: 0.9244\n",
      "Epoch: 914, Train Loss: 0.8638\n",
      "Epoch: 915, Train Loss: 0.9244\n",
      "Epoch: 916, Train Loss: 1.2526\n",
      "Epoch: 917, Train Loss: 0.8811\n",
      "Epoch: 918, Train Loss: 1.0386\n",
      "Epoch: 919, Train Loss: 1.2978\n",
      "Epoch: 920, Train Loss: 0.8831\n",
      "Epoch: 921, Train Loss: 1.3991\n",
      "Epoch: 922, Train Loss: 1.0274\n",
      "Epoch: 923, Train Loss: 1.0669\n",
      "Epoch: 924, Train Loss: 1.2955\n",
      "Epoch: 925, Train Loss: 0.8921\n",
      "Epoch: 926, Train Loss: 1.1824\n",
      "Epoch: 927, Train Loss: 0.8400\n",
      "Epoch: 928, Train Loss: 1.2665\n",
      "Epoch: 929, Train Loss: 0.9404\n",
      "Epoch: 930, Train Loss: 0.9001\n",
      "Epoch: 931, Train Loss: 1.2393\n",
      "Epoch: 932, Train Loss: 0.8824\n",
      "Epoch: 933, Train Loss: 1.0060\n",
      "Epoch: 934, Train Loss: 0.8915\n",
      "Epoch: 935, Train Loss: 1.0006\n",
      "Epoch: 936, Train Loss: 1.0028\n",
      "Epoch: 937, Train Loss: 0.8438\n",
      "Epoch: 938, Train Loss: 1.0634\n",
      "Epoch: 939, Train Loss: 0.8161\n",
      "Epoch: 940, Train Loss: 0.9520\n",
      "Epoch: 941, Train Loss: 0.8292\n",
      "Epoch: 942, Train Loss: 0.8394\n",
      "Epoch: 943, Train Loss: 0.8847\n",
      "Epoch: 944, Train Loss: 0.8243\n",
      "Epoch: 945, Train Loss: 0.8368\n",
      "Epoch: 946, Train Loss: 0.9037\n",
      "Epoch: 947, Train Loss: 0.9406\n",
      "Epoch: 948, Train Loss: 0.7775\n",
      "Epoch: 949, Train Loss: 0.9261\n",
      "Epoch: 950, Train Loss: 0.8624\n",
      "Epoch: 951, Train Loss: 0.8591\n",
      "Epoch: 952, Train Loss: 1.0038\n",
      "Epoch: 953, Train Loss: 0.8457\n",
      "Epoch: 954, Train Loss: 0.9382\n",
      "Epoch: 955, Train Loss: 0.8117\n",
      "Epoch: 956, Train Loss: 1.0319\n",
      "Epoch: 957, Train Loss: 1.1127\n",
      "Epoch: 958, Train Loss: 0.7946\n",
      "Epoch: 959, Train Loss: 0.9759\n",
      "Epoch: 960, Train Loss: 0.8226\n",
      "Epoch: 961, Train Loss: 0.8027\n",
      "Epoch: 962, Train Loss: 0.7946\n",
      "Epoch: 963, Train Loss: 0.7952\n",
      "Epoch: 964, Train Loss: 0.8338\n",
      "Epoch: 965, Train Loss: 0.8674\n",
      "Epoch: 966, Train Loss: 0.7062\n",
      "Epoch: 967, Train Loss: 0.8882\n",
      "Epoch: 968, Train Loss: 0.8335\n",
      "Epoch: 969, Train Loss: 0.7212\n",
      "Epoch: 970, Train Loss: 0.8532\n",
      "Epoch: 971, Train Loss: 0.9084\n",
      "Epoch: 972, Train Loss: 0.8867\n",
      "Epoch: 973, Train Loss: 1.0574\n",
      "Epoch: 974, Train Loss: 0.9443\n",
      "Epoch: 975, Train Loss: 0.8909\n",
      "Epoch: 976, Train Loss: 0.8029\n",
      "Epoch: 977, Train Loss: 0.8023\n",
      "Epoch: 978, Train Loss: 0.9164\n",
      "Epoch: 979, Train Loss: 0.8255\n",
      "Epoch: 980, Train Loss: 0.8850\n",
      "Epoch: 981, Train Loss: 0.8575\n",
      "Epoch: 982, Train Loss: 0.8750\n",
      "Epoch: 983, Train Loss: 1.1726\n",
      "Epoch: 984, Train Loss: 0.7922\n",
      "Epoch: 985, Train Loss: 0.7763\n",
      "Epoch: 986, Train Loss: 0.8460\n",
      "Epoch: 987, Train Loss: 0.8063\n",
      "Epoch: 988, Train Loss: 0.9465\n",
      "Epoch: 989, Train Loss: 0.8171\n",
      "Epoch: 990, Train Loss: 0.7684\n",
      "Epoch: 991, Train Loss: 0.9482\n",
      "Epoch: 992, Train Loss: 0.7222\n",
      "Epoch: 993, Train Loss: 0.9021\n",
      "Epoch: 994, Train Loss: 0.7661\n",
      "Epoch: 995, Train Loss: 0.8046\n",
      "Epoch: 996, Train Loss: 0.7756\n",
      "Epoch: 997, Train Loss: 0.7698\n",
      "Epoch: 998, Train Loss: 0.7310\n",
      "Epoch: 999, Train Loss: 0.8564\n"
     ]
    }
   ],
   "source": [
    "# Función de entrenamiento\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "for epoch in range(1000):  # Número de épocas\n",
    "    train_loss = train()\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 16.4173, Test Loss: 11.2194\n",
      "Epoch: 1, Train Loss: 10.9807, Test Loss: 11.5172\n",
      "Epoch: 2, Train Loss: 12.3859, Test Loss: 10.2881\n",
      "Epoch: 3, Train Loss: 10.7344, Test Loss: 8.3172\n",
      "Epoch: 4, Train Loss: 8.3789, Test Loss: 7.8686\n",
      "Epoch: 5, Train Loss: 7.6325, Test Loss: 8.7540\n",
      "Epoch: 6, Train Loss: 8.3636, Test Loss: 9.1917\n",
      "Epoch: 7, Train Loss: 8.5513, Test Loss: 8.5733\n",
      "Epoch: 8, Train Loss: 8.4555, Test Loss: 7.5017\n",
      "Epoch: 9, Train Loss: 7.2515, Test Loss: 6.9882\n",
      "Epoch: 10, Train Loss: 6.8937, Test Loss: 7.1446\n",
      "Epoch: 11, Train Loss: 6.6668, Test Loss: 7.3642\n",
      "Epoch: 12, Train Loss: 7.0957, Test Loss: 7.1082\n",
      "Epoch: 13, Train Loss: 6.8600, Test Loss: 6.6706\n",
      "Epoch: 14, Train Loss: 6.5596, Test Loss: 6.3404\n",
      "Epoch: 15, Train Loss: 5.9469, Test Loss: 6.3204\n",
      "Epoch: 16, Train Loss: 5.7463, Test Loss: 6.5786\n",
      "Epoch: 17, Train Loss: 6.1429, Test Loss: 6.7403\n",
      "Epoch: 18, Train Loss: 6.2495, Test Loss: 6.5659\n",
      "Epoch: 19, Train Loss: 5.8989, Test Loss: 6.2763\n",
      "Epoch: 20, Train Loss: 6.0277, Test Loss: 6.1000\n",
      "Epoch: 21, Train Loss: 5.9417, Test Loss: 6.0603\n",
      "Epoch: 22, Train Loss: 5.3687, Test Loss: 6.1113\n",
      "Epoch: 23, Train Loss: 5.4818, Test Loss: 6.0953\n",
      "Epoch: 24, Train Loss: 5.4283, Test Loss: 5.9560\n",
      "Epoch: 25, Train Loss: 5.4654, Test Loss: 5.8833\n",
      "Epoch: 26, Train Loss: 5.0032, Test Loss: 6.0361\n",
      "Epoch: 27, Train Loss: 5.2524, Test Loss: 6.1335\n",
      "Epoch: 28, Train Loss: 5.4996, Test Loss: 6.0543\n",
      "Epoch: 29, Train Loss: 5.1848, Test Loss: 5.9630\n",
      "Epoch: 30, Train Loss: 5.2521, Test Loss: 5.8267\n",
      "Epoch: 31, Train Loss: 5.0015, Test Loss: 5.8113\n",
      "Epoch: 32, Train Loss: 5.0250, Test Loss: 5.8364\n",
      "Epoch: 33, Train Loss: 5.2983, Test Loss: 5.7778\n",
      "Epoch: 34, Train Loss: 4.8128, Test Loss: 5.7787\n",
      "Epoch: 35, Train Loss: 5.0885, Test Loss: 5.7745\n",
      "Epoch: 36, Train Loss: 4.9062, Test Loss: 5.6928\n",
      "Epoch: 37, Train Loss: 4.7652, Test Loss: 5.7658\n",
      "Epoch: 38, Train Loss: 4.7269, Test Loss: 5.7781\n",
      "Epoch: 39, Train Loss: 4.8792, Test Loss: 5.7020\n",
      "Epoch: 40, Train Loss: 4.8409, Test Loss: 5.7092\n",
      "Epoch: 41, Train Loss: 4.8416, Test Loss: 5.7062\n",
      "Epoch: 42, Train Loss: 4.6329, Test Loss: 5.6864\n",
      "Epoch: 43, Train Loss: 4.7168, Test Loss: 5.6509\n",
      "Epoch: 44, Train Loss: 4.7609, Test Loss: 5.5836\n",
      "Epoch: 45, Train Loss: 4.7532, Test Loss: 5.5701\n",
      "Epoch: 46, Train Loss: 4.7779, Test Loss: 5.6281\n",
      "Epoch: 47, Train Loss: 4.6369, Test Loss: 5.6369\n",
      "Epoch: 48, Train Loss: 4.7192, Test Loss: 5.6502\n",
      "Epoch: 49, Train Loss: 4.3619, Test Loss: 5.6310\n",
      "Epoch: 50, Train Loss: 4.6273, Test Loss: 5.5463\n",
      "Epoch: 51, Train Loss: 4.1317, Test Loss: 5.4942\n",
      "Epoch: 52, Train Loss: 4.4404, Test Loss: 5.4194\n",
      "Epoch: 53, Train Loss: 4.5686, Test Loss: 5.4010\n",
      "Epoch: 54, Train Loss: 4.3545, Test Loss: 5.4230\n",
      "Epoch: 55, Train Loss: 4.2699, Test Loss: 5.5024\n",
      "Epoch: 56, Train Loss: 4.4406, Test Loss: 5.5296\n",
      "Epoch: 57, Train Loss: 4.2550, Test Loss: 5.5411\n",
      "Epoch: 58, Train Loss: 4.2263, Test Loss: 5.4470\n",
      "Epoch: 59, Train Loss: 4.1485, Test Loss: 5.3930\n",
      "Epoch: 60, Train Loss: 4.2685, Test Loss: 5.3814\n",
      "Epoch: 61, Train Loss: 4.2452, Test Loss: 5.4881\n",
      "Epoch: 62, Train Loss: 4.2581, Test Loss: 5.5307\n",
      "Epoch: 63, Train Loss: 4.2838, Test Loss: 5.4174\n",
      "Epoch: 64, Train Loss: 4.0927, Test Loss: 5.3765\n",
      "Epoch: 65, Train Loss: 4.3307, Test Loss: 5.4399\n",
      "Epoch: 66, Train Loss: 4.1857, Test Loss: 5.7365\n",
      "Epoch: 67, Train Loss: 4.1825, Test Loss: 5.5574\n",
      "Epoch: 68, Train Loss: 4.3119, Test Loss: 5.4619\n",
      "Epoch: 69, Train Loss: 4.4344, Test Loss: 5.3764\n",
      "Epoch: 70, Train Loss: 4.1573, Test Loss: 5.4756\n",
      "Epoch: 71, Train Loss: 4.2725, Test Loss: 5.5374\n",
      "Epoch: 72, Train Loss: 4.0597, Test Loss: 5.3068\n",
      "Epoch: 73, Train Loss: 4.0408, Test Loss: 5.3623\n",
      "Epoch: 74, Train Loss: 4.1304, Test Loss: 5.5720\n",
      "Epoch: 75, Train Loss: 4.1232, Test Loss: 5.6161\n",
      "Epoch: 76, Train Loss: 4.0503, Test Loss: 5.3313\n",
      "Epoch: 77, Train Loss: 3.8673, Test Loss: 5.1844\n",
      "Epoch: 78, Train Loss: 4.0925, Test Loss: 5.3313\n",
      "Epoch: 79, Train Loss: 4.1282, Test Loss: 5.6781\n",
      "Epoch: 80, Train Loss: 4.2416, Test Loss: 5.4681\n",
      "Epoch: 81, Train Loss: 3.9953, Test Loss: 5.2687\n",
      "Epoch: 82, Train Loss: 3.7597, Test Loss: 5.2560\n",
      "Epoch: 83, Train Loss: 3.9270, Test Loss: 5.4465\n",
      "Epoch: 84, Train Loss: 3.7244, Test Loss: 5.6128\n",
      "Epoch: 85, Train Loss: 4.1124, Test Loss: 5.3183\n",
      "Epoch: 86, Train Loss: 3.7666, Test Loss: 5.2600\n",
      "Epoch: 87, Train Loss: 3.8543, Test Loss: 5.4653\n",
      "Epoch: 88, Train Loss: 3.7691, Test Loss: 5.6910\n",
      "Epoch: 89, Train Loss: 3.8302, Test Loss: 5.3826\n",
      "Epoch: 90, Train Loss: 3.8535, Test Loss: 5.1608\n",
      "Epoch: 91, Train Loss: 4.1343, Test Loss: 5.4254\n",
      "Epoch: 92, Train Loss: 3.8049, Test Loss: 5.9464\n",
      "Epoch: 93, Train Loss: 3.9495, Test Loss: 5.6626\n",
      "Epoch: 94, Train Loss: 3.7764, Test Loss: 5.2265\n",
      "Epoch: 95, Train Loss: 3.8698, Test Loss: 5.1728\n",
      "Epoch: 96, Train Loss: 3.8008, Test Loss: 5.4798\n",
      "Epoch: 97, Train Loss: 3.7833, Test Loss: 5.9733\n",
      "Epoch: 98, Train Loss: 3.8042, Test Loss: 5.4796\n",
      "Epoch: 99, Train Loss: 3.4950, Test Loss: 5.2041\n",
      "Epoch: 100, Train Loss: 3.7086, Test Loss: 5.4737\n",
      "Epoch: 101, Train Loss: 3.6325, Test Loss: 5.9367\n",
      "Epoch: 102, Train Loss: 3.8220, Test Loss: 5.4195\n",
      "Epoch: 103, Train Loss: 3.5343, Test Loss: 5.1193\n",
      "Epoch: 104, Train Loss: 3.7982, Test Loss: 5.2623\n",
      "Epoch: 105, Train Loss: 3.6505, Test Loss: 6.0240\n",
      "Epoch: 106, Train Loss: 3.6442, Test Loss: 5.8692\n",
      "Epoch: 107, Train Loss: 3.6426, Test Loss: 5.1169\n",
      "Epoch: 108, Train Loss: 3.7517, Test Loss: 5.0659\n",
      "Epoch: 109, Train Loss: 3.9233, Test Loss: 5.6646\n",
      "Epoch: 110, Train Loss: 3.7607, Test Loss: 6.0649\n",
      "Epoch: 111, Train Loss: 3.8416, Test Loss: 5.4132\n",
      "Epoch: 112, Train Loss: 3.6997, Test Loss: 4.9951\n",
      "Epoch: 113, Train Loss: 3.9492, Test Loss: 5.3647\n",
      "Epoch: 114, Train Loss: 3.5199, Test Loss: 6.0912\n",
      "Epoch: 115, Train Loss: 4.0926, Test Loss: 5.6652\n",
      "Epoch: 116, Train Loss: 3.6163, Test Loss: 5.1692\n",
      "Epoch: 117, Train Loss: 3.6347, Test Loss: 5.1143\n",
      "Epoch: 118, Train Loss: 3.8591, Test Loss: 5.7251\n",
      "Epoch: 119, Train Loss: 3.4553, Test Loss: 6.1299\n",
      "Epoch: 120, Train Loss: 3.8541, Test Loss: 5.6717\n",
      "Epoch: 121, Train Loss: 3.5080, Test Loss: 5.1180\n",
      "Epoch: 122, Train Loss: 3.8379, Test Loss: 5.3355\n",
      "Epoch: 123, Train Loss: 3.5540, Test Loss: 6.2439\n",
      "Epoch: 124, Train Loss: 3.7019, Test Loss: 6.1390\n",
      "Epoch: 125, Train Loss: 3.5491, Test Loss: 5.2305\n",
      "Epoch: 126, Train Loss: 3.5670, Test Loss: 5.0743\n",
      "Epoch: 127, Train Loss: 3.5973, Test Loss: 5.6912\n",
      "Epoch: 128, Train Loss: 3.4983, Test Loss: 6.3719\n",
      "Epoch: 129, Train Loss: 3.5282, Test Loss: 5.8833\n",
      "Epoch: 130, Train Loss: 3.4956, Test Loss: 5.2258\n",
      "Epoch: 131, Train Loss: 3.6085, Test Loss: 5.3200\n",
      "Epoch: 132, Train Loss: 3.5381, Test Loss: 6.0869\n",
      "Epoch: 133, Train Loss: 3.3105, Test Loss: 6.0891\n",
      "Epoch: 134, Train Loss: 3.5352, Test Loss: 5.5746\n",
      "Epoch: 135, Train Loss: 3.4898, Test Loss: 5.3030\n",
      "Epoch: 136, Train Loss: 3.5442, Test Loss: 5.7083\n",
      "Epoch: 137, Train Loss: 3.5149, Test Loss: 6.3956\n",
      "Epoch: 138, Train Loss: 3.7737, Test Loss: 5.7543\n",
      "Epoch: 139, Train Loss: 3.5971, Test Loss: 5.2305\n",
      "Epoch: 140, Train Loss: 3.4026, Test Loss: 5.6093\n",
      "Epoch: 141, Train Loss: 3.4921, Test Loss: 6.0777\n",
      "Epoch: 142, Train Loss: 3.3450, Test Loss: 5.8635\n",
      "Epoch: 143, Train Loss: 3.1813, Test Loss: 5.3586\n",
      "Epoch: 144, Train Loss: 3.3998, Test Loss: 5.2941\n",
      "Epoch: 145, Train Loss: 3.3414, Test Loss: 5.7734\n",
      "Epoch: 146, Train Loss: 3.3777, Test Loss: 6.4550\n",
      "Epoch: 147, Train Loss: 3.5325, Test Loss: 6.0955\n",
      "Epoch: 148, Train Loss: 3.3288, Test Loss: 5.3697\n",
      "Epoch: 149, Train Loss: 3.3982, Test Loss: 5.4338\n",
      "Epoch: 150, Train Loss: 3.4805, Test Loss: 6.1307\n",
      "Epoch: 151, Train Loss: 3.4662, Test Loss: 6.0810\n",
      "Epoch: 152, Train Loss: 3.2271, Test Loss: 5.4706\n",
      "Epoch: 153, Train Loss: 3.2216, Test Loss: 5.2729\n",
      "Epoch: 154, Train Loss: 3.2694, Test Loss: 5.8499\n",
      "Epoch: 155, Train Loss: 3.4729, Test Loss: 6.3729\n",
      "Epoch: 156, Train Loss: 3.2822, Test Loss: 6.0236\n",
      "Epoch: 157, Train Loss: 3.3806, Test Loss: 5.5806\n",
      "Epoch: 158, Train Loss: 3.2011, Test Loss: 5.7952\n",
      "Epoch: 159, Train Loss: 3.6290, Test Loss: 6.4464\n",
      "Epoch: 160, Train Loss: 3.4137, Test Loss: 6.2335\n",
      "Epoch: 161, Train Loss: 3.1792, Test Loss: 5.7405\n",
      "Epoch: 162, Train Loss: 3.2719, Test Loss: 5.7864\n",
      "Epoch: 163, Train Loss: 3.3019, Test Loss: 6.0820\n",
      "Epoch: 164, Train Loss: 3.0864, Test Loss: 6.4815\n",
      "Epoch: 165, Train Loss: 3.3843, Test Loss: 6.0363\n",
      "Epoch: 166, Train Loss: 3.1614, Test Loss: 6.1190\n",
      "Epoch: 167, Train Loss: 3.2454, Test Loss: 6.1371\n",
      "Epoch: 168, Train Loss: 3.1821, Test Loss: 6.4296\n",
      "Epoch: 169, Train Loss: 3.3812, Test Loss: 6.2467\n",
      "Epoch: 170, Train Loss: 3.3066, Test Loss: 5.7458\n",
      "Epoch: 171, Train Loss: 3.2954, Test Loss: 6.2157\n",
      "Epoch: 172, Train Loss: 3.2507, Test Loss: 6.1546\n",
      "Epoch: 173, Train Loss: 3.2889, Test Loss: 5.9700\n",
      "Epoch: 174, Train Loss: 2.8731, Test Loss: 5.8846\n",
      "Epoch: 175, Train Loss: 3.2868, Test Loss: 6.2340\n",
      "Epoch: 176, Train Loss: 3.1599, Test Loss: 6.5298\n",
      "Epoch: 177, Train Loss: 3.5405, Test Loss: 6.0575\n",
      "Epoch: 178, Train Loss: 3.1567, Test Loss: 5.5809\n",
      "Epoch: 179, Train Loss: 3.2660, Test Loss: 6.1494\n",
      "Epoch: 180, Train Loss: 3.2999, Test Loss: 6.2991\n",
      "Epoch: 181, Train Loss: 3.1544, Test Loss: 5.8557\n",
      "Epoch: 182, Train Loss: 3.2113, Test Loss: 5.9374\n",
      "Epoch: 183, Train Loss: 3.2442, Test Loss: 6.6251\n",
      "Epoch: 184, Train Loss: 3.0099, Test Loss: 6.1020\n",
      "Epoch: 185, Train Loss: 3.1223, Test Loss: 5.9094\n",
      "Epoch: 186, Train Loss: 3.2484, Test Loss: 5.8640\n",
      "Epoch: 187, Train Loss: 3.2160, Test Loss: 6.1221\n",
      "Epoch: 188, Train Loss: 3.1687, Test Loss: 5.9986\n",
      "Epoch: 189, Train Loss: 3.0748, Test Loss: 5.8364\n",
      "Epoch: 190, Train Loss: 3.0342, Test Loss: 6.1389\n",
      "Epoch: 191, Train Loss: 3.1285, Test Loss: 5.9444\n",
      "Epoch: 192, Train Loss: 2.9787, Test Loss: 6.0918\n",
      "Epoch: 193, Train Loss: 3.0898, Test Loss: 6.0021\n",
      "Epoch: 194, Train Loss: 3.0062, Test Loss: 6.2711\n",
      "Epoch: 195, Train Loss: 3.2384, Test Loss: 5.8669\n",
      "Epoch: 196, Train Loss: 2.9558, Test Loss: 5.8763\n",
      "Epoch: 197, Train Loss: 3.1446, Test Loss: 5.9875\n",
      "Epoch: 198, Train Loss: 3.3001, Test Loss: 6.2327\n",
      "Epoch: 199, Train Loss: 3.0992, Test Loss: 5.8431\n",
      "Epoch: 200, Train Loss: 3.4068, Test Loss: 5.6001\n",
      "Epoch: 201, Train Loss: 3.0389, Test Loss: 6.0297\n",
      "Epoch: 202, Train Loss: 3.0932, Test Loss: 6.8536\n",
      "Epoch: 203, Train Loss: 3.1540, Test Loss: 5.9865\n",
      "Epoch: 204, Train Loss: 3.3116, Test Loss: 5.3458\n",
      "Epoch: 205, Train Loss: 3.2192, Test Loss: 6.1768\n",
      "Epoch: 206, Train Loss: 2.8602, Test Loss: 6.7756\n",
      "Epoch: 207, Train Loss: 2.9920, Test Loss: 5.9476\n",
      "Epoch: 208, Train Loss: 3.0939, Test Loss: 5.4816\n",
      "Epoch: 209, Train Loss: 3.3676, Test Loss: 6.0193\n",
      "Epoch: 210, Train Loss: 3.1887, Test Loss: 6.6242\n",
      "Epoch: 211, Train Loss: 3.1951, Test Loss: 5.8696\n",
      "Epoch: 212, Train Loss: 2.9913, Test Loss: 5.8761\n",
      "Epoch: 213, Train Loss: 2.9846, Test Loss: 6.1130\n",
      "Epoch: 214, Train Loss: 3.0993, Test Loss: 5.9053\n",
      "Epoch: 215, Train Loss: 2.7820, Test Loss: 5.7980\n",
      "Epoch: 216, Train Loss: 2.9781, Test Loss: 6.2656\n",
      "Epoch: 217, Train Loss: 3.1694, Test Loss: 6.7761\n",
      "Epoch: 218, Train Loss: 3.0471, Test Loss: 6.1937\n",
      "Epoch: 219, Train Loss: 3.0096, Test Loss: 5.8613\n",
      "Epoch: 220, Train Loss: 2.9054, Test Loss: 6.4429\n",
      "Epoch: 221, Train Loss: 3.1827, Test Loss: 6.8970\n",
      "Epoch: 222, Train Loss: 3.3596, Test Loss: 5.5058\n",
      "Epoch: 223, Train Loss: 2.9007, Test Loss: 5.6867\n",
      "Epoch: 224, Train Loss: 3.0227, Test Loss: 7.1074\n",
      "Epoch: 225, Train Loss: 3.0254, Test Loss: 6.7221\n",
      "Epoch: 226, Train Loss: 2.9780, Test Loss: 5.7841\n",
      "Epoch: 227, Train Loss: 2.9277, Test Loss: 6.6163\n",
      "Epoch: 228, Train Loss: 2.9639, Test Loss: 7.0872\n",
      "Epoch: 229, Train Loss: 3.0043, Test Loss: 5.6584\n",
      "Epoch: 230, Train Loss: 3.3475, Test Loss: 5.9118\n",
      "Epoch: 231, Train Loss: 2.9908, Test Loss: 6.5702\n",
      "Epoch: 232, Train Loss: 2.9357, Test Loss: 6.4670\n",
      "Epoch: 233, Train Loss: 2.7574, Test Loss: 5.6572\n",
      "Epoch: 234, Train Loss: 3.0153, Test Loss: 6.4839\n",
      "Epoch: 235, Train Loss: 2.8628, Test Loss: 6.3427\n",
      "Epoch: 236, Train Loss: 3.1276, Test Loss: 6.1526\n",
      "Epoch: 237, Train Loss: 2.9363, Test Loss: 5.9689\n",
      "Epoch: 238, Train Loss: 2.9715, Test Loss: 6.2190\n",
      "Epoch: 239, Train Loss: 3.0795, Test Loss: 6.5774\n",
      "Epoch: 240, Train Loss: 2.9540, Test Loss: 5.9135\n",
      "Epoch: 241, Train Loss: 2.9524, Test Loss: 5.7994\n",
      "Epoch: 242, Train Loss: 2.8426, Test Loss: 6.4195\n",
      "Epoch: 243, Train Loss: 2.9247, Test Loss: 6.1856\n",
      "Epoch: 244, Train Loss: 2.8208, Test Loss: 5.9687\n",
      "Epoch: 245, Train Loss: 2.7483, Test Loss: 6.2631\n",
      "Epoch: 246, Train Loss: 2.8891, Test Loss: 6.2445\n",
      "Epoch: 247, Train Loss: 2.7775, Test Loss: 5.6809\n",
      "Epoch: 248, Train Loss: 3.0040, Test Loss: 5.6353\n",
      "Epoch: 249, Train Loss: 2.7946, Test Loss: 5.7002\n",
      "Epoch: 250, Train Loss: 2.8539, Test Loss: 6.6366\n",
      "Epoch: 251, Train Loss: 2.9850, Test Loss: 5.8339\n",
      "Epoch: 252, Train Loss: 2.8536, Test Loss: 5.3972\n",
      "Epoch: 253, Train Loss: 2.9241, Test Loss: 5.8937\n",
      "Epoch: 254, Train Loss: 3.0431, Test Loss: 6.4110\n",
      "Epoch: 255, Train Loss: 2.9870, Test Loss: 6.4320\n",
      "Epoch: 256, Train Loss: 2.9671, Test Loss: 5.5911\n",
      "Epoch: 257, Train Loss: 2.8517, Test Loss: 6.5649\n",
      "Epoch: 258, Train Loss: 2.8498, Test Loss: 6.2477\n",
      "Epoch: 259, Train Loss: 3.1395, Test Loss: 5.2403\n",
      "Epoch: 260, Train Loss: 3.1396, Test Loss: 6.7766\n",
      "Epoch: 261, Train Loss: 2.9433, Test Loss: 7.0289\n",
      "Epoch: 262, Train Loss: 3.1119, Test Loss: 5.5942\n",
      "Epoch: 263, Train Loss: 2.9816, Test Loss: 6.2955\n",
      "Epoch: 264, Train Loss: 2.8484, Test Loss: 7.1004\n",
      "Epoch: 265, Train Loss: 3.1209, Test Loss: 5.5524\n",
      "Epoch: 266, Train Loss: 2.9593, Test Loss: 5.9593\n",
      "Epoch: 267, Train Loss: 3.0170, Test Loss: 7.2866\n",
      "Epoch: 268, Train Loss: 2.9603, Test Loss: 6.5786\n",
      "Epoch: 269, Train Loss: 2.8018, Test Loss: 5.7536\n",
      "Epoch: 270, Train Loss: 2.9310, Test Loss: 6.5511\n",
      "Epoch: 271, Train Loss: 2.6991, Test Loss: 6.8885\n",
      "Epoch: 272, Train Loss: 2.7458, Test Loss: 6.0286\n",
      "Epoch: 273, Train Loss: 2.7674, Test Loss: 5.8199\n",
      "Epoch: 274, Train Loss: 2.8121, Test Loss: 6.7569\n",
      "Epoch: 275, Train Loss: 2.8341, Test Loss: 6.9488\n",
      "Epoch: 276, Train Loss: 2.9292, Test Loss: 5.7200\n",
      "Epoch: 277, Train Loss: 2.9437, Test Loss: 6.4917\n",
      "Epoch: 278, Train Loss: 2.6855, Test Loss: 6.9475\n",
      "Epoch: 279, Train Loss: 3.1922, Test Loss: 5.6992\n",
      "Epoch: 280, Train Loss: 2.8288, Test Loss: 5.9240\n",
      "Epoch: 281, Train Loss: 2.7358, Test Loss: 6.7593\n",
      "Epoch: 282, Train Loss: 2.9505, Test Loss: 6.6224\n",
      "Epoch: 283, Train Loss: 2.9917, Test Loss: 5.4148\n",
      "Epoch: 284, Train Loss: 2.7220, Test Loss: 6.2479\n",
      "Epoch: 285, Train Loss: 2.9316, Test Loss: 6.8299\n",
      "Epoch: 286, Train Loss: 2.9663, Test Loss: 6.0791\n",
      "Epoch: 287, Train Loss: 2.5449, Test Loss: 5.4912\n",
      "Epoch: 288, Train Loss: 2.9446, Test Loss: 6.3901\n",
      "Epoch: 289, Train Loss: 2.9284, Test Loss: 6.7702\n",
      "Epoch: 290, Train Loss: 2.8515, Test Loss: 5.8396\n",
      "Epoch: 291, Train Loss: 2.8424, Test Loss: 5.8405\n",
      "Epoch: 292, Train Loss: 2.6195, Test Loss: 6.7488\n",
      "Epoch: 293, Train Loss: 2.9650, Test Loss: 6.8957\n",
      "Epoch: 294, Train Loss: 3.0055, Test Loss: 5.7495\n",
      "Epoch: 295, Train Loss: 2.7246, Test Loss: 6.3555\n",
      "Epoch: 296, Train Loss: 2.6072, Test Loss: 6.8368\n",
      "Epoch: 297, Train Loss: 2.7021, Test Loss: 6.3126\n",
      "Epoch: 298, Train Loss: 2.7921, Test Loss: 6.2989\n",
      "Epoch: 299, Train Loss: 2.7981, Test Loss: 6.9008\n",
      "Epoch: 300, Train Loss: 2.7395, Test Loss: 5.9574\n",
      "Epoch: 301, Train Loss: 2.6779, Test Loss: 6.2190\n",
      "Epoch: 302, Train Loss: 2.8041, Test Loss: 6.8728\n",
      "Epoch: 303, Train Loss: 2.5860, Test Loss: 6.2201\n",
      "Epoch: 304, Train Loss: 2.7420, Test Loss: 5.7885\n",
      "Epoch: 305, Train Loss: 2.7241, Test Loss: 6.5508\n",
      "Epoch: 306, Train Loss: 3.0413, Test Loss: 6.4261\n",
      "Epoch: 307, Train Loss: 2.8805, Test Loss: 6.2039\n",
      "Epoch: 308, Train Loss: 2.5132, Test Loss: 6.2864\n",
      "Epoch: 309, Train Loss: 2.6123, Test Loss: 6.5281\n",
      "Epoch: 310, Train Loss: 2.6812, Test Loss: 6.8840\n",
      "Epoch: 311, Train Loss: 2.7020, Test Loss: 6.3508\n",
      "Epoch: 312, Train Loss: 2.7208, Test Loss: 6.3645\n",
      "Epoch: 313, Train Loss: 2.6814, Test Loss: 6.9908\n",
      "Epoch: 314, Train Loss: 2.5466, Test Loss: 5.9516\n",
      "Epoch: 315, Train Loss: 2.6242, Test Loss: 6.2612\n",
      "Epoch: 316, Train Loss: 2.7245, Test Loss: 7.5001\n",
      "Epoch: 317, Train Loss: 2.5844, Test Loss: 6.9469\n",
      "Epoch: 318, Train Loss: 2.7842, Test Loss: 5.4796\n",
      "Epoch: 319, Train Loss: 2.7844, Test Loss: 6.6053\n",
      "Epoch: 320, Train Loss: 2.6856, Test Loss: 7.4385\n",
      "Epoch: 321, Train Loss: 2.7923, Test Loss: 6.2075\n",
      "Epoch: 322, Train Loss: 2.8189, Test Loss: 5.7519\n",
      "Epoch: 323, Train Loss: 2.7688, Test Loss: 6.8815\n",
      "Epoch: 324, Train Loss: 2.7065, Test Loss: 6.7904\n",
      "Epoch: 325, Train Loss: 2.8221, Test Loss: 6.3519\n",
      "Epoch: 326, Train Loss: 2.4786, Test Loss: 5.8913\n",
      "Epoch: 327, Train Loss: 2.9132, Test Loss: 7.2120\n",
      "Epoch: 328, Train Loss: 2.5409, Test Loss: 6.8317\n",
      "Epoch: 329, Train Loss: 2.7089, Test Loss: 5.7447\n",
      "Epoch: 330, Train Loss: 2.8077, Test Loss: 7.0596\n",
      "Epoch: 331, Train Loss: 2.6422, Test Loss: 7.4435\n",
      "Epoch: 332, Train Loss: 2.7518, Test Loss: 6.5017\n",
      "Epoch: 333, Train Loss: 2.3781, Test Loss: 6.2911\n",
      "Epoch: 334, Train Loss: 2.7064, Test Loss: 7.0989\n",
      "Epoch: 335, Train Loss: 2.4669, Test Loss: 6.5367\n",
      "Epoch: 336, Train Loss: 2.7248, Test Loss: 6.1394\n",
      "Epoch: 337, Train Loss: 2.7000, Test Loss: 7.3350\n",
      "Epoch: 338, Train Loss: 2.8664, Test Loss: 6.6333\n",
      "Epoch: 339, Train Loss: 2.6693, Test Loss: 6.3650\n",
      "Epoch: 340, Train Loss: 2.4636, Test Loss: 6.6434\n",
      "Epoch: 341, Train Loss: 2.5902, Test Loss: 7.0802\n",
      "Epoch: 342, Train Loss: 2.5812, Test Loss: 6.5631\n",
      "Epoch: 343, Train Loss: 2.7400, Test Loss: 6.1945\n",
      "Epoch: 344, Train Loss: 2.6457, Test Loss: 6.9892\n",
      "Epoch: 345, Train Loss: 2.5345, Test Loss: 7.2497\n",
      "Epoch: 346, Train Loss: 2.4575, Test Loss: 6.9888\n",
      "Epoch: 347, Train Loss: 2.5710, Test Loss: 6.0144\n",
      "Epoch: 348, Train Loss: 2.6434, Test Loss: 7.4629\n",
      "Epoch: 349, Train Loss: 2.5739, Test Loss: 7.0155\n",
      "Epoch: 350, Train Loss: 2.4675, Test Loss: 6.1765\n",
      "Epoch: 351, Train Loss: 2.5807, Test Loss: 7.4067\n",
      "Epoch: 352, Train Loss: 2.9610, Test Loss: 6.2659\n",
      "Epoch: 353, Train Loss: 2.3801, Test Loss: 6.5717\n",
      "Epoch: 354, Train Loss: 2.5083, Test Loss: 7.5630\n",
      "Epoch: 355, Train Loss: 2.6764, Test Loss: 6.8825\n",
      "Epoch: 356, Train Loss: 2.5477, Test Loss: 5.9165\n",
      "Epoch: 357, Train Loss: 2.6783, Test Loss: 6.7838\n",
      "Epoch: 358, Train Loss: 2.5121, Test Loss: 7.7581\n",
      "Epoch: 359, Train Loss: 2.6248, Test Loss: 6.3370\n",
      "Epoch: 360, Train Loss: 2.9181, Test Loss: 6.6523\n",
      "Epoch: 361, Train Loss: 2.4109, Test Loss: 7.7037\n",
      "Epoch: 362, Train Loss: 2.6169, Test Loss: 6.8050\n",
      "Epoch: 363, Train Loss: 2.4208, Test Loss: 5.7857\n",
      "Epoch: 364, Train Loss: 2.7035, Test Loss: 7.5959\n",
      "Epoch: 365, Train Loss: 2.3989, Test Loss: 7.8945\n",
      "Epoch: 366, Train Loss: 2.5177, Test Loss: 5.5854\n",
      "Epoch: 367, Train Loss: 2.8169, Test Loss: 6.8116\n",
      "Epoch: 368, Train Loss: 2.5792, Test Loss: 8.0364\n",
      "Epoch: 369, Train Loss: 2.6528, Test Loss: 6.3564\n",
      "Epoch: 370, Train Loss: 2.4851, Test Loss: 6.2505\n",
      "Epoch: 371, Train Loss: 2.6463, Test Loss: 7.4918\n",
      "Epoch: 372, Train Loss: 2.6519, Test Loss: 6.6267\n",
      "Epoch: 373, Train Loss: 2.4177, Test Loss: 6.0228\n",
      "Epoch: 374, Train Loss: 2.6894, Test Loss: 7.5349\n",
      "Epoch: 375, Train Loss: 2.5251, Test Loss: 6.3741\n",
      "Epoch: 376, Train Loss: 2.4104, Test Loss: 6.8240\n",
      "Epoch: 377, Train Loss: 2.6449, Test Loss: 7.3453\n",
      "Epoch: 378, Train Loss: 2.4470, Test Loss: 6.4613\n",
      "Epoch: 379, Train Loss: 2.5544, Test Loss: 7.1451\n",
      "Epoch: 380, Train Loss: 2.4475, Test Loss: 6.5750\n",
      "Epoch: 381, Train Loss: 2.5122, Test Loss: 7.0023\n",
      "Epoch: 382, Train Loss: 2.3687, Test Loss: 7.0551\n",
      "Epoch: 383, Train Loss: 2.3776, Test Loss: 6.5779\n",
      "Epoch: 384, Train Loss: 2.5483, Test Loss: 6.9325\n",
      "Epoch: 385, Train Loss: 2.5220, Test Loss: 6.4348\n",
      "Epoch: 386, Train Loss: 2.4274, Test Loss: 6.5372\n",
      "Epoch: 387, Train Loss: 2.4489, Test Loss: 7.4408\n",
      "Epoch: 388, Train Loss: 2.7098, Test Loss: 6.7886\n",
      "Epoch: 389, Train Loss: 2.4640, Test Loss: 6.3276\n",
      "Epoch: 390, Train Loss: 2.4247, Test Loss: 7.4224\n",
      "Epoch: 391, Train Loss: 2.4237, Test Loss: 7.0263\n",
      "Epoch: 392, Train Loss: 2.5704, Test Loss: 5.9163\n",
      "Epoch: 393, Train Loss: 2.8154, Test Loss: 8.2215\n",
      "Epoch: 394, Train Loss: 2.5920, Test Loss: 7.0032\n",
      "Epoch: 395, Train Loss: 2.5289, Test Loss: 5.8316\n",
      "Epoch: 396, Train Loss: 2.4169, Test Loss: 7.9555\n",
      "Epoch: 397, Train Loss: 2.5000, Test Loss: 8.1108\n",
      "Epoch: 398, Train Loss: 2.3702, Test Loss: 5.6231\n",
      "Epoch: 399, Train Loss: 2.7397, Test Loss: 7.0843\n",
      "Epoch: 400, Train Loss: 2.4311, Test Loss: 7.5811\n",
      "Epoch: 401, Train Loss: 2.4672, Test Loss: 6.1612\n",
      "Epoch: 402, Train Loss: 2.3305, Test Loss: 6.3513\n",
      "Epoch: 403, Train Loss: 2.4576, Test Loss: 7.9895\n",
      "Epoch: 404, Train Loss: 2.2556, Test Loss: 6.5109\n",
      "Epoch: 405, Train Loss: 2.4172, Test Loss: 6.3784\n",
      "Epoch: 406, Train Loss: 2.4677, Test Loss: 6.6499\n",
      "Epoch: 407, Train Loss: 2.2282, Test Loss: 6.4619\n",
      "Epoch: 408, Train Loss: 2.3604, Test Loss: 6.9596\n",
      "Epoch: 409, Train Loss: 2.4118, Test Loss: 6.9524\n",
      "Epoch: 410, Train Loss: 2.2197, Test Loss: 6.6314\n",
      "Epoch: 411, Train Loss: 2.3715, Test Loss: 6.2432\n",
      "Epoch: 412, Train Loss: 2.4168, Test Loss: 7.0383\n",
      "Epoch: 413, Train Loss: 2.4991, Test Loss: 7.0515\n",
      "Epoch: 414, Train Loss: 2.3879, Test Loss: 6.3712\n",
      "Epoch: 415, Train Loss: 2.4340, Test Loss: 6.5123\n",
      "Epoch: 416, Train Loss: 2.3436, Test Loss: 7.5287\n",
      "Epoch: 417, Train Loss: 2.4319, Test Loss: 6.3410\n",
      "Epoch: 418, Train Loss: 2.3852, Test Loss: 6.2291\n",
      "Epoch: 419, Train Loss: 2.2549, Test Loss: 7.5953\n",
      "Epoch: 420, Train Loss: 2.5505, Test Loss: 6.2513\n",
      "Epoch: 421, Train Loss: 2.3363, Test Loss: 6.2928\n",
      "Epoch: 422, Train Loss: 2.3007, Test Loss: 6.8878\n",
      "Epoch: 423, Train Loss: 2.4249, Test Loss: 7.2270\n",
      "Epoch: 424, Train Loss: 2.4052, Test Loss: 6.8143\n",
      "Epoch: 425, Train Loss: 2.3748, Test Loss: 5.9281\n",
      "Epoch: 426, Train Loss: 2.4432, Test Loss: 8.0614\n",
      "Epoch: 427, Train Loss: 2.3166, Test Loss: 6.8756\n",
      "Epoch: 428, Train Loss: 2.3443, Test Loss: 5.7085\n",
      "Epoch: 429, Train Loss: 2.6353, Test Loss: 7.3508\n",
      "Epoch: 430, Train Loss: 2.2050, Test Loss: 7.8819\n",
      "Epoch: 431, Train Loss: 2.6929, Test Loss: 5.7679\n",
      "Epoch: 432, Train Loss: 2.3590, Test Loss: 6.0809\n",
      "Epoch: 433, Train Loss: 2.5329, Test Loss: 7.6489\n",
      "Epoch: 434, Train Loss: 2.2408, Test Loss: 6.8299\n",
      "Epoch: 435, Train Loss: 2.2779, Test Loss: 6.3584\n",
      "Epoch: 436, Train Loss: 2.3937, Test Loss: 6.2087\n",
      "Epoch: 437, Train Loss: 2.4612, Test Loss: 7.6428\n",
      "Epoch: 438, Train Loss: 2.3270, Test Loss: 6.2563\n",
      "Epoch: 439, Train Loss: 2.1743, Test Loss: 6.1957\n",
      "Epoch: 440, Train Loss: 2.2137, Test Loss: 7.1247\n",
      "Epoch: 441, Train Loss: 2.2906, Test Loss: 7.3287\n",
      "Epoch: 442, Train Loss: 2.3583, Test Loss: 6.9665\n",
      "Epoch: 443, Train Loss: 2.2356, Test Loss: 6.4895\n",
      "Epoch: 444, Train Loss: 2.3811, Test Loss: 6.7718\n",
      "Epoch: 445, Train Loss: 2.2407, Test Loss: 7.4977\n",
      "Epoch: 446, Train Loss: 2.4254, Test Loss: 6.1410\n",
      "Epoch: 447, Train Loss: 2.3986, Test Loss: 7.7340\n",
      "Epoch: 448, Train Loss: 2.3107, Test Loss: 7.2998\n",
      "Epoch: 449, Train Loss: 2.0914, Test Loss: 6.1612\n",
      "Epoch: 450, Train Loss: 2.4859, Test Loss: 7.6430\n",
      "Epoch: 451, Train Loss: 2.2086, Test Loss: 8.2359\n",
      "Epoch: 452, Train Loss: 2.3273, Test Loss: 6.4289\n",
      "Epoch: 453, Train Loss: 2.2284, Test Loss: 6.8991\n",
      "Epoch: 454, Train Loss: 2.4386, Test Loss: 9.0682\n",
      "Epoch: 455, Train Loss: 2.4094, Test Loss: 6.6535\n",
      "Epoch: 456, Train Loss: 2.2315, Test Loss: 6.3346\n",
      "Epoch: 457, Train Loss: 2.1601, Test Loss: 8.5566\n",
      "Epoch: 458, Train Loss: 2.5395, Test Loss: 6.5273\n",
      "Epoch: 459, Train Loss: 2.2990, Test Loss: 6.9921\n",
      "Epoch: 460, Train Loss: 2.4255, Test Loss: 7.5143\n",
      "Epoch: 461, Train Loss: 2.3986, Test Loss: 7.6587\n",
      "Epoch: 462, Train Loss: 2.4886, Test Loss: 5.9406\n",
      "Epoch: 463, Train Loss: 2.3928, Test Loss: 8.0085\n",
      "Epoch: 464, Train Loss: 2.4153, Test Loss: 7.4719\n",
      "Epoch: 465, Train Loss: 2.1005, Test Loss: 6.2380\n",
      "Epoch: 466, Train Loss: 2.4301, Test Loss: 7.8252\n",
      "Epoch: 467, Train Loss: 2.2591, Test Loss: 7.3499\n",
      "Epoch: 468, Train Loss: 2.3481, Test Loss: 6.1721\n",
      "Epoch: 469, Train Loss: 2.2694, Test Loss: 7.7379\n",
      "Epoch: 470, Train Loss: 2.2373, Test Loss: 7.8830\n",
      "Epoch: 471, Train Loss: 2.2698, Test Loss: 6.7537\n",
      "Epoch: 472, Train Loss: 2.2327, Test Loss: 7.1655\n",
      "Epoch: 473, Train Loss: 2.3630, Test Loss: 7.6200\n",
      "Epoch: 474, Train Loss: 2.4472, Test Loss: 6.5100\n",
      "Epoch: 475, Train Loss: 2.2781, Test Loss: 7.2557\n",
      "Epoch: 476, Train Loss: 2.1915, Test Loss: 7.7605\n",
      "Epoch: 477, Train Loss: 2.3906, Test Loss: 6.6371\n",
      "Epoch: 478, Train Loss: 2.2527, Test Loss: 7.7360\n",
      "Epoch: 479, Train Loss: 2.2986, Test Loss: 7.6051\n",
      "Epoch: 480, Train Loss: 2.3460, Test Loss: 6.8288\n",
      "Epoch: 481, Train Loss: 2.2071, Test Loss: 7.5611\n",
      "Epoch: 482, Train Loss: 2.0763, Test Loss: 7.3933\n",
      "Epoch: 483, Train Loss: 2.1543, Test Loss: 6.8695\n",
      "Epoch: 484, Train Loss: 2.2237, Test Loss: 8.2866\n",
      "Epoch: 485, Train Loss: 2.1622, Test Loss: 7.8571\n",
      "Epoch: 486, Train Loss: 2.2395, Test Loss: 6.7680\n",
      "Epoch: 487, Train Loss: 2.3459, Test Loss: 6.7771\n",
      "Epoch: 488, Train Loss: 2.0838, Test Loss: 8.2323\n",
      "Epoch: 489, Train Loss: 2.3001, Test Loss: 6.5086\n",
      "Epoch: 490, Train Loss: 2.1215, Test Loss: 7.3920\n",
      "Epoch: 491, Train Loss: 2.2238, Test Loss: 8.9764\n",
      "Epoch: 492, Train Loss: 2.1176, Test Loss: 6.7466\n",
      "Epoch: 493, Train Loss: 2.2958, Test Loss: 6.0867\n",
      "Epoch: 494, Train Loss: 2.4443, Test Loss: 8.9795\n",
      "Epoch: 495, Train Loss: 2.2463, Test Loss: 7.1015\n",
      "Epoch: 496, Train Loss: 2.2701, Test Loss: 5.7000\n",
      "Epoch: 497, Train Loss: 2.3952, Test Loss: 8.4852\n",
      "Epoch: 498, Train Loss: 2.3151, Test Loss: 7.5928\n",
      "Epoch: 499, Train Loss: 2.2654, Test Loss: 6.2634\n",
      "Epoch: 500, Train Loss: 2.2137, Test Loss: 6.9010\n",
      "Epoch: 501, Train Loss: 2.1170, Test Loss: 7.2041\n",
      "Epoch: 502, Train Loss: 2.3065, Test Loss: 7.2696\n",
      "Epoch: 503, Train Loss: 2.1746, Test Loss: 6.2210\n",
      "Epoch: 504, Train Loss: 2.4044, Test Loss: 7.6705\n",
      "Epoch: 505, Train Loss: 2.1058, Test Loss: 8.3461\n",
      "Epoch: 506, Train Loss: 2.2986, Test Loss: 5.5834\n",
      "Epoch: 507, Train Loss: 2.3406, Test Loss: 7.1781\n",
      "Epoch: 508, Train Loss: 2.3345, Test Loss: 8.5757\n",
      "Epoch: 509, Train Loss: 2.3822, Test Loss: 6.1553\n",
      "Epoch: 510, Train Loss: 2.1633, Test Loss: 5.7723\n",
      "Epoch: 511, Train Loss: 2.3570, Test Loss: 9.0075\n",
      "Epoch: 512, Train Loss: 2.4777, Test Loss: 7.0887\n",
      "Epoch: 513, Train Loss: 2.0171, Test Loss: 5.9673\n",
      "Epoch: 514, Train Loss: 2.3083, Test Loss: 7.3872\n",
      "Epoch: 515, Train Loss: 2.2368, Test Loss: 7.7843\n",
      "Epoch: 516, Train Loss: 2.0734, Test Loss: 5.7004\n",
      "Epoch: 517, Train Loss: 2.1338, Test Loss: 6.6407\n",
      "Epoch: 518, Train Loss: 2.0321, Test Loss: 8.1459\n",
      "Epoch: 519, Train Loss: 2.3497, Test Loss: 6.4482\n",
      "Epoch: 520, Train Loss: 2.0581, Test Loss: 5.8895\n",
      "Epoch: 521, Train Loss: 2.2622, Test Loss: 7.9752\n",
      "Epoch: 522, Train Loss: 2.3327, Test Loss: 7.3998\n",
      "Epoch: 523, Train Loss: 1.9741, Test Loss: 5.8701\n",
      "Epoch: 524, Train Loss: 2.2620, Test Loss: 7.5120\n",
      "Epoch: 525, Train Loss: 2.3116, Test Loss: 8.1485\n",
      "Epoch: 526, Train Loss: 2.2997, Test Loss: 6.2442\n",
      "Epoch: 527, Train Loss: 2.2235, Test Loss: 6.3936\n",
      "Epoch: 528, Train Loss: 2.2649, Test Loss: 8.3493\n",
      "Epoch: 529, Train Loss: 2.2434, Test Loss: 7.1549\n",
      "Epoch: 530, Train Loss: 2.1186, Test Loss: 6.1493\n",
      "Epoch: 531, Train Loss: 2.1616, Test Loss: 7.0500\n",
      "Epoch: 532, Train Loss: 2.0681, Test Loss: 8.2555\n",
      "Epoch: 533, Train Loss: 2.3047, Test Loss: 6.5944\n",
      "Epoch: 534, Train Loss: 2.1898, Test Loss: 6.2451\n",
      "Epoch: 535, Train Loss: 2.0893, Test Loss: 7.2468\n",
      "Epoch: 536, Train Loss: 2.0835, Test Loss: 7.5430\n",
      "Epoch: 537, Train Loss: 2.0802, Test Loss: 6.2484\n",
      "Epoch: 538, Train Loss: 2.2299, Test Loss: 6.3524\n",
      "Epoch: 539, Train Loss: 1.9266, Test Loss: 7.8236\n",
      "Epoch: 540, Train Loss: 2.0598, Test Loss: 7.6179\n",
      "Epoch: 541, Train Loss: 2.0284, Test Loss: 6.5276\n",
      "Epoch: 542, Train Loss: 2.3367, Test Loss: 8.0135\n",
      "Epoch: 543, Train Loss: 2.1675, Test Loss: 7.0477\n",
      "Epoch: 544, Train Loss: 2.2341, Test Loss: 5.8971\n",
      "Epoch: 545, Train Loss: 2.2454, Test Loss: 8.1802\n",
      "Epoch: 546, Train Loss: 2.4786, Test Loss: 6.4508\n",
      "Epoch: 547, Train Loss: 2.1362, Test Loss: 5.7203\n",
      "Epoch: 548, Train Loss: 2.1814, Test Loss: 7.8459\n",
      "Epoch: 549, Train Loss: 2.3431, Test Loss: 6.7528\n",
      "Epoch: 550, Train Loss: 2.0940, Test Loss: 5.5052\n",
      "Epoch: 551, Train Loss: 2.0964, Test Loss: 7.0993\n",
      "Epoch: 552, Train Loss: 2.0355, Test Loss: 7.3098\n",
      "Epoch: 553, Train Loss: 1.9099, Test Loss: 5.6136\n",
      "Epoch: 554, Train Loss: 2.3828, Test Loss: 7.1246\n",
      "Epoch: 555, Train Loss: 2.0218, Test Loss: 7.4359\n",
      "Epoch: 556, Train Loss: 2.1916, Test Loss: 6.1632\n",
      "Epoch: 557, Train Loss: 2.0628, Test Loss: 6.1693\n",
      "Epoch: 558, Train Loss: 2.1616, Test Loss: 6.6398\n",
      "Epoch: 559, Train Loss: 1.9850, Test Loss: 7.5570\n",
      "Epoch: 560, Train Loss: 2.0887, Test Loss: 6.5638\n",
      "Epoch: 561, Train Loss: 2.2450, Test Loss: 5.8799\n",
      "Epoch: 562, Train Loss: 2.1244, Test Loss: 7.1238\n",
      "Epoch: 563, Train Loss: 2.0042, Test Loss: 8.3665\n",
      "Epoch: 564, Train Loss: 2.4034, Test Loss: 5.7997\n",
      "Epoch: 565, Train Loss: 2.2953, Test Loss: 7.0733\n",
      "Epoch: 566, Train Loss: 2.0112, Test Loss: 7.5827\n",
      "Epoch: 567, Train Loss: 1.8733, Test Loss: 7.0285\n",
      "Epoch: 568, Train Loss: 2.1386, Test Loss: 6.2645\n",
      "Epoch: 569, Train Loss: 1.8777, Test Loss: 6.9131\n",
      "Epoch: 570, Train Loss: 1.9040, Test Loss: 7.7070\n",
      "Epoch: 571, Train Loss: 2.1769, Test Loss: 6.6555\n",
      "Epoch: 572, Train Loss: 2.0029, Test Loss: 6.0449\n",
      "Epoch: 573, Train Loss: 2.1421, Test Loss: 7.9753\n",
      "Epoch: 574, Train Loss: 2.0906, Test Loss: 7.6763\n",
      "Epoch: 575, Train Loss: 1.9867, Test Loss: 5.8686\n",
      "Epoch: 576, Train Loss: 2.1057, Test Loss: 6.7443\n",
      "Epoch: 577, Train Loss: 1.9873, Test Loss: 8.4395\n",
      "Epoch: 578, Train Loss: 2.1412, Test Loss: 7.1232\n",
      "Epoch: 579, Train Loss: 1.9597, Test Loss: 5.3599\n",
      "Epoch: 580, Train Loss: 2.4800, Test Loss: 8.6099\n",
      "Epoch: 581, Train Loss: 2.1767, Test Loss: 7.7974\n",
      "Epoch: 582, Train Loss: 2.1319, Test Loss: 5.3839\n",
      "Epoch: 583, Train Loss: 2.2274, Test Loss: 6.5795\n",
      "Epoch: 584, Train Loss: 2.0758, Test Loss: 8.4988\n",
      "Epoch: 585, Train Loss: 1.9379, Test Loss: 6.6611\n",
      "Epoch: 586, Train Loss: 1.8736, Test Loss: 6.3486\n",
      "Epoch: 587, Train Loss: 2.1683, Test Loss: 8.2842\n",
      "Epoch: 588, Train Loss: 2.2018, Test Loss: 7.2868\n",
      "Epoch: 589, Train Loss: 2.1346, Test Loss: 6.4232\n",
      "Epoch: 590, Train Loss: 2.0429, Test Loss: 6.7465\n",
      "Epoch: 591, Train Loss: 1.9374, Test Loss: 8.0677\n",
      "Epoch: 592, Train Loss: 2.1295, Test Loss: 7.0552\n",
      "Epoch: 593, Train Loss: 1.8907, Test Loss: 6.2209\n",
      "Epoch: 594, Train Loss: 2.0794, Test Loss: 6.6201\n",
      "Epoch: 595, Train Loss: 2.0025, Test Loss: 7.4327\n",
      "Epoch: 596, Train Loss: 1.9063, Test Loss: 6.5092\n",
      "Epoch: 597, Train Loss: 2.0779, Test Loss: 6.6837\n",
      "Epoch: 598, Train Loss: 2.0260, Test Loss: 6.9120\n",
      "Epoch: 599, Train Loss: 1.9700, Test Loss: 7.0326\n",
      "Epoch: 600, Train Loss: 1.9783, Test Loss: 7.2006\n",
      "Epoch: 601, Train Loss: 1.9878, Test Loss: 6.6374\n",
      "Epoch: 602, Train Loss: 1.9263, Test Loss: 7.4658\n",
      "Epoch: 603, Train Loss: 2.0350, Test Loss: 7.1930\n",
      "Epoch: 604, Train Loss: 1.9701, Test Loss: 6.4307\n",
      "Epoch: 605, Train Loss: 1.8431, Test Loss: 7.0134\n",
      "Epoch: 606, Train Loss: 2.0713, Test Loss: 7.1352\n",
      "Epoch: 607, Train Loss: 2.0621, Test Loss: 6.2378\n",
      "Epoch: 608, Train Loss: 2.1110, Test Loss: 6.9872\n",
      "Epoch: 609, Train Loss: 1.9806, Test Loss: 7.2924\n",
      "Epoch: 610, Train Loss: 2.0552, Test Loss: 7.0063\n",
      "Epoch: 611, Train Loss: 1.7527, Test Loss: 7.0727\n",
      "Epoch: 612, Train Loss: 1.9224, Test Loss: 6.1082\n",
      "Epoch: 613, Train Loss: 1.8666, Test Loss: 7.6832\n",
      "Epoch: 614, Train Loss: 1.9253, Test Loss: 7.9461\n",
      "Epoch: 615, Train Loss: 1.7637, Test Loss: 6.1081\n",
      "Epoch: 616, Train Loss: 1.9851, Test Loss: 6.4710\n",
      "Epoch: 617, Train Loss: 1.8349, Test Loss: 8.7752\n",
      "Epoch: 618, Train Loss: 2.2825, Test Loss: 6.4316\n",
      "Epoch: 619, Train Loss: 1.9034, Test Loss: 5.9692\n",
      "Epoch: 620, Train Loss: 1.9572, Test Loss: 7.9911\n",
      "Epoch: 621, Train Loss: 1.9369, Test Loss: 6.9493\n",
      "Epoch: 622, Train Loss: 1.9225, Test Loss: 5.8082\n",
      "Epoch: 623, Train Loss: 2.1660, Test Loss: 7.6491\n",
      "Epoch: 624, Train Loss: 1.9618, Test Loss: 7.7730\n",
      "Epoch: 625, Train Loss: 2.1877, Test Loss: 5.3302\n",
      "Epoch: 626, Train Loss: 2.0941, Test Loss: 6.6115\n",
      "Epoch: 627, Train Loss: 1.8353, Test Loss: 8.2233\n",
      "Epoch: 628, Train Loss: 1.9352, Test Loss: 6.6969\n",
      "Epoch: 629, Train Loss: 1.8350, Test Loss: 5.3971\n",
      "Epoch: 630, Train Loss: 2.1514, Test Loss: 7.6478\n",
      "Epoch: 631, Train Loss: 1.9636, Test Loss: 7.4076\n",
      "Epoch: 632, Train Loss: 2.1276, Test Loss: 5.8066\n",
      "Epoch: 633, Train Loss: 1.9048, Test Loss: 6.3780\n",
      "Epoch: 634, Train Loss: 1.9142, Test Loss: 7.3819\n",
      "Epoch: 635, Train Loss: 1.8775, Test Loss: 7.3703\n",
      "Epoch: 636, Train Loss: 1.8825, Test Loss: 5.8849\n",
      "Epoch: 637, Train Loss: 2.1128, Test Loss: 6.6247\n",
      "Epoch: 638, Train Loss: 1.8884, Test Loss: 8.8844\n",
      "Epoch: 639, Train Loss: 2.2707, Test Loss: 6.0949\n",
      "Epoch: 640, Train Loss: 1.7949, Test Loss: 5.4998\n",
      "Epoch: 641, Train Loss: 1.9922, Test Loss: 7.9883\n",
      "Epoch: 642, Train Loss: 1.9352, Test Loss: 7.5030\n",
      "Epoch: 643, Train Loss: 1.8835, Test Loss: 5.8228\n",
      "Epoch: 644, Train Loss: 1.8649, Test Loss: 6.4054\n",
      "Epoch: 645, Train Loss: 1.9563, Test Loss: 7.8510\n",
      "Epoch: 646, Train Loss: 1.9298, Test Loss: 6.3221\n",
      "Epoch: 647, Train Loss: 1.8416, Test Loss: 6.2581\n",
      "Epoch: 648, Train Loss: 1.8224, Test Loss: 7.9457\n",
      "Epoch: 649, Train Loss: 1.8691, Test Loss: 6.7690\n",
      "Epoch: 650, Train Loss: 1.7379, Test Loss: 5.7118\n",
      "Epoch: 651, Train Loss: 2.1214, Test Loss: 8.4668\n",
      "Epoch: 652, Train Loss: 2.1311, Test Loss: 6.7207\n",
      "Epoch: 653, Train Loss: 1.7778, Test Loss: 5.1773\n",
      "Epoch: 654, Train Loss: 2.3297, Test Loss: 8.8271\n",
      "Epoch: 655, Train Loss: 2.0497, Test Loss: 9.1181\n",
      "Epoch: 656, Train Loss: 2.1879, Test Loss: 5.2857\n",
      "Epoch: 657, Train Loss: 2.1725, Test Loss: 5.8890\n",
      "Epoch: 658, Train Loss: 1.8768, Test Loss: 8.6009\n",
      "Epoch: 659, Train Loss: 2.1424, Test Loss: 7.4020\n",
      "Epoch: 660, Train Loss: 1.9094, Test Loss: 5.2570\n",
      "Epoch: 661, Train Loss: 2.1673, Test Loss: 6.2968\n",
      "Epoch: 662, Train Loss: 1.6109, Test Loss: 8.9793\n",
      "Epoch: 663, Train Loss: 2.1925, Test Loss: 6.7525\n",
      "Epoch: 664, Train Loss: 1.9673, Test Loss: 5.3397\n",
      "Epoch: 665, Train Loss: 2.0232, Test Loss: 6.7777\n",
      "Epoch: 666, Train Loss: 1.9057, Test Loss: 7.5576\n",
      "Epoch: 667, Train Loss: 2.0524, Test Loss: 5.5752\n",
      "Epoch: 668, Train Loss: 1.8424, Test Loss: 4.9611\n",
      "Epoch: 669, Train Loss: 2.1822, Test Loss: 7.7944\n",
      "Epoch: 670, Train Loss: 1.8587, Test Loss: 8.2134\n",
      "Epoch: 671, Train Loss: 1.9223, Test Loss: 6.1000\n",
      "Epoch: 672, Train Loss: 1.7916, Test Loss: 6.0254\n",
      "Epoch: 673, Train Loss: 2.0971, Test Loss: 7.4893\n",
      "Epoch: 674, Train Loss: 1.8964, Test Loss: 7.9113\n",
      "Epoch: 675, Train Loss: 1.9670, Test Loss: 5.5140\n",
      "Epoch: 676, Train Loss: 1.8642, Test Loss: 5.8131\n",
      "Epoch: 677, Train Loss: 1.7766, Test Loss: 7.9137\n",
      "Epoch: 678, Train Loss: 1.9329, Test Loss: 7.0913\n",
      "Epoch: 679, Train Loss: 1.7749, Test Loss: 5.5305\n",
      "Epoch: 680, Train Loss: 1.9346, Test Loss: 5.9908\n",
      "Epoch: 681, Train Loss: 1.9257, Test Loss: 8.0513\n",
      "Epoch: 682, Train Loss: 1.8534, Test Loss: 6.9214\n",
      "Epoch: 683, Train Loss: 1.8699, Test Loss: 5.2360\n",
      "Epoch: 684, Train Loss: 1.8659, Test Loss: 6.1314\n",
      "Epoch: 685, Train Loss: 1.8798, Test Loss: 8.8392\n",
      "Epoch: 686, Train Loss: 2.2264, Test Loss: 6.2848\n",
      "Epoch: 687, Train Loss: 1.7799, Test Loss: 5.4609\n",
      "Epoch: 688, Train Loss: 2.0032, Test Loss: 7.4735\n",
      "Epoch: 689, Train Loss: 1.9565, Test Loss: 7.4894\n",
      "Epoch: 690, Train Loss: 1.7800, Test Loss: 6.2099\n",
      "Epoch: 691, Train Loss: 1.6586, Test Loss: 6.2475\n",
      "Epoch: 692, Train Loss: 1.8712, Test Loss: 7.3779\n",
      "Epoch: 693, Train Loss: 1.8010, Test Loss: 7.8698\n",
      "Epoch: 694, Train Loss: 1.8719, Test Loss: 5.6451\n",
      "Epoch: 695, Train Loss: 1.9465, Test Loss: 6.4759\n",
      "Epoch: 696, Train Loss: 1.7474, Test Loss: 8.4251\n",
      "Epoch: 697, Train Loss: 2.1080, Test Loss: 6.6164\n",
      "Epoch: 698, Train Loss: 1.7772, Test Loss: 5.3712\n",
      "Epoch: 699, Train Loss: 1.9068, Test Loss: 7.1628\n",
      "Epoch: 700, Train Loss: 1.7075, Test Loss: 7.7211\n",
      "Epoch: 701, Train Loss: 1.9116, Test Loss: 6.0723\n",
      "Epoch: 702, Train Loss: 1.8755, Test Loss: 5.8253\n",
      "Epoch: 703, Train Loss: 1.9293, Test Loss: 7.6015\n",
      "Epoch: 704, Train Loss: 1.8581, Test Loss: 7.2757\n",
      "Epoch: 705, Train Loss: 1.7628, Test Loss: 5.6652\n",
      "Epoch: 706, Train Loss: 1.7580, Test Loss: 6.4313\n",
      "Epoch: 707, Train Loss: 1.8386, Test Loss: 7.9472\n",
      "Epoch: 708, Train Loss: 1.8652, Test Loss: 6.9733\n",
      "Epoch: 709, Train Loss: 1.7224, Test Loss: 5.9075\n",
      "Epoch: 710, Train Loss: 1.8291, Test Loss: 7.3725\n",
      "Epoch: 711, Train Loss: 1.7627, Test Loss: 6.8455\n",
      "Epoch: 712, Train Loss: 1.7620, Test Loss: 6.1751\n",
      "Epoch: 713, Train Loss: 1.7141, Test Loss: 7.0572\n",
      "Epoch: 714, Train Loss: 1.7524, Test Loss: 6.7590\n",
      "Epoch: 715, Train Loss: 1.6922, Test Loss: 6.2394\n",
      "Epoch: 716, Train Loss: 1.7314, Test Loss: 6.7593\n",
      "Epoch: 717, Train Loss: 1.7475, Test Loss: 7.2506\n",
      "Epoch: 718, Train Loss: 1.8704, Test Loss: 6.4269\n",
      "Epoch: 719, Train Loss: 1.7066, Test Loss: 5.9271\n",
      "Epoch: 720, Train Loss: 1.6978, Test Loss: 7.1910\n",
      "Epoch: 721, Train Loss: 1.8416, Test Loss: 7.0712\n",
      "Epoch: 722, Train Loss: 1.7475, Test Loss: 6.3865\n",
      "Epoch: 723, Train Loss: 1.7697, Test Loss: 6.9730\n",
      "Epoch: 724, Train Loss: 1.7382, Test Loss: 7.4670\n",
      "Epoch: 725, Train Loss: 1.6555, Test Loss: 7.0645\n",
      "Epoch: 726, Train Loss: 1.5975, Test Loss: 6.8340\n",
      "Epoch: 727, Train Loss: 1.7671, Test Loss: 7.4166\n",
      "Epoch: 728, Train Loss: 1.8274, Test Loss: 7.5862\n",
      "Epoch: 729, Train Loss: 1.6267, Test Loss: 6.8269\n",
      "Epoch: 730, Train Loss: 1.6452, Test Loss: 7.0719\n",
      "Epoch: 731, Train Loss: 1.7310, Test Loss: 7.3326\n",
      "Epoch: 732, Train Loss: 1.7204, Test Loss: 7.1503\n",
      "Epoch: 733, Train Loss: 1.6887, Test Loss: 6.2193\n",
      "Epoch: 734, Train Loss: 1.8539, Test Loss: 7.6287\n",
      "Epoch: 735, Train Loss: 1.6587, Test Loss: 7.0065\n",
      "Epoch: 736, Train Loss: 1.7899, Test Loss: 5.8458\n",
      "Epoch: 737, Train Loss: 1.7359, Test Loss: 7.5938\n",
      "Epoch: 738, Train Loss: 1.6323, Test Loss: 7.6696\n",
      "Epoch: 739, Train Loss: 1.7086, Test Loss: 6.1718\n",
      "Epoch: 740, Train Loss: 1.9502, Test Loss: 7.5701\n",
      "Epoch: 741, Train Loss: 1.7813, Test Loss: 7.5151\n",
      "Epoch: 742, Train Loss: 1.5759, Test Loss: 6.2131\n",
      "Epoch: 743, Train Loss: 1.8138, Test Loss: 6.5500\n",
      "Epoch: 744, Train Loss: 1.7057, Test Loss: 7.8641\n",
      "Epoch: 745, Train Loss: 1.8453, Test Loss: 5.6921\n",
      "Epoch: 746, Train Loss: 1.7095, Test Loss: 6.4644\n",
      "Epoch: 747, Train Loss: 1.6977, Test Loss: 7.8932\n",
      "Epoch: 748, Train Loss: 1.9120, Test Loss: 6.0245\n",
      "Epoch: 749, Train Loss: 1.6935, Test Loss: 6.1168\n",
      "Epoch: 750, Train Loss: 1.6578, Test Loss: 7.2189\n",
      "Epoch: 751, Train Loss: 1.8215, Test Loss: 6.2601\n",
      "Epoch: 752, Train Loss: 1.6603, Test Loss: 5.6208\n",
      "Epoch: 753, Train Loss: 1.6668, Test Loss: 7.7006\n",
      "Epoch: 754, Train Loss: 1.6709, Test Loss: 7.4332\n",
      "Epoch: 755, Train Loss: 1.6953, Test Loss: 5.8097\n",
      "Epoch: 756, Train Loss: 1.6841, Test Loss: 6.3638\n",
      "Epoch: 757, Train Loss: 1.7054, Test Loss: 6.5801\n",
      "Epoch: 758, Train Loss: 1.7199, Test Loss: 6.8179\n",
      "Epoch: 759, Train Loss: 1.6228, Test Loss: 6.8213\n",
      "Epoch: 760, Train Loss: 1.5954, Test Loss: 7.0978\n",
      "Epoch: 761, Train Loss: 1.7231, Test Loss: 5.7862\n",
      "Epoch: 762, Train Loss: 1.6620, Test Loss: 7.0964\n",
      "Epoch: 763, Train Loss: 1.7399, Test Loss: 7.1674\n",
      "Epoch: 764, Train Loss: 1.6482, Test Loss: 5.0819\n",
      "Epoch: 765, Train Loss: 1.9084, Test Loss: 6.4279\n",
      "Epoch: 766, Train Loss: 1.6235, Test Loss: 8.5390\n",
      "Epoch: 767, Train Loss: 2.0192, Test Loss: 5.9642\n",
      "Epoch: 768, Train Loss: 1.7510, Test Loss: 5.3836\n",
      "Epoch: 769, Train Loss: 1.8537, Test Loss: 7.2742\n",
      "Epoch: 770, Train Loss: 1.5983, Test Loss: 6.7422\n",
      "Epoch: 771, Train Loss: 1.7321, Test Loss: 5.3125\n",
      "Epoch: 772, Train Loss: 1.7918, Test Loss: 6.5969\n",
      "Epoch: 773, Train Loss: 1.6485, Test Loss: 6.8126\n",
      "Epoch: 774, Train Loss: 1.6613, Test Loss: 6.4461\n",
      "Epoch: 775, Train Loss: 1.6776, Test Loss: 5.4413\n",
      "Epoch: 776, Train Loss: 1.6433, Test Loss: 7.0822\n",
      "Epoch: 777, Train Loss: 1.7294, Test Loss: 7.1666\n",
      "Epoch: 778, Train Loss: 1.7575, Test Loss: 5.6198\n",
      "Epoch: 779, Train Loss: 1.6947, Test Loss: 5.3646\n",
      "Epoch: 780, Train Loss: 1.7983, Test Loss: 8.0181\n",
      "Epoch: 781, Train Loss: 1.8889, Test Loss: 6.7922\n",
      "Epoch: 782, Train Loss: 1.7356, Test Loss: 5.2775\n",
      "Epoch: 783, Train Loss: 1.7700, Test Loss: 6.6765\n",
      "Epoch: 784, Train Loss: 1.6884, Test Loss: 6.9723\n",
      "Epoch: 785, Train Loss: 1.6456, Test Loss: 5.3815\n",
      "Epoch: 786, Train Loss: 1.7408, Test Loss: 6.0577\n",
      "Epoch: 787, Train Loss: 1.5719, Test Loss: 8.8072\n",
      "Epoch: 788, Train Loss: 2.0890, Test Loss: 5.6800\n",
      "Epoch: 789, Train Loss: 1.5511, Test Loss: 4.7690\n",
      "Epoch: 790, Train Loss: 2.1499, Test Loss: 8.1385\n",
      "Epoch: 791, Train Loss: 1.9941, Test Loss: 6.5862\n",
      "Epoch: 792, Train Loss: 1.4724, Test Loss: 5.4395\n",
      "Epoch: 793, Train Loss: 1.5948, Test Loss: 5.3947\n",
      "Epoch: 794, Train Loss: 1.6852, Test Loss: 6.9977\n",
      "Epoch: 795, Train Loss: 1.6402, Test Loss: 6.3856\n",
      "Epoch: 796, Train Loss: 1.5820, Test Loss: 5.3394\n",
      "Epoch: 797, Train Loss: 1.7572, Test Loss: 5.6762\n",
      "Epoch: 798, Train Loss: 1.7527, Test Loss: 7.2957\n",
      "Epoch: 799, Train Loss: 1.7765, Test Loss: 6.5481\n",
      "Epoch: 800, Train Loss: 1.6998, Test Loss: 4.7534\n",
      "Epoch: 801, Train Loss: 1.8501, Test Loss: 5.3721\n",
      "Epoch: 802, Train Loss: 1.5085, Test Loss: 7.9791\n",
      "Epoch: 803, Train Loss: 1.9151, Test Loss: 5.7520\n",
      "Epoch: 804, Train Loss: 1.6285, Test Loss: 5.3397\n",
      "Epoch: 805, Train Loss: 1.8531, Test Loss: 7.6125\n",
      "Epoch: 806, Train Loss: 1.8421, Test Loss: 6.3427\n",
      "Epoch: 807, Train Loss: 1.5318, Test Loss: 5.0271\n",
      "Epoch: 808, Train Loss: 1.7274, Test Loss: 6.1762\n",
      "Epoch: 809, Train Loss: 1.5304, Test Loss: 7.0102\n",
      "Epoch: 810, Train Loss: 1.5277, Test Loss: 6.6073\n",
      "Epoch: 811, Train Loss: 1.7559, Test Loss: 5.2891\n",
      "Epoch: 812, Train Loss: 1.7315, Test Loss: 5.8669\n",
      "Epoch: 813, Train Loss: 1.6180, Test Loss: 6.7237\n",
      "Epoch: 814, Train Loss: 1.4273, Test Loss: 6.0843\n",
      "Epoch: 815, Train Loss: 1.5291, Test Loss: 6.1595\n",
      "Epoch: 816, Train Loss: 1.6426, Test Loss: 6.3538\n",
      "Epoch: 817, Train Loss: 1.6783, Test Loss: 5.8645\n",
      "Epoch: 818, Train Loss: 1.5762, Test Loss: 6.4631\n",
      "Epoch: 819, Train Loss: 1.7343, Test Loss: 6.2095\n",
      "Epoch: 820, Train Loss: 1.5316, Test Loss: 5.7731\n",
      "Epoch: 821, Train Loss: 1.5122, Test Loss: 6.6247\n",
      "Epoch: 822, Train Loss: 1.4879, Test Loss: 6.3203\n",
      "Epoch: 823, Train Loss: 1.4942, Test Loss: 6.3035\n",
      "Epoch: 824, Train Loss: 1.4230, Test Loss: 5.9666\n",
      "Epoch: 825, Train Loss: 1.4627, Test Loss: 6.3031\n",
      "Epoch: 826, Train Loss: 1.6156, Test Loss: 6.2576\n",
      "Epoch: 827, Train Loss: 1.6209, Test Loss: 5.4499\n",
      "Epoch: 828, Train Loss: 1.4720, Test Loss: 6.7904\n",
      "Epoch: 829, Train Loss: 1.5091, Test Loss: 7.9783\n",
      "Epoch: 830, Train Loss: 1.8623, Test Loss: 5.1654\n",
      "Epoch: 831, Train Loss: 1.7640, Test Loss: 5.3961\n",
      "Epoch: 832, Train Loss: 1.9135, Test Loss: 9.0453\n",
      "Epoch: 833, Train Loss: 1.9844, Test Loss: 6.3945\n",
      "Epoch: 834, Train Loss: 1.5519, Test Loss: 4.8538\n",
      "Epoch: 835, Train Loss: 1.7605, Test Loss: 7.3773\n",
      "Epoch: 836, Train Loss: 1.5690, Test Loss: 8.2541\n",
      "Epoch: 837, Train Loss: 1.9936, Test Loss: 4.6035\n",
      "Epoch: 838, Train Loss: 1.8554, Test Loss: 5.1649\n",
      "Epoch: 839, Train Loss: 1.6882, Test Loss: 8.7493\n",
      "Epoch: 840, Train Loss: 2.2888, Test Loss: 6.1005\n",
      "Epoch: 841, Train Loss: 1.5627, Test Loss: 4.4414\n",
      "Epoch: 842, Train Loss: 1.8139, Test Loss: 5.3963\n",
      "Epoch: 843, Train Loss: 1.5113, Test Loss: 7.3647\n",
      "Epoch: 844, Train Loss: 1.6689, Test Loss: 5.7386\n",
      "Epoch: 845, Train Loss: 1.4463, Test Loss: 4.6494\n",
      "Epoch: 846, Train Loss: 1.6851, Test Loss: 6.4859\n",
      "Epoch: 847, Train Loss: 1.4742, Test Loss: 7.4469\n",
      "Epoch: 848, Train Loss: 1.8647, Test Loss: 4.8892\n",
      "Epoch: 849, Train Loss: 1.6097, Test Loss: 5.0022\n",
      "Epoch: 850, Train Loss: 1.6613, Test Loss: 6.4087\n",
      "Epoch: 851, Train Loss: 1.5518, Test Loss: 6.2547\n",
      "Epoch: 852, Train Loss: 1.4326, Test Loss: 5.4359\n",
      "Epoch: 853, Train Loss: 1.5081, Test Loss: 5.9042\n",
      "Epoch: 854, Train Loss: 1.4773, Test Loss: 5.9222\n",
      "Epoch: 855, Train Loss: 1.6236, Test Loss: 5.4695\n",
      "Epoch: 856, Train Loss: 1.4921, Test Loss: 5.5022\n",
      "Epoch: 857, Train Loss: 1.6085, Test Loss: 6.5219\n",
      "Epoch: 858, Train Loss: 1.5543, Test Loss: 6.0837\n",
      "Epoch: 859, Train Loss: 1.5024, Test Loss: 5.4650\n",
      "Epoch: 860, Train Loss: 1.6041, Test Loss: 6.0702\n",
      "Epoch: 861, Train Loss: 1.3920, Test Loss: 6.6482\n",
      "Epoch: 862, Train Loss: 1.6267, Test Loss: 5.7119\n",
      "Epoch: 863, Train Loss: 1.5630, Test Loss: 5.1343\n",
      "Epoch: 864, Train Loss: 1.5118, Test Loss: 6.1353\n",
      "Epoch: 865, Train Loss: 1.4310, Test Loss: 7.1386\n",
      "Epoch: 866, Train Loss: 1.6133, Test Loss: 5.6788\n",
      "Epoch: 867, Train Loss: 1.5729, Test Loss: 6.3982\n",
      "Epoch: 868, Train Loss: 1.4627, Test Loss: 6.4503\n",
      "Epoch: 869, Train Loss: 1.3999, Test Loss: 6.1072\n",
      "Epoch: 870, Train Loss: 1.6575, Test Loss: 5.4335\n",
      "Epoch: 871, Train Loss: 1.5907, Test Loss: 7.0467\n",
      "Epoch: 872, Train Loss: 1.6947, Test Loss: 7.1084\n",
      "Epoch: 873, Train Loss: 1.5620, Test Loss: 4.8114\n",
      "Epoch: 874, Train Loss: 2.0363, Test Loss: 7.0605\n",
      "Epoch: 875, Train Loss: 1.6639, Test Loss: 7.9902\n",
      "Epoch: 876, Train Loss: 1.7361, Test Loss: 5.4914\n",
      "Epoch: 877, Train Loss: 1.6623, Test Loss: 5.1952\n",
      "Epoch: 878, Train Loss: 1.5742, Test Loss: 7.5496\n",
      "Epoch: 879, Train Loss: 1.6875, Test Loss: 6.6225\n",
      "Epoch: 880, Train Loss: 1.5853, Test Loss: 4.8623\n",
      "Epoch: 881, Train Loss: 1.6066, Test Loss: 5.4011\n",
      "Epoch: 882, Train Loss: 1.6357, Test Loss: 7.4415\n",
      "Epoch: 883, Train Loss: 1.6504, Test Loss: 6.9468\n",
      "Epoch: 884, Train Loss: 1.7171, Test Loss: 4.8683\n",
      "Epoch: 885, Train Loss: 1.6879, Test Loss: 5.1654\n",
      "Epoch: 886, Train Loss: 1.6284, Test Loss: 7.2847\n",
      "Epoch: 887, Train Loss: 1.6183, Test Loss: 6.9445\n",
      "Epoch: 888, Train Loss: 1.5077, Test Loss: 5.2034\n",
      "Epoch: 889, Train Loss: 1.6409, Test Loss: 5.2279\n",
      "Epoch: 890, Train Loss: 1.4994, Test Loss: 6.9096\n",
      "Epoch: 891, Train Loss: 1.4625, Test Loss: 6.6299\n",
      "Epoch: 892, Train Loss: 1.6442, Test Loss: 5.4122\n",
      "Epoch: 893, Train Loss: 1.6118, Test Loss: 6.2044\n",
      "Epoch: 894, Train Loss: 1.4495, Test Loss: 6.7387\n",
      "Epoch: 895, Train Loss: 1.5880, Test Loss: 5.4715\n",
      "Epoch: 896, Train Loss: 1.4213, Test Loss: 6.0090\n",
      "Epoch: 897, Train Loss: 1.2985, Test Loss: 6.5202\n",
      "Epoch: 898, Train Loss: 1.5574, Test Loss: 5.9359\n",
      "Epoch: 899, Train Loss: 1.4216, Test Loss: 5.4948\n",
      "Epoch: 900, Train Loss: 1.6293, Test Loss: 7.1837\n",
      "Epoch: 901, Train Loss: 1.7388, Test Loss: 6.6547\n",
      "Epoch: 902, Train Loss: 1.4568, Test Loss: 5.4197\n",
      "Epoch: 903, Train Loss: 1.5795, Test Loss: 6.2200\n",
      "Epoch: 904, Train Loss: 1.5888, Test Loss: 6.4226\n",
      "Epoch: 905, Train Loss: 1.4139, Test Loss: 6.0271\n",
      "Epoch: 906, Train Loss: 1.4966, Test Loss: 6.1788\n",
      "Epoch: 907, Train Loss: 1.5399, Test Loss: 5.8181\n",
      "Epoch: 908, Train Loss: 1.4866, Test Loss: 5.6117\n",
      "Epoch: 909, Train Loss: 1.4189, Test Loss: 6.3522\n",
      "Epoch: 910, Train Loss: 1.5231, Test Loss: 6.2885\n",
      "Epoch: 911, Train Loss: 1.4254, Test Loss: 5.4739\n",
      "Epoch: 912, Train Loss: 1.5059, Test Loss: 5.8455\n",
      "Epoch: 913, Train Loss: 1.4629, Test Loss: 8.4286\n",
      "Epoch: 914, Train Loss: 1.8647, Test Loss: 5.9196\n",
      "Epoch: 915, Train Loss: 1.4246, Test Loss: 4.4370\n",
      "Epoch: 916, Train Loss: 1.6640, Test Loss: 6.6312\n",
      "Epoch: 917, Train Loss: 1.4621, Test Loss: 7.6280\n",
      "Epoch: 918, Train Loss: 1.5339, Test Loss: 5.9078\n",
      "Epoch: 919, Train Loss: 1.3518, Test Loss: 5.0044\n",
      "Epoch: 920, Train Loss: 1.6368, Test Loss: 6.2925\n",
      "Epoch: 921, Train Loss: 1.4800, Test Loss: 7.2232\n",
      "Epoch: 922, Train Loss: 1.6565, Test Loss: 4.9693\n",
      "Epoch: 923, Train Loss: 1.5474, Test Loss: 5.7643\n",
      "Epoch: 924, Train Loss: 1.4515, Test Loss: 7.2204\n",
      "Epoch: 925, Train Loss: 1.6198, Test Loss: 5.3867\n",
      "Epoch: 926, Train Loss: 1.4256, Test Loss: 4.4191\n",
      "Epoch: 927, Train Loss: 1.8268, Test Loss: 7.1114\n",
      "Epoch: 928, Train Loss: 1.5762, Test Loss: 7.3870\n",
      "Epoch: 929, Train Loss: 1.6458, Test Loss: 4.6943\n",
      "Epoch: 930, Train Loss: 1.5294, Test Loss: 4.6764\n",
      "Epoch: 931, Train Loss: 1.6324, Test Loss: 7.0560\n",
      "Epoch: 932, Train Loss: 1.6326, Test Loss: 6.7177\n",
      "Epoch: 933, Train Loss: 1.4116, Test Loss: 4.9891\n",
      "Epoch: 934, Train Loss: 1.4332, Test Loss: 4.7228\n",
      "Epoch: 935, Train Loss: 1.5511, Test Loss: 6.4285\n",
      "Epoch: 936, Train Loss: 1.4214, Test Loss: 7.5150\n",
      "Epoch: 937, Train Loss: 1.7817, Test Loss: 4.9935\n",
      "Epoch: 938, Train Loss: 1.5369, Test Loss: 4.6763\n",
      "Epoch: 939, Train Loss: 1.5441, Test Loss: 6.3322\n",
      "Epoch: 940, Train Loss: 1.3243, Test Loss: 7.0227\n",
      "Epoch: 941, Train Loss: 1.6712, Test Loss: 5.0870\n",
      "Epoch: 942, Train Loss: 1.3376, Test Loss: 5.2789\n",
      "Epoch: 943, Train Loss: 1.3829, Test Loss: 5.9172\n",
      "Epoch: 944, Train Loss: 1.3060, Test Loss: 6.2556\n",
      "Epoch: 945, Train Loss: 1.4919, Test Loss: 5.0872\n",
      "Epoch: 946, Train Loss: 1.5013, Test Loss: 5.3918\n",
      "Epoch: 947, Train Loss: 1.4177, Test Loss: 6.3957\n",
      "Epoch: 948, Train Loss: 1.5008, Test Loss: 5.6975\n",
      "Epoch: 949, Train Loss: 1.3379, Test Loss: 5.4473\n",
      "Epoch: 950, Train Loss: 1.5080, Test Loss: 6.8363\n",
      "Epoch: 951, Train Loss: 1.4613, Test Loss: 5.9764\n",
      "Epoch: 952, Train Loss: 1.3542, Test Loss: 4.8295\n",
      "Epoch: 953, Train Loss: 1.3976, Test Loss: 5.8230\n",
      "Epoch: 954, Train Loss: 1.4331, Test Loss: 7.1949\n",
      "Epoch: 955, Train Loss: 1.4820, Test Loss: 5.8947\n",
      "Epoch: 956, Train Loss: 1.2460, Test Loss: 5.2778\n",
      "Epoch: 957, Train Loss: 1.4378, Test Loss: 5.5058\n",
      "Epoch: 958, Train Loss: 1.3365, Test Loss: 6.2342\n",
      "Epoch: 959, Train Loss: 1.3404, Test Loss: 6.6755\n",
      "Epoch: 960, Train Loss: 1.3926, Test Loss: 5.6919\n",
      "Epoch: 961, Train Loss: 1.2507, Test Loss: 5.2497\n",
      "Epoch: 962, Train Loss: 1.3791, Test Loss: 6.5445\n",
      "Epoch: 963, Train Loss: 1.3914, Test Loss: 5.7850\n",
      "Epoch: 964, Train Loss: 1.2893, Test Loss: 5.9401\n",
      "Epoch: 965, Train Loss: 1.3278, Test Loss: 5.9123\n",
      "Epoch: 966, Train Loss: 1.4102, Test Loss: 5.9303\n",
      "Epoch: 967, Train Loss: 1.3629, Test Loss: 5.2115\n",
      "Epoch: 968, Train Loss: 1.4603, Test Loss: 5.7158\n",
      "Epoch: 969, Train Loss: 1.3633, Test Loss: 6.7808\n",
      "Epoch: 970, Train Loss: 1.5487, Test Loss: 5.1213\n",
      "Epoch: 971, Train Loss: 1.4473, Test Loss: 4.9661\n",
      "Epoch: 972, Train Loss: 1.4066, Test Loss: 6.6807\n",
      "Epoch: 973, Train Loss: 1.5770, Test Loss: 6.5009\n",
      "Epoch: 974, Train Loss: 1.4113, Test Loss: 5.1335\n",
      "Epoch: 975, Train Loss: 1.3527, Test Loss: 5.5302\n",
      "Epoch: 976, Train Loss: 1.2742, Test Loss: 6.3478\n",
      "Epoch: 977, Train Loss: 1.3845, Test Loss: 5.6230\n",
      "Epoch: 978, Train Loss: 1.3221, Test Loss: 4.6616\n",
      "Epoch: 979, Train Loss: 1.4347, Test Loss: 5.7974\n",
      "Epoch: 980, Train Loss: 1.2325, Test Loss: 6.7346\n",
      "Epoch: 981, Train Loss: 1.3594, Test Loss: 5.3754\n",
      "Epoch: 982, Train Loss: 1.3630, Test Loss: 4.6837\n",
      "Epoch: 983, Train Loss: 1.5145, Test Loss: 5.7202\n",
      "Epoch: 984, Train Loss: 1.4167, Test Loss: 6.0106\n",
      "Epoch: 985, Train Loss: 1.3952, Test Loss: 5.4576\n",
      "Epoch: 986, Train Loss: 1.4354, Test Loss: 5.5513\n",
      "Epoch: 987, Train Loss: 1.3182, Test Loss: 6.1521\n",
      "Epoch: 988, Train Loss: 1.2978, Test Loss: 5.3795\n",
      "Epoch: 989, Train Loss: 1.2434, Test Loss: 5.3017\n",
      "Epoch: 990, Train Loss: 1.3547, Test Loss: 7.0549\n",
      "Epoch: 991, Train Loss: 1.3467, Test Loss: 5.6730\n",
      "Epoch: 992, Train Loss: 1.4382, Test Loss: 4.9235\n",
      "Epoch: 993, Train Loss: 1.3081, Test Loss: 5.7034\n",
      "Epoch: 994, Train Loss: 1.2488, Test Loss: 6.7585\n",
      "Epoch: 995, Train Loss: 1.2676, Test Loss: 6.0313\n",
      "Epoch: 996, Train Loss: 1.2771, Test Loss: 5.4708\n",
      "Epoch: 997, Train Loss: 1.4901, Test Loss: 5.8431\n",
      "Epoch: 998, Train Loss: 1.3594, Test Loss: 5.4930\n",
      "Epoch: 999, Train Loss: 1.3226, Test Loss: 5.1268\n",
      "Epoch: 1000, Train Loss: 1.3338, Test Loss: 6.4389\n",
      "Epoch: 1001, Train Loss: 1.2059, Test Loss: 6.0454\n",
      "Epoch: 1002, Train Loss: 1.2493, Test Loss: 4.4956\n",
      "Epoch: 1003, Train Loss: 1.4048, Test Loss: 5.3190\n",
      "Epoch: 1004, Train Loss: 1.2013, Test Loss: 6.9948\n",
      "Epoch: 1005, Train Loss: 1.5177, Test Loss: 5.4991\n",
      "Epoch: 1006, Train Loss: 1.2326, Test Loss: 4.7562\n",
      "Epoch: 1007, Train Loss: 1.5027, Test Loss: 6.8400\n",
      "Epoch: 1008, Train Loss: 1.5350, Test Loss: 5.8872\n",
      "Epoch: 1009, Train Loss: 1.2632, Test Loss: 4.7034\n",
      "Epoch: 1010, Train Loss: 1.4221, Test Loss: 5.5154\n",
      "Epoch: 1011, Train Loss: 1.2059, Test Loss: 6.5088\n",
      "Epoch: 1012, Train Loss: 1.2950, Test Loss: 6.1114\n",
      "Epoch: 1013, Train Loss: 1.4455, Test Loss: 4.4551\n",
      "Epoch: 1014, Train Loss: 1.5864, Test Loss: 5.6569\n",
      "Epoch: 1015, Train Loss: 1.4451, Test Loss: 6.3319\n",
      "Epoch: 1016, Train Loss: 1.3940, Test Loss: 5.2393\n",
      "Epoch: 1017, Train Loss: 1.3081, Test Loss: 5.3807\n",
      "Epoch: 1018, Train Loss: 1.2334, Test Loss: 5.6162\n",
      "Epoch: 1019, Train Loss: 1.2940, Test Loss: 4.9878\n",
      "Epoch: 1020, Train Loss: 1.3013, Test Loss: 4.7266\n",
      "Epoch: 1021, Train Loss: 1.2971, Test Loss: 6.4877\n",
      "Epoch: 1022, Train Loss: 1.3036, Test Loss: 5.6578\n",
      "Epoch: 1023, Train Loss: 1.2075, Test Loss: 4.7105\n",
      "Epoch: 1024, Train Loss: 1.3601, Test Loss: 5.9613\n",
      "Epoch: 1025, Train Loss: 1.3831, Test Loss: 5.7781\n",
      "Epoch: 1026, Train Loss: 1.2965, Test Loss: 4.6328\n",
      "Epoch: 1027, Train Loss: 1.3140, Test Loss: 5.4474\n",
      "Epoch: 1028, Train Loss: 1.2464, Test Loss: 6.6795\n",
      "Epoch: 1029, Train Loss: 1.4340, Test Loss: 5.6647\n",
      "Epoch: 1030, Train Loss: 1.1809, Test Loss: 4.8974\n",
      "Epoch: 1031, Train Loss: 1.5284, Test Loss: 5.7893\n",
      "Epoch: 1032, Train Loss: 1.2138, Test Loss: 6.5907\n",
      "Epoch: 1033, Train Loss: 1.3437, Test Loss: 5.2264\n",
      "Epoch: 1034, Train Loss: 1.3216, Test Loss: 4.8033\n",
      "Epoch: 1035, Train Loss: 1.3457, Test Loss: 6.2842\n",
      "Epoch: 1036, Train Loss: 1.3148, Test Loss: 6.1098\n",
      "Epoch: 1037, Train Loss: 1.2859, Test Loss: 5.6021\n",
      "Epoch: 1038, Train Loss: 1.2016, Test Loss: 4.8020\n",
      "Epoch: 1039, Train Loss: 1.3200, Test Loss: 5.5408\n",
      "Epoch: 1040, Train Loss: 1.3176, Test Loss: 7.7580\n",
      "Epoch: 1041, Train Loss: 1.7331, Test Loss: 5.0470\n",
      "Epoch: 1042, Train Loss: 1.2461, Test Loss: 4.5840\n",
      "Epoch: 1043, Train Loss: 1.4975, Test Loss: 6.8848\n",
      "Epoch: 1044, Train Loss: 1.2983, Test Loss: 6.6582\n",
      "Epoch: 1045, Train Loss: 1.3929, Test Loss: 4.4912\n",
      "Epoch: 1046, Train Loss: 1.3520, Test Loss: 5.0211\n",
      "Epoch: 1047, Train Loss: 1.2515, Test Loss: 6.8519\n",
      "Epoch: 1048, Train Loss: 1.4346, Test Loss: 5.6656\n",
      "Epoch: 1049, Train Loss: 1.2379, Test Loss: 4.4869\n",
      "Epoch: 1050, Train Loss: 1.3782, Test Loss: 5.1896\n",
      "Epoch: 1051, Train Loss: 1.3589, Test Loss: 6.0293\n",
      "Epoch: 1052, Train Loss: 1.3235, Test Loss: 5.2823\n",
      "Epoch: 1053, Train Loss: 1.1621, Test Loss: 4.5103\n",
      "Epoch: 1054, Train Loss: 1.3755, Test Loss: 5.6167\n",
      "Epoch: 1055, Train Loss: 1.4100, Test Loss: 6.7087\n",
      "Epoch: 1056, Train Loss: 1.3432, Test Loss: 5.1554\n",
      "Epoch: 1057, Train Loss: 1.1780, Test Loss: 4.3439\n",
      "Epoch: 1058, Train Loss: 1.4315, Test Loss: 5.6604\n",
      "Epoch: 1059, Train Loss: 1.2623, Test Loss: 6.1565\n",
      "Epoch: 1060, Train Loss: 1.3940, Test Loss: 4.8242\n",
      "Epoch: 1061, Train Loss: 1.2244, Test Loss: 4.4411\n",
      "Epoch: 1062, Train Loss: 1.4062, Test Loss: 6.6889\n",
      "Epoch: 1063, Train Loss: 1.4979, Test Loss: 6.1477\n",
      "Epoch: 1064, Train Loss: 1.2973, Test Loss: 4.5324\n",
      "Epoch: 1065, Train Loss: 1.4050, Test Loss: 5.0561\n",
      "Epoch: 1066, Train Loss: 1.3816, Test Loss: 7.7872\n",
      "Epoch: 1067, Train Loss: 1.7696, Test Loss: 5.7436\n",
      "Epoch: 1068, Train Loss: 1.2925, Test Loss: 4.0044\n",
      "Epoch: 1069, Train Loss: 1.7530, Test Loss: 5.6417\n",
      "Epoch: 1070, Train Loss: 1.1950, Test Loss: 7.7434\n",
      "Epoch: 1071, Train Loss: 1.8287, Test Loss: 4.8452\n",
      "Epoch: 1072, Train Loss: 1.3020, Test Loss: 4.0576\n",
      "Epoch: 1073, Train Loss: 1.5850, Test Loss: 5.8982\n",
      "Epoch: 1074, Train Loss: 1.3976, Test Loss: 6.0407\n",
      "Epoch: 1075, Train Loss: 1.2761, Test Loss: 4.6235\n",
      "Epoch: 1076, Train Loss: 1.3333, Test Loss: 4.8141\n",
      "Epoch: 1077, Train Loss: 1.2875, Test Loss: 5.5688\n",
      "Epoch: 1078, Train Loss: 1.2677, Test Loss: 5.3405\n",
      "Epoch: 1079, Train Loss: 1.2314, Test Loss: 4.8922\n",
      "Epoch: 1080, Train Loss: 1.2263, Test Loss: 4.9321\n",
      "Epoch: 1081, Train Loss: 1.1616, Test Loss: 5.8088\n",
      "Epoch: 1082, Train Loss: 1.3370, Test Loss: 5.2371\n",
      "Epoch: 1083, Train Loss: 1.3039, Test Loss: 4.5437\n",
      "Epoch: 1084, Train Loss: 1.2788, Test Loss: 5.3331\n",
      "Epoch: 1085, Train Loss: 1.2167, Test Loss: 5.6236\n",
      "Epoch: 1086, Train Loss: 1.1893, Test Loss: 5.4250\n",
      "Epoch: 1087, Train Loss: 1.2035, Test Loss: 5.0114\n",
      "Epoch: 1088, Train Loss: 1.2219, Test Loss: 4.9325\n",
      "Epoch: 1089, Train Loss: 1.2183, Test Loss: 5.7233\n",
      "Epoch: 1090, Train Loss: 1.3266, Test Loss: 5.3598\n",
      "Epoch: 1091, Train Loss: 1.1821, Test Loss: 4.7729\n",
      "Epoch: 1092, Train Loss: 1.2223, Test Loss: 4.9598\n",
      "Epoch: 1093, Train Loss: 1.3052, Test Loss: 6.8887\n",
      "Epoch: 1094, Train Loss: 1.4984, Test Loss: 5.4071\n",
      "Epoch: 1095, Train Loss: 1.1403, Test Loss: 4.3493\n",
      "Epoch: 1096, Train Loss: 1.4431, Test Loss: 5.1566\n",
      "Epoch: 1097, Train Loss: 1.1028, Test Loss: 6.0694\n",
      "Epoch: 1098, Train Loss: 1.3222, Test Loss: 4.9034\n",
      "Epoch: 1099, Train Loss: 1.2048, Test Loss: 4.7653\n",
      "Epoch: 1100, Train Loss: 1.1739, Test Loss: 5.0989\n",
      "Epoch: 1101, Train Loss: 1.2540, Test Loss: 5.7229\n",
      "Epoch: 1102, Train Loss: 1.2984, Test Loss: 5.8069\n",
      "Epoch: 1103, Train Loss: 1.1300, Test Loss: 4.3444\n",
      "Epoch: 1104, Train Loss: 1.3305, Test Loss: 4.7550\n",
      "Epoch: 1105, Train Loss: 1.2273, Test Loss: 6.0149\n",
      "Epoch: 1106, Train Loss: 1.2947, Test Loss: 5.2566\n",
      "Epoch: 1107, Train Loss: 1.1515, Test Loss: 4.5486\n",
      "Epoch: 1108, Train Loss: 1.2157, Test Loss: 4.9623\n",
      "Epoch: 1109, Train Loss: 1.2154, Test Loss: 6.3434\n",
      "Epoch: 1110, Train Loss: 1.2123, Test Loss: 5.1622\n",
      "Epoch: 1111, Train Loss: 1.1320, Test Loss: 4.5982\n",
      "Epoch: 1112, Train Loss: 1.2434, Test Loss: 5.6073\n",
      "Epoch: 1113, Train Loss: 1.2128, Test Loss: 6.4333\n",
      "Epoch: 1114, Train Loss: 1.3796, Test Loss: 4.7277\n",
      "Epoch: 1115, Train Loss: 1.1946, Test Loss: 4.2087\n",
      "Epoch: 1116, Train Loss: 1.4324, Test Loss: 6.4634\n",
      "Epoch: 1117, Train Loss: 1.3312, Test Loss: 6.0916\n",
      "Epoch: 1118, Train Loss: 1.1879, Test Loss: 4.3861\n",
      "Epoch: 1119, Train Loss: 1.1406, Test Loss: 4.7455\n",
      "Epoch: 1120, Train Loss: 1.2147, Test Loss: 6.4395\n",
      "Epoch: 1121, Train Loss: 1.3198, Test Loss: 5.5868\n",
      "Epoch: 1122, Train Loss: 1.2131, Test Loss: 4.2472\n",
      "Epoch: 1123, Train Loss: 1.2605, Test Loss: 4.4483\n",
      "Epoch: 1124, Train Loss: 1.2294, Test Loss: 6.4280\n",
      "Epoch: 1125, Train Loss: 1.3122, Test Loss: 6.1863\n",
      "Epoch: 1126, Train Loss: 1.5471, Test Loss: 3.7987\n",
      "Epoch: 1127, Train Loss: 1.4697, Test Loss: 3.9023\n",
      "Epoch: 1128, Train Loss: 1.3197, Test Loss: 6.5508\n",
      "Epoch: 1129, Train Loss: 1.4506, Test Loss: 6.5430\n",
      "Epoch: 1130, Train Loss: 1.3341, Test Loss: 4.0770\n",
      "Epoch: 1131, Train Loss: 1.2377, Test Loss: 3.6264\n",
      "Epoch: 1132, Train Loss: 1.7709, Test Loss: 5.9671\n",
      "Epoch: 1133, Train Loss: 1.1977, Test Loss: 7.5494\n",
      "Epoch: 1134, Train Loss: 1.8426, Test Loss: 4.1307\n",
      "Epoch: 1135, Train Loss: 1.4062, Test Loss: 3.8725\n",
      "Epoch: 1136, Train Loss: 1.3807, Test Loss: 5.4880\n",
      "Epoch: 1137, Train Loss: 1.1467, Test Loss: 6.3103\n",
      "Epoch: 1138, Train Loss: 1.5128, Test Loss: 4.5013\n",
      "Epoch: 1139, Train Loss: 1.1997, Test Loss: 3.9676\n",
      "Epoch: 1140, Train Loss: 1.4178, Test Loss: 4.6202\n",
      "Epoch: 1141, Train Loss: 1.2842, Test Loss: 5.7588\n",
      "Epoch: 1142, Train Loss: 1.3995, Test Loss: 5.3463\n",
      "Epoch: 1143, Train Loss: 1.2104, Test Loss: 4.5362\n",
      "Epoch: 1144, Train Loss: 1.2727, Test Loss: 4.5436\n",
      "Epoch: 1145, Train Loss: 1.1577, Test Loss: 5.7442\n",
      "Epoch: 1146, Train Loss: 1.1722, Test Loss: 5.5864\n",
      "Epoch: 1147, Train Loss: 1.1834, Test Loss: 4.4479\n",
      "Epoch: 1148, Train Loss: 1.0516, Test Loss: 4.1407\n",
      "Epoch: 1149, Train Loss: 1.2272, Test Loss: 5.0213\n",
      "Epoch: 1150, Train Loss: 1.1359, Test Loss: 5.7877\n",
      "Epoch: 1151, Train Loss: 1.2357, Test Loss: 4.8845\n",
      "Epoch: 1152, Train Loss: 1.1910, Test Loss: 4.5448\n",
      "Epoch: 1153, Train Loss: 1.1412, Test Loss: 4.8699\n",
      "Epoch: 1154, Train Loss: 1.1235, Test Loss: 5.1574\n",
      "Epoch: 1155, Train Loss: 1.1742, Test Loss: 5.0373\n",
      "Epoch: 1156, Train Loss: 1.1670, Test Loss: 4.6893\n",
      "Epoch: 1157, Train Loss: 1.1914, Test Loss: 4.8440\n",
      "Epoch: 1158, Train Loss: 1.1179, Test Loss: 5.2358\n",
      "Epoch: 1159, Train Loss: 1.1266, Test Loss: 4.7834\n",
      "Epoch: 1160, Train Loss: 1.1121, Test Loss: 4.7922\n",
      "Epoch: 1161, Train Loss: 1.0870, Test Loss: 4.8802\n",
      "Epoch: 1162, Train Loss: 1.0794, Test Loss: 4.9844\n",
      "Epoch: 1163, Train Loss: 1.1134, Test Loss: 5.0082\n",
      "Epoch: 1164, Train Loss: 1.0599, Test Loss: 5.1331\n",
      "Epoch: 1165, Train Loss: 1.0673, Test Loss: 5.3383\n",
      "Epoch: 1166, Train Loss: 1.0948, Test Loss: 5.0148\n",
      "Epoch: 1167, Train Loss: 1.0467, Test Loss: 5.0593\n",
      "Epoch: 1168, Train Loss: 1.0713, Test Loss: 4.7000\n",
      "Epoch: 1169, Train Loss: 1.1124, Test Loss: 5.1766\n",
      "Epoch: 1170, Train Loss: 1.1936, Test Loss: 5.6227\n",
      "Epoch: 1171, Train Loss: 1.2383, Test Loss: 4.6711\n",
      "Epoch: 1172, Train Loss: 1.1957, Test Loss: 5.0722\n",
      "Epoch: 1173, Train Loss: 1.0296, Test Loss: 6.1651\n",
      "Epoch: 1174, Train Loss: 1.3771, Test Loss: 4.9296\n",
      "Epoch: 1175, Train Loss: 1.1256, Test Loss: 3.9638\n",
      "Epoch: 1176, Train Loss: 1.3497, Test Loss: 5.0399\n",
      "Epoch: 1177, Train Loss: 1.0565, Test Loss: 6.8589\n",
      "Epoch: 1178, Train Loss: 1.3391, Test Loss: 5.1864\n",
      "Epoch: 1179, Train Loss: 1.1479, Test Loss: 4.1300\n",
      "Epoch: 1180, Train Loss: 1.2903, Test Loss: 4.8390\n",
      "Epoch: 1181, Train Loss: 1.1165, Test Loss: 6.3507\n",
      "Epoch: 1182, Train Loss: 1.2121, Test Loss: 5.0959\n",
      "Epoch: 1183, Train Loss: 1.1314, Test Loss: 4.0051\n",
      "Epoch: 1184, Train Loss: 1.3172, Test Loss: 4.9473\n",
      "Epoch: 1185, Train Loss: 0.9981, Test Loss: 5.9156\n",
      "Epoch: 1186, Train Loss: 1.1822, Test Loss: 5.1716\n",
      "Epoch: 1187, Train Loss: 1.0721, Test Loss: 4.1855\n",
      "Epoch: 1188, Train Loss: 1.2079, Test Loss: 4.4665\n",
      "Epoch: 1189, Train Loss: 1.1343, Test Loss: 6.0851\n",
      "Epoch: 1190, Train Loss: 1.1895, Test Loss: 5.1265\n",
      "Epoch: 1191, Train Loss: 1.1441, Test Loss: 4.1014\n",
      "Epoch: 1192, Train Loss: 1.3313, Test Loss: 4.9824\n",
      "Epoch: 1193, Train Loss: 1.0658, Test Loss: 6.2832\n",
      "Epoch: 1194, Train Loss: 1.2855, Test Loss: 4.8239\n",
      "Epoch: 1195, Train Loss: 1.1161, Test Loss: 4.2024\n",
      "Epoch: 1196, Train Loss: 1.2151, Test Loss: 5.0380\n",
      "Epoch: 1197, Train Loss: 1.0471, Test Loss: 5.7005\n",
      "Epoch: 1198, Train Loss: 1.0342, Test Loss: 4.8107\n",
      "Epoch: 1199, Train Loss: 1.1306, Test Loss: 4.5122\n",
      "Epoch: 1200, Train Loss: 1.0883, Test Loss: 5.1231\n",
      "Epoch: 1201, Train Loss: 0.9335, Test Loss: 5.3169\n",
      "Epoch: 1202, Train Loss: 1.1818, Test Loss: 4.1093\n",
      "Epoch: 1203, Train Loss: 1.1666, Test Loss: 4.5475\n",
      "Epoch: 1204, Train Loss: 1.1345, Test Loss: 5.7068\n",
      "Epoch: 1205, Train Loss: 1.1304, Test Loss: 5.1676\n",
      "Epoch: 1206, Train Loss: 1.0384, Test Loss: 4.1751\n",
      "Epoch: 1207, Train Loss: 1.1093, Test Loss: 4.7243\n",
      "Epoch: 1208, Train Loss: 1.0916, Test Loss: 6.4178\n",
      "Epoch: 1209, Train Loss: 1.3184, Test Loss: 4.8753\n",
      "Epoch: 1210, Train Loss: 1.0901, Test Loss: 3.9210\n",
      "Epoch: 1211, Train Loss: 1.3194, Test Loss: 5.0951\n",
      "Epoch: 1212, Train Loss: 1.0990, Test Loss: 5.7268\n",
      "Epoch: 1213, Train Loss: 1.1234, Test Loss: 5.0706\n",
      "Epoch: 1214, Train Loss: 1.0500, Test Loss: 3.9990\n",
      "Epoch: 1215, Train Loss: 1.3270, Test Loss: 4.5433\n",
      "Epoch: 1216, Train Loss: 1.0864, Test Loss: 5.2050\n",
      "Epoch: 1217, Train Loss: 1.1808, Test Loss: 5.1205\n",
      "Epoch: 1218, Train Loss: 1.0936, Test Loss: 4.3575\n",
      "Epoch: 1219, Train Loss: 1.0745, Test Loss: 4.6330\n",
      "Epoch: 1220, Train Loss: 1.0670, Test Loss: 4.9024\n",
      "Epoch: 1221, Train Loss: 0.9721, Test Loss: 4.9706\n",
      "Epoch: 1222, Train Loss: 1.0352, Test Loss: 4.5235\n",
      "Epoch: 1223, Train Loss: 1.1063, Test Loss: 4.9908\n",
      "Epoch: 1224, Train Loss: 1.0854, Test Loss: 4.9135\n",
      "Epoch: 1225, Train Loss: 1.1108, Test Loss: 4.3409\n",
      "Epoch: 1226, Train Loss: 1.0630, Test Loss: 4.7821\n",
      "Epoch: 1227, Train Loss: 0.9912, Test Loss: 5.6169\n",
      "Epoch: 1228, Train Loss: 1.0507, Test Loss: 5.4681\n",
      "Epoch: 1229, Train Loss: 1.0914, Test Loss: 4.2580\n",
      "Epoch: 1230, Train Loss: 1.1791, Test Loss: 4.9160\n",
      "Epoch: 1231, Train Loss: 1.0804, Test Loss: 5.4443\n",
      "Epoch: 1232, Train Loss: 1.0643, Test Loss: 4.9271\n",
      "Epoch: 1233, Train Loss: 1.0180, Test Loss: 4.4023\n",
      "Epoch: 1234, Train Loss: 1.0200, Test Loss: 4.6481\n",
      "Epoch: 1235, Train Loss: 1.1708, Test Loss: 6.0410\n",
      "Epoch: 1236, Train Loss: 1.1065, Test Loss: 6.1518\n",
      "Epoch: 1237, Train Loss: 1.1341, Test Loss: 4.3328\n",
      "Epoch: 1238, Train Loss: 1.1052, Test Loss: 3.9581\n",
      "Epoch: 1239, Train Loss: 1.1990, Test Loss: 5.3823\n",
      "Epoch: 1240, Train Loss: 1.0681, Test Loss: 6.1492\n",
      "Epoch: 1241, Train Loss: 1.3006, Test Loss: 4.4927\n",
      "Epoch: 1242, Train Loss: 1.0697, Test Loss: 4.2465\n",
      "Epoch: 1243, Train Loss: 1.2206, Test Loss: 5.8279\n",
      "Epoch: 1244, Train Loss: 1.1272, Test Loss: 5.8019\n",
      "Epoch: 1245, Train Loss: 1.0481, Test Loss: 4.7640\n",
      "Epoch: 1246, Train Loss: 1.0926, Test Loss: 4.2671\n",
      "Epoch: 1247, Train Loss: 1.1682, Test Loss: 4.7284\n",
      "Epoch: 1248, Train Loss: 0.9411, Test Loss: 5.5040\n",
      "Epoch: 1249, Train Loss: 1.2275, Test Loss: 4.4863\n",
      "Epoch: 1250, Train Loss: 1.2347, Test Loss: 4.7142\n",
      "Epoch: 1251, Train Loss: 1.0465, Test Loss: 5.1153\n",
      "Epoch: 1252, Train Loss: 1.0672, Test Loss: 5.2585\n",
      "Epoch: 1253, Train Loss: 1.0806, Test Loss: 4.6047\n",
      "Epoch: 1254, Train Loss: 1.2349, Test Loss: 4.8456\n",
      "Epoch: 1255, Train Loss: 0.9996, Test Loss: 5.1504\n",
      "Epoch: 1256, Train Loss: 1.1586, Test Loss: 4.8366\n",
      "Epoch: 1257, Train Loss: 1.0843, Test Loss: 4.3907\n",
      "Epoch: 1258, Train Loss: 1.2792, Test Loss: 5.7221\n",
      "Epoch: 1259, Train Loss: 1.1343, Test Loss: 5.1779\n",
      "Epoch: 1260, Train Loss: 0.9845, Test Loss: 4.4619\n",
      "Epoch: 1261, Train Loss: 1.1272, Test Loss: 4.4378\n",
      "Epoch: 1262, Train Loss: 1.0939, Test Loss: 5.1692\n",
      "Epoch: 1263, Train Loss: 1.0234, Test Loss: 6.0421\n",
      "Epoch: 1264, Train Loss: 1.1213, Test Loss: 4.8555\n",
      "Epoch: 1265, Train Loss: 1.0270, Test Loss: 4.5128\n",
      "Epoch: 1266, Train Loss: 0.9642, Test Loss: 5.1045\n",
      "Epoch: 1267, Train Loss: 1.1175, Test Loss: 4.9032\n",
      "Epoch: 1268, Train Loss: 1.1092, Test Loss: 4.4192\n",
      "Epoch: 1269, Train Loss: 0.9872, Test Loss: 4.4575\n",
      "Epoch: 1270, Train Loss: 1.0824, Test Loss: 5.1476\n",
      "Epoch: 1271, Train Loss: 1.0698, Test Loss: 4.8597\n",
      "Epoch: 1272, Train Loss: 1.0449, Test Loss: 4.4035\n",
      "Epoch: 1273, Train Loss: 0.9712, Test Loss: 4.6655\n",
      "Epoch: 1274, Train Loss: 1.0707, Test Loss: 5.6851\n",
      "Epoch: 1275, Train Loss: 1.0950, Test Loss: 5.1715\n",
      "Epoch: 1276, Train Loss: 1.0598, Test Loss: 4.4746\n",
      "Epoch: 1277, Train Loss: 1.0420, Test Loss: 4.5161\n",
      "Epoch: 1278, Train Loss: 0.9970, Test Loss: 4.9948\n",
      "Epoch: 1279, Train Loss: 1.0632, Test Loss: 5.1745\n",
      "Epoch: 1280, Train Loss: 1.0944, Test Loss: 4.6746\n",
      "Epoch: 1281, Train Loss: 1.0494, Test Loss: 4.5952\n",
      "Epoch: 1282, Train Loss: 1.0314, Test Loss: 5.2445\n",
      "Epoch: 1283, Train Loss: 1.1390, Test Loss: 4.9591\n",
      "Epoch: 1284, Train Loss: 1.0114, Test Loss: 4.0271\n",
      "Epoch: 1285, Train Loss: 1.0898, Test Loss: 4.5278\n",
      "Epoch: 1286, Train Loss: 0.9940, Test Loss: 5.0156\n",
      "Epoch: 1287, Train Loss: 1.0483, Test Loss: 5.1788\n",
      "Epoch: 1288, Train Loss: 1.0372, Test Loss: 4.3486\n",
      "Epoch: 1289, Train Loss: 1.0465, Test Loss: 4.5986\n",
      "Epoch: 1290, Train Loss: 1.1831, Test Loss: 5.7350\n",
      "Epoch: 1291, Train Loss: 1.1480, Test Loss: 4.5629\n",
      "Epoch: 1292, Train Loss: 0.9453, Test Loss: 4.0360\n",
      "Epoch: 1293, Train Loss: 1.0868, Test Loss: 4.8745\n",
      "Epoch: 1294, Train Loss: 0.9630, Test Loss: 5.5309\n",
      "Epoch: 1295, Train Loss: 1.0621, Test Loss: 4.5725\n",
      "Epoch: 1296, Train Loss: 1.0821, Test Loss: 4.9143\n",
      "Epoch: 1297, Train Loss: 0.9991, Test Loss: 4.5459\n",
      "Epoch: 1298, Train Loss: 1.0135, Test Loss: 4.0670\n",
      "Epoch: 1299, Train Loss: 1.0076, Test Loss: 5.1392\n",
      "Epoch: 1300, Train Loss: 1.0011, Test Loss: 5.3157\n",
      "Epoch: 1301, Train Loss: 1.0320, Test Loss: 4.3350\n",
      "Epoch: 1302, Train Loss: 1.0267, Test Loss: 4.4528\n",
      "Epoch: 1303, Train Loss: 0.9778, Test Loss: 5.2794\n",
      "Epoch: 1304, Train Loss: 0.9299, Test Loss: 4.9893\n",
      "Epoch: 1305, Train Loss: 1.0404, Test Loss: 4.8767\n",
      "Epoch: 1306, Train Loss: 0.9231, Test Loss: 4.6956\n",
      "Epoch: 1307, Train Loss: 1.0696, Test Loss: 4.6598\n",
      "Epoch: 1308, Train Loss: 0.9542, Test Loss: 5.2369\n",
      "Epoch: 1309, Train Loss: 0.9983, Test Loss: 5.1201\n",
      "Epoch: 1310, Train Loss: 0.9957, Test Loss: 4.3857\n",
      "Epoch: 1311, Train Loss: 1.0372, Test Loss: 4.6880\n",
      "Epoch: 1312, Train Loss: 0.9577, Test Loss: 4.8082\n",
      "Epoch: 1313, Train Loss: 0.8928, Test Loss: 4.6888\n",
      "Epoch: 1314, Train Loss: 0.9621, Test Loss: 4.9867\n",
      "Epoch: 1315, Train Loss: 0.9539, Test Loss: 4.9840\n",
      "Epoch: 1316, Train Loss: 0.9368, Test Loss: 4.8241\n",
      "Epoch: 1317, Train Loss: 0.8828, Test Loss: 4.8126\n",
      "Epoch: 1318, Train Loss: 0.9048, Test Loss: 5.1554\n",
      "Epoch: 1319, Train Loss: 1.0390, Test Loss: 4.4071\n",
      "Epoch: 1320, Train Loss: 0.9415, Test Loss: 4.7832\n",
      "Epoch: 1321, Train Loss: 0.9853, Test Loss: 6.1423\n",
      "Epoch: 1322, Train Loss: 1.1115, Test Loss: 4.7313\n",
      "Epoch: 1323, Train Loss: 0.9160, Test Loss: 3.8791\n",
      "Epoch: 1324, Train Loss: 1.2747, Test Loss: 5.7590\n",
      "Epoch: 1325, Train Loss: 1.1222, Test Loss: 5.7495\n",
      "Epoch: 1326, Train Loss: 1.1807, Test Loss: 3.9900\n",
      "Epoch: 1327, Train Loss: 1.0328, Test Loss: 3.9436\n",
      "Epoch: 1328, Train Loss: 1.1706, Test Loss: 5.9022\n",
      "Epoch: 1329, Train Loss: 1.2377, Test Loss: 4.7782\n",
      "Epoch: 1330, Train Loss: 0.9181, Test Loss: 3.7336\n",
      "Epoch: 1331, Train Loss: 1.1276, Test Loss: 4.4529\n",
      "Epoch: 1332, Train Loss: 0.8471, Test Loss: 6.2102\n",
      "Epoch: 1333, Train Loss: 1.1964, Test Loss: 5.0830\n",
      "Epoch: 1334, Train Loss: 1.1136, Test Loss: 3.6143\n",
      "Epoch: 1335, Train Loss: 1.2474, Test Loss: 4.2733\n",
      "Epoch: 1336, Train Loss: 0.9652, Test Loss: 6.1769\n",
      "Epoch: 1337, Train Loss: 1.2457, Test Loss: 5.0117\n",
      "Epoch: 1338, Train Loss: 0.9190, Test Loss: 4.0826\n",
      "Epoch: 1339, Train Loss: 0.9859, Test Loss: 4.3080\n",
      "Epoch: 1340, Train Loss: 0.9315, Test Loss: 5.6928\n",
      "Epoch: 1341, Train Loss: 1.1608, Test Loss: 4.8456\n",
      "Epoch: 1342, Train Loss: 0.9118, Test Loss: 4.2300\n",
      "Epoch: 1343, Train Loss: 1.0632, Test Loss: 5.0229\n",
      "Epoch: 1344, Train Loss: 0.9722, Test Loss: 4.7153\n",
      "Epoch: 1345, Train Loss: 0.9672, Test Loss: 4.4780\n",
      "Epoch: 1346, Train Loss: 1.0512, Test Loss: 5.2309\n",
      "Epoch: 1347, Train Loss: 1.0230, Test Loss: 4.7983\n",
      "Epoch: 1348, Train Loss: 0.9995, Test Loss: 4.3137\n",
      "Epoch: 1349, Train Loss: 0.9951, Test Loss: 4.7101\n",
      "Epoch: 1350, Train Loss: 1.0325, Test Loss: 4.8741\n",
      "Epoch: 1351, Train Loss: 0.9638, Test Loss: 5.2257\n",
      "Epoch: 1352, Train Loss: 0.9456, Test Loss: 5.4317\n",
      "Epoch: 1353, Train Loss: 1.0464, Test Loss: 4.5210\n",
      "Epoch: 1354, Train Loss: 0.9720, Test Loss: 4.1137\n",
      "Epoch: 1355, Train Loss: 0.9517, Test Loss: 4.1820\n",
      "Epoch: 1356, Train Loss: 0.8940, Test Loss: 5.4048\n",
      "Epoch: 1357, Train Loss: 0.9176, Test Loss: 5.2260\n",
      "Epoch: 1358, Train Loss: 1.0168, Test Loss: 4.7304\n",
      "Epoch: 1359, Train Loss: 1.0204, Test Loss: 4.8352\n",
      "Epoch: 1360, Train Loss: 0.9035, Test Loss: 5.1968\n",
      "Epoch: 1361, Train Loss: 0.8668, Test Loss: 4.9284\n",
      "Epoch: 1362, Train Loss: 0.9658, Test Loss: 4.3130\n",
      "Epoch: 1363, Train Loss: 0.9464, Test Loss: 5.0488\n",
      "Epoch: 1364, Train Loss: 0.9969, Test Loss: 5.3448\n",
      "Epoch: 1365, Train Loss: 0.9080, Test Loss: 4.6523\n",
      "Epoch: 1366, Train Loss: 0.9449, Test Loss: 4.0636\n",
      "Epoch: 1367, Train Loss: 1.1210, Test Loss: 4.8316\n",
      "Epoch: 1368, Train Loss: 0.9463, Test Loss: 5.4856\n",
      "Epoch: 1369, Train Loss: 0.9556, Test Loss: 5.1578\n",
      "Epoch: 1370, Train Loss: 1.0063, Test Loss: 4.5865\n",
      "Epoch: 1371, Train Loss: 1.0437, Test Loss: 4.9919\n",
      "Epoch: 1372, Train Loss: 0.9347, Test Loss: 4.7961\n",
      "Epoch: 1373, Train Loss: 0.9716, Test Loss: 4.3060\n",
      "Epoch: 1374, Train Loss: 1.1032, Test Loss: 4.8019\n",
      "Epoch: 1375, Train Loss: 0.9225, Test Loss: 5.2724\n",
      "Epoch: 1376, Train Loss: 0.9636, Test Loss: 4.8063\n",
      "Epoch: 1377, Train Loss: 1.0434, Test Loss: 3.8892\n",
      "Epoch: 1378, Train Loss: 1.1750, Test Loss: 5.0981\n",
      "Epoch: 1379, Train Loss: 1.0317, Test Loss: 5.6570\n",
      "Epoch: 1380, Train Loss: 1.0260, Test Loss: 4.4802\n",
      "Epoch: 1381, Train Loss: 1.0459, Test Loss: 4.4340\n",
      "Epoch: 1382, Train Loss: 0.9081, Test Loss: 5.3379\n",
      "Epoch: 1383, Train Loss: 0.9944, Test Loss: 4.9687\n",
      "Epoch: 1384, Train Loss: 1.0966, Test Loss: 3.7734\n",
      "Epoch: 1385, Train Loss: 1.2148, Test Loss: 4.9080\n",
      "Epoch: 1386, Train Loss: 1.0090, Test Loss: 5.2440\n",
      "Epoch: 1387, Train Loss: 0.9379, Test Loss: 4.1895\n",
      "Epoch: 1388, Train Loss: 0.8663, Test Loss: 4.1684\n",
      "Epoch: 1389, Train Loss: 0.9027, Test Loss: 4.9958\n",
      "Epoch: 1390, Train Loss: 1.0035, Test Loss: 5.2196\n",
      "Epoch: 1391, Train Loss: 0.9354, Test Loss: 4.4361\n",
      "Epoch: 1392, Train Loss: 0.8772, Test Loss: 4.3508\n",
      "Epoch: 1393, Train Loss: 0.9551, Test Loss: 5.2026\n",
      "Epoch: 1394, Train Loss: 0.9852, Test Loss: 5.3285\n",
      "Epoch: 1395, Train Loss: 1.0556, Test Loss: 4.4435\n",
      "Epoch: 1396, Train Loss: 0.9124, Test Loss: 4.6650\n",
      "Epoch: 1397, Train Loss: 0.8336, Test Loss: 4.8609\n",
      "Epoch: 1398, Train Loss: 0.8793, Test Loss: 4.5426\n",
      "Epoch: 1399, Train Loss: 0.8900, Test Loss: 4.4868\n",
      "Epoch: 1400, Train Loss: 0.9600, Test Loss: 4.8857\n",
      "Epoch: 1401, Train Loss: 0.9617, Test Loss: 4.4946\n",
      "Epoch: 1402, Train Loss: 1.0072, Test Loss: 4.9703\n",
      "Epoch: 1403, Train Loss: 0.8160, Test Loss: 5.1889\n",
      "Epoch: 1404, Train Loss: 0.9214, Test Loss: 4.3837\n",
      "Epoch: 1405, Train Loss: 1.0795, Test Loss: 4.8754\n",
      "Epoch: 1406, Train Loss: 0.9932, Test Loss: 5.2166\n",
      "Epoch: 1407, Train Loss: 0.9734, Test Loss: 4.9345\n",
      "Epoch: 1408, Train Loss: 0.8782, Test Loss: 4.6065\n",
      "Epoch: 1409, Train Loss: 0.8790, Test Loss: 4.6228\n",
      "Epoch: 1410, Train Loss: 0.9086, Test Loss: 4.4720\n",
      "Epoch: 1411, Train Loss: 0.9177, Test Loss: 5.2973\n",
      "Epoch: 1412, Train Loss: 0.9742, Test Loss: 4.7266\n",
      "Epoch: 1413, Train Loss: 0.8556, Test Loss: 4.2981\n",
      "Epoch: 1414, Train Loss: 0.9069, Test Loss: 4.8607\n",
      "Epoch: 1415, Train Loss: 0.9807, Test Loss: 5.6389\n",
      "Epoch: 1416, Train Loss: 0.9065, Test Loss: 4.7690\n",
      "Epoch: 1417, Train Loss: 0.8079, Test Loss: 4.2346\n",
      "Epoch: 1418, Train Loss: 0.9805, Test Loss: 5.1430\n",
      "Epoch: 1419, Train Loss: 0.9028, Test Loss: 4.7946\n",
      "Epoch: 1420, Train Loss: 0.8914, Test Loss: 4.6084\n",
      "Epoch: 1421, Train Loss: 0.8567, Test Loss: 4.0859\n",
      "Epoch: 1422, Train Loss: 1.0422, Test Loss: 5.6411\n",
      "Epoch: 1423, Train Loss: 0.9177, Test Loss: 5.8364\n",
      "Epoch: 1424, Train Loss: 1.0379, Test Loss: 4.0193\n",
      "Epoch: 1425, Train Loss: 0.9383, Test Loss: 4.0451\n",
      "Epoch: 1426, Train Loss: 1.0289, Test Loss: 5.7626\n",
      "Epoch: 1427, Train Loss: 0.9499, Test Loss: 5.5248\n",
      "Epoch: 1428, Train Loss: 0.9674, Test Loss: 4.0955\n",
      "Epoch: 1429, Train Loss: 1.0020, Test Loss: 3.8559\n",
      "Epoch: 1430, Train Loss: 1.0466, Test Loss: 5.1668\n",
      "Epoch: 1431, Train Loss: 0.9568, Test Loss: 5.6018\n",
      "Epoch: 1432, Train Loss: 0.9742, Test Loss: 4.2095\n",
      "Epoch: 1433, Train Loss: 0.9213, Test Loss: 4.0480\n",
      "Epoch: 1434, Train Loss: 0.9275, Test Loss: 5.4661\n",
      "Epoch: 1435, Train Loss: 1.0541, Test Loss: 4.6609\n",
      "Epoch: 1436, Train Loss: 0.8967, Test Loss: 3.9701\n",
      "Epoch: 1437, Train Loss: 0.9562, Test Loss: 4.4756\n",
      "Epoch: 1438, Train Loss: 0.8441, Test Loss: 4.8946\n",
      "Epoch: 1439, Train Loss: 0.9215, Test Loss: 5.0289\n",
      "Epoch: 1440, Train Loss: 0.9747, Test Loss: 4.7172\n",
      "Epoch: 1441, Train Loss: 0.8699, Test Loss: 4.8391\n",
      "Epoch: 1442, Train Loss: 0.9301, Test Loss: 4.6692\n",
      "Epoch: 1443, Train Loss: 1.0223, Test Loss: 4.4308\n",
      "Epoch: 1444, Train Loss: 0.9657, Test Loss: 4.6487\n",
      "Epoch: 1445, Train Loss: 0.9638, Test Loss: 4.4671\n",
      "Epoch: 1446, Train Loss: 0.8866, Test Loss: 4.5401\n",
      "Epoch: 1447, Train Loss: 0.8737, Test Loss: 4.7392\n",
      "Epoch: 1448, Train Loss: 0.8997, Test Loss: 5.2526\n",
      "Epoch: 1449, Train Loss: 0.9227, Test Loss: 4.4194\n",
      "Epoch: 1450, Train Loss: 0.9057, Test Loss: 4.3613\n",
      "Epoch: 1451, Train Loss: 0.9524, Test Loss: 4.3638\n",
      "Epoch: 1452, Train Loss: 0.9029, Test Loss: 4.5496\n",
      "Epoch: 1453, Train Loss: 0.9186, Test Loss: 4.6085\n",
      "Epoch: 1454, Train Loss: 0.8123, Test Loss: 4.5884\n",
      "Epoch: 1455, Train Loss: 0.8324, Test Loss: 4.5324\n",
      "Epoch: 1456, Train Loss: 0.8660, Test Loss: 4.6498\n",
      "Epoch: 1457, Train Loss: 0.8719, Test Loss: 4.1779\n",
      "Epoch: 1458, Train Loss: 1.0168, Test Loss: 5.0308\n",
      "Epoch: 1459, Train Loss: 0.8798, Test Loss: 4.7814\n",
      "Epoch: 1460, Train Loss: 0.8770, Test Loss: 3.8879\n",
      "Epoch: 1461, Train Loss: 0.9974, Test Loss: 4.2027\n",
      "Epoch: 1462, Train Loss: 0.9305, Test Loss: 5.2595\n",
      "Epoch: 1463, Train Loss: 0.8958, Test Loss: 4.7142\n",
      "Epoch: 1464, Train Loss: 0.8155, Test Loss: 4.1485\n",
      "Epoch: 1465, Train Loss: 0.9104, Test Loss: 4.6664\n",
      "Epoch: 1466, Train Loss: 0.9330, Test Loss: 5.4508\n",
      "Epoch: 1467, Train Loss: 1.0108, Test Loss: 4.5520\n",
      "Epoch: 1468, Train Loss: 0.8597, Test Loss: 4.0797\n",
      "Epoch: 1469, Train Loss: 1.0209, Test Loss: 4.8159\n",
      "Epoch: 1470, Train Loss: 0.8685, Test Loss: 5.7392\n",
      "Epoch: 1471, Train Loss: 1.0877, Test Loss: 3.9377\n",
      "Epoch: 1472, Train Loss: 1.0375, Test Loss: 4.1807\n",
      "Epoch: 1473, Train Loss: 0.8790, Test Loss: 5.3198\n",
      "Epoch: 1474, Train Loss: 0.8740, Test Loss: 5.2586\n",
      "Epoch: 1475, Train Loss: 1.0160, Test Loss: 4.0261\n",
      "Epoch: 1476, Train Loss: 1.0199, Test Loss: 4.1692\n",
      "Epoch: 1477, Train Loss: 0.8932, Test Loss: 5.1571\n",
      "Epoch: 1478, Train Loss: 1.0043, Test Loss: 5.2994\n",
      "Epoch: 1479, Train Loss: 0.8416, Test Loss: 4.3625\n",
      "Epoch: 1480, Train Loss: 0.8286, Test Loss: 4.1385\n",
      "Epoch: 1481, Train Loss: 0.9463, Test Loss: 5.0858\n",
      "Epoch: 1482, Train Loss: 0.8440, Test Loss: 5.1128\n",
      "Epoch: 1483, Train Loss: 0.9005, Test Loss: 4.9645\n",
      "Epoch: 1484, Train Loss: 0.8962, Test Loss: 4.4127\n",
      "Epoch: 1485, Train Loss: 0.8907, Test Loss: 4.2785\n",
      "Epoch: 1486, Train Loss: 1.0198, Test Loss: 4.7203\n",
      "Epoch: 1487, Train Loss: 0.8458, Test Loss: 5.1554\n",
      "Epoch: 1488, Train Loss: 0.9721, Test Loss: 4.1258\n",
      "Epoch: 1489, Train Loss: 0.9204, Test Loss: 4.6121\n",
      "Epoch: 1490, Train Loss: 0.8787, Test Loss: 4.6818\n",
      "Epoch: 1491, Train Loss: 0.8906, Test Loss: 4.8262\n",
      "Epoch: 1492, Train Loss: 0.8494, Test Loss: 4.8159\n",
      "Epoch: 1493, Train Loss: 0.8364, Test Loss: 5.0101\n",
      "Epoch: 1494, Train Loss: 0.8013, Test Loss: 5.2139\n",
      "Epoch: 1495, Train Loss: 0.8898, Test Loss: 4.5124\n",
      "Epoch: 1496, Train Loss: 0.8367, Test Loss: 4.3539\n",
      "Epoch: 1497, Train Loss: 0.8847, Test Loss: 4.4231\n",
      "Epoch: 1498, Train Loss: 0.8328, Test Loss: 4.7110\n",
      "Epoch: 1499, Train Loss: 0.7817, Test Loss: 5.1963\n",
      "Epoch: 1500, Train Loss: 0.9003, Test Loss: 4.2220\n",
      "Epoch: 1501, Train Loss: 0.8626, Test Loss: 4.1009\n",
      "Epoch: 1502, Train Loss: 0.8300, Test Loss: 5.0117\n",
      "Epoch: 1503, Train Loss: 0.8600, Test Loss: 5.4289\n",
      "Epoch: 1504, Train Loss: 1.0445, Test Loss: 4.0337\n",
      "Epoch: 1505, Train Loss: 0.9839, Test Loss: 4.0251\n",
      "Epoch: 1506, Train Loss: 0.9281, Test Loss: 5.1688\n",
      "Epoch: 1507, Train Loss: 0.9013, Test Loss: 5.2688\n",
      "Epoch: 1508, Train Loss: 0.9367, Test Loss: 3.9639\n",
      "Epoch: 1509, Train Loss: 1.0365, Test Loss: 4.2707\n",
      "Epoch: 1510, Train Loss: 0.9234, Test Loss: 5.3814\n",
      "Epoch: 1511, Train Loss: 0.9812, Test Loss: 4.4205\n",
      "Epoch: 1512, Train Loss: 0.9755, Test Loss: 4.3251\n",
      "Epoch: 1513, Train Loss: 0.9091, Test Loss: 5.0522\n",
      "Epoch: 1514, Train Loss: 0.9025, Test Loss: 4.8162\n",
      "Epoch: 1515, Train Loss: 0.8920, Test Loss: 4.1125\n",
      "Epoch: 1516, Train Loss: 0.9328, Test Loss: 4.7333\n",
      "Epoch: 1517, Train Loss: 0.9864, Test Loss: 5.7239\n",
      "Epoch: 1518, Train Loss: 1.0015, Test Loss: 4.3309\n",
      "Epoch: 1519, Train Loss: 0.8928, Test Loss: 3.9750\n",
      "Epoch: 1520, Train Loss: 0.9864, Test Loss: 5.6136\n",
      "Epoch: 1521, Train Loss: 1.0191, Test Loss: 5.2117\n",
      "Epoch: 1522, Train Loss: 1.0101, Test Loss: 3.7091\n",
      "Epoch: 1523, Train Loss: 1.0119, Test Loss: 3.7459\n",
      "Epoch: 1524, Train Loss: 1.0494, Test Loss: 5.4515\n",
      "Epoch: 1525, Train Loss: 0.9561, Test Loss: 5.9542\n",
      "Epoch: 1526, Train Loss: 1.1174, Test Loss: 3.7662\n",
      "Epoch: 1527, Train Loss: 1.0443, Test Loss: 3.8217\n",
      "Epoch: 1528, Train Loss: 1.0414, Test Loss: 5.2166\n",
      "Epoch: 1529, Train Loss: 1.0098, Test Loss: 4.9885\n",
      "Epoch: 1530, Train Loss: 1.0415, Test Loss: 3.8015\n",
      "Epoch: 1531, Train Loss: 1.0701, Test Loss: 3.9446\n",
      "Epoch: 1532, Train Loss: 0.8729, Test Loss: 4.7407\n",
      "Epoch: 1533, Train Loss: 0.8407, Test Loss: 4.7308\n",
      "Epoch: 1534, Train Loss: 0.7799, Test Loss: 4.2206\n",
      "Epoch: 1535, Train Loss: 0.8256, Test Loss: 4.4171\n",
      "Epoch: 1536, Train Loss: 0.9343, Test Loss: 4.2099\n",
      "Epoch: 1537, Train Loss: 0.8193, Test Loss: 4.9906\n",
      "Epoch: 1538, Train Loss: 0.9415, Test Loss: 4.9802\n",
      "Epoch: 1539, Train Loss: 0.9526, Test Loss: 3.8419\n",
      "Epoch: 1540, Train Loss: 0.8355, Test Loss: 4.1240\n",
      "Epoch: 1541, Train Loss: 0.8150, Test Loss: 5.2683\n",
      "Epoch: 1542, Train Loss: 1.0057, Test Loss: 4.4842\n",
      "Epoch: 1543, Train Loss: 0.8249, Test Loss: 3.9777\n",
      "Epoch: 1544, Train Loss: 0.8364, Test Loss: 4.3347\n",
      "Epoch: 1545, Train Loss: 0.8567, Test Loss: 4.5164\n",
      "Epoch: 1546, Train Loss: 0.9024, Test Loss: 5.0051\n",
      "Epoch: 1547, Train Loss: 0.9081, Test Loss: 4.3453\n",
      "Epoch: 1548, Train Loss: 0.8415, Test Loss: 3.9760\n",
      "Epoch: 1549, Train Loss: 0.9448, Test Loss: 4.7959\n",
      "Epoch: 1550, Train Loss: 0.8069, Test Loss: 5.2723\n",
      "Epoch: 1551, Train Loss: 0.8992, Test Loss: 4.1020\n",
      "Epoch: 1552, Train Loss: 0.8471, Test Loss: 3.5630\n",
      "Epoch: 1553, Train Loss: 1.0854, Test Loss: 4.7364\n",
      "Epoch: 1554, Train Loss: 0.8569, Test Loss: 5.4427\n",
      "Epoch: 1555, Train Loss: 0.8233, Test Loss: 4.7695\n",
      "Epoch: 1556, Train Loss: 0.8552, Test Loss: 3.7996\n",
      "Epoch: 1557, Train Loss: 0.8822, Test Loss: 4.0369\n",
      "Epoch: 1558, Train Loss: 0.8311, Test Loss: 4.9779\n",
      "Epoch: 1559, Train Loss: 0.8695, Test Loss: 4.7845\n",
      "Epoch: 1560, Train Loss: 0.8285, Test Loss: 3.9086\n",
      "Epoch: 1561, Train Loss: 0.9019, Test Loss: 3.9065\n",
      "Epoch: 1562, Train Loss: 0.8207, Test Loss: 5.0694\n",
      "Epoch: 1563, Train Loss: 0.9038, Test Loss: 4.9595\n",
      "Epoch: 1564, Train Loss: 0.9415, Test Loss: 3.9055\n",
      "Epoch: 1565, Train Loss: 0.8155, Test Loss: 3.8220\n",
      "Epoch: 1566, Train Loss: 0.9589, Test Loss: 4.8044\n",
      "Epoch: 1567, Train Loss: 0.8175, Test Loss: 4.8978\n",
      "Epoch: 1568, Train Loss: 0.9122, Test Loss: 3.6877\n",
      "Epoch: 1569, Train Loss: 1.0032, Test Loss: 4.0522\n",
      "Epoch: 1570, Train Loss: 0.8802, Test Loss: 5.3128\n",
      "Epoch: 1571, Train Loss: 0.8150, Test Loss: 5.5344\n",
      "Epoch: 1572, Train Loss: 1.0126, Test Loss: 4.0124\n",
      "Epoch: 1573, Train Loss: 0.9162, Test Loss: 3.8978\n",
      "Epoch: 1574, Train Loss: 0.8784, Test Loss: 4.6083\n",
      "Epoch: 1575, Train Loss: 0.8331, Test Loss: 4.7944\n",
      "Epoch: 1576, Train Loss: 0.8972, Test Loss: 4.0877\n",
      "Epoch: 1577, Train Loss: 0.8566, Test Loss: 3.8273\n",
      "Epoch: 1578, Train Loss: 0.8913, Test Loss: 4.3850\n",
      "Epoch: 1579, Train Loss: 0.9143, Test Loss: 4.4978\n",
      "Epoch: 1580, Train Loss: 0.8234, Test Loss: 4.5827\n",
      "Epoch: 1581, Train Loss: 0.8178, Test Loss: 4.6912\n",
      "Epoch: 1582, Train Loss: 0.8459, Test Loss: 4.2665\n",
      "Epoch: 1583, Train Loss: 0.8551, Test Loss: 4.5929\n",
      "Epoch: 1584, Train Loss: 0.8203, Test Loss: 4.4143\n",
      "Epoch: 1585, Train Loss: 0.7982, Test Loss: 4.8393\n",
      "Epoch: 1586, Train Loss: 0.7510, Test Loss: 4.8367\n",
      "Epoch: 1587, Train Loss: 0.8810, Test Loss: 3.9289\n",
      "Epoch: 1588, Train Loss: 0.8119, Test Loss: 3.7681\n",
      "Epoch: 1589, Train Loss: 1.0056, Test Loss: 5.5780\n",
      "Epoch: 1590, Train Loss: 0.8609, Test Loss: 5.6870\n",
      "Epoch: 1591, Train Loss: 0.9517, Test Loss: 4.1568\n",
      "Epoch: 1592, Train Loss: 0.8545, Test Loss: 3.6166\n",
      "Epoch: 1593, Train Loss: 1.0715, Test Loss: 4.7817\n",
      "Epoch: 1594, Train Loss: 0.8914, Test Loss: 5.0372\n",
      "Epoch: 1595, Train Loss: 0.9122, Test Loss: 4.3315\n",
      "Epoch: 1596, Train Loss: 0.8393, Test Loss: 3.8742\n",
      "Epoch: 1597, Train Loss: 0.9962, Test Loss: 4.9632\n",
      "Epoch: 1598, Train Loss: 0.9661, Test Loss: 4.3632\n",
      "Epoch: 1599, Train Loss: 0.8181, Test Loss: 4.1210\n",
      "Epoch: 1600, Train Loss: 0.8141, Test Loss: 4.2625\n",
      "Epoch: 1601, Train Loss: 0.9070, Test Loss: 5.1220\n",
      "Epoch: 1602, Train Loss: 0.9197, Test Loss: 4.7941\n",
      "Epoch: 1603, Train Loss: 0.8901, Test Loss: 3.8903\n",
      "Epoch: 1604, Train Loss: 0.8817, Test Loss: 4.2020\n",
      "Epoch: 1605, Train Loss: 0.8361, Test Loss: 4.1939\n",
      "Epoch: 1606, Train Loss: 0.8084, Test Loss: 4.6183\n",
      "Epoch: 1607, Train Loss: 0.8543, Test Loss: 4.6225\n",
      "Epoch: 1608, Train Loss: 0.8161, Test Loss: 4.2491\n",
      "Epoch: 1609, Train Loss: 0.7875, Test Loss: 4.7908\n",
      "Epoch: 1610, Train Loss: 0.9021, Test Loss: 4.6895\n",
      "Epoch: 1611, Train Loss: 0.8416, Test Loss: 4.2945\n",
      "Epoch: 1612, Train Loss: 0.8243, Test Loss: 4.6785\n",
      "Epoch: 1613, Train Loss: 0.9103, Test Loss: 4.4415\n",
      "Epoch: 1614, Train Loss: 0.8970, Test Loss: 4.8358\n",
      "Epoch: 1615, Train Loss: 0.7946, Test Loss: 4.7089\n",
      "Epoch: 1616, Train Loss: 0.8785, Test Loss: 4.4672\n",
      "Epoch: 1617, Train Loss: 0.8065, Test Loss: 3.9186\n",
      "Epoch: 1618, Train Loss: 0.8919, Test Loss: 4.2794\n",
      "Epoch: 1619, Train Loss: 0.8677, Test Loss: 5.1837\n",
      "Epoch: 1620, Train Loss: 0.8621, Test Loss: 4.6083\n",
      "Epoch: 1621, Train Loss: 0.8006, Test Loss: 4.0380\n",
      "Epoch: 1622, Train Loss: 0.8406, Test Loss: 4.0958\n",
      "Epoch: 1623, Train Loss: 0.7972, Test Loss: 4.4846\n",
      "Epoch: 1624, Train Loss: 0.7847, Test Loss: 4.5903\n",
      "Epoch: 1625, Train Loss: 0.8214, Test Loss: 4.7083\n",
      "Epoch: 1626, Train Loss: 0.7888, Test Loss: 4.7335\n",
      "Epoch: 1627, Train Loss: 0.7982, Test Loss: 4.5268\n",
      "Epoch: 1628, Train Loss: 0.9470, Test Loss: 3.9931\n",
      "Epoch: 1629, Train Loss: 0.7879, Test Loss: 4.2292\n",
      "Epoch: 1630, Train Loss: 0.7930, Test Loss: 4.7956\n",
      "Epoch: 1631, Train Loss: 0.7571, Test Loss: 4.8107\n",
      "Epoch: 1632, Train Loss: 0.8563, Test Loss: 4.3271\n",
      "Epoch: 1633, Train Loss: 0.8201, Test Loss: 4.1459\n",
      "Epoch: 1634, Train Loss: 0.8886, Test Loss: 4.3163\n",
      "Epoch: 1635, Train Loss: 0.8287, Test Loss: 5.1147\n",
      "Epoch: 1636, Train Loss: 0.8797, Test Loss: 4.1692\n",
      "Epoch: 1637, Train Loss: 0.8778, Test Loss: 4.0811\n",
      "Epoch: 1638, Train Loss: 0.8041, Test Loss: 4.5039\n",
      "Epoch: 1639, Train Loss: 0.8750, Test Loss: 5.3983\n",
      "Epoch: 1640, Train Loss: 0.9026, Test Loss: 4.2960\n",
      "Epoch: 1641, Train Loss: 0.8943, Test Loss: 3.6910\n",
      "Epoch: 1642, Train Loss: 0.9194, Test Loss: 4.2796\n",
      "Epoch: 1643, Train Loss: 0.7478, Test Loss: 5.4467\n",
      "Epoch: 1644, Train Loss: 0.9603, Test Loss: 4.5884\n",
      "Epoch: 1645, Train Loss: 0.8567, Test Loss: 3.5652\n",
      "Epoch: 1646, Train Loss: 1.0000, Test Loss: 4.1217\n",
      "Epoch: 1647, Train Loss: 0.7534, Test Loss: 5.5963\n",
      "Epoch: 1648, Train Loss: 0.9498, Test Loss: 5.1428\n",
      "Epoch: 1649, Train Loss: 0.8513, Test Loss: 3.8977\n",
      "Epoch: 1650, Train Loss: 0.8659, Test Loss: 3.7477\n",
      "Epoch: 1651, Train Loss: 0.8861, Test Loss: 4.7101\n",
      "Epoch: 1652, Train Loss: 0.9195, Test Loss: 4.7459\n",
      "Epoch: 1653, Train Loss: 0.8512, Test Loss: 3.9426\n",
      "Epoch: 1654, Train Loss: 0.8076, Test Loss: 3.6896\n",
      "Epoch: 1655, Train Loss: 0.8427, Test Loss: 4.5350\n",
      "Epoch: 1656, Train Loss: 0.8690, Test Loss: 4.6385\n",
      "Epoch: 1657, Train Loss: 0.7766, Test Loss: 4.0287\n",
      "Epoch: 1658, Train Loss: 0.8425, Test Loss: 4.3484\n",
      "Epoch: 1659, Train Loss: 0.8925, Test Loss: 4.8844\n",
      "Epoch: 1660, Train Loss: 0.8120, Test Loss: 4.3694\n",
      "Epoch: 1661, Train Loss: 0.7528, Test Loss: 4.1344\n",
      "Epoch: 1662, Train Loss: 0.8369, Test Loss: 4.2916\n",
      "Epoch: 1663, Train Loss: 0.7554, Test Loss: 4.4987\n",
      "Epoch: 1664, Train Loss: 0.8041, Test Loss: 4.2885\n",
      "Epoch: 1665, Train Loss: 0.8895, Test Loss: 4.2853\n",
      "Epoch: 1666, Train Loss: 0.8680, Test Loss: 4.6090\n",
      "Epoch: 1667, Train Loss: 0.8241, Test Loss: 4.2591\n",
      "Epoch: 1668, Train Loss: 0.8643, Test Loss: 4.5697\n",
      "Epoch: 1669, Train Loss: 0.9323, Test Loss: 4.6602\n",
      "Epoch: 1670, Train Loss: 0.8547, Test Loss: 4.2548\n",
      "Epoch: 1671, Train Loss: 0.7533, Test Loss: 3.6020\n",
      "Epoch: 1672, Train Loss: 0.9204, Test Loss: 4.3513\n",
      "Epoch: 1673, Train Loss: 0.9141, Test Loss: 5.1778\n",
      "Epoch: 1674, Train Loss: 0.8771, Test Loss: 4.3618\n",
      "Epoch: 1675, Train Loss: 0.8220, Test Loss: 3.7108\n",
      "Epoch: 1676, Train Loss: 0.8412, Test Loss: 4.3535\n",
      "Epoch: 1677, Train Loss: 0.8168, Test Loss: 5.0306\n",
      "Epoch: 1678, Train Loss: 0.8294, Test Loss: 4.2892\n",
      "Epoch: 1679, Train Loss: 0.7360, Test Loss: 3.8222\n",
      "Epoch: 1680, Train Loss: 0.9606, Test Loss: 4.5920\n",
      "Epoch: 1681, Train Loss: 0.7173, Test Loss: 4.8114\n",
      "Epoch: 1682, Train Loss: 0.8398, Test Loss: 4.2200\n",
      "Epoch: 1683, Train Loss: 0.8176, Test Loss: 4.1308\n",
      "Epoch: 1684, Train Loss: 0.8053, Test Loss: 4.3758\n",
      "Epoch: 1685, Train Loss: 0.7004, Test Loss: 4.6652\n",
      "Epoch: 1686, Train Loss: 0.7482, Test Loss: 4.3223\n",
      "Epoch: 1687, Train Loss: 0.7375, Test Loss: 4.0836\n",
      "Epoch: 1688, Train Loss: 0.7575, Test Loss: 4.1532\n",
      "Epoch: 1689, Train Loss: 0.7423, Test Loss: 4.4918\n",
      "Epoch: 1690, Train Loss: 0.8152, Test Loss: 4.3490\n",
      "Epoch: 1691, Train Loss: 0.7179, Test Loss: 4.2413\n",
      "Epoch: 1692, Train Loss: 0.7497, Test Loss: 4.6326\n",
      "Epoch: 1693, Train Loss: 0.8547, Test Loss: 4.1194\n",
      "Epoch: 1694, Train Loss: 0.8552, Test Loss: 3.9271\n",
      "Epoch: 1695, Train Loss: 0.7917, Test Loss: 4.3986\n",
      "Epoch: 1696, Train Loss: 0.6885, Test Loss: 5.3381\n",
      "Epoch: 1697, Train Loss: 0.9037, Test Loss: 4.0685\n",
      "Epoch: 1698, Train Loss: 0.8729, Test Loss: 3.9458\n",
      "Epoch: 1699, Train Loss: 0.7248, Test Loss: 4.5068\n",
      "Epoch: 1700, Train Loss: 0.7595, Test Loss: 5.4216\n",
      "Epoch: 1701, Train Loss: 0.8722, Test Loss: 4.5800\n",
      "Epoch: 1702, Train Loss: 0.7961, Test Loss: 3.6595\n",
      "Epoch: 1703, Train Loss: 0.9552, Test Loss: 4.5048\n",
      "Epoch: 1704, Train Loss: 0.7259, Test Loss: 5.2973\n",
      "Epoch: 1705, Train Loss: 0.8039, Test Loss: 5.0565\n",
      "Epoch: 1706, Train Loss: 0.8120, Test Loss: 3.5714\n",
      "Epoch: 1707, Train Loss: 0.9522, Test Loss: 3.9668\n",
      "Epoch: 1708, Train Loss: 0.7799, Test Loss: 5.0392\n",
      "Epoch: 1709, Train Loss: 0.7857, Test Loss: 4.8941\n",
      "Epoch: 1710, Train Loss: 0.8850, Test Loss: 3.6257\n",
      "Epoch: 1711, Train Loss: 0.8850, Test Loss: 3.7204\n",
      "Epoch: 1712, Train Loss: 0.7856, Test Loss: 5.0581\n",
      "Epoch: 1713, Train Loss: 0.8593, Test Loss: 5.3634\n",
      "Epoch: 1714, Train Loss: 0.8764, Test Loss: 3.9826\n",
      "Epoch: 1715, Train Loss: 0.8570, Test Loss: 3.3317\n",
      "Epoch: 1716, Train Loss: 1.0445, Test Loss: 4.0353\n",
      "Epoch: 1717, Train Loss: 0.8112, Test Loss: 5.1978\n",
      "Epoch: 1718, Train Loss: 1.0192, Test Loss: 4.2039\n",
      "Epoch: 1719, Train Loss: 0.8385, Test Loss: 3.1987\n",
      "Epoch: 1720, Train Loss: 1.0998, Test Loss: 3.9434\n",
      "Epoch: 1721, Train Loss: 0.8265, Test Loss: 6.1731\n",
      "Epoch: 1722, Train Loss: 1.1998, Test Loss: 4.5718\n",
      "Epoch: 1723, Train Loss: 0.7675, Test Loss: 3.5080\n",
      "Epoch: 1724, Train Loss: 1.0000, Test Loss: 3.7208\n",
      "Epoch: 1725, Train Loss: 0.8441, Test Loss: 5.1624\n",
      "Epoch: 1726, Train Loss: 1.0014, Test Loss: 4.8031\n",
      "Epoch: 1727, Train Loss: 0.9621, Test Loss: 3.4157\n",
      "Epoch: 1728, Train Loss: 0.9859, Test Loss: 3.4807\n",
      "Epoch: 1729, Train Loss: 1.0019, Test Loss: 5.6944\n",
      "Epoch: 1730, Train Loss: 1.1003, Test Loss: 5.3848\n",
      "Epoch: 1731, Train Loss: 0.8913, Test Loss: 3.6119\n",
      "Epoch: 1732, Train Loss: 0.8911, Test Loss: 3.5739\n",
      "Epoch: 1733, Train Loss: 0.8599, Test Loss: 4.3293\n",
      "Epoch: 1734, Train Loss: 0.7785, Test Loss: 4.8823\n",
      "Epoch: 1735, Train Loss: 0.8352, Test Loss: 4.1458\n",
      "Epoch: 1736, Train Loss: 0.7639, Test Loss: 3.4563\n",
      "Epoch: 1737, Train Loss: 0.9442, Test Loss: 4.2110\n",
      "Epoch: 1738, Train Loss: 0.7496, Test Loss: 5.1822\n",
      "Epoch: 1739, Train Loss: 1.0556, Test Loss: 4.0548\n",
      "Epoch: 1740, Train Loss: 0.8250, Test Loss: 3.4994\n",
      "Epoch: 1741, Train Loss: 0.8382, Test Loss: 3.8709\n",
      "Epoch: 1742, Train Loss: 0.8370, Test Loss: 4.9347\n",
      "Epoch: 1743, Train Loss: 0.8419, Test Loss: 4.7723\n",
      "Epoch: 1744, Train Loss: 0.8694, Test Loss: 3.5751\n",
      "Epoch: 1745, Train Loss: 0.8497, Test Loss: 3.6117\n",
      "Epoch: 1746, Train Loss: 0.8648, Test Loss: 4.8227\n",
      "Epoch: 1747, Train Loss: 0.7560, Test Loss: 5.4699\n",
      "Epoch: 1748, Train Loss: 0.8957, Test Loss: 4.2246\n",
      "Epoch: 1749, Train Loss: 0.7668, Test Loss: 3.4772\n",
      "Epoch: 1750, Train Loss: 1.1307, Test Loss: 4.2876\n",
      "Epoch: 1751, Train Loss: 0.7569, Test Loss: 5.3252\n",
      "Epoch: 1752, Train Loss: 0.9141, Test Loss: 4.4003\n",
      "Epoch: 1753, Train Loss: 0.8326, Test Loss: 3.7761\n",
      "Epoch: 1754, Train Loss: 0.7449, Test Loss: 3.8516\n",
      "Epoch: 1755, Train Loss: 0.7910, Test Loss: 4.7144\n",
      "Epoch: 1756, Train Loss: 0.8254, Test Loss: 4.8372\n",
      "Epoch: 1757, Train Loss: 0.7864, Test Loss: 4.0415\n",
      "Epoch: 1758, Train Loss: 0.7562, Test Loss: 3.4983\n",
      "Epoch: 1759, Train Loss: 1.0035, Test Loss: 4.3924\n",
      "Epoch: 1760, Train Loss: 0.7269, Test Loss: 5.2824\n",
      "Epoch: 1761, Train Loss: 0.8982, Test Loss: 4.2508\n",
      "Epoch: 1762, Train Loss: 0.7563, Test Loss: 3.8469\n",
      "Epoch: 1763, Train Loss: 0.7874, Test Loss: 4.4266\n",
      "Epoch: 1764, Train Loss: 0.7849, Test Loss: 4.7503\n",
      "Epoch: 1765, Train Loss: 0.7680, Test Loss: 4.4970\n",
      "Epoch: 1766, Train Loss: 0.7559, Test Loss: 4.3019\n",
      "Epoch: 1767, Train Loss: 0.7012, Test Loss: 4.1141\n",
      "Epoch: 1768, Train Loss: 0.7377, Test Loss: 4.0581\n",
      "Epoch: 1769, Train Loss: 0.7368, Test Loss: 4.2403\n",
      "Epoch: 1770, Train Loss: 0.7282, Test Loss: 4.5691\n",
      "Epoch: 1771, Train Loss: 0.8585, Test Loss: 3.9361\n",
      "Epoch: 1772, Train Loss: 0.7875, Test Loss: 4.4662\n",
      "Epoch: 1773, Train Loss: 0.7682, Test Loss: 4.8720\n",
      "Epoch: 1774, Train Loss: 0.7550, Test Loss: 4.5490\n",
      "Epoch: 1775, Train Loss: 0.7564, Test Loss: 4.1025\n",
      "Epoch: 1776, Train Loss: 0.7511, Test Loss: 3.8252\n",
      "Epoch: 1777, Train Loss: 0.8307, Test Loss: 4.6375\n",
      "Epoch: 1778, Train Loss: 0.8594, Test Loss: 5.2684\n",
      "Epoch: 1779, Train Loss: 0.7758, Test Loss: 4.4303\n",
      "Epoch: 1780, Train Loss: 0.7939, Test Loss: 3.4424\n",
      "Epoch: 1781, Train Loss: 1.0560, Test Loss: 3.9622\n",
      "Epoch: 1782, Train Loss: 0.7902, Test Loss: 5.7281\n",
      "Epoch: 1783, Train Loss: 1.1187, Test Loss: 4.4951\n",
      "Epoch: 1784, Train Loss: 0.7190, Test Loss: 3.5253\n",
      "Epoch: 1785, Train Loss: 0.8312, Test Loss: 3.6047\n",
      "Epoch: 1786, Train Loss: 0.8059, Test Loss: 4.6997\n",
      "Epoch: 1787, Train Loss: 0.7561, Test Loss: 4.9691\n",
      "Epoch: 1788, Train Loss: 0.7871, Test Loss: 4.2748\n",
      "Epoch: 1789, Train Loss: 0.7666, Test Loss: 3.9322\n",
      "Epoch: 1790, Train Loss: 0.8129, Test Loss: 3.9479\n",
      "Epoch: 1791, Train Loss: 0.8057, Test Loss: 4.5313\n",
      "Epoch: 1792, Train Loss: 0.7495, Test Loss: 5.0645\n",
      "Epoch: 1793, Train Loss: 0.8626, Test Loss: 3.9168\n",
      "Epoch: 1794, Train Loss: 0.7428, Test Loss: 3.7811\n",
      "Epoch: 1795, Train Loss: 0.8549, Test Loss: 4.3495\n",
      "Epoch: 1796, Train Loss: 0.8160, Test Loss: 4.8814\n",
      "Epoch: 1797, Train Loss: 0.7882, Test Loss: 4.4048\n",
      "Epoch: 1798, Train Loss: 0.8534, Test Loss: 3.4743\n",
      "Epoch: 1799, Train Loss: 0.8222, Test Loss: 3.6919\n",
      "Epoch: 1800, Train Loss: 0.7812, Test Loss: 4.1746\n",
      "Epoch: 1801, Train Loss: 0.7202, Test Loss: 4.5108\n",
      "Epoch: 1802, Train Loss: 0.8049, Test Loss: 4.4567\n",
      "Epoch: 1803, Train Loss: 0.7435, Test Loss: 3.9643\n",
      "Epoch: 1804, Train Loss: 0.7755, Test Loss: 4.0219\n",
      "Epoch: 1805, Train Loss: 0.7247, Test Loss: 4.3777\n",
      "Epoch: 1806, Train Loss: 0.7650, Test Loss: 4.6587\n",
      "Epoch: 1807, Train Loss: 0.7867, Test Loss: 4.0911\n",
      "Epoch: 1808, Train Loss: 0.7406, Test Loss: 3.8994\n",
      "Epoch: 1809, Train Loss: 0.8251, Test Loss: 3.7682\n",
      "Epoch: 1810, Train Loss: 0.7596, Test Loss: 4.6191\n",
      "Epoch: 1811, Train Loss: 0.8879, Test Loss: 5.1692\n",
      "Epoch: 1812, Train Loss: 0.8970, Test Loss: 3.9540\n",
      "Epoch: 1813, Train Loss: 0.7853, Test Loss: 3.8355\n",
      "Epoch: 1814, Train Loss: 0.7587, Test Loss: 4.4491\n",
      "Epoch: 1815, Train Loss: 0.7321, Test Loss: 4.7868\n",
      "Epoch: 1816, Train Loss: 0.7865, Test Loss: 4.0698\n",
      "Epoch: 1817, Train Loss: 0.7523, Test Loss: 3.5040\n",
      "Epoch: 1818, Train Loss: 0.8815, Test Loss: 4.2136\n",
      "Epoch: 1819, Train Loss: 0.7690, Test Loss: 5.0363\n",
      "Epoch: 1820, Train Loss: 0.8065, Test Loss: 4.6216\n",
      "Epoch: 1821, Train Loss: 0.7293, Test Loss: 3.9455\n",
      "Epoch: 1822, Train Loss: 0.8046, Test Loss: 4.2724\n",
      "Epoch: 1823, Train Loss: 0.7578, Test Loss: 5.0453\n",
      "Epoch: 1824, Train Loss: 0.7883, Test Loss: 4.5259\n",
      "Epoch: 1825, Train Loss: 0.7188, Test Loss: 3.7456\n",
      "Epoch: 1826, Train Loss: 0.8133, Test Loss: 4.1645\n",
      "Epoch: 1827, Train Loss: 0.7053, Test Loss: 5.2805\n",
      "Epoch: 1828, Train Loss: 1.0117, Test Loss: 3.8789\n",
      "Epoch: 1829, Train Loss: 0.7681, Test Loss: 3.6119\n",
      "Epoch: 1830, Train Loss: 0.7833, Test Loss: 4.2285\n",
      "Epoch: 1831, Train Loss: 0.6504, Test Loss: 4.8367\n",
      "Epoch: 1832, Train Loss: 0.7460, Test Loss: 4.4474\n",
      "Epoch: 1833, Train Loss: 0.7122, Test Loss: 3.6921\n",
      "Epoch: 1834, Train Loss: 0.7544, Test Loss: 3.9220\n",
      "Epoch: 1835, Train Loss: 0.7415, Test Loss: 4.5795\n",
      "Epoch: 1836, Train Loss: 0.7911, Test Loss: 4.6991\n",
      "Epoch: 1837, Train Loss: 0.7702, Test Loss: 3.9380\n",
      "Epoch: 1838, Train Loss: 0.7588, Test Loss: 3.7138\n",
      "Epoch: 1839, Train Loss: 0.7389, Test Loss: 4.3678\n",
      "Epoch: 1840, Train Loss: 0.7992, Test Loss: 4.3227\n",
      "Epoch: 1841, Train Loss: 0.7491, Test Loss: 3.6924\n",
      "Epoch: 1842, Train Loss: 0.7949, Test Loss: 4.1316\n",
      "Epoch: 1843, Train Loss: 0.7076, Test Loss: 4.3443\n",
      "Epoch: 1844, Train Loss: 0.7412, Test Loss: 4.8323\n",
      "Epoch: 1845, Train Loss: 0.7175, Test Loss: 4.3577\n",
      "Epoch: 1846, Train Loss: 0.7908, Test Loss: 4.1744\n",
      "Epoch: 1847, Train Loss: 0.8377, Test Loss: 4.9163\n",
      "Epoch: 1848, Train Loss: 0.8588, Test Loss: 4.7728\n",
      "Epoch: 1849, Train Loss: 0.7687, Test Loss: 3.8995\n",
      "Epoch: 1850, Train Loss: 0.7488, Test Loss: 4.0534\n",
      "Epoch: 1851, Train Loss: 0.7204, Test Loss: 4.3177\n",
      "Epoch: 1852, Train Loss: 0.6974, Test Loss: 4.5211\n",
      "Epoch: 1853, Train Loss: 0.7464, Test Loss: 4.5317\n",
      "Epoch: 1854, Train Loss: 0.7239, Test Loss: 3.9574\n",
      "Epoch: 1855, Train Loss: 0.7161, Test Loss: 3.9424\n",
      "Epoch: 1856, Train Loss: 0.6996, Test Loss: 4.3983\n",
      "Epoch: 1857, Train Loss: 0.6968, Test Loss: 4.4974\n",
      "Epoch: 1858, Train Loss: 0.7464, Test Loss: 4.0240\n",
      "Epoch: 1859, Train Loss: 0.7448, Test Loss: 4.1532\n",
      "Epoch: 1860, Train Loss: 0.7543, Test Loss: 4.5261\n",
      "Epoch: 1861, Train Loss: 0.6870, Test Loss: 4.6101\n",
      "Epoch: 1862, Train Loss: 0.7384, Test Loss: 3.8471\n",
      "Epoch: 1863, Train Loss: 0.8228, Test Loss: 4.0183\n",
      "Epoch: 1864, Train Loss: 0.7400, Test Loss: 4.8644\n",
      "Epoch: 1865, Train Loss: 0.8310, Test Loss: 4.3808\n",
      "Epoch: 1866, Train Loss: 0.7536, Test Loss: 3.5156\n",
      "Epoch: 1867, Train Loss: 0.8641, Test Loss: 4.1106\n",
      "Epoch: 1868, Train Loss: 0.7248, Test Loss: 4.8794\n",
      "Epoch: 1869, Train Loss: 0.8526, Test Loss: 3.9101\n",
      "Epoch: 1870, Train Loss: 0.7285, Test Loss: 3.6609\n",
      "Epoch: 1871, Train Loss: 0.7814, Test Loss: 4.3615\n",
      "Epoch: 1872, Train Loss: 0.7096, Test Loss: 5.0799\n",
      "Epoch: 1873, Train Loss: 0.8687, Test Loss: 4.1056\n",
      "Epoch: 1874, Train Loss: 0.7402, Test Loss: 3.7023\n",
      "Epoch: 1875, Train Loss: 0.8362, Test Loss: 4.4521\n",
      "Epoch: 1876, Train Loss: 0.7231, Test Loss: 5.2922\n",
      "Epoch: 1877, Train Loss: 0.8561, Test Loss: 4.3176\n",
      "Epoch: 1878, Train Loss: 0.7539, Test Loss: 3.7356\n",
      "Epoch: 1879, Train Loss: 0.7927, Test Loss: 4.1428\n",
      "Epoch: 1880, Train Loss: 0.7031, Test Loss: 4.8568\n",
      "Epoch: 1881, Train Loss: 0.7557, Test Loss: 4.5334\n",
      "Epoch: 1882, Train Loss: 0.7061, Test Loss: 3.8441\n",
      "Epoch: 1883, Train Loss: 0.7227, Test Loss: 3.7751\n",
      "Epoch: 1884, Train Loss: 0.7551, Test Loss: 4.8686\n",
      "Epoch: 1885, Train Loss: 0.8015, Test Loss: 4.3445\n",
      "Epoch: 1886, Train Loss: 0.7537, Test Loss: 4.3368\n",
      "Epoch: 1887, Train Loss: 0.7330, Test Loss: 4.1279\n",
      "Epoch: 1888, Train Loss: 0.7545, Test Loss: 4.5242\n",
      "Epoch: 1889, Train Loss: 0.7853, Test Loss: 4.3663\n",
      "Epoch: 1890, Train Loss: 0.7199, Test Loss: 3.7545\n",
      "Epoch: 1891, Train Loss: 0.7848, Test Loss: 4.3386\n",
      "Epoch: 1892, Train Loss: 0.7378, Test Loss: 4.6141\n",
      "Epoch: 1893, Train Loss: 0.7822, Test Loss: 4.1138\n",
      "Epoch: 1894, Train Loss: 0.6800, Test Loss: 3.8618\n",
      "Epoch: 1895, Train Loss: 0.7722, Test Loss: 4.0982\n",
      "Epoch: 1896, Train Loss: 0.7382, Test Loss: 4.7932\n",
      "Epoch: 1897, Train Loss: 0.7835, Test Loss: 4.1560\n",
      "Epoch: 1898, Train Loss: 0.7314, Test Loss: 3.5687\n",
      "Epoch: 1899, Train Loss: 0.8600, Test Loss: 4.0828\n",
      "Epoch: 1900, Train Loss: 0.6942, Test Loss: 5.4927\n",
      "Epoch: 1901, Train Loss: 0.9624, Test Loss: 4.4018\n",
      "Epoch: 1902, Train Loss: 0.8223, Test Loss: 3.2496\n",
      "Epoch: 1903, Train Loss: 1.1087, Test Loss: 3.7330\n",
      "Epoch: 1904, Train Loss: 0.7367, Test Loss: 4.9726\n",
      "Epoch: 1905, Train Loss: 0.8751, Test Loss: 4.5763\n",
      "Epoch: 1906, Train Loss: 0.6751, Test Loss: 3.9291\n",
      "Epoch: 1907, Train Loss: 0.7473, Test Loss: 3.8362\n",
      "Epoch: 1908, Train Loss: 0.7095, Test Loss: 4.2312\n",
      "Epoch: 1909, Train Loss: 0.6332, Test Loss: 4.9697\n",
      "Epoch: 1910, Train Loss: 0.8046, Test Loss: 4.1041\n",
      "Epoch: 1911, Train Loss: 0.7032, Test Loss: 3.5787\n",
      "Epoch: 1912, Train Loss: 0.7425, Test Loss: 3.9520\n",
      "Epoch: 1913, Train Loss: 0.7137, Test Loss: 4.7121\n",
      "Epoch: 1914, Train Loss: 0.7201, Test Loss: 4.5119\n",
      "Epoch: 1915, Train Loss: 0.7084, Test Loss: 3.9849\n",
      "Epoch: 1916, Train Loss: 0.8137, Test Loss: 4.3652\n",
      "Epoch: 1917, Train Loss: 0.6987, Test Loss: 4.6976\n",
      "Epoch: 1918, Train Loss: 0.8719, Test Loss: 3.8388\n",
      "Epoch: 1919, Train Loss: 0.7167, Test Loss: 3.5581\n",
      "Epoch: 1920, Train Loss: 0.7798, Test Loss: 4.3782\n",
      "Epoch: 1921, Train Loss: 0.7428, Test Loss: 4.8273\n",
      "Epoch: 1922, Train Loss: 0.8447, Test Loss: 3.8604\n",
      "Epoch: 1923, Train Loss: 0.7743, Test Loss: 3.7969\n",
      "Epoch: 1924, Train Loss: 0.8816, Test Loss: 5.3379\n",
      "Epoch: 1925, Train Loss: 0.9136, Test Loss: 4.8495\n",
      "Epoch: 1926, Train Loss: 0.8419, Test Loss: 3.7934\n",
      "Epoch: 1927, Train Loss: 0.7999, Test Loss: 3.4735\n",
      "Epoch: 1928, Train Loss: 0.6940, Test Loss: 4.2207\n",
      "Epoch: 1929, Train Loss: 0.7853, Test Loss: 4.9226\n",
      "Epoch: 1930, Train Loss: 0.8116, Test Loss: 4.0112\n",
      "Epoch: 1931, Train Loss: 0.7563, Test Loss: 3.6506\n",
      "Epoch: 1932, Train Loss: 0.7469, Test Loss: 4.0930\n",
      "Epoch: 1933, Train Loss: 0.7624, Test Loss: 4.5297\n",
      "Epoch: 1934, Train Loss: 0.7001, Test Loss: 4.1674\n",
      "Epoch: 1935, Train Loss: 0.8149, Test Loss: 3.7481\n",
      "Epoch: 1936, Train Loss: 0.7896, Test Loss: 4.3125\n",
      "Epoch: 1937, Train Loss: 0.6904, Test Loss: 4.8863\n",
      "Epoch: 1938, Train Loss: 0.7263, Test Loss: 4.5728\n",
      "Epoch: 1939, Train Loss: 0.7737, Test Loss: 3.8516\n",
      "Epoch: 1940, Train Loss: 0.7884, Test Loss: 3.7991\n",
      "Epoch: 1941, Train Loss: 0.7805, Test Loss: 4.4997\n",
      "Epoch: 1942, Train Loss: 0.8066, Test Loss: 4.3245\n",
      "Epoch: 1943, Train Loss: 0.7286, Test Loss: 4.2910\n",
      "Epoch: 1944, Train Loss: 0.7041, Test Loss: 4.3443\n",
      "Epoch: 1945, Train Loss: 0.7472, Test Loss: 3.8836\n",
      "Epoch: 1946, Train Loss: 0.6784, Test Loss: 3.6503\n",
      "Epoch: 1947, Train Loss: 0.7591, Test Loss: 4.1611\n",
      "Epoch: 1948, Train Loss: 0.7442, Test Loss: 4.8911\n",
      "Epoch: 1949, Train Loss: 0.6652, Test Loss: 4.8064\n",
      "Epoch: 1950, Train Loss: 0.7700, Test Loss: 3.8044\n",
      "Epoch: 1951, Train Loss: 0.7358, Test Loss: 3.5902\n",
      "Epoch: 1952, Train Loss: 0.7483, Test Loss: 4.0120\n",
      "Epoch: 1953, Train Loss: 0.6826, Test Loss: 4.7298\n",
      "Epoch: 1954, Train Loss: 0.7556, Test Loss: 4.3741\n",
      "Epoch: 1955, Train Loss: 0.6750, Test Loss: 3.6852\n",
      "Epoch: 1956, Train Loss: 0.7960, Test Loss: 3.8838\n",
      "Epoch: 1957, Train Loss: 0.7334, Test Loss: 4.4951\n",
      "Epoch: 1958, Train Loss: 0.7176, Test Loss: 4.3548\n",
      "Epoch: 1959, Train Loss: 0.7283, Test Loss: 3.9743\n",
      "Epoch: 1960, Train Loss: 0.6931, Test Loss: 4.0438\n",
      "Epoch: 1961, Train Loss: 0.7151, Test Loss: 4.4195\n",
      "Epoch: 1962, Train Loss: 0.6520, Test Loss: 4.1750\n",
      "Epoch: 1963, Train Loss: 0.7095, Test Loss: 3.6561\n",
      "Epoch: 1964, Train Loss: 0.7703, Test Loss: 4.0143\n",
      "Epoch: 1965, Train Loss: 0.7877, Test Loss: 5.3397\n",
      "Epoch: 1966, Train Loss: 0.8923, Test Loss: 4.4193\n",
      "Epoch: 1967, Train Loss: 0.7440, Test Loss: 3.4307\n",
      "Epoch: 1968, Train Loss: 0.7952, Test Loss: 3.5991\n",
      "Epoch: 1969, Train Loss: 0.7372, Test Loss: 4.5506\n",
      "Epoch: 1970, Train Loss: 0.7342, Test Loss: 5.0469\n",
      "Epoch: 1971, Train Loss: 0.8231, Test Loss: 4.1412\n",
      "Epoch: 1972, Train Loss: 0.6946, Test Loss: 3.5288\n",
      "Epoch: 1973, Train Loss: 0.8175, Test Loss: 3.9837\n",
      "Epoch: 1974, Train Loss: 0.6949, Test Loss: 5.0558\n",
      "Epoch: 1975, Train Loss: 0.7815, Test Loss: 4.5336\n",
      "Epoch: 1976, Train Loss: 0.6331, Test Loss: 3.7120\n",
      "Epoch: 1977, Train Loss: 0.6787, Test Loss: 3.3932\n",
      "Epoch: 1978, Train Loss: 0.9592, Test Loss: 4.4110\n",
      "Epoch: 1979, Train Loss: 0.6662, Test Loss: 5.3895\n",
      "Epoch: 1980, Train Loss: 0.9978, Test Loss: 3.7797\n",
      "Epoch: 1981, Train Loss: 0.6916, Test Loss: 3.0581\n",
      "Epoch: 1982, Train Loss: 1.2581, Test Loss: 4.1422\n",
      "Epoch: 1983, Train Loss: 0.7339, Test Loss: 5.2269\n",
      "Epoch: 1984, Train Loss: 0.8874, Test Loss: 4.2093\n",
      "Epoch: 1985, Train Loss: 0.6834, Test Loss: 3.3680\n",
      "Epoch: 1986, Train Loss: 0.8055, Test Loss: 3.4964\n",
      "Epoch: 1987, Train Loss: 0.7793, Test Loss: 4.8615\n",
      "Epoch: 1988, Train Loss: 0.6951, Test Loss: 5.8553\n",
      "Epoch: 1989, Train Loss: 1.1180, Test Loss: 3.6153\n",
      "Epoch: 1990, Train Loss: 0.6617, Test Loss: 3.0336\n",
      "Epoch: 1991, Train Loss: 1.1249, Test Loss: 3.4876\n",
      "Epoch: 1992, Train Loss: 0.7721, Test Loss: 5.7747\n",
      "Epoch: 1993, Train Loss: 1.2385, Test Loss: 4.6347\n",
      "Epoch: 1994, Train Loss: 0.7550, Test Loss: 3.3511\n",
      "Epoch: 1995, Train Loss: 0.7679, Test Loss: 3.1636\n",
      "Epoch: 1996, Train Loss: 1.0528, Test Loss: 3.7843\n",
      "Epoch: 1997, Train Loss: 0.6881, Test Loss: 5.2411\n",
      "Epoch: 1998, Train Loss: 0.9810, Test Loss: 4.3955\n",
      "Epoch: 1999, Train Loss: 0.7764, Test Loss: 3.3371\n",
      "Epoch: 2000, Train Loss: 0.8488, Test Loss: 3.3189\n",
      "Epoch: 2001, Train Loss: 0.9219, Test Loss: 4.1733\n",
      "Epoch: 2002, Train Loss: 0.6650, Test Loss: 5.5067\n",
      "Epoch: 2003, Train Loss: 1.1890, Test Loss: 3.9782\n",
      "Epoch: 2004, Train Loss: 0.8034, Test Loss: 3.0323\n",
      "Epoch: 2005, Train Loss: 1.0876, Test Loss: 3.1920\n",
      "Epoch: 2006, Train Loss: 0.8703, Test Loss: 4.7466\n",
      "Epoch: 2007, Train Loss: 0.8042, Test Loss: 5.8468\n",
      "Epoch: 2008, Train Loss: 1.0237, Test Loss: 4.0185\n",
      "Epoch: 2009, Train Loss: 0.6958, Test Loss: 3.1234\n",
      "Epoch: 2010, Train Loss: 1.1463, Test Loss: 3.3111\n",
      "Epoch: 2011, Train Loss: 0.8616, Test Loss: 4.7726\n",
      "Epoch: 2012, Train Loss: 0.8305, Test Loss: 5.4785\n",
      "Epoch: 2013, Train Loss: 1.0669, Test Loss: 3.7630\n",
      "Epoch: 2014, Train Loss: 0.6954, Test Loss: 3.0463\n",
      "Epoch: 2015, Train Loss: 1.0164, Test Loss: 3.2442\n",
      "Epoch: 2016, Train Loss: 0.9115, Test Loss: 4.8042\n",
      "Epoch: 2017, Train Loss: 0.7838, Test Loss: 5.4860\n",
      "Epoch: 2018, Train Loss: 0.9435, Test Loss: 4.0009\n",
      "Epoch: 2019, Train Loss: 0.7158, Test Loss: 3.1462\n",
      "Epoch: 2020, Train Loss: 1.0323, Test Loss: 3.4075\n",
      "Epoch: 2021, Train Loss: 0.7187, Test Loss: 4.4803\n",
      "Epoch: 2022, Train Loss: 0.7017, Test Loss: 5.2518\n",
      "Epoch: 2023, Train Loss: 0.9407, Test Loss: 4.3958\n",
      "Epoch: 2024, Train Loss: 0.7363, Test Loss: 3.3810\n",
      "Epoch: 2025, Train Loss: 0.9092, Test Loss: 3.4880\n",
      "Epoch: 2026, Train Loss: 0.7867, Test Loss: 4.1342\n",
      "Epoch: 2027, Train Loss: 0.6792, Test Loss: 4.7345\n",
      "Epoch: 2028, Train Loss: 0.7845, Test Loss: 4.0592\n",
      "Epoch: 2029, Train Loss: 0.6874, Test Loss: 3.4592\n",
      "Epoch: 2030, Train Loss: 0.7820, Test Loss: 3.5989\n",
      "Epoch: 2031, Train Loss: 0.6600, Test Loss: 4.1101\n",
      "Epoch: 2032, Train Loss: 0.7515, Test Loss: 4.3753\n",
      "Epoch: 2033, Train Loss: 0.6598, Test Loss: 4.3190\n",
      "Epoch: 2034, Train Loss: 0.6979, Test Loss: 4.0466\n",
      "Epoch: 2035, Train Loss: 0.6825, Test Loss: 4.0698\n",
      "Epoch: 2036, Train Loss: 0.6728, Test Loss: 4.1620\n",
      "Epoch: 2037, Train Loss: 0.6907, Test Loss: 3.9488\n",
      "Epoch: 2038, Train Loss: 0.7183, Test Loss: 3.8521\n",
      "Epoch: 2039, Train Loss: 0.7451, Test Loss: 4.2293\n",
      "Epoch: 2040, Train Loss: 0.6416, Test Loss: 4.3497\n",
      "Epoch: 2041, Train Loss: 0.7143, Test Loss: 4.2809\n",
      "Epoch: 2042, Train Loss: 0.6889, Test Loss: 3.8651\n",
      "Epoch: 2043, Train Loss: 0.6970, Test Loss: 3.8106\n",
      "Epoch: 2044, Train Loss: 0.7792, Test Loss: 4.3251\n",
      "Epoch: 2045, Train Loss: 0.7249, Test Loss: 4.6853\n",
      "Epoch: 2046, Train Loss: 0.8035, Test Loss: 3.8161\n",
      "Epoch: 2047, Train Loss: 0.7653, Test Loss: 3.5651\n",
      "Epoch: 2048, Train Loss: 0.8876, Test Loss: 4.1464\n",
      "Epoch: 2049, Train Loss: 0.6951, Test Loss: 4.3468\n",
      "Epoch: 2050, Train Loss: 0.6829, Test Loss: 4.2096\n",
      "Epoch: 2051, Train Loss: 0.6661, Test Loss: 4.0871\n",
      "Epoch: 2052, Train Loss: 0.6468, Test Loss: 3.9003\n",
      "Epoch: 2053, Train Loss: 0.6733, Test Loss: 4.1520\n",
      "Epoch: 2054, Train Loss: 0.6370, Test Loss: 4.4054\n",
      "Epoch: 2055, Train Loss: 0.7143, Test Loss: 4.3645\n",
      "Epoch: 2056, Train Loss: 0.7032, Test Loss: 3.8522\n",
      "Epoch: 2057, Train Loss: 0.6983, Test Loss: 3.6381\n",
      "Epoch: 2058, Train Loss: 0.7369, Test Loss: 4.3692\n",
      "Epoch: 2059, Train Loss: 0.7512, Test Loss: 4.4446\n",
      "Epoch: 2060, Train Loss: 0.7006, Test Loss: 4.0025\n",
      "Epoch: 2061, Train Loss: 0.7225, Test Loss: 3.9947\n",
      "Epoch: 2062, Train Loss: 0.6239, Test Loss: 4.2408\n",
      "Epoch: 2063, Train Loss: 0.6325, Test Loss: 4.0300\n",
      "Epoch: 2064, Train Loss: 0.6613, Test Loss: 4.1233\n",
      "Epoch: 2065, Train Loss: 0.7233, Test Loss: 4.1931\n",
      "Epoch: 2066, Train Loss: 0.6344, Test Loss: 4.2118\n",
      "Epoch: 2067, Train Loss: 0.6543, Test Loss: 4.0948\n",
      "Epoch: 2068, Train Loss: 0.6680, Test Loss: 3.8279\n",
      "Epoch: 2069, Train Loss: 0.7209, Test Loss: 3.9335\n",
      "Epoch: 2070, Train Loss: 0.6930, Test Loss: 4.6896\n",
      "Epoch: 2071, Train Loss: 0.6814, Test Loss: 5.0348\n",
      "Epoch: 2072, Train Loss: 0.8042, Test Loss: 3.9479\n",
      "Epoch: 2073, Train Loss: 0.6914, Test Loss: 3.5368\n",
      "Epoch: 2074, Train Loss: 0.7384, Test Loss: 3.8890\n",
      "Epoch: 2075, Train Loss: 0.7044, Test Loss: 4.7996\n",
      "Epoch: 2076, Train Loss: 0.7015, Test Loss: 4.7043\n",
      "Epoch: 2077, Train Loss: 0.6474, Test Loss: 4.0145\n",
      "Epoch: 2078, Train Loss: 0.7194, Test Loss: 4.2223\n",
      "Epoch: 2079, Train Loss: 0.6908, Test Loss: 4.6287\n",
      "Epoch: 2080, Train Loss: 0.6775, Test Loss: 4.4409\n",
      "Epoch: 2081, Train Loss: 0.7020, Test Loss: 4.1020\n",
      "Epoch: 2082, Train Loss: 0.7657, Test Loss: 3.7359\n",
      "Epoch: 2083, Train Loss: 0.8366, Test Loss: 4.3252\n",
      "Epoch: 2084, Train Loss: 0.7540, Test Loss: 5.7555\n",
      "Epoch: 2085, Train Loss: 0.8602, Test Loss: 4.5427\n",
      "Epoch: 2086, Train Loss: 0.7157, Test Loss: 3.5809\n",
      "Epoch: 2087, Train Loss: 0.8422, Test Loss: 3.5642\n",
      "Epoch: 2088, Train Loss: 0.8435, Test Loss: 4.4129\n",
      "Epoch: 2089, Train Loss: 0.6912, Test Loss: 4.9545\n",
      "Epoch: 2090, Train Loss: 0.7530, Test Loss: 4.1951\n",
      "Epoch: 2091, Train Loss: 0.6664, Test Loss: 3.5834\n",
      "Epoch: 2092, Train Loss: 0.8432, Test Loss: 4.0225\n",
      "Epoch: 2093, Train Loss: 0.7411, Test Loss: 5.3157\n",
      "Epoch: 2094, Train Loss: 0.8944, Test Loss: 4.5249\n",
      "Epoch: 2095, Train Loss: 0.7237, Test Loss: 3.5965\n",
      "Epoch: 2096, Train Loss: 0.7598, Test Loss: 3.6822\n",
      "Epoch: 2097, Train Loss: 0.7319, Test Loss: 4.4710\n",
      "Epoch: 2098, Train Loss: 0.7692, Test Loss: 4.5738\n",
      "Epoch: 2099, Train Loss: 0.7283, Test Loss: 3.8756\n",
      "Epoch: 2100, Train Loss: 0.6588, Test Loss: 3.6720\n",
      "Epoch: 2101, Train Loss: 0.7089, Test Loss: 4.1115\n",
      "Epoch: 2102, Train Loss: 0.6790, Test Loss: 4.2074\n",
      "Epoch: 2103, Train Loss: 0.6553, Test Loss: 4.1740\n",
      "Epoch: 2104, Train Loss: 0.7194, Test Loss: 4.1518\n",
      "Epoch: 2105, Train Loss: 0.6927, Test Loss: 4.3599\n",
      "Epoch: 2106, Train Loss: 0.7453, Test Loss: 4.3068\n",
      "Epoch: 2107, Train Loss: 0.6495, Test Loss: 3.9906\n",
      "Epoch: 2108, Train Loss: 0.6944, Test Loss: 3.8201\n",
      "Epoch: 2109, Train Loss: 0.6824, Test Loss: 4.3604\n",
      "Epoch: 2110, Train Loss: 0.6599, Test Loss: 4.8976\n",
      "Epoch: 2111, Train Loss: 0.6728, Test Loss: 4.2470\n",
      "Epoch: 2112, Train Loss: 0.6331, Test Loss: 3.8040\n",
      "Epoch: 2113, Train Loss: 0.6722, Test Loss: 4.1148\n",
      "Epoch: 2114, Train Loss: 0.6461, Test Loss: 4.3372\n",
      "Epoch: 2115, Train Loss: 0.7048, Test Loss: 4.3595\n",
      "Epoch: 2116, Train Loss: 0.7398, Test Loss: 4.1834\n",
      "Epoch: 2117, Train Loss: 0.6223, Test Loss: 4.0890\n",
      "Epoch: 2118, Train Loss: 0.6966, Test Loss: 4.6167\n",
      "Epoch: 2119, Train Loss: 0.6842, Test Loss: 4.6132\n",
      "Epoch: 2120, Train Loss: 0.7363, Test Loss: 3.7960\n",
      "Epoch: 2121, Train Loss: 0.7848, Test Loss: 3.6449\n",
      "Epoch: 2122, Train Loss: 0.7263, Test Loss: 3.9539\n",
      "Epoch: 2123, Train Loss: 0.6397, Test Loss: 4.7531\n",
      "Epoch: 2124, Train Loss: 0.7563, Test Loss: 4.3036\n",
      "Epoch: 2125, Train Loss: 0.6917, Test Loss: 3.7352\n",
      "Epoch: 2126, Train Loss: 0.7314, Test Loss: 4.0053\n",
      "Epoch: 2127, Train Loss: 0.6706, Test Loss: 4.4197\n",
      "Epoch: 2128, Train Loss: 0.6805, Test Loss: 4.6371\n",
      "Epoch: 2129, Train Loss: 0.7230, Test Loss: 4.0049\n",
      "Epoch: 2130, Train Loss: 0.6782, Test Loss: 3.8718\n",
      "Epoch: 2131, Train Loss: 0.6929, Test Loss: 4.2535\n",
      "Epoch: 2132, Train Loss: 0.6619, Test Loss: 4.1274\n",
      "Epoch: 2133, Train Loss: 0.6756, Test Loss: 3.7821\n",
      "Epoch: 2134, Train Loss: 0.6820, Test Loss: 3.8869\n",
      "Epoch: 2135, Train Loss: 0.6901, Test Loss: 4.2242\n",
      "Epoch: 2136, Train Loss: 0.7316, Test Loss: 4.4044\n",
      "Epoch: 2137, Train Loss: 0.6692, Test Loss: 4.1452\n",
      "Epoch: 2138, Train Loss: 0.6973, Test Loss: 4.1172\n",
      "Epoch: 2139, Train Loss: 0.6696, Test Loss: 4.0453\n",
      "Epoch: 2140, Train Loss: 0.6679, Test Loss: 4.5467\n",
      "Epoch: 2141, Train Loss: 0.6729, Test Loss: 5.0279\n",
      "Epoch: 2142, Train Loss: 0.7543, Test Loss: 4.3732\n",
      "Epoch: 2143, Train Loss: 0.7099, Test Loss: 3.5646\n",
      "Epoch: 2144, Train Loss: 0.7571, Test Loss: 3.8533\n",
      "Epoch: 2145, Train Loss: 0.6279, Test Loss: 4.2697\n",
      "Epoch: 2146, Train Loss: 0.6453, Test Loss: 4.5025\n",
      "Epoch: 2147, Train Loss: 0.6725, Test Loss: 4.5419\n",
      "Epoch: 2148, Train Loss: 0.6469, Test Loss: 4.0254\n",
      "Epoch: 2149, Train Loss: 0.6131, Test Loss: 3.9184\n",
      "Epoch: 2150, Train Loss: 0.6988, Test Loss: 4.4471\n",
      "Epoch: 2151, Train Loss: 0.6846, Test Loss: 4.6568\n",
      "Epoch: 2152, Train Loss: 0.7195, Test Loss: 3.8190\n",
      "Epoch: 2153, Train Loss: 0.6630, Test Loss: 3.6143\n",
      "Epoch: 2154, Train Loss: 0.8212, Test Loss: 4.5858\n",
      "Epoch: 2155, Train Loss: 0.6674, Test Loss: 5.0174\n",
      "Epoch: 2156, Train Loss: 0.6573, Test Loss: 4.7033\n",
      "Epoch: 2157, Train Loss: 0.6391, Test Loss: 4.2100\n",
      "Epoch: 2158, Train Loss: 0.6961, Test Loss: 3.8582\n",
      "Epoch: 2159, Train Loss: 0.7109, Test Loss: 4.0082\n",
      "Epoch: 2160, Train Loss: 0.6229, Test Loss: 4.7226\n",
      "Epoch: 2161, Train Loss: 0.6555, Test Loss: 4.3951\n",
      "Epoch: 2162, Train Loss: 0.6515, Test Loss: 3.7990\n",
      "Epoch: 2163, Train Loss: 0.6500, Test Loss: 3.8877\n",
      "Epoch: 2164, Train Loss: 0.6680, Test Loss: 4.6971\n",
      "Epoch: 2165, Train Loss: 0.6484, Test Loss: 4.9002\n",
      "Epoch: 2166, Train Loss: 0.6959, Test Loss: 4.1193\n",
      "Epoch: 2167, Train Loss: 0.6690, Test Loss: 3.5696\n",
      "Epoch: 2168, Train Loss: 0.7966, Test Loss: 4.1617\n",
      "Epoch: 2169, Train Loss: 0.6379, Test Loss: 4.8636\n",
      "Epoch: 2170, Train Loss: 0.6693, Test Loss: 4.5622\n",
      "Epoch: 2171, Train Loss: 0.6631, Test Loss: 3.7371\n",
      "Epoch: 2172, Train Loss: 0.6598, Test Loss: 3.8707\n",
      "Epoch: 2173, Train Loss: 0.7041, Test Loss: 4.3042\n",
      "Epoch: 2174, Train Loss: 0.7095, Test Loss: 4.3818\n",
      "Epoch: 2175, Train Loss: 0.6669, Test Loss: 4.1634\n",
      "Epoch: 2176, Train Loss: 0.6845, Test Loss: 3.8609\n",
      "Epoch: 2177, Train Loss: 0.7257, Test Loss: 4.3698\n",
      "Epoch: 2178, Train Loss: 0.6819, Test Loss: 4.4353\n",
      "Epoch: 2179, Train Loss: 0.6211, Test Loss: 4.5021\n",
      "Epoch: 2180, Train Loss: 0.6954, Test Loss: 4.1060\n",
      "Epoch: 2181, Train Loss: 0.6091, Test Loss: 3.9804\n",
      "Epoch: 2182, Train Loss: 0.6238, Test Loss: 4.4133\n",
      "Epoch: 2183, Train Loss: 0.6423, Test Loss: 4.4932\n",
      "Epoch: 2184, Train Loss: 0.6566, Test Loss: 4.6647\n",
      "Epoch: 2185, Train Loss: 0.7316, Test Loss: 3.8827\n",
      "Epoch: 2186, Train Loss: 0.6776, Test Loss: 3.9079\n",
      "Epoch: 2187, Train Loss: 0.6545, Test Loss: 4.3206\n",
      "Epoch: 2188, Train Loss: 0.7016, Test Loss: 4.6443\n",
      "Epoch: 2189, Train Loss: 0.7036, Test Loss: 4.2401\n",
      "Epoch: 2190, Train Loss: 0.6767, Test Loss: 3.8646\n",
      "Epoch: 2191, Train Loss: 0.6901, Test Loss: 4.1675\n",
      "Epoch: 2192, Train Loss: 0.6538, Test Loss: 4.7460\n",
      "Epoch: 2193, Train Loss: 0.6579, Test Loss: 4.5918\n",
      "Epoch: 2194, Train Loss: 0.7052, Test Loss: 3.9855\n",
      "Epoch: 2195, Train Loss: 0.7305, Test Loss: 4.3053\n",
      "Epoch: 2196, Train Loss: 0.6533, Test Loss: 4.6108\n",
      "Epoch: 2197, Train Loss: 0.6867, Test Loss: 4.1477\n",
      "Epoch: 2198, Train Loss: 0.6317, Test Loss: 3.8198\n",
      "Epoch: 2199, Train Loss: 0.6983, Test Loss: 4.0188\n",
      "Epoch: 2200, Train Loss: 0.6629, Test Loss: 4.4717\n",
      "Epoch: 2201, Train Loss: 0.6679, Test Loss: 4.1883\n",
      "Epoch: 2202, Train Loss: 0.6852, Test Loss: 3.8321\n",
      "Epoch: 2203, Train Loss: 0.7390, Test Loss: 4.0378\n",
      "Epoch: 2204, Train Loss: 0.6730, Test Loss: 4.6608\n",
      "Epoch: 2205, Train Loss: 0.7468, Test Loss: 4.1801\n",
      "Epoch: 2206, Train Loss: 0.6531, Test Loss: 3.9621\n",
      "Epoch: 2207, Train Loss: 0.6331, Test Loss: 4.1407\n",
      "Epoch: 2208, Train Loss: 0.6827, Test Loss: 4.1287\n",
      "Epoch: 2209, Train Loss: 0.6173, Test Loss: 4.2089\n",
      "Epoch: 2210, Train Loss: 0.6596, Test Loss: 4.5774\n",
      "Epoch: 2211, Train Loss: 0.6936, Test Loss: 4.5347\n",
      "Epoch: 2212, Train Loss: 0.6624, Test Loss: 3.9912\n",
      "Epoch: 2213, Train Loss: 0.6791, Test Loss: 3.8265\n",
      "Epoch: 2214, Train Loss: 0.7422, Test Loss: 4.3427\n",
      "Epoch: 2215, Train Loss: 0.6076, Test Loss: 4.6333\n",
      "Epoch: 2216, Train Loss: 0.8134, Test Loss: 4.0437\n",
      "Epoch: 2217, Train Loss: 0.7742, Test Loss: 4.2547\n",
      "Epoch: 2218, Train Loss: 0.6967, Test Loss: 4.4352\n",
      "Epoch: 2219, Train Loss: 0.7209, Test Loss: 4.3649\n",
      "Epoch: 2220, Train Loss: 0.7011, Test Loss: 4.4197\n",
      "Epoch: 2221, Train Loss: 0.6784, Test Loss: 4.0793\n",
      "Epoch: 2222, Train Loss: 0.6902, Test Loss: 4.1576\n",
      "Epoch: 2223, Train Loss: 0.6966, Test Loss: 4.7538\n",
      "Epoch: 2224, Train Loss: 0.7039, Test Loss: 4.4850\n",
      "Epoch: 2225, Train Loss: 0.7498, Test Loss: 4.0654\n",
      "Epoch: 2226, Train Loss: 0.6810, Test Loss: 4.3114\n",
      "Epoch: 2227, Train Loss: 0.6535, Test Loss: 4.4479\n",
      "Epoch: 2228, Train Loss: 0.7451, Test Loss: 4.4540\n",
      "Epoch: 2229, Train Loss: 0.7699, Test Loss: 3.9790\n",
      "Epoch: 2230, Train Loss: 0.7175, Test Loss: 3.9612\n",
      "Epoch: 2231, Train Loss: 0.7188, Test Loss: 4.4171\n",
      "Epoch: 2232, Train Loss: 0.6990, Test Loss: 5.2482\n",
      "Epoch: 2233, Train Loss: 0.8311, Test Loss: 4.1483\n",
      "Epoch: 2234, Train Loss: 0.6386, Test Loss: 3.3637\n",
      "Epoch: 2235, Train Loss: 0.9058, Test Loss: 3.9871\n",
      "Epoch: 2236, Train Loss: 0.7067, Test Loss: 5.0507\n",
      "Epoch: 2237, Train Loss: 0.8164, Test Loss: 4.6109\n",
      "Epoch: 2238, Train Loss: 0.6300, Test Loss: 3.9342\n",
      "Epoch: 2239, Train Loss: 0.7465, Test Loss: 4.2787\n",
      "Epoch: 2240, Train Loss: 0.6866, Test Loss: 5.0041\n",
      "Epoch: 2241, Train Loss: 0.7488, Test Loss: 4.3984\n",
      "Epoch: 2242, Train Loss: 0.7330, Test Loss: 4.0719\n",
      "Epoch: 2243, Train Loss: 0.7008, Test Loss: 4.0746\n",
      "Epoch: 2244, Train Loss: 0.6642, Test Loss: 4.6735\n",
      "Epoch: 2245, Train Loss: 0.7658, Test Loss: 4.3398\n",
      "Epoch: 2246, Train Loss: 0.6379, Test Loss: 3.9543\n",
      "Epoch: 2247, Train Loss: 0.6982, Test Loss: 4.4989\n",
      "Epoch: 2248, Train Loss: 0.6675, Test Loss: 4.7795\n",
      "Epoch: 2249, Train Loss: 0.6846, Test Loss: 4.4548\n",
      "Epoch: 2250, Train Loss: 0.6322, Test Loss: 3.8378\n",
      "Epoch: 2251, Train Loss: 0.6400, Test Loss: 3.7919\n",
      "Epoch: 2252, Train Loss: 0.6971, Test Loss: 4.5852\n",
      "Epoch: 2253, Train Loss: 0.6559, Test Loss: 5.0363\n",
      "Epoch: 2254, Train Loss: 0.7105, Test Loss: 4.1436\n",
      "Epoch: 2255, Train Loss: 0.6892, Test Loss: 3.4483\n",
      "Epoch: 2256, Train Loss: 0.8363, Test Loss: 4.0348\n",
      "Epoch: 2257, Train Loss: 0.6508, Test Loss: 5.1754\n",
      "Epoch: 2258, Train Loss: 0.7695, Test Loss: 4.7175\n",
      "Epoch: 2259, Train Loss: 0.6951, Test Loss: 3.5593\n",
      "Epoch: 2260, Train Loss: 0.7278, Test Loss: 3.6067\n",
      "Epoch: 2261, Train Loss: 0.8424, Test Loss: 5.0762\n",
      "Epoch: 2262, Train Loss: 0.6866, Test Loss: 5.6657\n",
      "Epoch: 2263, Train Loss: 0.9809, Test Loss: 3.7822\n",
      "Epoch: 2264, Train Loss: 0.6889, Test Loss: 3.2229\n",
      "Epoch: 2265, Train Loss: 0.8647, Test Loss: 3.8963\n",
      "Epoch: 2266, Train Loss: 0.6440, Test Loss: 5.3610\n",
      "Epoch: 2267, Train Loss: 0.8236, Test Loss: 4.7645\n",
      "Epoch: 2268, Train Loss: 0.7383, Test Loss: 3.4258\n",
      "Epoch: 2269, Train Loss: 0.8235, Test Loss: 3.4613\n",
      "Epoch: 2270, Train Loss: 0.7271, Test Loss: 4.1053\n",
      "Epoch: 2271, Train Loss: 0.6316, Test Loss: 4.6986\n",
      "Epoch: 2272, Train Loss: 0.7457, Test Loss: 4.2388\n",
      "Epoch: 2273, Train Loss: 0.6428, Test Loss: 3.5052\n",
      "Epoch: 2274, Train Loss: 0.8199, Test Loss: 3.6962\n",
      "Epoch: 2275, Train Loss: 0.7076, Test Loss: 4.4928\n",
      "Epoch: 2276, Train Loss: 0.6671, Test Loss: 4.6955\n",
      "Epoch: 2277, Train Loss: 0.7249, Test Loss: 3.7256\n",
      "Epoch: 2278, Train Loss: 0.6429, Test Loss: 3.4238\n",
      "Epoch: 2279, Train Loss: 0.7733, Test Loss: 4.1355\n",
      "Epoch: 2280, Train Loss: 0.6212, Test Loss: 5.1193\n",
      "Epoch: 2281, Train Loss: 0.8104, Test Loss: 4.3328\n",
      "Epoch: 2282, Train Loss: 0.7469, Test Loss: 3.3331\n",
      "Epoch: 2283, Train Loss: 0.8221, Test Loss: 3.4740\n",
      "Epoch: 2284, Train Loss: 0.7097, Test Loss: 4.5305\n",
      "Epoch: 2285, Train Loss: 0.7152, Test Loss: 4.6699\n",
      "Epoch: 2286, Train Loss: 0.7151, Test Loss: 4.0072\n",
      "Epoch: 2287, Train Loss: 0.6461, Test Loss: 3.4246\n",
      "Epoch: 2288, Train Loss: 0.8092, Test Loss: 3.9641\n",
      "Epoch: 2289, Train Loss: 0.6688, Test Loss: 4.8687\n",
      "Epoch: 2290, Train Loss: 0.7117, Test Loss: 4.6200\n",
      "Epoch: 2291, Train Loss: 0.7975, Test Loss: 3.5711\n",
      "Epoch: 2292, Train Loss: 0.7451, Test Loss: 3.6430\n",
      "Epoch: 2293, Train Loss: 0.6862, Test Loss: 4.4436\n",
      "Epoch: 2294, Train Loss: 0.6193, Test Loss: 4.7008\n",
      "Epoch: 2295, Train Loss: 0.7401, Test Loss: 3.7262\n",
      "Epoch: 2296, Train Loss: 0.7342, Test Loss: 3.6603\n",
      "Epoch: 2297, Train Loss: 0.7210, Test Loss: 4.3093\n",
      "Epoch: 2298, Train Loss: 0.6686, Test Loss: 4.5757\n",
      "Epoch: 2299, Train Loss: 0.6593, Test Loss: 3.8895\n",
      "Epoch: 2300, Train Loss: 0.6132, Test Loss: 3.5948\n",
      "Epoch: 2301, Train Loss: 0.7757, Test Loss: 4.4315\n",
      "Epoch: 2302, Train Loss: 0.6302, Test Loss: 4.8967\n",
      "Epoch: 2303, Train Loss: 0.7644, Test Loss: 3.9578\n",
      "Epoch: 2304, Train Loss: 0.6520, Test Loss: 3.2798\n",
      "Epoch: 2305, Train Loss: 0.7721, Test Loss: 3.6639\n",
      "Epoch: 2306, Train Loss: 0.6347, Test Loss: 5.0651\n",
      "Epoch: 2307, Train Loss: 0.8247, Test Loss: 4.6218\n",
      "Epoch: 2308, Train Loss: 0.7317, Test Loss: 3.5695\n",
      "Epoch: 2309, Train Loss: 0.6572, Test Loss: 3.4283\n",
      "Epoch: 2310, Train Loss: 0.7968, Test Loss: 4.3045\n",
      "Epoch: 2311, Train Loss: 0.7883, Test Loss: 4.7819\n",
      "Epoch: 2312, Train Loss: 0.6522, Test Loss: 4.2264\n",
      "Epoch: 2313, Train Loss: 0.6384, Test Loss: 3.7487\n",
      "Epoch: 2314, Train Loss: 0.6613, Test Loss: 3.6397\n",
      "Epoch: 2315, Train Loss: 0.7495, Test Loss: 4.3846\n",
      "Epoch: 2316, Train Loss: 0.6165, Test Loss: 4.9427\n",
      "Epoch: 2317, Train Loss: 0.6627, Test Loss: 4.3113\n",
      "Epoch: 2318, Train Loss: 0.6203, Test Loss: 3.7329\n",
      "Epoch: 2319, Train Loss: 0.7345, Test Loss: 4.0750\n",
      "Epoch: 2320, Train Loss: 0.6390, Test Loss: 4.7316\n",
      "Epoch: 2321, Train Loss: 0.6965, Test Loss: 4.3190\n",
      "Epoch: 2322, Train Loss: 0.7062, Test Loss: 3.7168\n",
      "Epoch: 2323, Train Loss: 0.6376, Test Loss: 3.8705\n",
      "Epoch: 2324, Train Loss: 0.6838, Test Loss: 4.4150\n",
      "Epoch: 2325, Train Loss: 0.6694, Test Loss: 4.9496\n",
      "Epoch: 2326, Train Loss: 0.6887, Test Loss: 4.4563\n",
      "Epoch: 2327, Train Loss: 0.6116, Test Loss: 3.7654\n",
      "Epoch: 2328, Train Loss: 0.6804, Test Loss: 3.8553\n",
      "Epoch: 2329, Train Loss: 0.7188, Test Loss: 4.3993\n",
      "Epoch: 2330, Train Loss: 0.6570, Test Loss: 4.4021\n",
      "Epoch: 2331, Train Loss: 0.6428, Test Loss: 4.0246\n",
      "Epoch: 2332, Train Loss: 0.6326, Test Loss: 3.6935\n",
      "Epoch: 2333, Train Loss: 0.6406, Test Loss: 4.0415\n",
      "Epoch: 2334, Train Loss: 0.6470, Test Loss: 4.6330\n",
      "Epoch: 2335, Train Loss: 0.7049, Test Loss: 4.3591\n",
      "Epoch: 2336, Train Loss: 0.6490, Test Loss: 3.7375\n",
      "Epoch: 2337, Train Loss: 0.6440, Test Loss: 3.9587\n",
      "Epoch: 2338, Train Loss: 0.6127, Test Loss: 4.7629\n",
      "Epoch: 2339, Train Loss: 0.6341, Test Loss: 4.5572\n",
      "Epoch: 2340, Train Loss: 0.6232, Test Loss: 3.8214\n",
      "Epoch: 2341, Train Loss: 0.6013, Test Loss: 3.5834\n",
      "Epoch: 2342, Train Loss: 0.7303, Test Loss: 4.5609\n",
      "Epoch: 2343, Train Loss: 0.6269, Test Loss: 4.8596\n",
      "Epoch: 2344, Train Loss: 0.7284, Test Loss: 3.7756\n",
      "Epoch: 2345, Train Loss: 0.6381, Test Loss: 3.4442\n",
      "Epoch: 2346, Train Loss: 0.8303, Test Loss: 4.2940\n",
      "Epoch: 2347, Train Loss: 0.6793, Test Loss: 4.7823\n",
      "Epoch: 2348, Train Loss: 0.6722, Test Loss: 4.3178\n",
      "Epoch: 2349, Train Loss: 0.6190, Test Loss: 3.7029\n",
      "Epoch: 2350, Train Loss: 0.7889, Test Loss: 4.2148\n",
      "Epoch: 2351, Train Loss: 0.6529, Test Loss: 5.0080\n",
      "Epoch: 2352, Train Loss: 0.6909, Test Loss: 4.6896\n",
      "Epoch: 2353, Train Loss: 0.6564, Test Loss: 3.7232\n",
      "Epoch: 2354, Train Loss: 0.6586, Test Loss: 3.5331\n",
      "Epoch: 2355, Train Loss: 0.7166, Test Loss: 4.1816\n",
      "Epoch: 2356, Train Loss: 0.6005, Test Loss: 5.4246\n",
      "Epoch: 2357, Train Loss: 0.7842, Test Loss: 4.4013\n",
      "Epoch: 2358, Train Loss: 0.6240, Test Loss: 3.5758\n",
      "Epoch: 2359, Train Loss: 0.7419, Test Loss: 3.7434\n",
      "Epoch: 2360, Train Loss: 0.6509, Test Loss: 4.4727\n",
      "Epoch: 2361, Train Loss: 0.7045, Test Loss: 4.2679\n",
      "Epoch: 2362, Train Loss: 0.6244, Test Loss: 3.9362\n",
      "Epoch: 2363, Train Loss: 0.6754, Test Loss: 4.0775\n",
      "Epoch: 2364, Train Loss: 0.6378, Test Loss: 4.1710\n",
      "Epoch: 2365, Train Loss: 0.6554, Test Loss: 4.1180\n",
      "Epoch: 2366, Train Loss: 0.6775, Test Loss: 4.1677\n",
      "Epoch: 2367, Train Loss: 0.7019, Test Loss: 4.0300\n",
      "Epoch: 2368, Train Loss: 0.6337, Test Loss: 4.3422\n",
      "Epoch: 2369, Train Loss: 0.6073, Test Loss: 4.2739\n",
      "Epoch: 2370, Train Loss: 0.6113, Test Loss: 4.3193\n",
      "Epoch: 2371, Train Loss: 0.5998, Test Loss: 4.0659\n",
      "Epoch: 2372, Train Loss: 0.6240, Test Loss: 4.1209\n",
      "Epoch: 2373, Train Loss: 0.6440, Test Loss: 4.2056\n",
      "Epoch: 2374, Train Loss: 0.6503, Test Loss: 4.0668\n",
      "Epoch: 2375, Train Loss: 0.6747, Test Loss: 3.7634\n",
      "Epoch: 2376, Train Loss: 0.6680, Test Loss: 4.2695\n",
      "Epoch: 2377, Train Loss: 0.6640, Test Loss: 4.5812\n",
      "Epoch: 2378, Train Loss: 0.6413, Test Loss: 4.2740\n",
      "Epoch: 2379, Train Loss: 0.6254, Test Loss: 3.7137\n",
      "Epoch: 2380, Train Loss: 0.6792, Test Loss: 3.8352\n",
      "Epoch: 2381, Train Loss: 0.6462, Test Loss: 4.7281\n",
      "Epoch: 2382, Train Loss: 0.7085, Test Loss: 4.5067\n",
      "Epoch: 2383, Train Loss: 0.6098, Test Loss: 3.9271\n",
      "Epoch: 2384, Train Loss: 0.6569, Test Loss: 3.7568\n",
      "Epoch: 2385, Train Loss: 0.6228, Test Loss: 4.0370\n",
      "Epoch: 2386, Train Loss: 0.6157, Test Loss: 4.2854\n",
      "Epoch: 2387, Train Loss: 0.6530, Test Loss: 4.4242\n",
      "Epoch: 2388, Train Loss: 0.6191, Test Loss: 4.3246\n",
      "Epoch: 2389, Train Loss: 0.6382, Test Loss: 4.0986\n",
      "Epoch: 2390, Train Loss: 0.6042, Test Loss: 3.9521\n",
      "Epoch: 2391, Train Loss: 0.6224, Test Loss: 3.9920\n",
      "Epoch: 2392, Train Loss: 0.6190, Test Loss: 4.2235\n",
      "Epoch: 2393, Train Loss: 0.6354, Test Loss: 4.1060\n",
      "Epoch: 2394, Train Loss: 0.6465, Test Loss: 3.6935\n",
      "Epoch: 2395, Train Loss: 0.6323, Test Loss: 3.8518\n",
      "Epoch: 2396, Train Loss: 0.6634, Test Loss: 4.3249\n",
      "Epoch: 2397, Train Loss: 0.7107, Test Loss: 4.1036\n",
      "Epoch: 2398, Train Loss: 0.6343, Test Loss: 4.0560\n",
      "Epoch: 2399, Train Loss: 0.6683, Test Loss: 4.3987\n",
      "Epoch: 2400, Train Loss: 0.6429, Test Loss: 4.5326\n",
      "Epoch: 2401, Train Loss: 0.6298, Test Loss: 4.0799\n",
      "Epoch: 2402, Train Loss: 0.6362, Test Loss: 4.0056\n",
      "Epoch: 2403, Train Loss: 0.6614, Test Loss: 3.9978\n",
      "Epoch: 2404, Train Loss: 0.5967, Test Loss: 4.0348\n",
      "Epoch: 2405, Train Loss: 0.6298, Test Loss: 3.9794\n",
      "Epoch: 2406, Train Loss: 0.7619, Test Loss: 4.7997\n",
      "Epoch: 2407, Train Loss: 0.6635, Test Loss: 4.6334\n",
      "Epoch: 2408, Train Loss: 0.6594, Test Loss: 3.6313\n",
      "Epoch: 2409, Train Loss: 0.8200, Test Loss: 3.6822\n",
      "Epoch: 2410, Train Loss: 0.6743, Test Loss: 4.1917\n",
      "Epoch: 2411, Train Loss: 0.5801, Test Loss: 4.9497\n",
      "Epoch: 2412, Train Loss: 0.6969, Test Loss: 4.3931\n",
      "Epoch: 2413, Train Loss: 0.6600, Test Loss: 3.6469\n",
      "Epoch: 2414, Train Loss: 0.6766, Test Loss: 3.7025\n",
      "Epoch: 2415, Train Loss: 0.6897, Test Loss: 4.4154\n",
      "Epoch: 2416, Train Loss: 0.6287, Test Loss: 4.8860\n",
      "Epoch: 2417, Train Loss: 0.6343, Test Loss: 4.3792\n",
      "Epoch: 2418, Train Loss: 0.6157, Test Loss: 3.7680\n",
      "Epoch: 2419, Train Loss: 0.6635, Test Loss: 3.6451\n",
      "Epoch: 2420, Train Loss: 0.6640, Test Loss: 3.9857\n",
      "Epoch: 2421, Train Loss: 0.6351, Test Loss: 4.4949\n",
      "Epoch: 2422, Train Loss: 0.6421, Test Loss: 4.6619\n",
      "Epoch: 2423, Train Loss: 0.7334, Test Loss: 4.1880\n",
      "Epoch: 2424, Train Loss: 0.6189, Test Loss: 3.8212\n",
      "Epoch: 2425, Train Loss: 0.6670, Test Loss: 4.1953\n",
      "Epoch: 2426, Train Loss: 0.6026, Test Loss: 4.4421\n",
      "Epoch: 2427, Train Loss: 0.6699, Test Loss: 4.3273\n",
      "Epoch: 2428, Train Loss: 0.6540, Test Loss: 3.6292\n",
      "Epoch: 2429, Train Loss: 0.6362, Test Loss: 3.7574\n",
      "Epoch: 2430, Train Loss: 0.5908, Test Loss: 4.0970\n",
      "Epoch: 2431, Train Loss: 0.5974, Test Loss: 4.4603\n",
      "Epoch: 2432, Train Loss: 0.6399, Test Loss: 4.3540\n",
      "Epoch: 2433, Train Loss: 0.6464, Test Loss: 3.8535\n",
      "Epoch: 2434, Train Loss: 0.6632, Test Loss: 4.1568\n",
      "Epoch: 2435, Train Loss: 0.5999, Test Loss: 4.2392\n",
      "Epoch: 2436, Train Loss: 0.6358, Test Loss: 3.9419\n",
      "Epoch: 2437, Train Loss: 0.5749, Test Loss: 3.9528\n",
      "Epoch: 2438, Train Loss: 0.6126, Test Loss: 3.9733\n",
      "Epoch: 2439, Train Loss: 0.6605, Test Loss: 4.9874\n",
      "Epoch: 2440, Train Loss: 0.7201, Test Loss: 4.6427\n",
      "Epoch: 2441, Train Loss: 0.6598, Test Loss: 3.8438\n",
      "Epoch: 2442, Train Loss: 0.6606, Test Loss: 3.8450\n",
      "Epoch: 2443, Train Loss: 0.6565, Test Loss: 3.9761\n",
      "Epoch: 2444, Train Loss: 0.6439, Test Loss: 4.7669\n",
      "Epoch: 2445, Train Loss: 0.6561, Test Loss: 4.6727\n",
      "Epoch: 2446, Train Loss: 0.6592, Test Loss: 3.7889\n",
      "Epoch: 2447, Train Loss: 0.6499, Test Loss: 3.5961\n",
      "Epoch: 2448, Train Loss: 0.7337, Test Loss: 4.6563\n",
      "Epoch: 2449, Train Loss: 0.6046, Test Loss: 5.2835\n",
      "Epoch: 2450, Train Loss: 0.8811, Test Loss: 3.6481\n",
      "Epoch: 2451, Train Loss: 0.6447, Test Loss: 3.3432\n",
      "Epoch: 2452, Train Loss: 0.8815, Test Loss: 4.4531\n",
      "Epoch: 2453, Train Loss: 0.5910, Test Loss: 5.5903\n",
      "Epoch: 2454, Train Loss: 0.8011, Test Loss: 4.3159\n",
      "Epoch: 2455, Train Loss: 0.6001, Test Loss: 3.6431\n",
      "Epoch: 2456, Train Loss: 0.6491, Test Loss: 3.7575\n",
      "Epoch: 2457, Train Loss: 0.6260, Test Loss: 4.5954\n",
      "Epoch: 2458, Train Loss: 0.5830, Test Loss: 5.3984\n",
      "Epoch: 2459, Train Loss: 0.8554, Test Loss: 3.8840\n",
      "Epoch: 2460, Train Loss: 0.6390, Test Loss: 3.5255\n",
      "Epoch: 2461, Train Loss: 0.7200, Test Loss: 3.9515\n",
      "Epoch: 2462, Train Loss: 0.6093, Test Loss: 4.7098\n",
      "Epoch: 2463, Train Loss: 0.7414, Test Loss: 4.3080\n",
      "Epoch: 2464, Train Loss: 0.7076, Test Loss: 3.6320\n",
      "Epoch: 2465, Train Loss: 0.6998, Test Loss: 3.9156\n",
      "Epoch: 2466, Train Loss: 0.6723, Test Loss: 4.4234\n",
      "Epoch: 2467, Train Loss: 0.6548, Test Loss: 4.7205\n",
      "Epoch: 2468, Train Loss: 0.6790, Test Loss: 4.1253\n",
      "Epoch: 2469, Train Loss: 0.6913, Test Loss: 4.0826\n",
      "Epoch: 2470, Train Loss: 0.6840, Test Loss: 3.9358\n",
      "Epoch: 2471, Train Loss: 0.6227, Test Loss: 4.1081\n",
      "Epoch: 2472, Train Loss: 0.6037, Test Loss: 4.5494\n",
      "Epoch: 2473, Train Loss: 0.6422, Test Loss: 4.7531\n",
      "Epoch: 2474, Train Loss: 0.6651, Test Loss: 4.5700\n",
      "Epoch: 2475, Train Loss: 0.6420, Test Loss: 3.6820\n",
      "Epoch: 2476, Train Loss: 0.6848, Test Loss: 3.8528\n",
      "Epoch: 2477, Train Loss: 0.6994, Test Loss: 5.0386\n",
      "Epoch: 2478, Train Loss: 0.7408, Test Loss: 5.0751\n",
      "Epoch: 2479, Train Loss: 0.7328, Test Loss: 3.7632\n",
      "Epoch: 2480, Train Loss: 0.6506, Test Loss: 3.6084\n",
      "Epoch: 2481, Train Loss: 0.7506, Test Loss: 4.7159\n",
      "Epoch: 2482, Train Loss: 0.7297, Test Loss: 4.5191\n",
      "Epoch: 2483, Train Loss: 0.6387, Test Loss: 4.1821\n",
      "Epoch: 2484, Train Loss: 0.6147, Test Loss: 3.9387\n",
      "Epoch: 2485, Train Loss: 0.6300, Test Loss: 4.2429\n",
      "Epoch: 2486, Train Loss: 0.6392, Test Loss: 4.2023\n",
      "Epoch: 2487, Train Loss: 0.5897, Test Loss: 4.2851\n",
      "Epoch: 2488, Train Loss: 0.6257, Test Loss: 4.0053\n",
      "Epoch: 2489, Train Loss: 0.6565, Test Loss: 4.0429\n",
      "Epoch: 2490, Train Loss: 0.6484, Test Loss: 4.5457\n",
      "Epoch: 2491, Train Loss: 0.6256, Test Loss: 4.5306\n",
      "Epoch: 2492, Train Loss: 0.6640, Test Loss: 3.9705\n",
      "Epoch: 2493, Train Loss: 0.6328, Test Loss: 3.6436\n",
      "Epoch: 2494, Train Loss: 0.6988, Test Loss: 4.5036\n",
      "Epoch: 2495, Train Loss: 0.6077, Test Loss: 5.4273\n",
      "Epoch: 2496, Train Loss: 0.7798, Test Loss: 4.1889\n",
      "Epoch: 2497, Train Loss: 0.6436, Test Loss: 3.5096\n",
      "Epoch: 2498, Train Loss: 0.6534, Test Loss: 3.6229\n",
      "Epoch: 2499, Train Loss: 0.7120, Test Loss: 4.4942\n",
      "Epoch: 2500, Train Loss: 0.6333, Test Loss: 5.1347\n",
      "Epoch: 2501, Train Loss: 0.7163, Test Loss: 4.0817\n",
      "Epoch: 2502, Train Loss: 0.6585, Test Loss: 3.4301\n",
      "Epoch: 2503, Train Loss: 0.7777, Test Loss: 4.0784\n",
      "Epoch: 2504, Train Loss: 0.6263, Test Loss: 5.4620\n",
      "Epoch: 2505, Train Loss: 0.8623, Test Loss: 4.3961\n",
      "Epoch: 2506, Train Loss: 0.6212, Test Loss: 3.5054\n",
      "Epoch: 2507, Train Loss: 0.7055, Test Loss: 3.6467\n",
      "Epoch: 2508, Train Loss: 0.7126, Test Loss: 4.4993\n",
      "Epoch: 2509, Train Loss: 0.7227, Test Loss: 4.5666\n",
      "Epoch: 2510, Train Loss: 0.7010, Test Loss: 3.7212\n",
      "Epoch: 2511, Train Loss: 0.6657, Test Loss: 3.7599\n",
      "Epoch: 2512, Train Loss: 0.6708, Test Loss: 4.3777\n",
      "Epoch: 2513, Train Loss: 0.7047, Test Loss: 5.0693\n",
      "Epoch: 2514, Train Loss: 0.7744, Test Loss: 4.3562\n",
      "Epoch: 2515, Train Loss: 0.6115, Test Loss: 3.7341\n",
      "Epoch: 2516, Train Loss: 0.6866, Test Loss: 3.6438\n",
      "Epoch: 2517, Train Loss: 0.6529, Test Loss: 4.4211\n",
      "Epoch: 2518, Train Loss: 0.6563, Test Loss: 4.9174\n",
      "Epoch: 2519, Train Loss: 0.7622, Test Loss: 3.8676\n",
      "Epoch: 2520, Train Loss: 0.6755, Test Loss: 4.0010\n",
      "Epoch: 2521, Train Loss: 0.6470, Test Loss: 4.3117\n",
      "Epoch: 2522, Train Loss: 0.6216, Test Loss: 4.3850\n",
      "Epoch: 2523, Train Loss: 0.7259, Test Loss: 4.0540\n",
      "Epoch: 2524, Train Loss: 0.6729, Test Loss: 3.7676\n",
      "Epoch: 2525, Train Loss: 0.6472, Test Loss: 4.0355\n",
      "Epoch: 2526, Train Loss: 0.6357, Test Loss: 4.7214\n",
      "Epoch: 2527, Train Loss: 0.6682, Test Loss: 4.2303\n",
      "Epoch: 2528, Train Loss: 0.6429, Test Loss: 4.2601\n",
      "Epoch: 2529, Train Loss: 0.6311, Test Loss: 4.2977\n",
      "Epoch: 2530, Train Loss: 0.6556, Test Loss: 4.0275\n",
      "Epoch: 2531, Train Loss: 0.6432, Test Loss: 3.8241\n",
      "Epoch: 2532, Train Loss: 0.6013, Test Loss: 4.1591\n",
      "Epoch: 2533, Train Loss: 0.6505, Test Loss: 4.4715\n",
      "Epoch: 2534, Train Loss: 0.6612, Test Loss: 4.0713\n",
      "Epoch: 2535, Train Loss: 0.6219, Test Loss: 3.5637\n",
      "Epoch: 2536, Train Loss: 0.6654, Test Loss: 3.9496\n",
      "Epoch: 2537, Train Loss: 0.6768, Test Loss: 4.7797\n",
      "Epoch: 2538, Train Loss: 0.7147, Test Loss: 4.5919\n",
      "Epoch: 2539, Train Loss: 0.6123, Test Loss: 3.9758\n",
      "Epoch: 2540, Train Loss: 0.6300, Test Loss: 3.6241\n",
      "Epoch: 2541, Train Loss: 0.7523, Test Loss: 4.4418\n",
      "Epoch: 2542, Train Loss: 0.6519, Test Loss: 5.0101\n",
      "Epoch: 2543, Train Loss: 0.7032, Test Loss: 4.0541\n",
      "Epoch: 2544, Train Loss: 0.6509, Test Loss: 3.4441\n",
      "Epoch: 2545, Train Loss: 0.7443, Test Loss: 4.0530\n",
      "Epoch: 2546, Train Loss: 0.6126, Test Loss: 4.9950\n",
      "Epoch: 2547, Train Loss: 0.7274, Test Loss: 4.4650\n",
      "Epoch: 2548, Train Loss: 0.6505, Test Loss: 3.4740\n",
      "Epoch: 2549, Train Loss: 0.7360, Test Loss: 3.7340\n",
      "Epoch: 2550, Train Loss: 0.6554, Test Loss: 4.5127\n",
      "Epoch: 2551, Train Loss: 0.5896, Test Loss: 4.6725\n",
      "Epoch: 2552, Train Loss: 0.6764, Test Loss: 3.9379\n",
      "Epoch: 2553, Train Loss: 0.6036, Test Loss: 3.3756\n",
      "Epoch: 2554, Train Loss: 0.6817, Test Loss: 3.6906\n",
      "Epoch: 2555, Train Loss: 0.6591, Test Loss: 5.0063\n",
      "Epoch: 2556, Train Loss: 0.7469, Test Loss: 4.7225\n",
      "Epoch: 2557, Train Loss: 0.6982, Test Loss: 3.5908\n",
      "Epoch: 2558, Train Loss: 0.6953, Test Loss: 3.6106\n",
      "Epoch: 2559, Train Loss: 0.7170, Test Loss: 4.8048\n",
      "Epoch: 2560, Train Loss: 0.6341, Test Loss: 5.6471\n",
      "Epoch: 2561, Train Loss: 1.0248, Test Loss: 3.7164\n",
      "Epoch: 2562, Train Loss: 0.6224, Test Loss: 3.0525\n",
      "Epoch: 2563, Train Loss: 1.0852, Test Loss: 3.6469\n",
      "Epoch: 2564, Train Loss: 0.6275, Test Loss: 5.2077\n",
      "Epoch: 2565, Train Loss: 0.8203, Test Loss: 5.2290\n",
      "Epoch: 2566, Train Loss: 0.7933, Test Loss: 3.6551\n",
      "Epoch: 2567, Train Loss: 0.6361, Test Loss: 3.2522\n",
      "Epoch: 2568, Train Loss: 0.8064, Test Loss: 3.5758\n",
      "Epoch: 2569, Train Loss: 0.6370, Test Loss: 4.6688\n",
      "Epoch: 2570, Train Loss: 0.6765, Test Loss: 4.6949\n",
      "Epoch: 2571, Train Loss: 0.6781, Test Loss: 3.6606\n",
      "Epoch: 2572, Train Loss: 0.6216, Test Loss: 3.3572\n",
      "Epoch: 2573, Train Loss: 0.6625, Test Loss: 3.8102\n",
      "Epoch: 2574, Train Loss: 0.5995, Test Loss: 4.6697\n",
      "Epoch: 2575, Train Loss: 0.7237, Test Loss: 4.2306\n",
      "Epoch: 2576, Train Loss: 0.6664, Test Loss: 3.4294\n",
      "Epoch: 2577, Train Loss: 0.7853, Test Loss: 3.8425\n",
      "Epoch: 2578, Train Loss: 0.6102, Test Loss: 4.9254\n",
      "Epoch: 2579, Train Loss: 0.7508, Test Loss: 4.4579\n",
      "Epoch: 2580, Train Loss: 0.6124, Test Loss: 3.4612\n",
      "Epoch: 2581, Train Loss: 0.6447, Test Loss: 3.3307\n",
      "Epoch: 2582, Train Loss: 0.8094, Test Loss: 4.2139\n",
      "Epoch: 2583, Train Loss: 0.6042, Test Loss: 5.0315\n",
      "Epoch: 2584, Train Loss: 0.7537, Test Loss: 4.0717\n",
      "Epoch: 2585, Train Loss: 0.5744, Test Loss: 3.4461\n",
      "Epoch: 2586, Train Loss: 0.6590, Test Loss: 3.5785\n",
      "Epoch: 2587, Train Loss: 0.6059, Test Loss: 4.0657\n",
      "Epoch: 2588, Train Loss: 0.5758, Test Loss: 4.9765\n",
      "Epoch: 2589, Train Loss: 0.6162, Test Loss: 4.7859\n",
      "Epoch: 2590, Train Loss: 0.6583, Test Loss: 4.0066\n",
      "Epoch: 2591, Train Loss: 0.6736, Test Loss: 3.4522\n",
      "Epoch: 2592, Train Loss: 0.7574, Test Loss: 3.8758\n",
      "Epoch: 2593, Train Loss: 0.6136, Test Loss: 5.0582\n",
      "Epoch: 2594, Train Loss: 0.6648, Test Loss: 5.0689\n",
      "Epoch: 2595, Train Loss: 0.6800, Test Loss: 3.8741\n",
      "Epoch: 2596, Train Loss: 0.5978, Test Loss: 3.3236\n",
      "Epoch: 2597, Train Loss: 0.7446, Test Loss: 3.7299\n",
      "Epoch: 2598, Train Loss: 0.5981, Test Loss: 4.7878\n",
      "Epoch: 2599, Train Loss: 0.6576, Test Loss: 4.9345\n",
      "Epoch: 2600, Train Loss: 0.7984, Test Loss: 3.6491\n",
      "Epoch: 2601, Train Loss: 0.6126, Test Loss: 3.4272\n",
      "Epoch: 2602, Train Loss: 0.7394, Test Loss: 4.0686\n",
      "Epoch: 2603, Train Loss: 0.5825, Test Loss: 4.5949\n",
      "Epoch: 2604, Train Loss: 0.5811, Test Loss: 4.5343\n",
      "Epoch: 2605, Train Loss: 0.6326, Test Loss: 3.8819\n",
      "Epoch: 2606, Train Loss: 0.6112, Test Loss: 3.7948\n",
      "Epoch: 2607, Train Loss: 0.5979, Test Loss: 4.0190\n",
      "Epoch: 2608, Train Loss: 0.6064, Test Loss: 4.1545\n",
      "Epoch: 2609, Train Loss: 0.6257, Test Loss: 4.1665\n",
      "Epoch: 2610, Train Loss: 0.6143, Test Loss: 4.3289\n",
      "Epoch: 2611, Train Loss: 0.5766, Test Loss: 4.1078\n",
      "Epoch: 2612, Train Loss: 0.5840, Test Loss: 4.1976\n",
      "Epoch: 2613, Train Loss: 0.5974, Test Loss: 4.3640\n",
      "Epoch: 2614, Train Loss: 0.6224, Test Loss: 4.3723\n",
      "Epoch: 2615, Train Loss: 0.6444, Test Loss: 3.9778\n",
      "Epoch: 2616, Train Loss: 0.5880, Test Loss: 3.8662\n",
      "Epoch: 2617, Train Loss: 0.6320, Test Loss: 4.2227\n",
      "Epoch: 2618, Train Loss: 0.5927, Test Loss: 4.4506\n",
      "Epoch: 2619, Train Loss: 0.6221, Test Loss: 4.5061\n",
      "Epoch: 2620, Train Loss: 0.6416, Test Loss: 3.9141\n",
      "Epoch: 2621, Train Loss: 0.6300, Test Loss: 3.8010\n",
      "Epoch: 2622, Train Loss: 0.6549, Test Loss: 4.3461\n",
      "Epoch: 2623, Train Loss: 0.5888, Test Loss: 4.9202\n",
      "Epoch: 2624, Train Loss: 0.6848, Test Loss: 4.1290\n",
      "Epoch: 2625, Train Loss: 0.5689, Test Loss: 3.7905\n",
      "Epoch: 2626, Train Loss: 0.5805, Test Loss: 3.7156\n",
      "Epoch: 2627, Train Loss: 0.6259, Test Loss: 4.0412\n",
      "Epoch: 2628, Train Loss: 0.6610, Test Loss: 4.0594\n",
      "Epoch: 2629, Train Loss: 0.6478, Test Loss: 3.8892\n",
      "Epoch: 2630, Train Loss: 0.6051, Test Loss: 4.2560\n",
      "Epoch: 2631, Train Loss: 0.6395, Test Loss: 4.3742\n",
      "Epoch: 2632, Train Loss: 0.6079, Test Loss: 4.1538\n",
      "Epoch: 2633, Train Loss: 0.5964, Test Loss: 3.8056\n",
      "Epoch: 2634, Train Loss: 0.7180, Test Loss: 3.6376\n",
      "Epoch: 2635, Train Loss: 0.6316, Test Loss: 4.0732\n",
      "Epoch: 2636, Train Loss: 0.6916, Test Loss: 4.8441\n",
      "Epoch: 2637, Train Loss: 0.6605, Test Loss: 4.3168\n",
      "Epoch: 2638, Train Loss: 0.5802, Test Loss: 3.9580\n",
      "Epoch: 2639, Train Loss: 0.6344, Test Loss: 3.8439\n",
      "Epoch: 2640, Train Loss: 0.6502, Test Loss: 4.3632\n",
      "Epoch: 2641, Train Loss: 0.6657, Test Loss: 4.2223\n",
      "Epoch: 2642, Train Loss: 0.6193, Test Loss: 3.8084\n",
      "Epoch: 2643, Train Loss: 0.5715, Test Loss: 3.6903\n",
      "Epoch: 2644, Train Loss: 0.6368, Test Loss: 4.0665\n",
      "Epoch: 2645, Train Loss: 0.6689, Test Loss: 4.2776\n",
      "Epoch: 2646, Train Loss: 0.5749, Test Loss: 4.1935\n",
      "Epoch: 2647, Train Loss: 0.5822, Test Loss: 4.3590\n",
      "Epoch: 2648, Train Loss: 0.6038, Test Loss: 4.2444\n",
      "Epoch: 2649, Train Loss: 0.6062, Test Loss: 4.0307\n",
      "Epoch: 2650, Train Loss: 0.6138, Test Loss: 3.7759\n",
      "Epoch: 2651, Train Loss: 0.6802, Test Loss: 4.3605\n",
      "Epoch: 2652, Train Loss: 0.6526, Test Loss: 4.2573\n",
      "Epoch: 2653, Train Loss: 0.5698, Test Loss: 3.8968\n",
      "Epoch: 2654, Train Loss: 0.6114, Test Loss: 4.0863\n",
      "Epoch: 2655, Train Loss: 0.6138, Test Loss: 4.5979\n",
      "Epoch: 2656, Train Loss: 0.6355, Test Loss: 4.1972\n",
      "Epoch: 2657, Train Loss: 0.6216, Test Loss: 4.1008\n",
      "Epoch: 2658, Train Loss: 0.5992, Test Loss: 4.3613\n",
      "Epoch: 2659, Train Loss: 0.6237, Test Loss: 4.2259\n",
      "Epoch: 2660, Train Loss: 0.5989, Test Loss: 4.1174\n",
      "Epoch: 2661, Train Loss: 0.5652, Test Loss: 4.1021\n",
      "Epoch: 2662, Train Loss: 0.6187, Test Loss: 3.7668\n",
      "Epoch: 2663, Train Loss: 0.6201, Test Loss: 4.0776\n",
      "Epoch: 2664, Train Loss: 0.5638, Test Loss: 4.3790\n",
      "Epoch: 2665, Train Loss: 0.6205, Test Loss: 4.2870\n",
      "Epoch: 2666, Train Loss: 0.5781, Test Loss: 3.8529\n",
      "Epoch: 2667, Train Loss: 0.6036, Test Loss: 3.7188\n",
      "Epoch: 2668, Train Loss: 0.5816, Test Loss: 3.8904\n",
      "Epoch: 2669, Train Loss: 0.5950, Test Loss: 4.5347\n",
      "Epoch: 2670, Train Loss: 0.6480, Test Loss: 4.2518\n",
      "Epoch: 2671, Train Loss: 0.6434, Test Loss: 3.9752\n",
      "Epoch: 2672, Train Loss: 0.5968, Test Loss: 3.8025\n",
      "Epoch: 2673, Train Loss: 0.6369, Test Loss: 4.1622\n",
      "Epoch: 2674, Train Loss: 0.5992, Test Loss: 4.5091\n",
      "Epoch: 2675, Train Loss: 0.5743, Test Loss: 4.1514\n",
      "Epoch: 2676, Train Loss: 0.6024, Test Loss: 4.4368\n",
      "Epoch: 2677, Train Loss: 0.6058, Test Loss: 4.1125\n",
      "Epoch: 2678, Train Loss: 0.5613, Test Loss: 4.1382\n",
      "Epoch: 2679, Train Loss: 0.6073, Test Loss: 4.0225\n",
      "Epoch: 2680, Train Loss: 0.5972, Test Loss: 4.2202\n",
      "Epoch: 2681, Train Loss: 0.6086, Test Loss: 4.0658\n",
      "Epoch: 2682, Train Loss: 0.5981, Test Loss: 3.9604\n",
      "Epoch: 2683, Train Loss: 0.6365, Test Loss: 4.4815\n",
      "Epoch: 2684, Train Loss: 0.6223, Test Loss: 4.1557\n",
      "Epoch: 2685, Train Loss: 0.6157, Test Loss: 3.9853\n",
      "Epoch: 2686, Train Loss: 0.5761, Test Loss: 3.9426\n",
      "Epoch: 2687, Train Loss: 0.5521, Test Loss: 4.2465\n",
      "Epoch: 2688, Train Loss: 0.5922, Test Loss: 4.7682\n",
      "Epoch: 2689, Train Loss: 0.6487, Test Loss: 4.2476\n",
      "Epoch: 2690, Train Loss: 0.6274, Test Loss: 3.5583\n",
      "Epoch: 2691, Train Loss: 0.6226, Test Loss: 3.6871\n",
      "Epoch: 2692, Train Loss: 0.7437, Test Loss: 5.0335\n",
      "Epoch: 2693, Train Loss: 0.6168, Test Loss: 5.2988\n",
      "Epoch: 2694, Train Loss: 0.7878, Test Loss: 3.7687\n",
      "Epoch: 2695, Train Loss: 0.6280, Test Loss: 3.3640\n",
      "Epoch: 2696, Train Loss: 0.8665, Test Loss: 4.2609\n",
      "Epoch: 2697, Train Loss: 0.5808, Test Loss: 5.2165\n",
      "Epoch: 2698, Train Loss: 0.8251, Test Loss: 4.0972\n",
      "Epoch: 2699, Train Loss: 0.6384, Test Loss: 3.4884\n",
      "Epoch: 2700, Train Loss: 0.7149, Test Loss: 3.8413\n",
      "Epoch: 2701, Train Loss: 0.6594, Test Loss: 4.6739\n",
      "Epoch: 2702, Train Loss: 0.6955, Test Loss: 4.3654\n",
      "Epoch: 2703, Train Loss: 0.6072, Test Loss: 3.5806\n",
      "Epoch: 2704, Train Loss: 0.6291, Test Loss: 3.5503\n",
      "Epoch: 2705, Train Loss: 0.6543, Test Loss: 4.2916\n",
      "Epoch: 2706, Train Loss: 0.6116, Test Loss: 4.7573\n",
      "Epoch: 2707, Train Loss: 0.6011, Test Loss: 4.2947\n",
      "Epoch: 2708, Train Loss: 0.5733, Test Loss: 3.6637\n",
      "Epoch: 2709, Train Loss: 0.6648, Test Loss: 3.9719\n",
      "Epoch: 2710, Train Loss: 0.6258, Test Loss: 3.9494\n",
      "Epoch: 2711, Train Loss: 0.6407, Test Loss: 4.0448\n",
      "Epoch: 2712, Train Loss: 0.6116, Test Loss: 4.7368\n",
      "Epoch: 2713, Train Loss: 0.6160, Test Loss: 4.2785\n",
      "Epoch: 2714, Train Loss: 0.5972, Test Loss: 3.8708\n",
      "Epoch: 2715, Train Loss: 0.5798, Test Loss: 3.9589\n",
      "Epoch: 2716, Train Loss: 0.5703, Test Loss: 4.0761\n",
      "Epoch: 2717, Train Loss: 0.5568, Test Loss: 4.3155\n",
      "Epoch: 2718, Train Loss: 0.6255, Test Loss: 3.7603\n",
      "Epoch: 2719, Train Loss: 0.5676, Test Loss: 3.5740\n",
      "Epoch: 2720, Train Loss: 0.6503, Test Loss: 4.3815\n",
      "Epoch: 2721, Train Loss: 0.6104, Test Loss: 5.0156\n",
      "Epoch: 2722, Train Loss: 0.7161, Test Loss: 4.0901\n",
      "Epoch: 2723, Train Loss: 0.6132, Test Loss: 3.3872\n",
      "Epoch: 2724, Train Loss: 0.7229, Test Loss: 3.7621\n",
      "Epoch: 2725, Train Loss: 0.6431, Test Loss: 4.7273\n",
      "Epoch: 2726, Train Loss: 0.6623, Test Loss: 4.6851\n",
      "Epoch: 2727, Train Loss: 0.6181, Test Loss: 3.6096\n",
      "Epoch: 2728, Train Loss: 0.6061, Test Loss: 3.5471\n",
      "Epoch: 2729, Train Loss: 0.6700, Test Loss: 4.3692\n",
      "Epoch: 2730, Train Loss: 0.5905, Test Loss: 4.5943\n",
      "Epoch: 2731, Train Loss: 0.5892, Test Loss: 3.9891\n",
      "Epoch: 2732, Train Loss: 0.5991, Test Loss: 3.5132\n",
      "Epoch: 2733, Train Loss: 0.6718, Test Loss: 3.5028\n",
      "Epoch: 2734, Train Loss: 0.6196, Test Loss: 3.9645\n",
      "Epoch: 2735, Train Loss: 0.5900, Test Loss: 4.7144\n",
      "Epoch: 2736, Train Loss: 0.6306, Test Loss: 4.3971\n",
      "Epoch: 2737, Train Loss: 0.6249, Test Loss: 3.7045\n",
      "Epoch: 2738, Train Loss: 0.6634, Test Loss: 3.8802\n",
      "Epoch: 2739, Train Loss: 0.5564, Test Loss: 4.3315\n",
      "Epoch: 2740, Train Loss: 0.6127, Test Loss: 4.1692\n",
      "Epoch: 2741, Train Loss: 0.6130, Test Loss: 3.7321\n",
      "Epoch: 2742, Train Loss: 0.6982, Test Loss: 3.7426\n",
      "Epoch: 2743, Train Loss: 0.6104, Test Loss: 4.0261\n",
      "Epoch: 2744, Train Loss: 0.5905, Test Loss: 4.6212\n",
      "Epoch: 2745, Train Loss: 0.7069, Test Loss: 4.8888\n",
      "Epoch: 2746, Train Loss: 0.6556, Test Loss: 4.0135\n",
      "Epoch: 2747, Train Loss: 0.6173, Test Loss: 3.8080\n",
      "Epoch: 2748, Train Loss: 0.6648, Test Loss: 4.4101\n",
      "Epoch: 2749, Train Loss: 0.6692, Test Loss: 4.5130\n",
      "Epoch: 2750, Train Loss: 0.7156, Test Loss: 3.7894\n",
      "Epoch: 2751, Train Loss: 0.6449, Test Loss: 3.8765\n",
      "Epoch: 2752, Train Loss: 0.6485, Test Loss: 4.6685\n",
      "Epoch: 2753, Train Loss: 0.6233, Test Loss: 5.0771\n",
      "Epoch: 2754, Train Loss: 0.7171, Test Loss: 3.6849\n",
      "Epoch: 2755, Train Loss: 0.6100, Test Loss: 3.0998\n",
      "Epoch: 2756, Train Loss: 0.9690, Test Loss: 3.9680\n",
      "Epoch: 2757, Train Loss: 0.5985, Test Loss: 5.0665\n",
      "Epoch: 2758, Train Loss: 0.7327, Test Loss: 4.4523\n",
      "Epoch: 2759, Train Loss: 0.6523, Test Loss: 3.7111\n",
      "Epoch: 2760, Train Loss: 0.6266, Test Loss: 3.5142\n",
      "Epoch: 2761, Train Loss: 0.6591, Test Loss: 4.2320\n",
      "Epoch: 2762, Train Loss: 0.5688, Test Loss: 5.0323\n",
      "Epoch: 2763, Train Loss: 0.7092, Test Loss: 4.1713\n",
      "Epoch: 2764, Train Loss: 0.6016, Test Loss: 3.7208\n",
      "Epoch: 2765, Train Loss: 0.6653, Test Loss: 3.7844\n",
      "Epoch: 2766, Train Loss: 0.6165, Test Loss: 4.4982\n",
      "Epoch: 2767, Train Loss: 0.5914, Test Loss: 4.7572\n",
      "Epoch: 2768, Train Loss: 0.7088, Test Loss: 3.8074\n",
      "Epoch: 2769, Train Loss: 0.5877, Test Loss: 3.4097\n",
      "Epoch: 2770, Train Loss: 0.6690, Test Loss: 3.9415\n",
      "Epoch: 2771, Train Loss: 0.6410, Test Loss: 4.7915\n",
      "Epoch: 2772, Train Loss: 0.6798, Test Loss: 4.6985\n",
      "Epoch: 2773, Train Loss: 0.6720, Test Loss: 3.6466\n",
      "Epoch: 2774, Train Loss: 0.5897, Test Loss: 3.4533\n",
      "Epoch: 2775, Train Loss: 0.7342, Test Loss: 4.3104\n",
      "Epoch: 2776, Train Loss: 0.5703, Test Loss: 5.0853\n",
      "Epoch: 2777, Train Loss: 0.7406, Test Loss: 4.0005\n",
      "Epoch: 2778, Train Loss: 0.5712, Test Loss: 3.3159\n",
      "Epoch: 2779, Train Loss: 0.7689, Test Loss: 3.8378\n",
      "Epoch: 2780, Train Loss: 0.5476, Test Loss: 4.5777\n",
      "Epoch: 2781, Train Loss: 0.7030, Test Loss: 4.3650\n",
      "Epoch: 2782, Train Loss: 0.5489, Test Loss: 3.9231\n",
      "Epoch: 2783, Train Loss: 0.6286, Test Loss: 3.9287\n",
      "Epoch: 2784, Train Loss: 0.5654, Test Loss: 4.1322\n",
      "Epoch: 2785, Train Loss: 0.5991, Test Loss: 4.4446\n",
      "Epoch: 2786, Train Loss: 0.5642, Test Loss: 4.6508\n",
      "Epoch: 2787, Train Loss: 0.6114, Test Loss: 4.0036\n",
      "Epoch: 2788, Train Loss: 0.5743, Test Loss: 3.7105\n",
      "Epoch: 2789, Train Loss: 0.6464, Test Loss: 3.8200\n",
      "Epoch: 2790, Train Loss: 0.5858, Test Loss: 4.4387\n",
      "Epoch: 2791, Train Loss: 0.6392, Test Loss: 4.6896\n",
      "Epoch: 2792, Train Loss: 0.6562, Test Loss: 4.1249\n",
      "Epoch: 2793, Train Loss: 0.6266, Test Loss: 3.6535\n",
      "Epoch: 2794, Train Loss: 0.7011, Test Loss: 3.8929\n",
      "Epoch: 2795, Train Loss: 0.6260, Test Loss: 4.8378\n",
      "Epoch: 2796, Train Loss: 0.6570, Test Loss: 4.8265\n",
      "Epoch: 2797, Train Loss: 0.6407, Test Loss: 3.9465\n",
      "Epoch: 2798, Train Loss: 0.6212, Test Loss: 3.7765\n",
      "Epoch: 2799, Train Loss: 0.5911, Test Loss: 4.0793\n",
      "Epoch: 2800, Train Loss: 0.5822, Test Loss: 4.4196\n",
      "Epoch: 2801, Train Loss: 0.6108, Test Loss: 4.6162\n",
      "Epoch: 2802, Train Loss: 0.5929, Test Loss: 4.2181\n",
      "Epoch: 2803, Train Loss: 0.5442, Test Loss: 3.9535\n",
      "Epoch: 2804, Train Loss: 0.5655, Test Loss: 4.2530\n",
      "Epoch: 2805, Train Loss: 0.6201, Test Loss: 4.8419\n",
      "Epoch: 2806, Train Loss: 0.6220, Test Loss: 4.5739\n",
      "Epoch: 2807, Train Loss: 0.6030, Test Loss: 3.7656\n",
      "Epoch: 2808, Train Loss: 0.6229, Test Loss: 3.7153\n",
      "Epoch: 2809, Train Loss: 0.6489, Test Loss: 4.2585\n",
      "Epoch: 2810, Train Loss: 0.5744, Test Loss: 4.3930\n",
      "Epoch: 2811, Train Loss: 0.6231, Test Loss: 4.1799\n",
      "Epoch: 2812, Train Loss: 0.6813, Test Loss: 3.7710\n",
      "Epoch: 2813, Train Loss: 0.5865, Test Loss: 3.9155\n",
      "Epoch: 2814, Train Loss: 0.6482, Test Loss: 4.1657\n",
      "Epoch: 2815, Train Loss: 0.6148, Test Loss: 4.7236\n",
      "Epoch: 2816, Train Loss: 0.5843, Test Loss: 4.5281\n",
      "Epoch: 2817, Train Loss: 0.5730, Test Loss: 4.0129\n",
      "Epoch: 2818, Train Loss: 0.6191, Test Loss: 3.9840\n",
      "Epoch: 2819, Train Loss: 0.5638, Test Loss: 4.1368\n",
      "Epoch: 2820, Train Loss: 0.6157, Test Loss: 4.4519\n",
      "Epoch: 2821, Train Loss: 0.5974, Test Loss: 4.2730\n",
      "Epoch: 2822, Train Loss: 0.6259, Test Loss: 4.1296\n",
      "Epoch: 2823, Train Loss: 0.6749, Test Loss: 3.7129\n",
      "Epoch: 2824, Train Loss: 0.7676, Test Loss: 4.3810\n",
      "Epoch: 2825, Train Loss: 0.5943, Test Loss: 4.8237\n",
      "Epoch: 2826, Train Loss: 0.7076, Test Loss: 3.9018\n",
      "Epoch: 2827, Train Loss: 0.5957, Test Loss: 3.5693\n",
      "Epoch: 2828, Train Loss: 0.6445, Test Loss: 3.9844\n",
      "Epoch: 2829, Train Loss: 0.6486, Test Loss: 4.8241\n",
      "Epoch: 2830, Train Loss: 0.6664, Test Loss: 4.5473\n",
      "Epoch: 2831, Train Loss: 0.6573, Test Loss: 4.0663\n",
      "Epoch: 2832, Train Loss: 0.6572, Test Loss: 4.0629\n",
      "Epoch: 2833, Train Loss: 0.6179, Test Loss: 4.3773\n",
      "Epoch: 2834, Train Loss: 0.6289, Test Loss: 4.6236\n",
      "Epoch: 2835, Train Loss: 0.6325, Test Loss: 4.2481\n",
      "Epoch: 2836, Train Loss: 0.6226, Test Loss: 4.1665\n",
      "Epoch: 2837, Train Loss: 0.6435, Test Loss: 4.3597\n",
      "Epoch: 2838, Train Loss: 0.6096, Test Loss: 4.1926\n",
      "Epoch: 2839, Train Loss: 0.6451, Test Loss: 3.8550\n",
      "Epoch: 2840, Train Loss: 0.5836, Test Loss: 3.5660\n",
      "Epoch: 2841, Train Loss: 0.6432, Test Loss: 3.8891\n",
      "Epoch: 2842, Train Loss: 0.5510, Test Loss: 4.4615\n",
      "Epoch: 2843, Train Loss: 0.6313, Test Loss: 4.5698\n",
      "Epoch: 2844, Train Loss: 0.6702, Test Loss: 4.0594\n",
      "Epoch: 2845, Train Loss: 0.5713, Test Loss: 3.7858\n",
      "Epoch: 2846, Train Loss: 0.6477, Test Loss: 4.2834\n",
      "Epoch: 2847, Train Loss: 0.5940, Test Loss: 4.6654\n",
      "Epoch: 2848, Train Loss: 0.6082, Test Loss: 4.0878\n",
      "Epoch: 2849, Train Loss: 0.5872, Test Loss: 3.8705\n",
      "Epoch: 2850, Train Loss: 0.6190, Test Loss: 4.3321\n",
      "Epoch: 2851, Train Loss: 0.5549, Test Loss: 4.9093\n",
      "Epoch: 2852, Train Loss: 0.6826, Test Loss: 4.2824\n",
      "Epoch: 2853, Train Loss: 0.6012, Test Loss: 3.5775\n",
      "Epoch: 2854, Train Loss: 0.6413, Test Loss: 3.8098\n",
      "Epoch: 2855, Train Loss: 0.6052, Test Loss: 4.8385\n",
      "Epoch: 2856, Train Loss: 0.6880, Test Loss: 4.6524\n",
      "Epoch: 2857, Train Loss: 0.6685, Test Loss: 3.5491\n",
      "Epoch: 2858, Train Loss: 0.6212, Test Loss: 3.2683\n",
      "Epoch: 2859, Train Loss: 0.8216, Test Loss: 4.2896\n",
      "Epoch: 2860, Train Loss: 0.6152, Test Loss: 5.6397\n",
      "Epoch: 2861, Train Loss: 0.9321, Test Loss: 4.2227\n",
      "Epoch: 2862, Train Loss: 0.5815, Test Loss: 3.6575\n",
      "Epoch: 2863, Train Loss: 0.6925, Test Loss: 4.1087\n",
      "Epoch: 2864, Train Loss: 0.6679, Test Loss: 5.3985\n",
      "Epoch: 2865, Train Loss: 0.8919, Test Loss: 4.2773\n",
      "Epoch: 2866, Train Loss: 0.6539, Test Loss: 3.3016\n",
      "Epoch: 2867, Train Loss: 0.8254, Test Loss: 3.4742\n",
      "Epoch: 2868, Train Loss: 0.6964, Test Loss: 4.7478\n",
      "Epoch: 2869, Train Loss: 0.6655, Test Loss: 5.5217\n",
      "Epoch: 2870, Train Loss: 0.8982, Test Loss: 3.9615\n",
      "Epoch: 2871, Train Loss: 0.5541, Test Loss: 3.2461\n",
      "Epoch: 2872, Train Loss: 0.6866, Test Loss: 3.3456\n",
      "Epoch: 2873, Train Loss: 0.6872, Test Loss: 4.2937\n",
      "Epoch: 2874, Train Loss: 0.5980, Test Loss: 5.1335\n",
      "Epoch: 2875, Train Loss: 0.7392, Test Loss: 4.1841\n",
      "Epoch: 2876, Train Loss: 0.5667, Test Loss: 3.5387\n",
      "Epoch: 2877, Train Loss: 0.6993, Test Loss: 3.7751\n",
      "Epoch: 2878, Train Loss: 0.5841, Test Loss: 4.5515\n",
      "Epoch: 2879, Train Loss: 0.6578, Test Loss: 4.5052\n",
      "Epoch: 2880, Train Loss: 0.5777, Test Loss: 4.0283\n",
      "Epoch: 2881, Train Loss: 0.5925, Test Loss: 3.5809\n",
      "Epoch: 2882, Train Loss: 0.6357, Test Loss: 3.7420\n",
      "Epoch: 2883, Train Loss: 0.5771, Test Loss: 4.3021\n",
      "Epoch: 2884, Train Loss: 0.6028, Test Loss: 4.5278\n",
      "Epoch: 2885, Train Loss: 0.6113, Test Loss: 4.1945\n",
      "Epoch: 2886, Train Loss: 0.5239, Test Loss: 3.9752\n",
      "Epoch: 2887, Train Loss: 0.5670, Test Loss: 4.1601\n",
      "Epoch: 2888, Train Loss: 0.5472, Test Loss: 4.2916\n",
      "Epoch: 2889, Train Loss: 0.6230, Test Loss: 3.9044\n",
      "Epoch: 2890, Train Loss: 0.5468, Test Loss: 3.7184\n",
      "Epoch: 2891, Train Loss: 0.6097, Test Loss: 4.0321\n",
      "Epoch: 2892, Train Loss: 0.5440, Test Loss: 4.2224\n",
      "Epoch: 2893, Train Loss: 0.5787, Test Loss: 3.9310\n",
      "Epoch: 2894, Train Loss: 0.5497, Test Loss: 3.6161\n",
      "Epoch: 2895, Train Loss: 0.6267, Test Loss: 3.9574\n",
      "Epoch: 2896, Train Loss: 0.6210, Test Loss: 4.6528\n",
      "Epoch: 2897, Train Loss: 0.6001, Test Loss: 4.4046\n",
      "Epoch: 2898, Train Loss: 0.6234, Test Loss: 3.7673\n",
      "Epoch: 2899, Train Loss: 0.5597, Test Loss: 3.7360\n",
      "Epoch: 2900, Train Loss: 0.7302, Test Loss: 4.7896\n",
      "Epoch: 2901, Train Loss: 0.7092, Test Loss: 4.7143\n",
      "Epoch: 2902, Train Loss: 0.6394, Test Loss: 3.6733\n",
      "Epoch: 2903, Train Loss: 0.5673, Test Loss: 3.3728\n",
      "Epoch: 2904, Train Loss: 0.7010, Test Loss: 4.2187\n",
      "Epoch: 2905, Train Loss: 0.5306, Test Loss: 5.4039\n",
      "Epoch: 2906, Train Loss: 0.9618, Test Loss: 4.0166\n",
      "Epoch: 2907, Train Loss: 0.5805, Test Loss: 3.2134\n",
      "Epoch: 2908, Train Loss: 0.7852, Test Loss: 3.5068\n",
      "Epoch: 2909, Train Loss: 0.6186, Test Loss: 4.8262\n",
      "Epoch: 2910, Train Loss: 0.6953, Test Loss: 4.7501\n",
      "Epoch: 2911, Train Loss: 0.6279, Test Loss: 3.7073\n",
      "Epoch: 2912, Train Loss: 0.6279, Test Loss: 3.5253\n",
      "Epoch: 2913, Train Loss: 0.6093, Test Loss: 3.9896\n",
      "Epoch: 2914, Train Loss: 0.6228, Test Loss: 4.4580\n",
      "Epoch: 2915, Train Loss: 0.5848, Test Loss: 4.2595\n",
      "Epoch: 2916, Train Loss: 0.6383, Test Loss: 3.7104\n",
      "Epoch: 2917, Train Loss: 0.6125, Test Loss: 3.7190\n",
      "Epoch: 2918, Train Loss: 0.6288, Test Loss: 3.8721\n",
      "Epoch: 2919, Train Loss: 0.5334, Test Loss: 4.0904\n",
      "Epoch: 2920, Train Loss: 0.5640, Test Loss: 4.1413\n",
      "Epoch: 2921, Train Loss: 0.5849, Test Loss: 3.9983\n",
      "Epoch: 2922, Train Loss: 0.5635, Test Loss: 4.0670\n",
      "Epoch: 2923, Train Loss: 0.5766, Test Loss: 3.8689\n",
      "Epoch: 2924, Train Loss: 0.6294, Test Loss: 4.2222\n",
      "Epoch: 2925, Train Loss: 0.5720, Test Loss: 4.0954\n",
      "Epoch: 2926, Train Loss: 0.5761, Test Loss: 4.2024\n",
      "Epoch: 2927, Train Loss: 0.6001, Test Loss: 4.2185\n",
      "Epoch: 2928, Train Loss: 0.5154, Test Loss: 4.1113\n",
      "Epoch: 2929, Train Loss: 0.5418, Test Loss: 3.7928\n",
      "Epoch: 2930, Train Loss: 0.6122, Test Loss: 4.0551\n",
      "Epoch: 2931, Train Loss: 0.5754, Test Loss: 4.3547\n",
      "Epoch: 2932, Train Loss: 0.5864, Test Loss: 4.0878\n",
      "Epoch: 2933, Train Loss: 0.5748, Test Loss: 3.8267\n",
      "Epoch: 2934, Train Loss: 0.5750, Test Loss: 3.9675\n",
      "Epoch: 2935, Train Loss: 0.5191, Test Loss: 4.1355\n",
      "Epoch: 2936, Train Loss: 0.5406, Test Loss: 4.1803\n",
      "Epoch: 2937, Train Loss: 0.5944, Test Loss: 3.9429\n",
      "Epoch: 2938, Train Loss: 0.5952, Test Loss: 4.1648\n",
      "Epoch: 2939, Train Loss: 0.5541, Test Loss: 3.9912\n",
      "Epoch: 2940, Train Loss: 0.5754, Test Loss: 3.8330\n",
      "Epoch: 2941, Train Loss: 0.6377, Test Loss: 4.4040\n",
      "Epoch: 2942, Train Loss: 0.5385, Test Loss: 4.7112\n",
      "Epoch: 2943, Train Loss: 0.6157, Test Loss: 4.1029\n",
      "Epoch: 2944, Train Loss: 0.5410, Test Loss: 3.7101\n",
      "Epoch: 2945, Train Loss: 0.6196, Test Loss: 3.7782\n",
      "Epoch: 2946, Train Loss: 0.5772, Test Loss: 4.1806\n",
      "Epoch: 2947, Train Loss: 0.5355, Test Loss: 4.8529\n",
      "Epoch: 2948, Train Loss: 0.6652, Test Loss: 4.1277\n",
      "Epoch: 2949, Train Loss: 0.6227, Test Loss: 3.3993\n",
      "Epoch: 2950, Train Loss: 0.6514, Test Loss: 3.6528\n",
      "Epoch: 2951, Train Loss: 0.6043, Test Loss: 4.6801\n",
      "Epoch: 2952, Train Loss: 0.6904, Test Loss: 4.4640\n",
      "Epoch: 2953, Train Loss: 0.6873, Test Loss: 3.5053\n",
      "Epoch: 2954, Train Loss: 0.7555, Test Loss: 3.8625\n",
      "Epoch: 2955, Train Loss: 0.6316, Test Loss: 5.0695\n",
      "Epoch: 2956, Train Loss: 0.7483, Test Loss: 4.8749\n",
      "Epoch: 2957, Train Loss: 0.5753, Test Loss: 4.2120\n",
      "Epoch: 2958, Train Loss: 0.7039, Test Loss: 3.2754\n",
      "Epoch: 2959, Train Loss: 0.7816, Test Loss: 3.4536\n",
      "Epoch: 2960, Train Loss: 0.6138, Test Loss: 4.5184\n",
      "Epoch: 2961, Train Loss: 0.6157, Test Loss: 5.0533\n",
      "Epoch: 2962, Train Loss: 0.7701, Test Loss: 3.9323\n",
      "Epoch: 2963, Train Loss: 0.5867, Test Loss: 3.5281\n",
      "Epoch: 2964, Train Loss: 0.6430, Test Loss: 3.6512\n",
      "Epoch: 2965, Train Loss: 0.6279, Test Loss: 4.1534\n",
      "Epoch: 2966, Train Loss: 0.5978, Test Loss: 4.4692\n",
      "Epoch: 2967, Train Loss: 0.5940, Test Loss: 4.4129\n",
      "Epoch: 2968, Train Loss: 0.6068, Test Loss: 3.6582\n",
      "Epoch: 2969, Train Loss: 0.6106, Test Loss: 3.5400\n",
      "Epoch: 2970, Train Loss: 0.6359, Test Loss: 3.9767\n",
      "Epoch: 2971, Train Loss: 0.5939, Test Loss: 4.2816\n",
      "Epoch: 2972, Train Loss: 0.5856, Test Loss: 4.0271\n",
      "Epoch: 2973, Train Loss: 0.5659, Test Loss: 3.9588\n",
      "Epoch: 2974, Train Loss: 0.6164, Test Loss: 4.1025\n",
      "Epoch: 2975, Train Loss: 0.5371, Test Loss: 3.9614\n",
      "Epoch: 2976, Train Loss: 0.5468, Test Loss: 4.1082\n",
      "Epoch: 2977, Train Loss: 0.5312, Test Loss: 4.3099\n",
      "Epoch: 2978, Train Loss: 0.5887, Test Loss: 4.3177\n",
      "Epoch: 2979, Train Loss: 0.5949, Test Loss: 3.7148\n",
      "Epoch: 2980, Train Loss: 0.5460, Test Loss: 3.5140\n",
      "Epoch: 2981, Train Loss: 0.6035, Test Loss: 3.8942\n",
      "Epoch: 2982, Train Loss: 0.5529, Test Loss: 4.3780\n",
      "Epoch: 2983, Train Loss: 0.5686, Test Loss: 4.2062\n",
      "Epoch: 2984, Train Loss: 0.5683, Test Loss: 3.4028\n",
      "Epoch: 2985, Train Loss: 0.6918, Test Loss: 3.7589\n",
      "Epoch: 2986, Train Loss: 0.6452, Test Loss: 4.7886\n",
      "Epoch: 2987, Train Loss: 0.6269, Test Loss: 4.7598\n",
      "Epoch: 2988, Train Loss: 0.5939, Test Loss: 3.9561\n",
      "Epoch: 2989, Train Loss: 0.5484, Test Loss: 3.4349\n",
      "Epoch: 2990, Train Loss: 0.6800, Test Loss: 3.5972\n",
      "Epoch: 2991, Train Loss: 0.6251, Test Loss: 4.5266\n",
      "Epoch: 2992, Train Loss: 0.6355, Test Loss: 4.3405\n",
      "Epoch: 2993, Train Loss: 0.5632, Test Loss: 3.6760\n",
      "Epoch: 2994, Train Loss: 0.6084, Test Loss: 3.4913\n",
      "Epoch: 2995, Train Loss: 0.6656, Test Loss: 4.0451\n",
      "Epoch: 2996, Train Loss: 0.5329, Test Loss: 4.8774\n",
      "Epoch: 2997, Train Loss: 0.6712, Test Loss: 4.1993\n",
      "Epoch: 2998, Train Loss: 0.5978, Test Loss: 3.3767\n",
      "Epoch: 2999, Train Loss: 0.6630, Test Loss: 3.4026\n",
      "Epoch: 3000, Train Loss: 0.6911, Test Loss: 4.2669\n",
      "Epoch: 3001, Train Loss: 0.6310, Test Loss: 4.4858\n",
      "Epoch: 3002, Train Loss: 0.5946, Test Loss: 3.9389\n",
      "Epoch: 3003, Train Loss: 0.5352, Test Loss: 3.5798\n",
      "Epoch: 3004, Train Loss: 0.6902, Test Loss: 4.2597\n",
      "Epoch: 3005, Train Loss: 0.5936, Test Loss: 4.5349\n",
      "Epoch: 3006, Train Loss: 0.5488, Test Loss: 4.0015\n",
      "Epoch: 3007, Train Loss: 0.5444, Test Loss: 3.5890\n",
      "Epoch: 3008, Train Loss: 0.6214, Test Loss: 4.0318\n",
      "Epoch: 3009, Train Loss: 0.5579, Test Loss: 4.5408\n",
      "Epoch: 3010, Train Loss: 0.5682, Test Loss: 4.3260\n",
      "Epoch: 3011, Train Loss: 0.5422, Test Loss: 3.9569\n",
      "Epoch: 3012, Train Loss: 0.5707, Test Loss: 4.1055\n",
      "Epoch: 3013, Train Loss: 0.5953, Test Loss: 4.5327\n",
      "Epoch: 3014, Train Loss: 0.5454, Test Loss: 4.3680\n",
      "Epoch: 3015, Train Loss: 0.5348, Test Loss: 3.8905\n",
      "Epoch: 3016, Train Loss: 0.6046, Test Loss: 3.5680\n",
      "Epoch: 3017, Train Loss: 0.6135, Test Loss: 3.8985\n",
      "Epoch: 3018, Train Loss: 0.5615, Test Loss: 4.1994\n",
      "Epoch: 3019, Train Loss: 0.5895, Test Loss: 4.0642\n",
      "Epoch: 3020, Train Loss: 0.5820, Test Loss: 3.8454\n",
      "Epoch: 3021, Train Loss: 0.6259, Test Loss: 4.2888\n",
      "Epoch: 3022, Train Loss: 0.5510, Test Loss: 4.6290\n",
      "Epoch: 3023, Train Loss: 0.5952, Test Loss: 4.3672\n",
      "Epoch: 3024, Train Loss: 0.6298, Test Loss: 3.8386\n",
      "Epoch: 3025, Train Loss: 0.6713, Test Loss: 4.1572\n",
      "Epoch: 3026, Train Loss: 0.5899, Test Loss: 4.9056\n",
      "Epoch: 3027, Train Loss: 0.6743, Test Loss: 4.3362\n",
      "Epoch: 3028, Train Loss: 0.6511, Test Loss: 3.6856\n",
      "Epoch: 3029, Train Loss: 0.6555, Test Loss: 3.8627\n",
      "Epoch: 3030, Train Loss: 0.6063, Test Loss: 4.2963\n",
      "Epoch: 3031, Train Loss: 0.5731, Test Loss: 4.2989\n",
      "Epoch: 3032, Train Loss: 0.6049, Test Loss: 3.9814\n",
      "Epoch: 3033, Train Loss: 0.6331, Test Loss: 3.9346\n",
      "Epoch: 3034, Train Loss: 0.5750, Test Loss: 4.2252\n",
      "Epoch: 3035, Train Loss: 0.5417, Test Loss: 4.5603\n",
      "Epoch: 3036, Train Loss: 0.7349, Test Loss: 3.8601\n",
      "Epoch: 3037, Train Loss: 0.6070, Test Loss: 3.3678\n",
      "Epoch: 3038, Train Loss: 0.7232, Test Loss: 3.8112\n",
      "Epoch: 3039, Train Loss: 0.6272, Test Loss: 4.8834\n",
      "Epoch: 3040, Train Loss: 0.6137, Test Loss: 5.1271\n",
      "Epoch: 3041, Train Loss: 0.6566, Test Loss: 4.0467\n",
      "Epoch: 3042, Train Loss: 0.5955, Test Loss: 3.6171\n",
      "Epoch: 3043, Train Loss: 0.5763, Test Loss: 3.8872\n",
      "Epoch: 3044, Train Loss: 0.5736, Test Loss: 4.6605\n",
      "Epoch: 3045, Train Loss: 0.6769, Test Loss: 4.5515\n",
      "Epoch: 3046, Train Loss: 0.6259, Test Loss: 3.8938\n",
      "Epoch: 3047, Train Loss: 0.6012, Test Loss: 3.8898\n",
      "Epoch: 3048, Train Loss: 0.6134, Test Loss: 4.1164\n",
      "Epoch: 3049, Train Loss: 0.5474, Test Loss: 4.3903\n",
      "Epoch: 3050, Train Loss: 0.5954, Test Loss: 4.2182\n",
      "Epoch: 3051, Train Loss: 0.5438, Test Loss: 4.0266\n",
      "Epoch: 3052, Train Loss: 0.5571, Test Loss: 4.2598\n",
      "Epoch: 3053, Train Loss: 0.5386, Test Loss: 4.6175\n",
      "Epoch: 3054, Train Loss: 0.5869, Test Loss: 4.2420\n",
      "Epoch: 3055, Train Loss: 0.5602, Test Loss: 3.8789\n",
      "Epoch: 3056, Train Loss: 0.5490, Test Loss: 3.7151\n",
      "Epoch: 3057, Train Loss: 0.5978, Test Loss: 4.1040\n",
      "Epoch: 3058, Train Loss: 0.6102, Test Loss: 4.2606\n",
      "Epoch: 3059, Train Loss: 0.5629, Test Loss: 4.2095\n",
      "Epoch: 3060, Train Loss: 0.5894, Test Loss: 3.9005\n",
      "Epoch: 3061, Train Loss: 0.6001, Test Loss: 4.1605\n",
      "Epoch: 3062, Train Loss: 0.5557, Test Loss: 4.1904\n",
      "Epoch: 3063, Train Loss: 0.5420, Test Loss: 3.9486\n",
      "Epoch: 3064, Train Loss: 0.5921, Test Loss: 3.9436\n",
      "Epoch: 3065, Train Loss: 0.5936, Test Loss: 3.9769\n",
      "Epoch: 3066, Train Loss: 0.5661, Test Loss: 4.0990\n",
      "Epoch: 3067, Train Loss: 0.5602, Test Loss: 4.2854\n",
      "Epoch: 3068, Train Loss: 0.5670, Test Loss: 4.1930\n",
      "Epoch: 3069, Train Loss: 0.6561, Test Loss: 3.6652\n",
      "Epoch: 3070, Train Loss: 0.6667, Test Loss: 4.1782\n",
      "Epoch: 3071, Train Loss: 0.5780, Test Loss: 4.2611\n",
      "Epoch: 3072, Train Loss: 0.5826, Test Loss: 4.2607\n",
      "Epoch: 3073, Train Loss: 0.5479, Test Loss: 3.9635\n",
      "Epoch: 3074, Train Loss: 0.5645, Test Loss: 4.1807\n",
      "Epoch: 3075, Train Loss: 0.5354, Test Loss: 4.5301\n",
      "Epoch: 3076, Train Loss: 0.5987, Test Loss: 4.0203\n",
      "Epoch: 3077, Train Loss: 0.6121, Test Loss: 4.1465\n",
      "Epoch: 3078, Train Loss: 0.5916, Test Loss: 4.3027\n",
      "Epoch: 3079, Train Loss: 0.5939, Test Loss: 4.7951\n",
      "Epoch: 3080, Train Loss: 0.7074, Test Loss: 4.0849\n",
      "Epoch: 3081, Train Loss: 0.5559, Test Loss: 3.8312\n",
      "Epoch: 3082, Train Loss: 0.6531, Test Loss: 4.5033\n",
      "Epoch: 3083, Train Loss: 0.6502, Test Loss: 4.4488\n",
      "Epoch: 3084, Train Loss: 0.5849, Test Loss: 4.1627\n",
      "Epoch: 3085, Train Loss: 0.5501, Test Loss: 3.9185\n",
      "Epoch: 3086, Train Loss: 0.5742, Test Loss: 4.0942\n",
      "Epoch: 3087, Train Loss: 0.5080, Test Loss: 4.2912\n",
      "Epoch: 3088, Train Loss: 0.6198, Test Loss: 4.1074\n",
      "Epoch: 3089, Train Loss: 0.5886, Test Loss: 3.5082\n",
      "Epoch: 3090, Train Loss: 0.6033, Test Loss: 3.6907\n",
      "Epoch: 3091, Train Loss: 0.5844, Test Loss: 4.8953\n",
      "Epoch: 3092, Train Loss: 0.6524, Test Loss: 5.0728\n",
      "Epoch: 3093, Train Loss: 0.5995, Test Loss: 4.2306\n",
      "Epoch: 3094, Train Loss: 0.5432, Test Loss: 3.7371\n",
      "Epoch: 3095, Train Loss: 0.6283, Test Loss: 4.3026\n",
      "Epoch: 3096, Train Loss: 0.6111, Test Loss: 4.4350\n",
      "Epoch: 3097, Train Loss: 0.6160, Test Loss: 3.8768\n",
      "Epoch: 3098, Train Loss: 0.5482, Test Loss: 3.7384\n",
      "Epoch: 3099, Train Loss: 0.6239, Test Loss: 4.0595\n",
      "Epoch: 3100, Train Loss: 0.5362, Test Loss: 4.5974\n",
      "Epoch: 3101, Train Loss: 0.6274, Test Loss: 4.0586\n",
      "Epoch: 3102, Train Loss: 0.5725, Test Loss: 3.8247\n",
      "Epoch: 3103, Train Loss: 0.5621, Test Loss: 4.2354\n",
      "Epoch: 3104, Train Loss: 0.5375, Test Loss: 4.4233\n",
      "Epoch: 3105, Train Loss: 0.6288, Test Loss: 3.9273\n",
      "Epoch: 3106, Train Loss: 0.5352, Test Loss: 3.6174\n",
      "Epoch: 3107, Train Loss: 0.5736, Test Loss: 3.9480\n",
      "Epoch: 3108, Train Loss: 0.5904, Test Loss: 4.7035\n",
      "Epoch: 3109, Train Loss: 0.6469, Test Loss: 4.3873\n",
      "Epoch: 3110, Train Loss: 0.5503, Test Loss: 3.9610\n",
      "Epoch: 3111, Train Loss: 0.5427, Test Loss: 3.7262\n",
      "Epoch: 3112, Train Loss: 0.5315, Test Loss: 3.8060\n",
      "Epoch: 3113, Train Loss: 0.5743, Test Loss: 4.6202\n",
      "Epoch: 3114, Train Loss: 0.5727, Test Loss: 4.6456\n",
      "Epoch: 3115, Train Loss: 0.6280, Test Loss: 3.8559\n",
      "Epoch: 3116, Train Loss: 0.5974, Test Loss: 3.6928\n",
      "Epoch: 3117, Train Loss: 0.5775, Test Loss: 4.2933\n",
      "Epoch: 3118, Train Loss: 0.5308, Test Loss: 4.8398\n",
      "Epoch: 3119, Train Loss: 0.5609, Test Loss: 4.2787\n",
      "Epoch: 3120, Train Loss: 0.5743, Test Loss: 3.6206\n",
      "Epoch: 3121, Train Loss: 0.6389, Test Loss: 3.9328\n",
      "Epoch: 3122, Train Loss: 0.5791, Test Loss: 4.2565\n",
      "Epoch: 3123, Train Loss: 0.6394, Test Loss: 3.8766\n",
      "Epoch: 3124, Train Loss: 0.5774, Test Loss: 4.0132\n",
      "Epoch: 3125, Train Loss: 0.5529, Test Loss: 4.3218\n",
      "Epoch: 3126, Train Loss: 0.5329, Test Loss: 4.2029\n",
      "Epoch: 3127, Train Loss: 0.6060, Test Loss: 4.2061\n",
      "Epoch: 3128, Train Loss: 0.5674, Test Loss: 4.2193\n",
      "Epoch: 3129, Train Loss: 0.5593, Test Loss: 3.8668\n",
      "Epoch: 3130, Train Loss: 0.5907, Test Loss: 3.9335\n",
      "Epoch: 3131, Train Loss: 0.6014, Test Loss: 4.0698\n",
      "Epoch: 3132, Train Loss: 0.5659, Test Loss: 4.8044\n",
      "Epoch: 3133, Train Loss: 0.6186, Test Loss: 4.2137\n",
      "Epoch: 3134, Train Loss: 0.5478, Test Loss: 3.5575\n",
      "Epoch: 3135, Train Loss: 0.7371, Test Loss: 4.3243\n",
      "Epoch: 3136, Train Loss: 0.5607, Test Loss: 5.3580\n",
      "Epoch: 3137, Train Loss: 0.6666, Test Loss: 4.6248\n",
      "Epoch: 3138, Train Loss: 0.5708, Test Loss: 3.5330\n",
      "Epoch: 3139, Train Loss: 0.6650, Test Loss: 3.3996\n",
      "Epoch: 3140, Train Loss: 0.6994, Test Loss: 4.3038\n",
      "Epoch: 3141, Train Loss: 0.5682, Test Loss: 5.2092\n",
      "Epoch: 3142, Train Loss: 0.8172, Test Loss: 4.0988\n",
      "Epoch: 3143, Train Loss: 0.6099, Test Loss: 3.3115\n",
      "Epoch: 3144, Train Loss: 0.6611, Test Loss: 3.5378\n",
      "Epoch: 3145, Train Loss: 0.7112, Test Loss: 5.1602\n",
      "Epoch: 3146, Train Loss: 0.7448, Test Loss: 4.9120\n",
      "Epoch: 3147, Train Loss: 0.6329, Test Loss: 3.6473\n",
      "Epoch: 3148, Train Loss: 0.6064, Test Loss: 3.3954\n",
      "Epoch: 3149, Train Loss: 0.6713, Test Loss: 3.8611\n",
      "Epoch: 3150, Train Loss: 0.5358, Test Loss: 4.5568\n",
      "Epoch: 3151, Train Loss: 0.6101, Test Loss: 4.3908\n",
      "Epoch: 3152, Train Loss: 0.5977, Test Loss: 3.8020\n",
      "Epoch: 3153, Train Loss: 0.6040, Test Loss: 3.7652\n",
      "Epoch: 3154, Train Loss: 0.5228, Test Loss: 3.8927\n",
      "Epoch: 3155, Train Loss: 0.5545, Test Loss: 4.2458\n",
      "Epoch: 3156, Train Loss: 0.5687, Test Loss: 4.1304\n",
      "Epoch: 3157, Train Loss: 0.5258, Test Loss: 3.8899\n",
      "Epoch: 3158, Train Loss: 0.5738, Test Loss: 3.9321\n",
      "Epoch: 3159, Train Loss: 0.5348, Test Loss: 4.1700\n",
      "Epoch: 3160, Train Loss: 0.5502, Test Loss: 3.9441\n",
      "Epoch: 3161, Train Loss: 0.5572, Test Loss: 3.9028\n",
      "Epoch: 3162, Train Loss: 0.5187, Test Loss: 3.9851\n",
      "Epoch: 3163, Train Loss: 0.5413, Test Loss: 4.0041\n",
      "Epoch: 3164, Train Loss: 0.5610, Test Loss: 4.0945\n",
      "Epoch: 3165, Train Loss: 0.5291, Test Loss: 4.2210\n",
      "Epoch: 3166, Train Loss: 0.5422, Test Loss: 4.3209\n",
      "Epoch: 3167, Train Loss: 0.5716, Test Loss: 3.6972\n",
      "Epoch: 3168, Train Loss: 0.5329, Test Loss: 3.6378\n",
      "Epoch: 3169, Train Loss: 0.5708, Test Loss: 3.8238\n",
      "Epoch: 3170, Train Loss: 0.5565, Test Loss: 4.5405\n",
      "Epoch: 3171, Train Loss: 0.5867, Test Loss: 4.3955\n",
      "Epoch: 3172, Train Loss: 0.5943, Test Loss: 3.7966\n",
      "Epoch: 3173, Train Loss: 0.6162, Test Loss: 3.9945\n",
      "Epoch: 3174, Train Loss: 0.5261, Test Loss: 4.3076\n",
      "Epoch: 3175, Train Loss: 0.5271, Test Loss: 4.2305\n",
      "Epoch: 3176, Train Loss: 0.5974, Test Loss: 3.9385\n",
      "Epoch: 3177, Train Loss: 0.6240, Test Loss: 4.0490\n",
      "Epoch: 3178, Train Loss: 0.5741, Test Loss: 3.8562\n",
      "Epoch: 3179, Train Loss: 0.6057, Test Loss: 4.0706\n",
      "Epoch: 3180, Train Loss: 0.5833, Test Loss: 4.4981\n",
      "Epoch: 3181, Train Loss: 0.6229, Test Loss: 4.3626\n",
      "Epoch: 3182, Train Loss: 0.5628, Test Loss: 4.0758\n",
      "Epoch: 3183, Train Loss: 0.5494, Test Loss: 3.7714\n",
      "Epoch: 3184, Train Loss: 0.5421, Test Loss: 3.8700\n",
      "Epoch: 3185, Train Loss: 0.5712, Test Loss: 4.2858\n",
      "Epoch: 3186, Train Loss: 0.5524, Test Loss: 4.9946\n",
      "Epoch: 3187, Train Loss: 0.7243, Test Loss: 4.2208\n",
      "Epoch: 3188, Train Loss: 0.5594, Test Loss: 3.4452\n",
      "Epoch: 3189, Train Loss: 0.6914, Test Loss: 4.0118\n",
      "Epoch: 3190, Train Loss: 0.6076, Test Loss: 5.1075\n",
      "Epoch: 3191, Train Loss: 0.6731, Test Loss: 4.5205\n",
      "Epoch: 3192, Train Loss: 0.5729, Test Loss: 3.6515\n",
      "Epoch: 3193, Train Loss: 0.5954, Test Loss: 3.4250\n",
      "Epoch: 3194, Train Loss: 0.6997, Test Loss: 4.2738\n",
      "Epoch: 3195, Train Loss: 0.5550, Test Loss: 5.0025\n",
      "Epoch: 3196, Train Loss: 0.7078, Test Loss: 3.9924\n",
      "Epoch: 3197, Train Loss: 0.5439, Test Loss: 3.5101\n",
      "Epoch: 3198, Train Loss: 0.6269, Test Loss: 3.8071\n",
      "Epoch: 3199, Train Loss: 0.5442, Test Loss: 4.8300\n",
      "Epoch: 3200, Train Loss: 0.6891, Test Loss: 4.2618\n",
      "Epoch: 3201, Train Loss: 0.5541, Test Loss: 3.8446\n",
      "Epoch: 3202, Train Loss: 0.5907, Test Loss: 3.5778\n",
      "Epoch: 3203, Train Loss: 0.6188, Test Loss: 4.0626\n",
      "Epoch: 3204, Train Loss: 0.5399, Test Loss: 4.4951\n",
      "Epoch: 3205, Train Loss: 0.5498, Test Loss: 4.3680\n",
      "Epoch: 3206, Train Loss: 0.5528, Test Loss: 3.9455\n",
      "Epoch: 3207, Train Loss: 0.5762, Test Loss: 3.8942\n",
      "Epoch: 3208, Train Loss: 0.5203, Test Loss: 3.9327\n",
      "Epoch: 3209, Train Loss: 0.5665, Test Loss: 4.4089\n",
      "Epoch: 3210, Train Loss: 0.5788, Test Loss: 4.3071\n",
      "Epoch: 3211, Train Loss: 0.5554, Test Loss: 3.7813\n",
      "Epoch: 3212, Train Loss: 0.5769, Test Loss: 4.0124\n",
      "Epoch: 3213, Train Loss: 0.5867, Test Loss: 4.6692\n",
      "Epoch: 3214, Train Loss: 0.5478, Test Loss: 5.0241\n",
      "Epoch: 3215, Train Loss: 0.7110, Test Loss: 3.8925\n",
      "Epoch: 3216, Train Loss: 0.5321, Test Loss: 3.3721\n",
      "Epoch: 3217, Train Loss: 0.7752, Test Loss: 4.0906\n",
      "Epoch: 3218, Train Loss: 0.5788, Test Loss: 5.1098\n",
      "Epoch: 3219, Train Loss: 0.7023, Test Loss: 4.3962\n",
      "Epoch: 3220, Train Loss: 0.5490, Test Loss: 3.5746\n",
      "Epoch: 3221, Train Loss: 0.5502, Test Loss: 3.4925\n",
      "Epoch: 3222, Train Loss: 0.6586, Test Loss: 4.5428\n",
      "Epoch: 3223, Train Loss: 0.5615, Test Loss: 4.8355\n",
      "Epoch: 3224, Train Loss: 0.5680, Test Loss: 4.2888\n",
      "Epoch: 3225, Train Loss: 0.5098, Test Loss: 3.6930\n",
      "Epoch: 3226, Train Loss: 0.5810, Test Loss: 3.6311\n",
      "Epoch: 3227, Train Loss: 0.5490, Test Loss: 4.0522\n",
      "Epoch: 3228, Train Loss: 0.5660, Test Loss: 4.6428\n",
      "Epoch: 3229, Train Loss: 0.5688, Test Loss: 4.7443\n",
      "Epoch: 3230, Train Loss: 0.6359, Test Loss: 4.0229\n",
      "Epoch: 3231, Train Loss: 0.5163, Test Loss: 3.4769\n",
      "Epoch: 3232, Train Loss: 0.7028, Test Loss: 4.0131\n",
      "Epoch: 3233, Train Loss: 0.5704, Test Loss: 5.0783\n",
      "Epoch: 3234, Train Loss: 0.7169, Test Loss: 4.3744\n",
      "Epoch: 3235, Train Loss: 0.6293, Test Loss: 3.4826\n",
      "Epoch: 3236, Train Loss: 0.6037, Test Loss: 3.5271\n",
      "Epoch: 3237, Train Loss: 0.6035, Test Loss: 4.4807\n",
      "Epoch: 3238, Train Loss: 0.6378, Test Loss: 4.5505\n",
      "Epoch: 3239, Train Loss: 0.6304, Test Loss: 3.7280\n",
      "Epoch: 3240, Train Loss: 0.5657, Test Loss: 3.3733\n",
      "Epoch: 3241, Train Loss: 0.6583, Test Loss: 3.7702\n",
      "Epoch: 3242, Train Loss: 0.5484, Test Loss: 4.4603\n",
      "Epoch: 3243, Train Loss: 0.6281, Test Loss: 4.2322\n",
      "Epoch: 3244, Train Loss: 0.5602, Test Loss: 3.7471\n",
      "Epoch: 3245, Train Loss: 0.5252, Test Loss: 3.5770\n",
      "Epoch: 3246, Train Loss: 0.5798, Test Loss: 3.7775\n",
      "Epoch: 3247, Train Loss: 0.5290, Test Loss: 4.3789\n",
      "Epoch: 3248, Train Loss: 0.5989, Test Loss: 4.4432\n",
      "Epoch: 3249, Train Loss: 0.5683, Test Loss: 4.0190\n",
      "Epoch: 3250, Train Loss: 0.5135, Test Loss: 3.7476\n",
      "Epoch: 3251, Train Loss: 0.5247, Test Loss: 3.7119\n",
      "Epoch: 3252, Train Loss: 0.5515, Test Loss: 4.1684\n",
      "Epoch: 3253, Train Loss: 0.5096, Test Loss: 4.7955\n",
      "Epoch: 3254, Train Loss: 0.5777, Test Loss: 4.3558\n",
      "Epoch: 3255, Train Loss: 0.5561, Test Loss: 3.9328\n",
      "Epoch: 3256, Train Loss: 0.5372, Test Loss: 3.7300\n",
      "Epoch: 3257, Train Loss: 0.5744, Test Loss: 4.0120\n",
      "Epoch: 3258, Train Loss: 0.5175, Test Loss: 4.3065\n",
      "Epoch: 3259, Train Loss: 0.5309, Test Loss: 3.9782\n",
      "Epoch: 3260, Train Loss: 0.5177, Test Loss: 3.8173\n",
      "Epoch: 3261, Train Loss: 0.5700, Test Loss: 3.8736\n",
      "Epoch: 3262, Train Loss: 0.5337, Test Loss: 4.3529\n",
      "Epoch: 3263, Train Loss: 0.5354, Test Loss: 4.3255\n",
      "Epoch: 3264, Train Loss: 0.6180, Test Loss: 3.6134\n",
      "Epoch: 3265, Train Loss: 0.5931, Test Loss: 3.5497\n",
      "Epoch: 3266, Train Loss: 0.5840, Test Loss: 4.0909\n",
      "Epoch: 3267, Train Loss: 0.5380, Test Loss: 4.6397\n",
      "Epoch: 3268, Train Loss: 0.5982, Test Loss: 4.3255\n",
      "Epoch: 3269, Train Loss: 0.5968, Test Loss: 3.5527\n",
      "Epoch: 3270, Train Loss: 0.5726, Test Loss: 3.5998\n",
      "Epoch: 3271, Train Loss: 0.5641, Test Loss: 4.2149\n",
      "Epoch: 3272, Train Loss: 0.5463, Test Loss: 4.3984\n",
      "Epoch: 3273, Train Loss: 0.5554, Test Loss: 3.9240\n",
      "Epoch: 3274, Train Loss: 0.5669, Test Loss: 3.8410\n",
      "Epoch: 3275, Train Loss: 0.5704, Test Loss: 4.0533\n",
      "Epoch: 3276, Train Loss: 0.4975, Test Loss: 4.4446\n",
      "Epoch: 3277, Train Loss: 0.5735, Test Loss: 4.6846\n",
      "Epoch: 3278, Train Loss: 0.6088, Test Loss: 4.1582\n",
      "Epoch: 3279, Train Loss: 0.6270, Test Loss: 3.4854\n",
      "Epoch: 3280, Train Loss: 0.6963, Test Loss: 4.0243\n",
      "Epoch: 3281, Train Loss: 0.5836, Test Loss: 5.4798\n",
      "Epoch: 3282, Train Loss: 0.8413, Test Loss: 4.3305\n",
      "Epoch: 3283, Train Loss: 0.5248, Test Loss: 3.5537\n",
      "Epoch: 3284, Train Loss: 0.6315, Test Loss: 3.6114\n",
      "Epoch: 3285, Train Loss: 0.5643, Test Loss: 4.0050\n",
      "Epoch: 3286, Train Loss: 0.5565, Test Loss: 4.6907\n",
      "Epoch: 3287, Train Loss: 0.5980, Test Loss: 4.1381\n",
      "Epoch: 3288, Train Loss: 0.5310, Test Loss: 3.9413\n",
      "Epoch: 3289, Train Loss: 0.5782, Test Loss: 4.0606\n",
      "Epoch: 3290, Train Loss: 0.5253, Test Loss: 4.3905\n",
      "Epoch: 3291, Train Loss: 0.5622, Test Loss: 4.0561\n",
      "Epoch: 3292, Train Loss: 0.5826, Test Loss: 3.7561\n",
      "Epoch: 3293, Train Loss: 0.5461, Test Loss: 3.9094\n",
      "Epoch: 3294, Train Loss: 0.5280, Test Loss: 4.4389\n",
      "Epoch: 3295, Train Loss: 0.5849, Test Loss: 4.6181\n",
      "Epoch: 3296, Train Loss: 0.6055, Test Loss: 3.9015\n",
      "Epoch: 3297, Train Loss: 0.5645, Test Loss: 3.8911\n",
      "Epoch: 3298, Train Loss: 0.5353, Test Loss: 4.2692\n",
      "Epoch: 3299, Train Loss: 0.5197, Test Loss: 4.7335\n",
      "Epoch: 3300, Train Loss: 0.6174, Test Loss: 4.1971\n",
      "Epoch: 3301, Train Loss: 0.5643, Test Loss: 3.5994\n",
      "Epoch: 3302, Train Loss: 0.6209, Test Loss: 4.0502\n",
      "Epoch: 3303, Train Loss: 0.5152, Test Loss: 4.5412\n",
      "Epoch: 3304, Train Loss: 0.6209, Test Loss: 4.2546\n",
      "Epoch: 3305, Train Loss: 0.5338, Test Loss: 3.8462\n",
      "Epoch: 3306, Train Loss: 0.5536, Test Loss: 3.7389\n",
      "Epoch: 3307, Train Loss: 0.5871, Test Loss: 4.5604\n",
      "Epoch: 3308, Train Loss: 0.5702, Test Loss: 4.7133\n",
      "Epoch: 3309, Train Loss: 0.5775, Test Loss: 3.9221\n",
      "Epoch: 3310, Train Loss: 0.5053, Test Loss: 3.7073\n",
      "Epoch: 3311, Train Loss: 0.6242, Test Loss: 4.2035\n",
      "Epoch: 3312, Train Loss: 0.5179, Test Loss: 4.4635\n",
      "Epoch: 3313, Train Loss: 0.5612, Test Loss: 4.1321\n",
      "Epoch: 3314, Train Loss: 0.5041, Test Loss: 3.9121\n",
      "Epoch: 3315, Train Loss: 0.5295, Test Loss: 3.9894\n",
      "Epoch: 3316, Train Loss: 0.5442, Test Loss: 4.3689\n",
      "Epoch: 3317, Train Loss: 0.5351, Test Loss: 4.5343\n",
      "Epoch: 3318, Train Loss: 0.6053, Test Loss: 4.0219\n",
      "Epoch: 3319, Train Loss: 0.5247, Test Loss: 3.7218\n",
      "Epoch: 3320, Train Loss: 0.6487, Test Loss: 4.2317\n",
      "Epoch: 3321, Train Loss: 0.5734, Test Loss: 4.5006\n",
      "Epoch: 3322, Train Loss: 0.5529, Test Loss: 4.2434\n",
      "Epoch: 3323, Train Loss: 0.5125, Test Loss: 3.6699\n",
      "Epoch: 3324, Train Loss: 0.6775, Test Loss: 4.2816\n",
      "Epoch: 3325, Train Loss: 0.5667, Test Loss: 4.6435\n",
      "Epoch: 3326, Train Loss: 0.5748, Test Loss: 4.3122\n",
      "Epoch: 3327, Train Loss: 0.5401, Test Loss: 3.7831\n",
      "Epoch: 3328, Train Loss: 0.5220, Test Loss: 3.7098\n",
      "Epoch: 3329, Train Loss: 0.5325, Test Loss: 4.0387\n",
      "Epoch: 3330, Train Loss: 0.4962, Test Loss: 4.2823\n",
      "Epoch: 3331, Train Loss: 0.5556, Test Loss: 3.9944\n",
      "Epoch: 3332, Train Loss: 0.5486, Test Loss: 3.8236\n",
      "Epoch: 3333, Train Loss: 0.5552, Test Loss: 3.6561\n",
      "Epoch: 3334, Train Loss: 0.5710, Test Loss: 3.7258\n",
      "Epoch: 3335, Train Loss: 0.5471, Test Loss: 4.6856\n",
      "Epoch: 3336, Train Loss: 0.5749, Test Loss: 4.6002\n",
      "Epoch: 3337, Train Loss: 0.6106, Test Loss: 3.5394\n",
      "Epoch: 3338, Train Loss: 0.5667, Test Loss: 3.1689\n",
      "Epoch: 3339, Train Loss: 0.6763, Test Loss: 3.7661\n",
      "Epoch: 3340, Train Loss: 0.5562, Test Loss: 4.7417\n",
      "Epoch: 3341, Train Loss: 0.6349, Test Loss: 4.3053\n",
      "Epoch: 3342, Train Loss: 0.5231, Test Loss: 3.8327\n",
      "Epoch: 3343, Train Loss: 0.5348, Test Loss: 3.7587\n",
      "Epoch: 3344, Train Loss: 0.5966, Test Loss: 4.1357\n",
      "Epoch: 3345, Train Loss: 0.5392, Test Loss: 4.1473\n",
      "Epoch: 3346, Train Loss: 0.4991, Test Loss: 3.8050\n",
      "Epoch: 3347, Train Loss: 0.5387, Test Loss: 3.6028\n",
      "Epoch: 3348, Train Loss: 0.5469, Test Loss: 3.7932\n",
      "Epoch: 3349, Train Loss: 0.5509, Test Loss: 4.3715\n",
      "Epoch: 3350, Train Loss: 0.5595, Test Loss: 4.5228\n",
      "Epoch: 3351, Train Loss: 0.5722, Test Loss: 4.1776\n",
      "Epoch: 3352, Train Loss: 0.5380, Test Loss: 3.8895\n",
      "Epoch: 3353, Train Loss: 0.5953, Test Loss: 4.3040\n",
      "Epoch: 3354, Train Loss: 0.6166, Test Loss: 3.8272\n",
      "Epoch: 3355, Train Loss: 0.5358, Test Loss: 3.5535\n",
      "Epoch: 3356, Train Loss: 0.5251, Test Loss: 3.5228\n",
      "Epoch: 3357, Train Loss: 0.5997, Test Loss: 4.3334\n",
      "Epoch: 3358, Train Loss: 0.5685, Test Loss: 4.6661\n",
      "Epoch: 3359, Train Loss: 0.5835, Test Loss: 4.1242\n",
      "Epoch: 3360, Train Loss: 0.5058, Test Loss: 3.7154\n",
      "Epoch: 3361, Train Loss: 0.5753, Test Loss: 3.8211\n",
      "Epoch: 3362, Train Loss: 0.5491, Test Loss: 4.4686\n",
      "Epoch: 3363, Train Loss: 0.5457, Test Loss: 4.3445\n",
      "Epoch: 3364, Train Loss: 0.5923, Test Loss: 3.6304\n",
      "Epoch: 3365, Train Loss: 0.5808, Test Loss: 3.7203\n",
      "Epoch: 3366, Train Loss: 0.6161, Test Loss: 4.1829\n",
      "Epoch: 3367, Train Loss: 0.5224, Test Loss: 4.3780\n",
      "Epoch: 3368, Train Loss: 0.5713, Test Loss: 3.9121\n",
      "Epoch: 3369, Train Loss: 0.5326, Test Loss: 3.6378\n",
      "Epoch: 3370, Train Loss: 0.5625, Test Loss: 3.7393\n",
      "Epoch: 3371, Train Loss: 0.5728, Test Loss: 4.7852\n",
      "Epoch: 3372, Train Loss: 0.6178, Test Loss: 4.3732\n",
      "Epoch: 3373, Train Loss: 0.5373, Test Loss: 3.6078\n",
      "Epoch: 3374, Train Loss: 0.5255, Test Loss: 3.5493\n",
      "Epoch: 3375, Train Loss: 0.6360, Test Loss: 4.4146\n",
      "Epoch: 3376, Train Loss: 0.6222, Test Loss: 4.7592\n",
      "Epoch: 3377, Train Loss: 0.6366, Test Loss: 3.7137\n",
      "Epoch: 3378, Train Loss: 0.5474, Test Loss: 3.2884\n",
      "Epoch: 3379, Train Loss: 0.7069, Test Loss: 3.9432\n",
      "Epoch: 3380, Train Loss: 0.5798, Test Loss: 4.4349\n",
      "Epoch: 3381, Train Loss: 0.6537, Test Loss: 3.8962\n",
      "Epoch: 3382, Train Loss: 0.5075, Test Loss: 3.5088\n",
      "Epoch: 3383, Train Loss: 0.5216, Test Loss: 3.5201\n",
      "Epoch: 3384, Train Loss: 0.5584, Test Loss: 4.0405\n",
      "Epoch: 3385, Train Loss: 0.5164, Test Loss: 4.2964\n",
      "Epoch: 3386, Train Loss: 0.5100, Test Loss: 4.0809\n",
      "Epoch: 3387, Train Loss: 0.5909, Test Loss: 3.4796\n",
      "Epoch: 3388, Train Loss: 0.6388, Test Loss: 3.7941\n",
      "Epoch: 3389, Train Loss: 0.5090, Test Loss: 4.4600\n",
      "Epoch: 3390, Train Loss: 0.5606, Test Loss: 4.1884\n",
      "Epoch: 3391, Train Loss: 0.5656, Test Loss: 3.3868\n",
      "Epoch: 3392, Train Loss: 0.5696, Test Loss: 3.3541\n",
      "Epoch: 3393, Train Loss: 0.5853, Test Loss: 4.1383\n",
      "Epoch: 3394, Train Loss: 0.5619, Test Loss: 4.7317\n",
      "Epoch: 3395, Train Loss: 0.6895, Test Loss: 3.7797\n",
      "Epoch: 3396, Train Loss: 0.5097, Test Loss: 3.3334\n",
      "Epoch: 3397, Train Loss: 0.6830, Test Loss: 3.9930\n",
      "Epoch: 3398, Train Loss: 0.5439, Test Loss: 4.8805\n",
      "Epoch: 3399, Train Loss: 0.7019, Test Loss: 4.1143\n",
      "Epoch: 3400, Train Loss: 0.5223, Test Loss: 3.5769\n",
      "Epoch: 3401, Train Loss: 0.5613, Test Loss: 3.6527\n",
      "Epoch: 3402, Train Loss: 0.5406, Test Loss: 4.2074\n",
      "Epoch: 3403, Train Loss: 0.6007, Test Loss: 4.5235\n",
      "Epoch: 3404, Train Loss: 0.5850, Test Loss: 4.2871\n",
      "Epoch: 3405, Train Loss: 0.5072, Test Loss: 3.9451\n",
      "Epoch: 3406, Train Loss: 0.5442, Test Loss: 4.1428\n",
      "Epoch: 3407, Train Loss: 0.5064, Test Loss: 4.2993\n",
      "Epoch: 3408, Train Loss: 0.5373, Test Loss: 4.0960\n",
      "Epoch: 3409, Train Loss: 0.5179, Test Loss: 3.9927\n",
      "Epoch: 3410, Train Loss: 0.5218, Test Loss: 3.6348\n",
      "Epoch: 3411, Train Loss: 0.5427, Test Loss: 3.6964\n",
      "Epoch: 3412, Train Loss: 0.5522, Test Loss: 3.8460\n",
      "Epoch: 3413, Train Loss: 0.5725, Test Loss: 4.3265\n",
      "Epoch: 3414, Train Loss: 0.5121, Test Loss: 4.2444\n",
      "Epoch: 3415, Train Loss: 0.5213, Test Loss: 4.1455\n",
      "Epoch: 3416, Train Loss: 0.5231, Test Loss: 3.8932\n",
      "Epoch: 3417, Train Loss: 0.5708, Test Loss: 3.8627\n",
      "Epoch: 3418, Train Loss: 0.5056, Test Loss: 4.1385\n",
      "Epoch: 3419, Train Loss: 0.5255, Test Loss: 4.4945\n",
      "Epoch: 3420, Train Loss: 0.5096, Test Loss: 4.2819\n",
      "Epoch: 3421, Train Loss: 0.5513, Test Loss: 3.7472\n",
      "Epoch: 3422, Train Loss: 0.5319, Test Loss: 3.7507\n",
      "Epoch: 3423, Train Loss: 0.5395, Test Loss: 4.2316\n",
      "Epoch: 3424, Train Loss: 0.5288, Test Loss: 4.6407\n",
      "Epoch: 3425, Train Loss: 0.5676, Test Loss: 4.2693\n",
      "Epoch: 3426, Train Loss: 0.5087, Test Loss: 3.6649\n",
      "Epoch: 3427, Train Loss: 0.6346, Test Loss: 3.8987\n",
      "Epoch: 3428, Train Loss: 0.5574, Test Loss: 4.2962\n",
      "Epoch: 3429, Train Loss: 0.5547, Test Loss: 4.2575\n",
      "Epoch: 3430, Train Loss: 0.5361, Test Loss: 4.4059\n",
      "Epoch: 3431, Train Loss: 0.5536, Test Loss: 4.3104\n",
      "Epoch: 3432, Train Loss: 0.5550, Test Loss: 3.7365\n",
      "Epoch: 3433, Train Loss: 0.5748, Test Loss: 3.7461\n",
      "Epoch: 3434, Train Loss: 0.5802, Test Loss: 4.4808\n",
      "Epoch: 3435, Train Loss: 0.5637, Test Loss: 4.7952\n",
      "Epoch: 3436, Train Loss: 0.6088, Test Loss: 4.1401\n",
      "Epoch: 3437, Train Loss: 0.5231, Test Loss: 3.4648\n",
      "Epoch: 3438, Train Loss: 0.5611, Test Loss: 3.6643\n",
      "Epoch: 3439, Train Loss: 0.5909, Test Loss: 4.4731\n",
      "Epoch: 3440, Train Loss: 0.6107, Test Loss: 4.4740\n",
      "Epoch: 3441, Train Loss: 0.5686, Test Loss: 3.8019\n",
      "Epoch: 3442, Train Loss: 0.5898, Test Loss: 3.4866\n",
      "Epoch: 3443, Train Loss: 0.5599, Test Loss: 3.7222\n",
      "Epoch: 3444, Train Loss: 0.5541, Test Loss: 4.0059\n",
      "Epoch: 3445, Train Loss: 0.5582, Test Loss: 4.5190\n",
      "Epoch: 3446, Train Loss: 0.5559, Test Loss: 4.1974\n",
      "Epoch: 3447, Train Loss: 0.5730, Test Loss: 3.8272\n",
      "Epoch: 3448, Train Loss: 0.5423, Test Loss: 3.7067\n",
      "Epoch: 3449, Train Loss: 0.5384, Test Loss: 4.1290\n",
      "Epoch: 3450, Train Loss: 0.5209, Test Loss: 4.4835\n",
      "Epoch: 3451, Train Loss: 0.5470, Test Loss: 4.0967\n",
      "Epoch: 3452, Train Loss: 0.5174, Test Loss: 3.9796\n",
      "Epoch: 3453, Train Loss: 0.5589, Test Loss: 4.1548\n",
      "Epoch: 3454, Train Loss: 0.5207, Test Loss: 4.2536\n",
      "Epoch: 3455, Train Loss: 0.5214, Test Loss: 4.0743\n",
      "Epoch: 3456, Train Loss: 0.5109, Test Loss: 3.6936\n",
      "Epoch: 3457, Train Loss: 0.5325, Test Loss: 3.6209\n",
      "Epoch: 3458, Train Loss: 0.5517, Test Loss: 4.1194\n",
      "Epoch: 3459, Train Loss: 0.5184, Test Loss: 4.3550\n",
      "Epoch: 3460, Train Loss: 0.5166, Test Loss: 4.4323\n",
      "Epoch: 3461, Train Loss: 0.5624, Test Loss: 4.2123\n",
      "Epoch: 3462, Train Loss: 0.5238, Test Loss: 3.7271\n",
      "Epoch: 3463, Train Loss: 0.6002, Test Loss: 3.8038\n",
      "Epoch: 3464, Train Loss: 0.5443, Test Loss: 4.2302\n",
      "Epoch: 3465, Train Loss: 0.5330, Test Loss: 4.3723\n",
      "Epoch: 3466, Train Loss: 0.5215, Test Loss: 4.0197\n",
      "Epoch: 3467, Train Loss: 0.4739, Test Loss: 3.7441\n",
      "Epoch: 3468, Train Loss: 0.5146, Test Loss: 3.6895\n",
      "Epoch: 3469, Train Loss: 0.5919, Test Loss: 4.2925\n",
      "Epoch: 3470, Train Loss: 0.5634, Test Loss: 4.6099\n",
      "Epoch: 3471, Train Loss: 0.5773, Test Loss: 3.9851\n",
      "Epoch: 3472, Train Loss: 0.5449, Test Loss: 3.5682\n",
      "Epoch: 3473, Train Loss: 0.6251, Test Loss: 4.0068\n",
      "Epoch: 3474, Train Loss: 0.5534, Test Loss: 4.4893\n",
      "Epoch: 3475, Train Loss: 0.6223, Test Loss: 3.9771\n",
      "Epoch: 3476, Train Loss: 0.5921, Test Loss: 3.7105\n",
      "Epoch: 3477, Train Loss: 0.5124, Test Loss: 3.7589\n",
      "Epoch: 3478, Train Loss: 0.5453, Test Loss: 4.2549\n",
      "Epoch: 3479, Train Loss: 0.5093, Test Loss: 4.7057\n",
      "Epoch: 3480, Train Loss: 0.5434, Test Loss: 4.3669\n",
      "Epoch: 3481, Train Loss: 0.5453, Test Loss: 3.9717\n",
      "Epoch: 3482, Train Loss: 0.5423, Test Loss: 4.0127\n",
      "Epoch: 3483, Train Loss: 0.5105, Test Loss: 4.4988\n",
      "Epoch: 3484, Train Loss: 0.5675, Test Loss: 4.1009\n",
      "Epoch: 3485, Train Loss: 0.4864, Test Loss: 3.8809\n",
      "Epoch: 3486, Train Loss: 0.4979, Test Loss: 4.0337\n",
      "Epoch: 3487, Train Loss: 0.5168, Test Loss: 4.2460\n",
      "Epoch: 3488, Train Loss: 0.5405, Test Loss: 4.1435\n",
      "Epoch: 3489, Train Loss: 0.5533, Test Loss: 3.7253\n",
      "Epoch: 3490, Train Loss: 0.5376, Test Loss: 3.5812\n",
      "Epoch: 3491, Train Loss: 0.5647, Test Loss: 4.3511\n",
      "Epoch: 3492, Train Loss: 0.5699, Test Loss: 4.5576\n",
      "Epoch: 3493, Train Loss: 0.5739, Test Loss: 3.9524\n",
      "Epoch: 3494, Train Loss: 0.5487, Test Loss: 3.8018\n",
      "Epoch: 3495, Train Loss: 0.5607, Test Loss: 4.1111\n",
      "Epoch: 3496, Train Loss: 0.5545, Test Loss: 4.3718\n",
      "Epoch: 3497, Train Loss: 0.5363, Test Loss: 4.1131\n",
      "Epoch: 3498, Train Loss: 0.5199, Test Loss: 3.7672\n",
      "Epoch: 3499, Train Loss: 0.4984, Test Loss: 3.8641\n",
      "Epoch: 3500, Train Loss: 0.5487, Test Loss: 4.2744\n",
      "Epoch: 3501, Train Loss: 0.5792, Test Loss: 4.6705\n",
      "Epoch: 3502, Train Loss: 0.5756, Test Loss: 3.9397\n",
      "Epoch: 3503, Train Loss: 0.5071, Test Loss: 3.4320\n",
      "Epoch: 3504, Train Loss: 0.6709, Test Loss: 3.8049\n",
      "Epoch: 3505, Train Loss: 0.5215, Test Loss: 4.9466\n",
      "Epoch: 3506, Train Loss: 0.6899, Test Loss: 4.3941\n",
      "Epoch: 3507, Train Loss: 0.5831, Test Loss: 3.3792\n",
      "Epoch: 3508, Train Loss: 0.5905, Test Loss: 3.3906\n",
      "Epoch: 3509, Train Loss: 0.6236, Test Loss: 4.5632\n",
      "Epoch: 3510, Train Loss: 0.6036, Test Loss: 4.6633\n",
      "Epoch: 3511, Train Loss: 0.5684, Test Loss: 4.0936\n",
      "Epoch: 3512, Train Loss: 0.5014, Test Loss: 3.4815\n",
      "Epoch: 3513, Train Loss: 0.6465, Test Loss: 3.7806\n",
      "Epoch: 3514, Train Loss: 0.5734, Test Loss: 4.9223\n",
      "Epoch: 3515, Train Loss: 0.7244, Test Loss: 4.5494\n",
      "Epoch: 3516, Train Loss: 0.5444, Test Loss: 3.7438\n",
      "Epoch: 3517, Train Loss: 0.5247, Test Loss: 3.2862\n",
      "Epoch: 3518, Train Loss: 0.5952, Test Loss: 3.4786\n",
      "Epoch: 3519, Train Loss: 0.5860, Test Loss: 4.6308\n",
      "Epoch: 3520, Train Loss: 0.6224, Test Loss: 4.8080\n",
      "Epoch: 3521, Train Loss: 0.6144, Test Loss: 3.7796\n",
      "Epoch: 3522, Train Loss: 0.5721, Test Loss: 3.4082\n",
      "Epoch: 3523, Train Loss: 0.5799, Test Loss: 3.6276\n",
      "Epoch: 3524, Train Loss: 0.5357, Test Loss: 4.1991\n",
      "Epoch: 3525, Train Loss: 0.5026, Test Loss: 4.4640\n",
      "Epoch: 3526, Train Loss: 0.5741, Test Loss: 3.8097\n",
      "Epoch: 3527, Train Loss: 0.4956, Test Loss: 3.5353\n",
      "Epoch: 3528, Train Loss: 0.6917, Test Loss: 4.3478\n",
      "Epoch: 3529, Train Loss: 0.5355, Test Loss: 4.8200\n",
      "Epoch: 3530, Train Loss: 0.5610, Test Loss: 4.3376\n",
      "Epoch: 3531, Train Loss: 0.5450, Test Loss: 3.7845\n",
      "Epoch: 3532, Train Loss: 0.5885, Test Loss: 3.6835\n",
      "Epoch: 3533, Train Loss: 0.5512, Test Loss: 4.1535\n",
      "Epoch: 3534, Train Loss: 0.5341, Test Loss: 4.7090\n",
      "Epoch: 3535, Train Loss: 0.6496, Test Loss: 4.0310\n",
      "Epoch: 3536, Train Loss: 0.5912, Test Loss: 3.4066\n",
      "Epoch: 3537, Train Loss: 0.6349, Test Loss: 3.6894\n",
      "Epoch: 3538, Train Loss: 0.5367, Test Loss: 4.5998\n",
      "Epoch: 3539, Train Loss: 0.6721, Test Loss: 4.1733\n",
      "Epoch: 3540, Train Loss: 0.5437, Test Loss: 3.7413\n",
      "Epoch: 3541, Train Loss: 0.5306, Test Loss: 3.7187\n",
      "Epoch: 3542, Train Loss: 0.5664, Test Loss: 3.8748\n",
      "Epoch: 3543, Train Loss: 0.5806, Test Loss: 4.2634\n",
      "Epoch: 3544, Train Loss: 0.5679, Test Loss: 4.8158\n",
      "Epoch: 3545, Train Loss: 0.6043, Test Loss: 4.1115\n",
      "Epoch: 3546, Train Loss: 0.4924, Test Loss: 3.5673\n",
      "Epoch: 3547, Train Loss: 0.6939, Test Loss: 3.9024\n",
      "Epoch: 3548, Train Loss: 0.5906, Test Loss: 3.9299\n",
      "Epoch: 3549, Train Loss: 0.5377, Test Loss: 4.1849\n",
      "Epoch: 3550, Train Loss: 0.5360, Test Loss: 4.2391\n",
      "Epoch: 3551, Train Loss: 0.5762, Test Loss: 3.9379\n",
      "Epoch: 3552, Train Loss: 0.5520, Test Loss: 4.0039\n",
      "Epoch: 3553, Train Loss: 0.5686, Test Loss: 4.1916\n",
      "Epoch: 3554, Train Loss: 0.5744, Test Loss: 4.1088\n",
      "Epoch: 3555, Train Loss: 0.5559, Test Loss: 3.7705\n",
      "Epoch: 3556, Train Loss: 0.5455, Test Loss: 3.6174\n",
      "Epoch: 3557, Train Loss: 0.6111, Test Loss: 4.3174\n",
      "Epoch: 3558, Train Loss: 0.5019, Test Loss: 4.7382\n",
      "Epoch: 3559, Train Loss: 0.5842, Test Loss: 4.0460\n",
      "Epoch: 3560, Train Loss: 0.5224, Test Loss: 3.4817\n",
      "Epoch: 3561, Train Loss: 0.6232, Test Loss: 3.8448\n",
      "Epoch: 3562, Train Loss: 0.5186, Test Loss: 4.2174\n",
      "Epoch: 3563, Train Loss: 0.5903, Test Loss: 3.9885\n",
      "Epoch: 3564, Train Loss: 0.5212, Test Loss: 3.6703\n",
      "Epoch: 3565, Train Loss: 0.5889, Test Loss: 3.8201\n",
      "Epoch: 3566, Train Loss: 0.6299, Test Loss: 4.8593\n",
      "Epoch: 3567, Train Loss: 0.6515, Test Loss: 4.5441\n",
      "Epoch: 3568, Train Loss: 0.5017, Test Loss: 3.6764\n",
      "Epoch: 3569, Train Loss: 0.5444, Test Loss: 3.5436\n",
      "Epoch: 3570, Train Loss: 0.6345, Test Loss: 4.6150\n",
      "Epoch: 3571, Train Loss: 0.6082, Test Loss: 4.5683\n",
      "Epoch: 3572, Train Loss: 0.5757, Test Loss: 3.6839\n",
      "Epoch: 3573, Train Loss: 0.5123, Test Loss: 3.3171\n",
      "Epoch: 3574, Train Loss: 0.5988, Test Loss: 3.7204\n",
      "Epoch: 3575, Train Loss: 0.6070, Test Loss: 4.7657\n",
      "Epoch: 3576, Train Loss: 0.6264, Test Loss: 4.5205\n",
      "Epoch: 3577, Train Loss: 0.5030, Test Loss: 3.7048\n",
      "Epoch: 3578, Train Loss: 0.5469, Test Loss: 3.7269\n",
      "Epoch: 3579, Train Loss: 0.5528, Test Loss: 4.0774\n",
      "Epoch: 3580, Train Loss: 0.5347, Test Loss: 4.4015\n",
      "Epoch: 3581, Train Loss: 0.5486, Test Loss: 4.3833\n",
      "Epoch: 3582, Train Loss: 0.4969, Test Loss: 3.9757\n",
      "Epoch: 3583, Train Loss: 0.5087, Test Loss: 3.7528\n",
      "Epoch: 3584, Train Loss: 0.5527, Test Loss: 4.1645\n",
      "Epoch: 3585, Train Loss: 0.5012, Test Loss: 4.3569\n",
      "Epoch: 3586, Train Loss: 0.5760, Test Loss: 3.9378\n",
      "Epoch: 3587, Train Loss: 0.4838, Test Loss: 3.7020\n",
      "Epoch: 3588, Train Loss: 0.5462, Test Loss: 3.9672\n",
      "Epoch: 3589, Train Loss: 0.5370, Test Loss: 4.1196\n",
      "Epoch: 3590, Train Loss: 0.5757, Test Loss: 4.1381\n",
      "Epoch: 3591, Train Loss: 0.5607, Test Loss: 3.9169\n",
      "Epoch: 3592, Train Loss: 0.5243, Test Loss: 4.2039\n",
      "Epoch: 3593, Train Loss: 0.6078, Test Loss: 3.8544\n",
      "Epoch: 3594, Train Loss: 0.5374, Test Loss: 3.9825\n",
      "Epoch: 3595, Train Loss: 0.4836, Test Loss: 4.0833\n",
      "Epoch: 3596, Train Loss: 0.6001, Test Loss: 4.5485\n",
      "Epoch: 3597, Train Loss: 0.5711, Test Loss: 4.5666\n",
      "Epoch: 3598, Train Loss: 0.5902, Test Loss: 3.9179\n",
      "Epoch: 3599, Train Loss: 0.5376, Test Loss: 3.7097\n",
      "Epoch: 3600, Train Loss: 0.5426, Test Loss: 3.8588\n",
      "Epoch: 3601, Train Loss: 0.5416, Test Loss: 4.4764\n",
      "Epoch: 3602, Train Loss: 0.5671, Test Loss: 4.4064\n",
      "Epoch: 3603, Train Loss: 0.5339, Test Loss: 3.8761\n",
      "Epoch: 3604, Train Loss: 0.5700, Test Loss: 3.9329\n",
      "Epoch: 3605, Train Loss: 0.5279, Test Loss: 4.2265\n",
      "Epoch: 3606, Train Loss: 0.5185, Test Loss: 4.4930\n",
      "Epoch: 3607, Train Loss: 0.5458, Test Loss: 4.0366\n",
      "Epoch: 3608, Train Loss: 0.5660, Test Loss: 3.7021\n",
      "Epoch: 3609, Train Loss: 0.5607, Test Loss: 4.0196\n",
      "Epoch: 3610, Train Loss: 0.4977, Test Loss: 4.6788\n",
      "Epoch: 3611, Train Loss: 0.5599, Test Loss: 4.5734\n",
      "Epoch: 3612, Train Loss: 0.5628, Test Loss: 3.8588\n",
      "Epoch: 3613, Train Loss: 0.5413, Test Loss: 3.8925\n",
      "Epoch: 3614, Train Loss: 0.5084, Test Loss: 4.1773\n",
      "Epoch: 3615, Train Loss: 0.4801, Test Loss: 4.5008\n",
      "Epoch: 3616, Train Loss: 0.5832, Test Loss: 4.1705\n",
      "Epoch: 3617, Train Loss: 0.5055, Test Loss: 3.9032\n",
      "Epoch: 3618, Train Loss: 0.5185, Test Loss: 3.8328\n",
      "Epoch: 3619, Train Loss: 0.5111, Test Loss: 4.2024\n",
      "Epoch: 3620, Train Loss: 0.5036, Test Loss: 4.4496\n",
      "Epoch: 3621, Train Loss: 0.5275, Test Loss: 4.2556\n",
      "Epoch: 3622, Train Loss: 0.5105, Test Loss: 3.7704\n",
      "Epoch: 3623, Train Loss: 0.6252, Test Loss: 4.4583\n",
      "Epoch: 3624, Train Loss: 0.5004, Test Loss: 4.7701\n",
      "Epoch: 3625, Train Loss: 0.5686, Test Loss: 4.1609\n",
      "Epoch: 3626, Train Loss: 0.5406, Test Loss: 3.9022\n",
      "Epoch: 3627, Train Loss: 0.5499, Test Loss: 4.1625\n",
      "Epoch: 3628, Train Loss: 0.5235, Test Loss: 4.4329\n",
      "Epoch: 3629, Train Loss: 0.5472, Test Loss: 4.2374\n",
      "Epoch: 3630, Train Loss: 0.4823, Test Loss: 3.8585\n",
      "Epoch: 3631, Train Loss: 0.5421, Test Loss: 3.6977\n",
      "Epoch: 3632, Train Loss: 0.5369, Test Loss: 4.1868\n",
      "Epoch: 3633, Train Loss: 0.4708, Test Loss: 4.7815\n",
      "Epoch: 3634, Train Loss: 0.5958, Test Loss: 4.1661\n",
      "Epoch: 3635, Train Loss: 0.5195, Test Loss: 3.9157\n",
      "Epoch: 3636, Train Loss: 0.5025, Test Loss: 3.9095\n",
      "Epoch: 3637, Train Loss: 0.5219, Test Loss: 4.6375\n",
      "Epoch: 3638, Train Loss: 0.5396, Test Loss: 4.3764\n",
      "Epoch: 3639, Train Loss: 0.5283, Test Loss: 4.1552\n",
      "Epoch: 3640, Train Loss: 0.4735, Test Loss: 3.9834\n",
      "Epoch: 3641, Train Loss: 0.5613, Test Loss: 3.8933\n",
      "Epoch: 3642, Train Loss: 0.5186, Test Loss: 4.0057\n",
      "Epoch: 3643, Train Loss: 0.5300, Test Loss: 4.3612\n",
      "Epoch: 3644, Train Loss: 0.5156, Test Loss: 4.3923\n",
      "Epoch: 3645, Train Loss: 0.5469, Test Loss: 3.9666\n",
      "Epoch: 3646, Train Loss: 0.4907, Test Loss: 3.7147\n",
      "Epoch: 3647, Train Loss: 0.5158, Test Loss: 4.0114\n",
      "Epoch: 3648, Train Loss: 0.5856, Test Loss: 5.1340\n",
      "Epoch: 3649, Train Loss: 0.5999, Test Loss: 4.9336\n",
      "Epoch: 3650, Train Loss: 0.6023, Test Loss: 3.5869\n",
      "Epoch: 3651, Train Loss: 0.6360, Test Loss: 3.4660\n",
      "Epoch: 3652, Train Loss: 0.7036, Test Loss: 4.5085\n",
      "Epoch: 3653, Train Loss: 0.5284, Test Loss: 5.4134\n",
      "Epoch: 3654, Train Loss: 0.7259, Test Loss: 4.1902\n",
      "Epoch: 3655, Train Loss: 0.4974, Test Loss: 3.3618\n",
      "Epoch: 3656, Train Loss: 0.7899, Test Loss: 3.8639\n",
      "Epoch: 3657, Train Loss: 0.5090, Test Loss: 4.4353\n",
      "Epoch: 3658, Train Loss: 0.5919, Test Loss: 4.1393\n",
      "Epoch: 3659, Train Loss: 0.5843, Test Loss: 3.5733\n",
      "Epoch: 3660, Train Loss: 0.5219, Test Loss: 3.5305\n",
      "Epoch: 3661, Train Loss: 0.5534, Test Loss: 3.9632\n",
      "Epoch: 3662, Train Loss: 0.5755, Test Loss: 4.7474\n",
      "Epoch: 3663, Train Loss: 0.7054, Test Loss: 4.0120\n",
      "Epoch: 3664, Train Loss: 0.4995, Test Loss: 3.5233\n",
      "Epoch: 3665, Train Loss: 0.5655, Test Loss: 3.7739\n",
      "Epoch: 3666, Train Loss: 0.5884, Test Loss: 5.1884\n",
      "Epoch: 3667, Train Loss: 0.7405, Test Loss: 4.6371\n",
      "Epoch: 3668, Train Loss: 0.6052, Test Loss: 3.4459\n",
      "Epoch: 3669, Train Loss: 0.5234, Test Loss: 3.1473\n",
      "Epoch: 3670, Train Loss: 0.6984, Test Loss: 3.6955\n",
      "Epoch: 3671, Train Loss: 0.5689, Test Loss: 4.9790\n",
      "Epoch: 3672, Train Loss: 0.6963, Test Loss: 4.7296\n",
      "Epoch: 3673, Train Loss: 0.5579, Test Loss: 3.7102\n",
      "Epoch: 3674, Train Loss: 0.5498, Test Loss: 3.5249\n",
      "Epoch: 3675, Train Loss: 0.6493, Test Loss: 3.9326\n",
      "Epoch: 3676, Train Loss: 0.5266, Test Loss: 4.7214\n",
      "Epoch: 3677, Train Loss: 0.6988, Test Loss: 4.0746\n",
      "Epoch: 3678, Train Loss: 0.5673, Test Loss: 3.1899\n",
      "Epoch: 3679, Train Loss: 0.7424, Test Loss: 3.6221\n",
      "Epoch: 3680, Train Loss: 0.5380, Test Loss: 4.7874\n",
      "Epoch: 3681, Train Loss: 0.6842, Test Loss: 4.5268\n",
      "Epoch: 3682, Train Loss: 0.5680, Test Loss: 3.8629\n",
      "Epoch: 3683, Train Loss: 0.4978, Test Loss: 3.5167\n",
      "Epoch: 3684, Train Loss: 0.5785, Test Loss: 3.7463\n",
      "Epoch: 3685, Train Loss: 0.5443, Test Loss: 4.1039\n",
      "Epoch: 3686, Train Loss: 0.5209, Test Loss: 4.2837\n",
      "Epoch: 3687, Train Loss: 0.5509, Test Loss: 3.9048\n",
      "Epoch: 3688, Train Loss: 0.5233, Test Loss: 3.5119\n",
      "Epoch: 3689, Train Loss: 0.5909, Test Loss: 3.7792\n",
      "Epoch: 3690, Train Loss: 0.5321, Test Loss: 4.6431\n",
      "Epoch: 3691, Train Loss: 0.5470, Test Loss: 4.5677\n",
      "Epoch: 3692, Train Loss: 0.5661, Test Loss: 3.6665\n",
      "Epoch: 3693, Train Loss: 0.5205, Test Loss: 3.4603\n",
      "Epoch: 3694, Train Loss: 0.5952, Test Loss: 4.0074\n",
      "Epoch: 3695, Train Loss: 0.5471, Test Loss: 4.6938\n",
      "Epoch: 3696, Train Loss: 0.6157, Test Loss: 4.1920\n",
      "Epoch: 3697, Train Loss: 0.5568, Test Loss: 3.4200\n",
      "Epoch: 3698, Train Loss: 0.5500, Test Loss: 3.2671\n",
      "Epoch: 3699, Train Loss: 0.6249, Test Loss: 4.0812\n",
      "Epoch: 3700, Train Loss: 0.5366, Test Loss: 5.3651\n",
      "Epoch: 3701, Train Loss: 0.8002, Test Loss: 4.2547\n",
      "Epoch: 3702, Train Loss: 0.5619, Test Loss: 3.1576\n",
      "Epoch: 3703, Train Loss: 0.6902, Test Loss: 3.0759\n",
      "Epoch: 3704, Train Loss: 0.7595, Test Loss: 4.0761\n",
      "Epoch: 3705, Train Loss: 0.5451, Test Loss: 4.9896\n",
      "Epoch: 3706, Train Loss: 0.7457, Test Loss: 4.0162\n",
      "Epoch: 3707, Train Loss: 0.5487, Test Loss: 3.4044\n",
      "Epoch: 3708, Train Loss: 0.5333, Test Loss: 3.4042\n",
      "Epoch: 3709, Train Loss: 0.5711, Test Loss: 4.0685\n",
      "Epoch: 3710, Train Loss: 0.5754, Test Loss: 4.4950\n",
      "Epoch: 3711, Train Loss: 0.6350, Test Loss: 3.7070\n",
      "Epoch: 3712, Train Loss: 0.5730, Test Loss: 3.3875\n",
      "Epoch: 3713, Train Loss: 0.6162, Test Loss: 3.6763\n",
      "Epoch: 3714, Train Loss: 0.4836, Test Loss: 4.3554\n",
      "Epoch: 3715, Train Loss: 0.5782, Test Loss: 4.1132\n",
      "Epoch: 3716, Train Loss: 0.5743, Test Loss: 3.6028\n",
      "Epoch: 3717, Train Loss: 0.5436, Test Loss: 3.4784\n",
      "Epoch: 3718, Train Loss: 0.5601, Test Loss: 3.9122\n",
      "Epoch: 3719, Train Loss: 0.5328, Test Loss: 4.1743\n",
      "Epoch: 3720, Train Loss: 0.5429, Test Loss: 3.8471\n",
      "Epoch: 3721, Train Loss: 0.5300, Test Loss: 3.6093\n",
      "Epoch: 3722, Train Loss: 0.5339, Test Loss: 3.5841\n",
      "Epoch: 3723, Train Loss: 0.5729, Test Loss: 4.4633\n",
      "Epoch: 3724, Train Loss: 0.6069, Test Loss: 4.6670\n",
      "Epoch: 3725, Train Loss: 0.6726, Test Loss: 3.7708\n",
      "Epoch: 3726, Train Loss: 0.5076, Test Loss: 3.3641\n",
      "Epoch: 3727, Train Loss: 0.6030, Test Loss: 3.5998\n",
      "Epoch: 3728, Train Loss: 0.5237, Test Loss: 4.1631\n",
      "Epoch: 3729, Train Loss: 0.5173, Test Loss: 4.2552\n",
      "Epoch: 3730, Train Loss: 0.5810, Test Loss: 3.8169\n",
      "Epoch: 3731, Train Loss: 0.5688, Test Loss: 3.8661\n",
      "Epoch: 3732, Train Loss: 0.5618, Test Loss: 3.9305\n",
      "Epoch: 3733, Train Loss: 0.5352, Test Loss: 4.0955\n",
      "Epoch: 3734, Train Loss: 0.4795, Test Loss: 4.1581\n",
      "Epoch: 3735, Train Loss: 0.5827, Test Loss: 4.0280\n",
      "Epoch: 3736, Train Loss: 0.5394, Test Loss: 3.7389\n",
      "Epoch: 3737, Train Loss: 0.5165, Test Loss: 3.8755\n",
      "Epoch: 3738, Train Loss: 0.5453, Test Loss: 4.1849\n",
      "Epoch: 3739, Train Loss: 0.5299, Test Loss: 4.0749\n",
      "Epoch: 3740, Train Loss: 0.5275, Test Loss: 4.0109\n",
      "Epoch: 3741, Train Loss: 0.5178, Test Loss: 3.6980\n",
      "Epoch: 3742, Train Loss: 0.5397, Test Loss: 3.9175\n",
      "Epoch: 3743, Train Loss: 0.4915, Test Loss: 4.3442\n",
      "Epoch: 3744, Train Loss: 0.5074, Test Loss: 4.2512\n",
      "Epoch: 3745, Train Loss: 0.5176, Test Loss: 3.8827\n",
      "Epoch: 3746, Train Loss: 0.5717, Test Loss: 4.0628\n",
      "Epoch: 3747, Train Loss: 0.5035, Test Loss: 4.3814\n",
      "Epoch: 3748, Train Loss: 0.5472, Test Loss: 3.9406\n",
      "Epoch: 3749, Train Loss: 0.4931, Test Loss: 3.8067\n",
      "Epoch: 3750, Train Loss: 0.5232, Test Loss: 4.2568\n",
      "Epoch: 3751, Train Loss: 0.5223, Test Loss: 4.3802\n",
      "Epoch: 3752, Train Loss: 0.5706, Test Loss: 3.7411\n",
      "Epoch: 3753, Train Loss: 0.5394, Test Loss: 3.5556\n",
      "Epoch: 3754, Train Loss: 0.5893, Test Loss: 3.9450\n",
      "Epoch: 3755, Train Loss: 0.5410, Test Loss: 4.7127\n",
      "Epoch: 3756, Train Loss: 0.5614, Test Loss: 4.3723\n",
      "Epoch: 3757, Train Loss: 0.5343, Test Loss: 3.8719\n",
      "Epoch: 3758, Train Loss: 0.5030, Test Loss: 3.6459\n",
      "Epoch: 3759, Train Loss: 0.5454, Test Loss: 4.0453\n",
      "Epoch: 3760, Train Loss: 0.5334, Test Loss: 4.5465\n",
      "Epoch: 3761, Train Loss: 0.5493, Test Loss: 4.2766\n",
      "Epoch: 3762, Train Loss: 0.5312, Test Loss: 3.5615\n",
      "Epoch: 3763, Train Loss: 0.5263, Test Loss: 3.4055\n",
      "Epoch: 3764, Train Loss: 0.5663, Test Loss: 3.9699\n",
      "Epoch: 3765, Train Loss: 0.4985, Test Loss: 4.5612\n",
      "Epoch: 3766, Train Loss: 0.6226, Test Loss: 3.9188\n",
      "Epoch: 3767, Train Loss: 0.4972, Test Loss: 3.3243\n",
      "Epoch: 3768, Train Loss: 0.6540, Test Loss: 3.6392\n",
      "Epoch: 3769, Train Loss: 0.5241, Test Loss: 4.3922\n",
      "Epoch: 3770, Train Loss: 0.5209, Test Loss: 4.6955\n",
      "Epoch: 3771, Train Loss: 0.6786, Test Loss: 3.8223\n",
      "Epoch: 3772, Train Loss: 0.5249, Test Loss: 3.3726\n",
      "Epoch: 3773, Train Loss: 0.6385, Test Loss: 3.9040\n",
      "Epoch: 3774, Train Loss: 0.5030, Test Loss: 4.3334\n",
      "Epoch: 3775, Train Loss: 0.5650, Test Loss: 4.0241\n",
      "Epoch: 3776, Train Loss: 0.5362, Test Loss: 3.7525\n",
      "Epoch: 3777, Train Loss: 0.5412, Test Loss: 3.8891\n",
      "Epoch: 3778, Train Loss: 0.4900, Test Loss: 4.1963\n",
      "Epoch: 3779, Train Loss: 0.5473, Test Loss: 3.9680\n",
      "Epoch: 3780, Train Loss: 0.5363, Test Loss: 3.8325\n",
      "Epoch: 3781, Train Loss: 0.5219, Test Loss: 3.9868\n",
      "Epoch: 3782, Train Loss: 0.5091, Test Loss: 3.9275\n",
      "Epoch: 3783, Train Loss: 0.5145, Test Loss: 4.2055\n",
      "Epoch: 3784, Train Loss: 0.5265, Test Loss: 4.3689\n",
      "Epoch: 3785, Train Loss: 0.4971, Test Loss: 3.8982\n",
      "Epoch: 3786, Train Loss: 0.5067, Test Loss: 3.3768\n",
      "Epoch: 3787, Train Loss: 0.6208, Test Loss: 3.7470\n",
      "Epoch: 3788, Train Loss: 0.4933, Test Loss: 4.5528\n",
      "Epoch: 3789, Train Loss: 0.5665, Test Loss: 4.3832\n",
      "Epoch: 3790, Train Loss: 0.5253, Test Loss: 3.8704\n",
      "Epoch: 3791, Train Loss: 0.4790, Test Loss: 3.7396\n",
      "Epoch: 3792, Train Loss: 0.5644, Test Loss: 4.0404\n",
      "Epoch: 3793, Train Loss: 0.4946, Test Loss: 4.1072\n",
      "Epoch: 3794, Train Loss: 0.5143, Test Loss: 4.0966\n",
      "Epoch: 3795, Train Loss: 0.5671, Test Loss: 4.0121\n",
      "Epoch: 3796, Train Loss: 0.4877, Test Loss: 4.0126\n",
      "Epoch: 3797, Train Loss: 0.5703, Test Loss: 3.8720\n",
      "Epoch: 3798, Train Loss: 0.5523, Test Loss: 3.7241\n",
      "Epoch: 3799, Train Loss: 0.5287, Test Loss: 4.1029\n",
      "Epoch: 3800, Train Loss: 0.5101, Test Loss: 4.2304\n",
      "Epoch: 3801, Train Loss: 0.5153, Test Loss: 4.0729\n",
      "Epoch: 3802, Train Loss: 0.4901, Test Loss: 3.5838\n",
      "Epoch: 3803, Train Loss: 0.5681, Test Loss: 3.5274\n",
      "Epoch: 3804, Train Loss: 0.5609, Test Loss: 4.0508\n",
      "Epoch: 3805, Train Loss: 0.5328, Test Loss: 4.2675\n",
      "Epoch: 3806, Train Loss: 0.5153, Test Loss: 4.4526\n",
      "Epoch: 3807, Train Loss: 0.5523, Test Loss: 3.7277\n",
      "Epoch: 3808, Train Loss: 0.5459, Test Loss: 3.8507\n",
      "Epoch: 3809, Train Loss: 0.5185, Test Loss: 4.3968\n",
      "Epoch: 3810, Train Loss: 0.5196, Test Loss: 4.1569\n",
      "Epoch: 3811, Train Loss: 0.5259, Test Loss: 3.9599\n",
      "Epoch: 3812, Train Loss: 0.5124, Test Loss: 3.9298\n",
      "Epoch: 3813, Train Loss: 0.5233, Test Loss: 4.4018\n",
      "Epoch: 3814, Train Loss: 0.5158, Test Loss: 4.4719\n",
      "Epoch: 3815, Train Loss: 0.5632, Test Loss: 3.6557\n",
      "Epoch: 3816, Train Loss: 0.5180, Test Loss: 3.5386\n",
      "Epoch: 3817, Train Loss: 0.5484, Test Loss: 4.1843\n",
      "Epoch: 3818, Train Loss: 0.4770, Test Loss: 4.8483\n",
      "Epoch: 3819, Train Loss: 0.6158, Test Loss: 4.0593\n",
      "Epoch: 3820, Train Loss: 0.5679, Test Loss: 3.4264\n",
      "Epoch: 3821, Train Loss: 0.5657, Test Loss: 3.5248\n",
      "Epoch: 3822, Train Loss: 0.5273, Test Loss: 4.3137\n",
      "Epoch: 3823, Train Loss: 0.5110, Test Loss: 4.4727\n",
      "Epoch: 3824, Train Loss: 0.5544, Test Loss: 3.9249\n",
      "Epoch: 3825, Train Loss: 0.5483, Test Loss: 3.5375\n",
      "Epoch: 3826, Train Loss: 0.5527, Test Loss: 3.8342\n",
      "Epoch: 3827, Train Loss: 0.5063, Test Loss: 4.0693\n",
      "Epoch: 3828, Train Loss: 0.5158, Test Loss: 4.2641\n",
      "Epoch: 3829, Train Loss: 0.4986, Test Loss: 4.0971\n",
      "Epoch: 3830, Train Loss: 0.4883, Test Loss: 3.8278\n",
      "Epoch: 3831, Train Loss: 0.4863, Test Loss: 3.9495\n",
      "Epoch: 3832, Train Loss: 0.5146, Test Loss: 4.2170\n",
      "Epoch: 3833, Train Loss: 0.4994, Test Loss: 4.2745\n",
      "Epoch: 3834, Train Loss: 0.4896, Test Loss: 3.8145\n",
      "Epoch: 3835, Train Loss: 0.5019, Test Loss: 3.6198\n",
      "Epoch: 3836, Train Loss: 0.5227, Test Loss: 3.4327\n",
      "Epoch: 3837, Train Loss: 0.5384, Test Loss: 3.8504\n",
      "Epoch: 3838, Train Loss: 0.5577, Test Loss: 4.8137\n",
      "Epoch: 3839, Train Loss: 0.5764, Test Loss: 4.3803\n",
      "Epoch: 3840, Train Loss: 0.5472, Test Loss: 3.4683\n",
      "Epoch: 3841, Train Loss: 0.5719, Test Loss: 3.3521\n",
      "Epoch: 3842, Train Loss: 0.5897, Test Loss: 4.1132\n",
      "Epoch: 3843, Train Loss: 0.5313, Test Loss: 4.6264\n",
      "Epoch: 3844, Train Loss: 0.6134, Test Loss: 4.0707\n",
      "Epoch: 3845, Train Loss: 0.4864, Test Loss: 3.4282\n",
      "Epoch: 3846, Train Loss: 0.5898, Test Loss: 3.5540\n",
      "Epoch: 3847, Train Loss: 0.5458, Test Loss: 4.4621\n",
      "Epoch: 3848, Train Loss: 0.5783, Test Loss: 4.4855\n",
      "Epoch: 3849, Train Loss: 0.5232, Test Loss: 3.7215\n",
      "Epoch: 3850, Train Loss: 0.5082, Test Loss: 3.3009\n",
      "Epoch: 3851, Train Loss: 0.5519, Test Loss: 3.4756\n",
      "Epoch: 3852, Train Loss: 0.5866, Test Loss: 4.7831\n",
      "Epoch: 3853, Train Loss: 0.5731, Test Loss: 4.9288\n",
      "Epoch: 3854, Train Loss: 0.7007, Test Loss: 3.3794\n",
      "Epoch: 3855, Train Loss: 0.6085, Test Loss: 3.1353\n",
      "Epoch: 3856, Train Loss: 0.7758, Test Loss: 4.2882\n",
      "Epoch: 3857, Train Loss: 0.5414, Test Loss: 5.4190\n",
      "Epoch: 3858, Train Loss: 0.8215, Test Loss: 4.1162\n",
      "Epoch: 3859, Train Loss: 0.5056, Test Loss: 3.3226\n",
      "Epoch: 3860, Train Loss: 0.6188, Test Loss: 3.3766\n",
      "Epoch: 3861, Train Loss: 0.6625, Test Loss: 4.4133\n",
      "Epoch: 3862, Train Loss: 0.5567, Test Loss: 4.5787\n",
      "Epoch: 3863, Train Loss: 0.5984, Test Loss: 3.8821\n",
      "Epoch: 3864, Train Loss: 0.5010, Test Loss: 3.2663\n",
      "Epoch: 3865, Train Loss: 0.5586, Test Loss: 3.3195\n",
      "Epoch: 3866, Train Loss: 0.5879, Test Loss: 4.2328\n",
      "Epoch: 3867, Train Loss: 0.5073, Test Loss: 4.8138\n",
      "Epoch: 3868, Train Loss: 0.6096, Test Loss: 3.9497\n",
      "Epoch: 3869, Train Loss: 0.5254, Test Loss: 3.2339\n",
      "Epoch: 3870, Train Loss: 0.5778, Test Loss: 3.3310\n",
      "Epoch: 3871, Train Loss: 0.5998, Test Loss: 4.0931\n",
      "Epoch: 3872, Train Loss: 0.5410, Test Loss: 4.3988\n",
      "Epoch: 3873, Train Loss: 0.5296, Test Loss: 4.0076\n",
      "Epoch: 3874, Train Loss: 0.5143, Test Loss: 3.4821\n",
      "Epoch: 3875, Train Loss: 0.5439, Test Loss: 3.4961\n",
      "Epoch: 3876, Train Loss: 0.5563, Test Loss: 4.1418\n",
      "Epoch: 3877, Train Loss: 0.4857, Test Loss: 4.5533\n",
      "Epoch: 3878, Train Loss: 0.6018, Test Loss: 3.8705\n",
      "Epoch: 3879, Train Loss: 0.5037, Test Loss: 3.4129\n",
      "Epoch: 3880, Train Loss: 0.5360, Test Loss: 3.4896\n",
      "Epoch: 3881, Train Loss: 0.5388, Test Loss: 4.1499\n",
      "Epoch: 3882, Train Loss: 0.5204, Test Loss: 4.3485\n",
      "Epoch: 3883, Train Loss: 0.5631, Test Loss: 3.6216\n",
      "Epoch: 3884, Train Loss: 0.5023, Test Loss: 3.3162\n",
      "Epoch: 3885, Train Loss: 0.6226, Test Loss: 3.8817\n",
      "Epoch: 3886, Train Loss: 0.4908, Test Loss: 4.5875\n",
      "Epoch: 3887, Train Loss: 0.5271, Test Loss: 4.4774\n",
      "Epoch: 3888, Train Loss: 0.5622, Test Loss: 3.5585\n",
      "Epoch: 3889, Train Loss: 0.4980, Test Loss: 3.3429\n",
      "Epoch: 3890, Train Loss: 0.5517, Test Loss: 3.7800\n",
      "Epoch: 3891, Train Loss: 0.4709, Test Loss: 4.3501\n",
      "Epoch: 3892, Train Loss: 0.5599, Test Loss: 4.0772\n",
      "Epoch: 3893, Train Loss: 0.4821, Test Loss: 3.6976\n",
      "Epoch: 3894, Train Loss: 0.5146, Test Loss: 3.6585\n",
      "Epoch: 3895, Train Loss: 0.5246, Test Loss: 3.9273\n",
      "Epoch: 3896, Train Loss: 0.4980, Test Loss: 3.8599\n",
      "Epoch: 3897, Train Loss: 0.5034, Test Loss: 3.7742\n",
      "Epoch: 3898, Train Loss: 0.5341, Test Loss: 3.7704\n",
      "Epoch: 3899, Train Loss: 0.6008, Test Loss: 3.8913\n",
      "Epoch: 3900, Train Loss: 0.4741, Test Loss: 4.0629\n",
      "Epoch: 3901, Train Loss: 0.5171, Test Loss: 4.1611\n",
      "Epoch: 3902, Train Loss: 0.5083, Test Loss: 4.1607\n",
      "Epoch: 3903, Train Loss: 0.5277, Test Loss: 3.9897\n",
      "Epoch: 3904, Train Loss: 0.5366, Test Loss: 3.6276\n",
      "Epoch: 3905, Train Loss: 0.5218, Test Loss: 3.6633\n",
      "Epoch: 3906, Train Loss: 0.5087, Test Loss: 4.2643\n",
      "Epoch: 3907, Train Loss: 0.5546, Test Loss: 4.2168\n",
      "Epoch: 3908, Train Loss: 0.5948, Test Loss: 3.4621\n",
      "Epoch: 3909, Train Loss: 0.5731, Test Loss: 3.5257\n",
      "Epoch: 3910, Train Loss: 0.5323, Test Loss: 3.9428\n",
      "Epoch: 3911, Train Loss: 0.5266, Test Loss: 4.3549\n",
      "Epoch: 3912, Train Loss: 0.5411, Test Loss: 4.3264\n",
      "Epoch: 3913, Train Loss: 0.5513, Test Loss: 3.6522\n",
      "Epoch: 3914, Train Loss: 0.5477, Test Loss: 3.4091\n",
      "Epoch: 3915, Train Loss: 0.5437, Test Loss: 3.7020\n",
      "Epoch: 3916, Train Loss: 0.4959, Test Loss: 4.4995\n",
      "Epoch: 3917, Train Loss: 0.5070, Test Loss: 4.6327\n",
      "Epoch: 3918, Train Loss: 0.5126, Test Loss: 3.8265\n",
      "Epoch: 3919, Train Loss: 0.5309, Test Loss: 3.3300\n",
      "Epoch: 3920, Train Loss: 0.5812, Test Loss: 3.5681\n",
      "Epoch: 3921, Train Loss: 0.5086, Test Loss: 4.4349\n",
      "Epoch: 3922, Train Loss: 0.5430, Test Loss: 4.3152\n",
      "Epoch: 3923, Train Loss: 0.5225, Test Loss: 3.8685\n",
      "Epoch: 3924, Train Loss: 0.5357, Test Loss: 3.7743\n",
      "Epoch: 3925, Train Loss: 0.4887, Test Loss: 4.0078\n",
      "Epoch: 3926, Train Loss: 0.4690, Test Loss: 4.0420\n",
      "Epoch: 3927, Train Loss: 0.5402, Test Loss: 4.0472\n",
      "Epoch: 3928, Train Loss: 0.4795, Test Loss: 4.0515\n",
      "Epoch: 3929, Train Loss: 0.5248, Test Loss: 4.0147\n",
      "Epoch: 3930, Train Loss: 0.5211, Test Loss: 3.8181\n",
      "Epoch: 3931, Train Loss: 0.4869, Test Loss: 3.7438\n",
      "Epoch: 3932, Train Loss: 0.4500, Test Loss: 3.8770\n",
      "Epoch: 3933, Train Loss: 0.4784, Test Loss: 4.0745\n",
      "Epoch: 3934, Train Loss: 0.5243, Test Loss: 4.1718\n",
      "Epoch: 3935, Train Loss: 0.4894, Test Loss: 4.0436\n",
      "Epoch: 3936, Train Loss: 0.4788, Test Loss: 3.8327\n",
      "Epoch: 3937, Train Loss: 0.4912, Test Loss: 3.8013\n",
      "Epoch: 3938, Train Loss: 0.5504, Test Loss: 4.5619\n",
      "Epoch: 3939, Train Loss: 0.5013, Test Loss: 4.6934\n",
      "Epoch: 3940, Train Loss: 0.5189, Test Loss: 4.1158\n",
      "Epoch: 3941, Train Loss: 0.5302, Test Loss: 3.5737\n",
      "Epoch: 3942, Train Loss: 0.4934, Test Loss: 3.4944\n",
      "Epoch: 3943, Train Loss: 0.5436, Test Loss: 3.9466\n",
      "Epoch: 3944, Train Loss: 0.4844, Test Loss: 4.5861\n",
      "Epoch: 3945, Train Loss: 0.5275, Test Loss: 4.4966\n",
      "Epoch: 3946, Train Loss: 0.6352, Test Loss: 3.7612\n",
      "Epoch: 3947, Train Loss: 0.4777, Test Loss: 3.3697\n",
      "Epoch: 3948, Train Loss: 0.5732, Test Loss: 3.5855\n",
      "Epoch: 3949, Train Loss: 0.5448, Test Loss: 4.6974\n",
      "Epoch: 3950, Train Loss: 0.5591, Test Loss: 4.9296\n",
      "Epoch: 3951, Train Loss: 0.6315, Test Loss: 3.6788\n",
      "Epoch: 3952, Train Loss: 0.5004, Test Loss: 3.2206\n",
      "Epoch: 3953, Train Loss: 0.5964, Test Loss: 3.5756\n",
      "Epoch: 3954, Train Loss: 0.4793, Test Loss: 4.7397\n",
      "Epoch: 3955, Train Loss: 0.6292, Test Loss: 4.7237\n",
      "Epoch: 3956, Train Loss: 0.5909, Test Loss: 3.5164\n",
      "Epoch: 3957, Train Loss: 0.5310, Test Loss: 3.2249\n",
      "Epoch: 3958, Train Loss: 0.5461, Test Loss: 3.5798\n",
      "Epoch: 3959, Train Loss: 0.4804, Test Loss: 4.1841\n",
      "Epoch: 3960, Train Loss: 0.5833, Test Loss: 4.0963\n",
      "Epoch: 3961, Train Loss: 0.5254, Test Loss: 3.6994\n",
      "Epoch: 3962, Train Loss: 0.5772, Test Loss: 3.4137\n",
      "Epoch: 3963, Train Loss: 0.5512, Test Loss: 3.8990\n",
      "Epoch: 3964, Train Loss: 0.5618, Test Loss: 4.3200\n",
      "Epoch: 3965, Train Loss: 0.5422, Test Loss: 3.7622\n",
      "Epoch: 3966, Train Loss: 0.5424, Test Loss: 3.7034\n",
      "Epoch: 3967, Train Loss: 0.5453, Test Loss: 4.0442\n",
      "Epoch: 3968, Train Loss: 0.4917, Test Loss: 4.3092\n",
      "Epoch: 3969, Train Loss: 0.5168, Test Loss: 3.9853\n",
      "Epoch: 3970, Train Loss: 0.4858, Test Loss: 3.7064\n",
      "Epoch: 3971, Train Loss: 0.5187, Test Loss: 3.8600\n",
      "Epoch: 3972, Train Loss: 0.5354, Test Loss: 4.5598\n",
      "Epoch: 3973, Train Loss: 0.6081, Test Loss: 4.1198\n",
      "Epoch: 3974, Train Loss: 0.5553, Test Loss: 3.2718\n",
      "Epoch: 3975, Train Loss: 0.5525, Test Loss: 3.2289\n",
      "Epoch: 3976, Train Loss: 0.6618, Test Loss: 4.1896\n",
      "Epoch: 3977, Train Loss: 0.4950, Test Loss: 5.1027\n",
      "Epoch: 3978, Train Loss: 0.6526, Test Loss: 4.3299\n",
      "Epoch: 3979, Train Loss: 0.5215, Test Loss: 3.5397\n",
      "Epoch: 3980, Train Loss: 0.5025, Test Loss: 3.4675\n",
      "Epoch: 3981, Train Loss: 0.5750, Test Loss: 3.8493\n",
      "Epoch: 3982, Train Loss: 0.4922, Test Loss: 4.7592\n",
      "Epoch: 3983, Train Loss: 0.6075, Test Loss: 4.3484\n",
      "Epoch: 3984, Train Loss: 0.5212, Test Loss: 3.4019\n",
      "Epoch: 3985, Train Loss: 0.6580, Test Loss: 3.6317\n",
      "Epoch: 3986, Train Loss: 0.5593, Test Loss: 4.7700\n",
      "Epoch: 3987, Train Loss: 0.5712, Test Loss: 5.0542\n",
      "Epoch: 3988, Train Loss: 0.7244, Test Loss: 3.6407\n",
      "Epoch: 3989, Train Loss: 0.5106, Test Loss: 3.2159\n",
      "Epoch: 3990, Train Loss: 0.6648, Test Loss: 3.6757\n",
      "Epoch: 3991, Train Loss: 0.5050, Test Loss: 4.6592\n",
      "Epoch: 3992, Train Loss: 0.6772, Test Loss: 4.1207\n",
      "Epoch: 3993, Train Loss: 0.5294, Test Loss: 3.3118\n",
      "Epoch: 3994, Train Loss: 0.6191, Test Loss: 3.4871\n",
      "Epoch: 3995, Train Loss: 0.5802, Test Loss: 4.4948\n",
      "Epoch: 3996, Train Loss: 0.5564, Test Loss: 4.5730\n",
      "Epoch: 3997, Train Loss: 0.6398, Test Loss: 3.4654\n",
      "Epoch: 3998, Train Loss: 0.5786, Test Loss: 3.0983\n",
      "Epoch: 3999, Train Loss: 0.7399, Test Loss: 3.6530\n",
      "Epoch: 4000, Train Loss: 0.5319, Test Loss: 4.7802\n",
      "Epoch: 4001, Train Loss: 0.6800, Test Loss: 4.5853\n",
      "Epoch: 4002, Train Loss: 0.6174, Test Loss: 3.4602\n",
      "Epoch: 4003, Train Loss: 0.5165, Test Loss: 3.1493\n",
      "Epoch: 4004, Train Loss: 0.6444, Test Loss: 3.8090\n",
      "Epoch: 4005, Train Loss: 0.5138, Test Loss: 4.8652\n",
      "Epoch: 4006, Train Loss: 0.5822, Test Loss: 4.5433\n",
      "Epoch: 4007, Train Loss: 0.6145, Test Loss: 3.3460\n",
      "Epoch: 4008, Train Loss: 0.5263, Test Loss: 2.9581\n",
      "Epoch: 4009, Train Loss: 0.6209, Test Loss: 3.3578\n",
      "Epoch: 4010, Train Loss: 0.6028, Test Loss: 4.8919\n",
      "Epoch: 4011, Train Loss: 0.6142, Test Loss: 4.9602\n",
      "Epoch: 4012, Train Loss: 0.7394, Test Loss: 3.3043\n",
      "Epoch: 4013, Train Loss: 0.5447, Test Loss: 2.9747\n",
      "Epoch: 4014, Train Loss: 0.7350, Test Loss: 3.4082\n",
      "Epoch: 4015, Train Loss: 0.5437, Test Loss: 4.6363\n",
      "Epoch: 4016, Train Loss: 0.6076, Test Loss: 4.6872\n",
      "Epoch: 4017, Train Loss: 0.7046, Test Loss: 3.3860\n",
      "Epoch: 4018, Train Loss: 0.5464, Test Loss: 3.0690\n",
      "Epoch: 4019, Train Loss: 0.7564, Test Loss: 3.5896\n",
      "Epoch: 4020, Train Loss: 0.5107, Test Loss: 4.3951\n",
      "Epoch: 4021, Train Loss: 0.6446, Test Loss: 4.1105\n",
      "Epoch: 4022, Train Loss: 0.5469, Test Loss: 3.4235\n",
      "Epoch: 4023, Train Loss: 0.5283, Test Loss: 3.2223\n",
      "Epoch: 4024, Train Loss: 0.6022, Test Loss: 3.6549\n",
      "Epoch: 4025, Train Loss: 0.4740, Test Loss: 4.4114\n",
      "Epoch: 4026, Train Loss: 0.5906, Test Loss: 4.1237\n",
      "Epoch: 4027, Train Loss: 0.5226, Test Loss: 3.4838\n",
      "Epoch: 4028, Train Loss: 0.5523, Test Loss: 3.3636\n",
      "Epoch: 4029, Train Loss: 0.5626, Test Loss: 3.8276\n",
      "Epoch: 4030, Train Loss: 0.4896, Test Loss: 4.0359\n",
      "Epoch: 4031, Train Loss: 0.5330, Test Loss: 3.8681\n",
      "Epoch: 4032, Train Loss: 0.5107, Test Loss: 3.9744\n",
      "Epoch: 4033, Train Loss: 0.5599, Test Loss: 3.8545\n",
      "Epoch: 4034, Train Loss: 0.5078, Test Loss: 3.9093\n",
      "Epoch: 4035, Train Loss: 0.5164, Test Loss: 3.8744\n",
      "Epoch: 4036, Train Loss: 0.4839, Test Loss: 3.8117\n",
      "Epoch: 4037, Train Loss: 0.5049, Test Loss: 3.7946\n",
      "Epoch: 4038, Train Loss: 0.4872, Test Loss: 4.0259\n",
      "Epoch: 4039, Train Loss: 0.5261, Test Loss: 3.8811\n",
      "Epoch: 4040, Train Loss: 0.5133, Test Loss: 3.7837\n",
      "Epoch: 4041, Train Loss: 0.5250, Test Loss: 3.9498\n",
      "Epoch: 4042, Train Loss: 0.4731, Test Loss: 3.7648\n",
      "Epoch: 4043, Train Loss: 0.5387, Test Loss: 3.5493\n",
      "Epoch: 4044, Train Loss: 0.5049, Test Loss: 3.6167\n",
      "Epoch: 4045, Train Loss: 0.4851, Test Loss: 3.9668\n",
      "Epoch: 4046, Train Loss: 0.5078, Test Loss: 3.9652\n",
      "Epoch: 4047, Train Loss: 0.4999, Test Loss: 3.8396\n",
      "Epoch: 4048, Train Loss: 0.4832, Test Loss: 3.5727\n",
      "Epoch: 4049, Train Loss: 0.5246, Test Loss: 3.8100\n",
      "Epoch: 4050, Train Loss: 0.5172, Test Loss: 3.9073\n",
      "Epoch: 4051, Train Loss: 0.5476, Test Loss: 3.7526\n",
      "Epoch: 4052, Train Loss: 0.4848, Test Loss: 3.6514\n",
      "Epoch: 4053, Train Loss: 0.5600, Test Loss: 3.8476\n",
      "Epoch: 4054, Train Loss: 0.5360, Test Loss: 3.9714\n",
      "Epoch: 4055, Train Loss: 0.4823, Test Loss: 4.0700\n",
      "Epoch: 4056, Train Loss: 0.5485, Test Loss: 3.7394\n",
      "Epoch: 4057, Train Loss: 0.5388, Test Loss: 3.8820\n",
      "Epoch: 4058, Train Loss: 0.5574, Test Loss: 4.5800\n",
      "Epoch: 4059, Train Loss: 0.5898, Test Loss: 4.0191\n",
      "Epoch: 4060, Train Loss: 0.4893, Test Loss: 3.4502\n",
      "Epoch: 4061, Train Loss: 0.5427, Test Loss: 3.3847\n",
      "Epoch: 4062, Train Loss: 0.5090, Test Loss: 3.8216\n",
      "Epoch: 4063, Train Loss: 0.5178, Test Loss: 4.4478\n",
      "Epoch: 4064, Train Loss: 0.5428, Test Loss: 4.3553\n",
      "Epoch: 4065, Train Loss: 0.5354, Test Loss: 3.7836\n",
      "Epoch: 4066, Train Loss: 0.4987, Test Loss: 3.6267\n",
      "Epoch: 4067, Train Loss: 0.4976, Test Loss: 3.9300\n",
      "Epoch: 4068, Train Loss: 0.5417, Test Loss: 4.4585\n",
      "Epoch: 4069, Train Loss: 0.6284, Test Loss: 3.9366\n",
      "Epoch: 4070, Train Loss: 0.4671, Test Loss: 3.3668\n",
      "Epoch: 4071, Train Loss: 0.5669, Test Loss: 3.5100\n",
      "Epoch: 4072, Train Loss: 0.5811, Test Loss: 4.4002\n",
      "Epoch: 4073, Train Loss: 0.5834, Test Loss: 4.3472\n",
      "Epoch: 4074, Train Loss: 0.5466, Test Loss: 3.6779\n",
      "Epoch: 4075, Train Loss: 0.5341, Test Loss: 3.6265\n",
      "Epoch: 4076, Train Loss: 0.5266, Test Loss: 3.7666\n",
      "Epoch: 4077, Train Loss: 0.5087, Test Loss: 4.0634\n",
      "Epoch: 4078, Train Loss: 0.4724, Test Loss: 3.9654\n",
      "Epoch: 4079, Train Loss: 0.5771, Test Loss: 4.0754\n",
      "Epoch: 4080, Train Loss: 0.4687, Test Loss: 4.0373\n",
      "Epoch: 4081, Train Loss: 0.4677, Test Loss: 3.9314\n",
      "Epoch: 4082, Train Loss: 0.4716, Test Loss: 3.7665\n",
      "Epoch: 4083, Train Loss: 0.5284, Test Loss: 3.7809\n",
      "Epoch: 4084, Train Loss: 0.5207, Test Loss: 4.2240\n",
      "Epoch: 4085, Train Loss: 0.5327, Test Loss: 4.2760\n",
      "Epoch: 4086, Train Loss: 0.5042, Test Loss: 3.9089\n",
      "Epoch: 4087, Train Loss: 0.5042, Test Loss: 3.6198\n",
      "Epoch: 4088, Train Loss: 0.5160, Test Loss: 3.8659\n",
      "Epoch: 4089, Train Loss: 0.5318, Test Loss: 4.1673\n",
      "Epoch: 4090, Train Loss: 0.4718, Test Loss: 4.1790\n",
      "Epoch: 4091, Train Loss: 0.5102, Test Loss: 3.6229\n",
      "Epoch: 4092, Train Loss: 0.5471, Test Loss: 3.6932\n",
      "Epoch: 4093, Train Loss: 0.5612, Test Loss: 4.4051\n",
      "Epoch: 4094, Train Loss: 0.5086, Test Loss: 4.7263\n",
      "Epoch: 4095, Train Loss: 0.5494, Test Loss: 3.9311\n",
      "Epoch: 4096, Train Loss: 0.4817, Test Loss: 3.5120\n",
      "Epoch: 4097, Train Loss: 0.5343, Test Loss: 3.8368\n",
      "Epoch: 4098, Train Loss: 0.5273, Test Loss: 4.1634\n",
      "Epoch: 4099, Train Loss: 0.5452, Test Loss: 3.9963\n",
      "Epoch: 4100, Train Loss: 0.5010, Test Loss: 3.6219\n",
      "Epoch: 4101, Train Loss: 0.4976, Test Loss: 3.6261\n",
      "Epoch: 4102, Train Loss: 0.5364, Test Loss: 4.0438\n",
      "Epoch: 4103, Train Loss: 0.5139, Test Loss: 4.5791\n",
      "Epoch: 4104, Train Loss: 0.5468, Test Loss: 4.4201\n",
      "Epoch: 4105, Train Loss: 0.5380, Test Loss: 3.7144\n",
      "Epoch: 4106, Train Loss: 0.5208, Test Loss: 3.4775\n",
      "Epoch: 4107, Train Loss: 0.5641, Test Loss: 3.9999\n",
      "Epoch: 4108, Train Loss: 0.4827, Test Loss: 4.4882\n",
      "Epoch: 4109, Train Loss: 0.5679, Test Loss: 4.1413\n",
      "Epoch: 4110, Train Loss: 0.4893, Test Loss: 3.4773\n",
      "Epoch: 4111, Train Loss: 0.5538, Test Loss: 3.7155\n",
      "Epoch: 4112, Train Loss: 0.5490, Test Loss: 4.1951\n",
      "Epoch: 4113, Train Loss: 0.5608, Test Loss: 4.0179\n",
      "Epoch: 4114, Train Loss: 0.4858, Test Loss: 3.4609\n",
      "Epoch: 4115, Train Loss: 0.5284, Test Loss: 3.4916\n",
      "Epoch: 4116, Train Loss: 0.5798, Test Loss: 4.3607\n",
      "Epoch: 4117, Train Loss: 0.5597, Test Loss: 4.4240\n",
      "Epoch: 4118, Train Loss: 0.5573, Test Loss: 3.6924\n",
      "Epoch: 4119, Train Loss: 0.4969, Test Loss: 3.2746\n",
      "Epoch: 4120, Train Loss: 0.6417, Test Loss: 3.8584\n",
      "Epoch: 4121, Train Loss: 0.4948, Test Loss: 4.4751\n",
      "Epoch: 4122, Train Loss: 0.5056, Test Loss: 4.2295\n",
      "Epoch: 4123, Train Loss: 0.4735, Test Loss: 3.7424\n",
      "Epoch: 4124, Train Loss: 0.5384, Test Loss: 3.6766\n",
      "Epoch: 4125, Train Loss: 0.5364, Test Loss: 4.0604\n",
      "Epoch: 4126, Train Loss: 0.4723, Test Loss: 4.4253\n",
      "Epoch: 4127, Train Loss: 0.5613, Test Loss: 3.7755\n",
      "Epoch: 4128, Train Loss: 0.5295, Test Loss: 3.3091\n",
      "Epoch: 4129, Train Loss: 0.5063, Test Loss: 3.3448\n",
      "Epoch: 4130, Train Loss: 0.5886, Test Loss: 4.0508\n",
      "Epoch: 4131, Train Loss: 0.4945, Test Loss: 4.5977\n",
      "Epoch: 4132, Train Loss: 0.6244, Test Loss: 3.7399\n",
      "Epoch: 4133, Train Loss: 0.5338, Test Loss: 3.2013\n",
      "Epoch: 4134, Train Loss: 0.6982, Test Loss: 3.7255\n",
      "Epoch: 4135, Train Loss: 0.4714, Test Loss: 4.2318\n",
      "Epoch: 4136, Train Loss: 0.5437, Test Loss: 4.2046\n",
      "Epoch: 4137, Train Loss: 0.5171, Test Loss: 3.8486\n",
      "Epoch: 4138, Train Loss: 0.4924, Test Loss: 3.7662\n",
      "Epoch: 4139, Train Loss: 0.5276, Test Loss: 4.0092\n",
      "Epoch: 4140, Train Loss: 0.4617, Test Loss: 4.1965\n",
      "Epoch: 4141, Train Loss: 0.4935, Test Loss: 3.7963\n",
      "Epoch: 4142, Train Loss: 0.4986, Test Loss: 3.9450\n",
      "Epoch: 4143, Train Loss: 0.4749, Test Loss: 4.0065\n",
      "Epoch: 4144, Train Loss: 0.4758, Test Loss: 4.1638\n",
      "Epoch: 4145, Train Loss: 0.5244, Test Loss: 4.0416\n",
      "Epoch: 4146, Train Loss: 0.4896, Test Loss: 3.9161\n",
      "Epoch: 4147, Train Loss: 0.4768, Test Loss: 3.8309\n",
      "Epoch: 4148, Train Loss: 0.5047, Test Loss: 3.9566\n",
      "Epoch: 4149, Train Loss: 0.4804, Test Loss: 3.6645\n",
      "Epoch: 4150, Train Loss: 0.4674, Test Loss: 3.7491\n",
      "Epoch: 4151, Train Loss: 0.4842, Test Loss: 4.1039\n",
      "Epoch: 4152, Train Loss: 0.4991, Test Loss: 4.1026\n",
      "Epoch: 4153, Train Loss: 0.4944, Test Loss: 3.7810\n",
      "Epoch: 4154, Train Loss: 0.5065, Test Loss: 3.4478\n",
      "Epoch: 4155, Train Loss: 0.5226, Test Loss: 3.8308\n",
      "Epoch: 4156, Train Loss: 0.4754, Test Loss: 4.2320\n",
      "Epoch: 4157, Train Loss: 0.5154, Test Loss: 3.9485\n",
      "Epoch: 4158, Train Loss: 0.5439, Test Loss: 4.0222\n",
      "Epoch: 4159, Train Loss: 0.4972, Test Loss: 3.7466\n",
      "Epoch: 4160, Train Loss: 0.5417, Test Loss: 3.8376\n",
      "Epoch: 4161, Train Loss: 0.4729, Test Loss: 4.4104\n",
      "Epoch: 4162, Train Loss: 0.4811, Test Loss: 4.4038\n",
      "Epoch: 4163, Train Loss: 0.5661, Test Loss: 3.4645\n",
      "Epoch: 4164, Train Loss: 0.5384, Test Loss: 3.3544\n",
      "Epoch: 4165, Train Loss: 0.6039, Test Loss: 4.1840\n",
      "Epoch: 4166, Train Loss: 0.5312, Test Loss: 4.4811\n",
      "Epoch: 4167, Train Loss: 0.5487, Test Loss: 3.8612\n",
      "Epoch: 4168, Train Loss: 0.5140, Test Loss: 3.2618\n",
      "Epoch: 4169, Train Loss: 0.5470, Test Loss: 3.3098\n",
      "Epoch: 4170, Train Loss: 0.5625, Test Loss: 3.9317\n",
      "Epoch: 4171, Train Loss: 0.5067, Test Loss: 4.5796\n",
      "Epoch: 4172, Train Loss: 0.5885, Test Loss: 4.0189\n",
      "Epoch: 4173, Train Loss: 0.5292, Test Loss: 3.3469\n",
      "Epoch: 4174, Train Loss: 0.5879, Test Loss: 3.5278\n",
      "Epoch: 4175, Train Loss: 0.5828, Test Loss: 4.5057\n",
      "Epoch: 4176, Train Loss: 0.6161, Test Loss: 4.4410\n",
      "Epoch: 4177, Train Loss: 0.5448, Test Loss: 3.5662\n",
      "Epoch: 4178, Train Loss: 0.5181, Test Loss: 3.4681\n",
      "Epoch: 4179, Train Loss: 0.5425, Test Loss: 3.7513\n",
      "Epoch: 4180, Train Loss: 0.4551, Test Loss: 4.0880\n",
      "Epoch: 4181, Train Loss: 0.5135, Test Loss: 3.8370\n",
      "Epoch: 4182, Train Loss: 0.5066, Test Loss: 3.8632\n",
      "Epoch: 4183, Train Loss: 0.4862, Test Loss: 3.8045\n",
      "Epoch: 4184, Train Loss: 0.5091, Test Loss: 3.8195\n",
      "Epoch: 4185, Train Loss: 0.4728, Test Loss: 3.7870\n",
      "Epoch: 4186, Train Loss: 0.4945, Test Loss: 3.8504\n",
      "Epoch: 4187, Train Loss: 0.4822, Test Loss: 4.1396\n",
      "Epoch: 4188, Train Loss: 0.5174, Test Loss: 4.3037\n",
      "Epoch: 4189, Train Loss: 0.5228, Test Loss: 3.8282\n",
      "Epoch: 4190, Train Loss: 0.4694, Test Loss: 3.4809\n",
      "Epoch: 4191, Train Loss: 0.5144, Test Loss: 3.8212\n",
      "Epoch: 4192, Train Loss: 0.4986, Test Loss: 4.0950\n",
      "Epoch: 4193, Train Loss: 0.4877, Test Loss: 4.1445\n",
      "Epoch: 4194, Train Loss: 0.4892, Test Loss: 4.0630\n",
      "Epoch: 4195, Train Loss: 0.5580, Test Loss: 4.0152\n",
      "Epoch: 4196, Train Loss: 0.5111, Test Loss: 3.8516\n",
      "Epoch: 4197, Train Loss: 0.5021, Test Loss: 3.8495\n",
      "Epoch: 4198, Train Loss: 0.5573, Test Loss: 3.8853\n",
      "Epoch: 4199, Train Loss: 0.4998, Test Loss: 3.8945\n",
      "Epoch: 4200, Train Loss: 0.4697, Test Loss: 3.7599\n",
      "Epoch: 4201, Train Loss: 0.5851, Test Loss: 4.2120\n",
      "Epoch: 4202, Train Loss: 0.4892, Test Loss: 4.2656\n",
      "Epoch: 4203, Train Loss: 0.5189, Test Loss: 4.0059\n",
      "Epoch: 4204, Train Loss: 0.4962, Test Loss: 3.7155\n",
      "Epoch: 4205, Train Loss: 0.5209, Test Loss: 3.8152\n",
      "Epoch: 4206, Train Loss: 0.4908, Test Loss: 4.2736\n",
      "Epoch: 4207, Train Loss: 0.4961, Test Loss: 4.2020\n",
      "Epoch: 4208, Train Loss: 0.4707, Test Loss: 4.0818\n",
      "Epoch: 4209, Train Loss: 0.4741, Test Loss: 4.0631\n",
      "Epoch: 4210, Train Loss: 0.4445, Test Loss: 3.8972\n",
      "Epoch: 4211, Train Loss: 0.5071, Test Loss: 4.0024\n",
      "Epoch: 4212, Train Loss: 0.4745, Test Loss: 4.0573\n",
      "Epoch: 4213, Train Loss: 0.5102, Test Loss: 3.6891\n",
      "Epoch: 4214, Train Loss: 0.5395, Test Loss: 3.6652\n",
      "Epoch: 4215, Train Loss: 0.5356, Test Loss: 4.5258\n",
      "Epoch: 4216, Train Loss: 0.5459, Test Loss: 4.6148\n",
      "Epoch: 4217, Train Loss: 0.6057, Test Loss: 3.6747\n",
      "Epoch: 4218, Train Loss: 0.5163, Test Loss: 3.4585\n",
      "Epoch: 4219, Train Loss: 0.5962, Test Loss: 4.1490\n",
      "Epoch: 4220, Train Loss: 0.4980, Test Loss: 4.4677\n",
      "Epoch: 4221, Train Loss: 0.4913, Test Loss: 4.0771\n",
      "Epoch: 4222, Train Loss: 0.5009, Test Loss: 3.6047\n",
      "Epoch: 4223, Train Loss: 0.5268, Test Loss: 3.7880\n",
      "Epoch: 4224, Train Loss: 0.5172, Test Loss: 4.1894\n",
      "Epoch: 4225, Train Loss: 0.4813, Test Loss: 4.6038\n",
      "Epoch: 4226, Train Loss: 0.4831, Test Loss: 4.2435\n",
      "Epoch: 4227, Train Loss: 0.4845, Test Loss: 3.7209\n",
      "Epoch: 4228, Train Loss: 0.5115, Test Loss: 3.8263\n",
      "Epoch: 4229, Train Loss: 0.5061, Test Loss: 4.0355\n",
      "Epoch: 4230, Train Loss: 0.5435, Test Loss: 4.1793\n",
      "Epoch: 4231, Train Loss: 0.4779, Test Loss: 3.9967\n",
      "Epoch: 4232, Train Loss: 0.4957, Test Loss: 4.1290\n",
      "Epoch: 4233, Train Loss: 0.4975, Test Loss: 4.1230\n",
      "Epoch: 4234, Train Loss: 0.5088, Test Loss: 4.1367\n",
      "Epoch: 4235, Train Loss: 0.4710, Test Loss: 4.0580\n",
      "Epoch: 4236, Train Loss: 0.4905, Test Loss: 4.2326\n",
      "Epoch: 4237, Train Loss: 0.5297, Test Loss: 3.9674\n",
      "Epoch: 4238, Train Loss: 0.4660, Test Loss: 3.6713\n",
      "Epoch: 4239, Train Loss: 0.5252, Test Loss: 4.0061\n",
      "Epoch: 4240, Train Loss: 0.4630, Test Loss: 4.3035\n",
      "Epoch: 4241, Train Loss: 0.5317, Test Loss: 3.9148\n",
      "Epoch: 4242, Train Loss: 0.4387, Test Loss: 3.6329\n",
      "Epoch: 4243, Train Loss: 0.5911, Test Loss: 4.0956\n",
      "Epoch: 4244, Train Loss: 0.4926, Test Loss: 4.7158\n",
      "Epoch: 4245, Train Loss: 0.5841, Test Loss: 3.9607\n",
      "Epoch: 4246, Train Loss: 0.4658, Test Loss: 3.3428\n",
      "Epoch: 4247, Train Loss: 0.6083, Test Loss: 3.6548\n",
      "Epoch: 4248, Train Loss: 0.5034, Test Loss: 4.3095\n",
      "Epoch: 4249, Train Loss: 0.5216, Test Loss: 4.1412\n",
      "Epoch: 4250, Train Loss: 0.4653, Test Loss: 3.9787\n",
      "Epoch: 4251, Train Loss: 0.4735, Test Loss: 4.1130\n",
      "Epoch: 4252, Train Loss: 0.4887, Test Loss: 4.1482\n",
      "Epoch: 4253, Train Loss: 0.5407, Test Loss: 3.9921\n",
      "Epoch: 4254, Train Loss: 0.4861, Test Loss: 3.8575\n",
      "Epoch: 4255, Train Loss: 0.4784, Test Loss: 4.0212\n",
      "Epoch: 4256, Train Loss: 0.5145, Test Loss: 4.4682\n",
      "Epoch: 4257, Train Loss: 0.5846, Test Loss: 4.1357\n",
      "Epoch: 4258, Train Loss: 0.4708, Test Loss: 3.7084\n",
      "Epoch: 4259, Train Loss: 0.5440, Test Loss: 4.1602\n",
      "Epoch: 4260, Train Loss: 0.4942, Test Loss: 4.8663\n",
      "Epoch: 4261, Train Loss: 0.5513, Test Loss: 4.4278\n",
      "Epoch: 4262, Train Loss: 0.4903, Test Loss: 3.5174\n",
      "Epoch: 4263, Train Loss: 0.5137, Test Loss: 3.3914\n",
      "Epoch: 4264, Train Loss: 0.5609, Test Loss: 4.1231\n",
      "Epoch: 4265, Train Loss: 0.4957, Test Loss: 4.5928\n",
      "Epoch: 4266, Train Loss: 0.5862, Test Loss: 3.9093\n",
      "Epoch: 4267, Train Loss: 0.4819, Test Loss: 3.3994\n",
      "Epoch: 4268, Train Loss: 0.5675, Test Loss: 3.5991\n",
      "Epoch: 4269, Train Loss: 0.5077, Test Loss: 4.0416\n",
      "Epoch: 4270, Train Loss: 0.4956, Test Loss: 4.3404\n",
      "Epoch: 4271, Train Loss: 0.5200, Test Loss: 4.2628\n",
      "Epoch: 4272, Train Loss: 0.4837, Test Loss: 3.8893\n",
      "Epoch: 4273, Train Loss: 0.5042, Test Loss: 3.5009\n",
      "Epoch: 4274, Train Loss: 0.5112, Test Loss: 3.5210\n",
      "Epoch: 4275, Train Loss: 0.5363, Test Loss: 4.0497\n",
      "Epoch: 4276, Train Loss: 0.4913, Test Loss: 4.4097\n",
      "Epoch: 4277, Train Loss: 0.5588, Test Loss: 3.7230\n",
      "Epoch: 4278, Train Loss: 0.5496, Test Loss: 3.5401\n",
      "Epoch: 4279, Train Loss: 0.5187, Test Loss: 3.8607\n",
      "Epoch: 4280, Train Loss: 0.5061, Test Loss: 4.3632\n",
      "Epoch: 4281, Train Loss: 0.5581, Test Loss: 4.1798\n",
      "Epoch: 4282, Train Loss: 0.5278, Test Loss: 3.7362\n",
      "Epoch: 4283, Train Loss: 0.4440, Test Loss: 3.3537\n",
      "Epoch: 4284, Train Loss: 0.4935, Test Loss: 3.5030\n",
      "Epoch: 4285, Train Loss: 0.6098, Test Loss: 4.5910\n",
      "Epoch: 4286, Train Loss: 0.5348, Test Loss: 4.6528\n",
      "Epoch: 4287, Train Loss: 0.5444, Test Loss: 3.6748\n",
      "Epoch: 4288, Train Loss: 0.5119, Test Loss: 3.0515\n",
      "Epoch: 4289, Train Loss: 0.6674, Test Loss: 3.3714\n",
      "Epoch: 4290, Train Loss: 0.5173, Test Loss: 4.5943\n",
      "Epoch: 4291, Train Loss: 0.5361, Test Loss: 4.9011\n",
      "Epoch: 4292, Train Loss: 0.5793, Test Loss: 3.8254\n",
      "Epoch: 4293, Train Loss: 0.5290, Test Loss: 3.4080\n",
      "Epoch: 4294, Train Loss: 0.5026, Test Loss: 3.4305\n",
      "Epoch: 4295, Train Loss: 0.5337, Test Loss: 4.1664\n",
      "Epoch: 4296, Train Loss: 0.4892, Test Loss: 4.5654\n",
      "Epoch: 4297, Train Loss: 0.6114, Test Loss: 3.7899\n",
      "Epoch: 4298, Train Loss: 0.5137, Test Loss: 3.2902\n",
      "Epoch: 4299, Train Loss: 0.5793, Test Loss: 3.5925\n",
      "Epoch: 4300, Train Loss: 0.4601, Test Loss: 4.2173\n",
      "Epoch: 4301, Train Loss: 0.5144, Test Loss: 4.2683\n",
      "Epoch: 4302, Train Loss: 0.5626, Test Loss: 3.4428\n",
      "Epoch: 4303, Train Loss: 0.5412, Test Loss: 3.2681\n",
      "Epoch: 4304, Train Loss: 0.5427, Test Loss: 3.7593\n",
      "Epoch: 4305, Train Loss: 0.5140, Test Loss: 4.2589\n",
      "Epoch: 4306, Train Loss: 0.5452, Test Loss: 4.1353\n",
      "Epoch: 4307, Train Loss: 0.5220, Test Loss: 3.6926\n",
      "Epoch: 4308, Train Loss: 0.5238, Test Loss: 3.6852\n",
      "Epoch: 4309, Train Loss: 0.4577, Test Loss: 3.8079\n",
      "Epoch: 4310, Train Loss: 0.4846, Test Loss: 4.1628\n",
      "Epoch: 4311, Train Loss: 0.5293, Test Loss: 4.0483\n",
      "Epoch: 4312, Train Loss: 0.4497, Test Loss: 3.8198\n",
      "Epoch: 4313, Train Loss: 0.4623, Test Loss: 3.5310\n",
      "Epoch: 4314, Train Loss: 0.5351, Test Loss: 3.8114\n",
      "Epoch: 4315, Train Loss: 0.4560, Test Loss: 4.4660\n",
      "Epoch: 4316, Train Loss: 0.5269, Test Loss: 4.1141\n",
      "Epoch: 4317, Train Loss: 0.4878, Test Loss: 3.3525\n",
      "Epoch: 4318, Train Loss: 0.5784, Test Loss: 3.7068\n",
      "Epoch: 4319, Train Loss: 0.4907, Test Loss: 4.5619\n",
      "Epoch: 4320, Train Loss: 0.5527, Test Loss: 4.4069\n",
      "Epoch: 4321, Train Loss: 0.5101, Test Loss: 3.6500\n",
      "Epoch: 4322, Train Loss: 0.4997, Test Loss: 3.4675\n",
      "Epoch: 4323, Train Loss: 0.5696, Test Loss: 3.9736\n",
      "Epoch: 4324, Train Loss: 0.4868, Test Loss: 4.7302\n",
      "Epoch: 4325, Train Loss: 0.5532, Test Loss: 4.5611\n",
      "Epoch: 4326, Train Loss: 0.5480, Test Loss: 3.6003\n",
      "Epoch: 4327, Train Loss: 0.5205, Test Loss: 3.3106\n",
      "Epoch: 4328, Train Loss: 0.6881, Test Loss: 3.8724\n",
      "Epoch: 4329, Train Loss: 0.4884, Test Loss: 4.4364\n",
      "Epoch: 4330, Train Loss: 0.5588, Test Loss: 4.0953\n",
      "Epoch: 4331, Train Loss: 0.4951, Test Loss: 3.4794\n",
      "Epoch: 4332, Train Loss: 0.4798, Test Loss: 3.3991\n",
      "Epoch: 4333, Train Loss: 0.5641, Test Loss: 3.9621\n",
      "Epoch: 4334, Train Loss: 0.4720, Test Loss: 4.8426\n",
      "Epoch: 4335, Train Loss: 0.7490, Test Loss: 3.7897\n",
      "Epoch: 4336, Train Loss: 0.5510, Test Loss: 3.0769\n",
      "Epoch: 4337, Train Loss: 0.6386, Test Loss: 3.1199\n",
      "Epoch: 4338, Train Loss: 0.5380, Test Loss: 3.6736\n",
      "Epoch: 4339, Train Loss: 0.4960, Test Loss: 4.3190\n",
      "Epoch: 4340, Train Loss: 0.5972, Test Loss: 4.0615\n",
      "Epoch: 4341, Train Loss: 0.5552, Test Loss: 3.6014\n",
      "Epoch: 4342, Train Loss: 0.5144, Test Loss: 3.6539\n",
      "Epoch: 4343, Train Loss: 0.5500, Test Loss: 4.1083\n",
      "Epoch: 4344, Train Loss: 0.4996, Test Loss: 4.3608\n",
      "Epoch: 4345, Train Loss: 0.5811, Test Loss: 3.6560\n",
      "Epoch: 4346, Train Loss: 0.5257, Test Loss: 3.3934\n",
      "Epoch: 4347, Train Loss: 0.5586, Test Loss: 3.6944\n",
      "Epoch: 4348, Train Loss: 0.5331, Test Loss: 3.8827\n",
      "Epoch: 4349, Train Loss: 0.5389, Test Loss: 3.9138\n",
      "Epoch: 4350, Train Loss: 0.4638, Test Loss: 3.7732\n",
      "Epoch: 4351, Train Loss: 0.5122, Test Loss: 3.5729\n",
      "Epoch: 4352, Train Loss: 0.5295, Test Loss: 3.6277\n",
      "Epoch: 4353, Train Loss: 0.5469, Test Loss: 4.1307\n",
      "Epoch: 4354, Train Loss: 0.5329, Test Loss: 4.1427\n",
      "Epoch: 4355, Train Loss: 0.5214, Test Loss: 3.7571\n",
      "Epoch: 4356, Train Loss: 0.5211, Test Loss: 3.3543\n",
      "Epoch: 4357, Train Loss: 0.5400, Test Loss: 3.6477\n",
      "Epoch: 4358, Train Loss: 0.4828, Test Loss: 4.3355\n",
      "Epoch: 4359, Train Loss: 0.5002, Test Loss: 4.7160\n",
      "Epoch: 4360, Train Loss: 0.5817, Test Loss: 3.9481\n",
      "Epoch: 4361, Train Loss: 0.5351, Test Loss: 3.5949\n",
      "Epoch: 4362, Train Loss: 0.4991, Test Loss: 3.8434\n",
      "Epoch: 4363, Train Loss: 0.5032, Test Loss: 3.9767\n",
      "Epoch: 4364, Train Loss: 0.4720, Test Loss: 3.9525\n",
      "Epoch: 4365, Train Loss: 0.4677, Test Loss: 3.6225\n",
      "Epoch: 4366, Train Loss: 0.5052, Test Loss: 3.5444\n",
      "Epoch: 4367, Train Loss: 0.5300, Test Loss: 3.8956\n",
      "Epoch: 4368, Train Loss: 0.4998, Test Loss: 3.7578\n",
      "Epoch: 4369, Train Loss: 0.5054, Test Loss: 3.5038\n",
      "Epoch: 4370, Train Loss: 0.4851, Test Loss: 3.6350\n",
      "Epoch: 4371, Train Loss: 0.4738, Test Loss: 3.8728\n",
      "Epoch: 4372, Train Loss: 0.5062, Test Loss: 3.8876\n",
      "Epoch: 4373, Train Loss: 0.5158, Test Loss: 3.5626\n",
      "Epoch: 4374, Train Loss: 0.4417, Test Loss: 3.5410\n",
      "Epoch: 4375, Train Loss: 0.4948, Test Loss: 3.8875\n",
      "Epoch: 4376, Train Loss: 0.4951, Test Loss: 3.8577\n",
      "Epoch: 4377, Train Loss: 0.5039, Test Loss: 3.8320\n",
      "Epoch: 4378, Train Loss: 0.4704, Test Loss: 3.9527\n",
      "Epoch: 4379, Train Loss: 0.4919, Test Loss: 3.8646\n",
      "Epoch: 4380, Train Loss: 0.5178, Test Loss: 3.6431\n",
      "Epoch: 4381, Train Loss: 0.4777, Test Loss: 3.9201\n",
      "Epoch: 4382, Train Loss: 0.5065, Test Loss: 4.1846\n",
      "Epoch: 4383, Train Loss: 0.4746, Test Loss: 4.1217\n",
      "Epoch: 4384, Train Loss: 0.4925, Test Loss: 3.7819\n",
      "Epoch: 4385, Train Loss: 0.4900, Test Loss: 3.2494\n",
      "Epoch: 4386, Train Loss: 0.6270, Test Loss: 3.5892\n",
      "Epoch: 4387, Train Loss: 0.4761, Test Loss: 4.3652\n",
      "Epoch: 4388, Train Loss: 0.5062, Test Loss: 4.6214\n",
      "Epoch: 4389, Train Loss: 0.6282, Test Loss: 3.4826\n",
      "Epoch: 4390, Train Loss: 0.4980, Test Loss: 3.1595\n",
      "Epoch: 4391, Train Loss: 0.6092, Test Loss: 3.6787\n",
      "Epoch: 4392, Train Loss: 0.5259, Test Loss: 4.3214\n",
      "Epoch: 4393, Train Loss: 0.5220, Test Loss: 4.2574\n",
      "Epoch: 4394, Train Loss: 0.5726, Test Loss: 3.4168\n",
      "Epoch: 4395, Train Loss: 0.5934, Test Loss: 3.5691\n",
      "Epoch: 4396, Train Loss: 0.4863, Test Loss: 4.1577\n",
      "Epoch: 4397, Train Loss: 0.4732, Test Loss: 4.4056\n",
      "Epoch: 4398, Train Loss: 0.5043, Test Loss: 3.7877\n",
      "Epoch: 4399, Train Loss: 0.5219, Test Loss: 3.4772\n",
      "Epoch: 4400, Train Loss: 0.4991, Test Loss: 3.4515\n",
      "Epoch: 4401, Train Loss: 0.5143, Test Loss: 3.9623\n",
      "Epoch: 4402, Train Loss: 0.5062, Test Loss: 4.1895\n",
      "Epoch: 4403, Train Loss: 0.5422, Test Loss: 3.6358\n",
      "Epoch: 4404, Train Loss: 0.5335, Test Loss: 3.4738\n",
      "Epoch: 4405, Train Loss: 0.5023, Test Loss: 3.7695\n",
      "Epoch: 4406, Train Loss: 0.4688, Test Loss: 4.0792\n",
      "Epoch: 4407, Train Loss: 0.4782, Test Loss: 4.0742\n",
      "Epoch: 4408, Train Loss: 0.4801, Test Loss: 3.8025\n",
      "Epoch: 4409, Train Loss: 0.5194, Test Loss: 3.6622\n",
      "Epoch: 4410, Train Loss: 0.4560, Test Loss: 3.6873\n",
      "Epoch: 4411, Train Loss: 0.5260, Test Loss: 3.8844\n",
      "Epoch: 4412, Train Loss: 0.5077, Test Loss: 4.0187\n",
      "Epoch: 4413, Train Loss: 0.4677, Test Loss: 3.7763\n",
      "Epoch: 4414, Train Loss: 0.5046, Test Loss: 3.6442\n",
      "Epoch: 4415, Train Loss: 0.4823, Test Loss: 3.8445\n",
      "Epoch: 4416, Train Loss: 0.4697, Test Loss: 3.9978\n",
      "Epoch: 4417, Train Loss: 0.5698, Test Loss: 4.3311\n",
      "Epoch: 4418, Train Loss: 0.4917, Test Loss: 4.2188\n",
      "Epoch: 4419, Train Loss: 0.4782, Test Loss: 3.7372\n",
      "Epoch: 4420, Train Loss: 0.4915, Test Loss: 3.4398\n",
      "Epoch: 4421, Train Loss: 0.5071, Test Loss: 3.7780\n",
      "Epoch: 4422, Train Loss: 0.5016, Test Loss: 4.1819\n",
      "Epoch: 4423, Train Loss: 0.4973, Test Loss: 4.5104\n",
      "Epoch: 4424, Train Loss: 0.5348, Test Loss: 3.9044\n",
      "Epoch: 4425, Train Loss: 0.5136, Test Loss: 3.5536\n",
      "Epoch: 4426, Train Loss: 0.5344, Test Loss: 3.8829\n",
      "Epoch: 4427, Train Loss: 0.4840, Test Loss: 4.2938\n",
      "Epoch: 4428, Train Loss: 0.5264, Test Loss: 3.9148\n",
      "Epoch: 4429, Train Loss: 0.5236, Test Loss: 3.3874\n",
      "Epoch: 4430, Train Loss: 0.4817, Test Loss: 3.2504\n",
      "Epoch: 4431, Train Loss: 0.5621, Test Loss: 3.8687\n",
      "Epoch: 4432, Train Loss: 0.5112, Test Loss: 4.5889\n",
      "Epoch: 4433, Train Loss: 0.5918, Test Loss: 3.8116\n",
      "Epoch: 4434, Train Loss: 0.4716, Test Loss: 3.3221\n",
      "Epoch: 4435, Train Loss: 0.5944, Test Loss: 3.7737\n",
      "Epoch: 4436, Train Loss: 0.4592, Test Loss: 4.3327\n",
      "Epoch: 4437, Train Loss: 0.5029, Test Loss: 4.2137\n",
      "Epoch: 4438, Train Loss: 0.4782, Test Loss: 3.6742\n",
      "Epoch: 4439, Train Loss: 0.4513, Test Loss: 3.5185\n",
      "Epoch: 4440, Train Loss: 0.5067, Test Loss: 3.9174\n",
      "Epoch: 4441, Train Loss: 0.4938, Test Loss: 3.9511\n",
      "Epoch: 4442, Train Loss: 0.4862, Test Loss: 4.1699\n",
      "Epoch: 4443, Train Loss: 0.4990, Test Loss: 3.7303\n",
      "Epoch: 4444, Train Loss: 0.4934, Test Loss: 3.3098\n",
      "Epoch: 4445, Train Loss: 0.5288, Test Loss: 3.2745\n",
      "Epoch: 4446, Train Loss: 0.6067, Test Loss: 4.2929\n",
      "Epoch: 4447, Train Loss: 0.5370, Test Loss: 4.8215\n",
      "Epoch: 4448, Train Loss: 0.6068, Test Loss: 3.8533\n",
      "Epoch: 4449, Train Loss: 0.4941, Test Loss: 3.0873\n",
      "Epoch: 4450, Train Loss: 0.6616, Test Loss: 3.2983\n",
      "Epoch: 4451, Train Loss: 0.5438, Test Loss: 4.2986\n",
      "Epoch: 4452, Train Loss: 0.5434, Test Loss: 4.8928\n",
      "Epoch: 4453, Train Loss: 0.6577, Test Loss: 3.8543\n",
      "Epoch: 4454, Train Loss: 0.4695, Test Loss: 3.2534\n",
      "Epoch: 4455, Train Loss: 0.5722, Test Loss: 3.3711\n",
      "Epoch: 4456, Train Loss: 0.5102, Test Loss: 4.1813\n",
      "Epoch: 4457, Train Loss: 0.5674, Test Loss: 4.1465\n",
      "Epoch: 4458, Train Loss: 0.5437, Test Loss: 3.6834\n",
      "Epoch: 4459, Train Loss: 0.4639, Test Loss: 3.4729\n",
      "Epoch: 4460, Train Loss: 0.5495, Test Loss: 3.7008\n",
      "Epoch: 4461, Train Loss: 0.4749, Test Loss: 3.9812\n",
      "Epoch: 4462, Train Loss: 0.4981, Test Loss: 4.2376\n",
      "Epoch: 4463, Train Loss: 0.5487, Test Loss: 3.8433\n",
      "Epoch: 4464, Train Loss: 0.4846, Test Loss: 3.6173\n",
      "Epoch: 4465, Train Loss: 0.4849, Test Loss: 3.6904\n",
      "Epoch: 4466, Train Loss: 0.5333, Test Loss: 4.1300\n",
      "Epoch: 4467, Train Loss: 0.5031, Test Loss: 4.0515\n",
      "Epoch: 4468, Train Loss: 0.4540, Test Loss: 4.0166\n",
      "Epoch: 4469, Train Loss: 0.5581, Test Loss: 3.4183\n",
      "Epoch: 4470, Train Loss: 0.5449, Test Loss: 3.6891\n",
      "Epoch: 4471, Train Loss: 0.5124, Test Loss: 4.0773\n",
      "Epoch: 4472, Train Loss: 0.4789, Test Loss: 4.3726\n",
      "Epoch: 4473, Train Loss: 0.4696, Test Loss: 4.0890\n",
      "Epoch: 4474, Train Loss: 0.5059, Test Loss: 3.4284\n",
      "Epoch: 4475, Train Loss: 0.5229, Test Loss: 3.4548\n",
      "Epoch: 4476, Train Loss: 0.5002, Test Loss: 3.8047\n",
      "Epoch: 4477, Train Loss: 0.4843, Test Loss: 3.8360\n",
      "Epoch: 4478, Train Loss: 0.4683, Test Loss: 3.7806\n",
      "Epoch: 4479, Train Loss: 0.4984, Test Loss: 3.8470\n",
      "Epoch: 4480, Train Loss: 0.4618, Test Loss: 4.0320\n",
      "Epoch: 4481, Train Loss: 0.4932, Test Loss: 3.7148\n",
      "Epoch: 4482, Train Loss: 0.4711, Test Loss: 3.4311\n",
      "Epoch: 4483, Train Loss: 0.6351, Test Loss: 3.8965\n",
      "Epoch: 4484, Train Loss: 0.4406, Test Loss: 3.9662\n",
      "Epoch: 4485, Train Loss: 0.4606, Test Loss: 3.8643\n",
      "Epoch: 4486, Train Loss: 0.4698, Test Loss: 3.6774\n",
      "Epoch: 4487, Train Loss: 0.4510, Test Loss: 3.9231\n",
      "Epoch: 4488, Train Loss: 0.5090, Test Loss: 3.8011\n",
      "Epoch: 4489, Train Loss: 0.4806, Test Loss: 3.4526\n",
      "Epoch: 4490, Train Loss: 0.5119, Test Loss: 3.7828\n",
      "Epoch: 4491, Train Loss: 0.4573, Test Loss: 4.0411\n",
      "Epoch: 4492, Train Loss: 0.5457, Test Loss: 3.6031\n",
      "Epoch: 4493, Train Loss: 0.4784, Test Loss: 3.5707\n",
      "Epoch: 4494, Train Loss: 0.4514, Test Loss: 3.7107\n",
      "Epoch: 4495, Train Loss: 0.5067, Test Loss: 4.0519\n",
      "Epoch: 4496, Train Loss: 0.4625, Test Loss: 4.0070\n",
      "Epoch: 4497, Train Loss: 0.4687, Test Loss: 3.5785\n",
      "Epoch: 4498, Train Loss: 0.4611, Test Loss: 3.5226\n",
      "Epoch: 4499, Train Loss: 0.5399, Test Loss: 4.2797\n",
      "Epoch: 4500, Train Loss: 0.4860, Test Loss: 4.5077\n",
      "Epoch: 4501, Train Loss: 0.5370, Test Loss: 3.7850\n",
      "Epoch: 4502, Train Loss: 0.4611, Test Loss: 3.5392\n",
      "Epoch: 4503, Train Loss: 0.4641, Test Loss: 3.6576\n",
      "Epoch: 4504, Train Loss: 0.5026, Test Loss: 4.1494\n",
      "Epoch: 4505, Train Loss: 0.4801, Test Loss: 4.0354\n",
      "Epoch: 4506, Train Loss: 0.4481, Test Loss: 3.7722\n",
      "Epoch: 4507, Train Loss: 0.4948, Test Loss: 3.4519\n",
      "Epoch: 4508, Train Loss: 0.5359, Test Loss: 3.6547\n",
      "Epoch: 4509, Train Loss: 0.4988, Test Loss: 4.1523\n",
      "Epoch: 4510, Train Loss: 0.5272, Test Loss: 3.9482\n",
      "Epoch: 4511, Train Loss: 0.4809, Test Loss: 3.5196\n",
      "Epoch: 4512, Train Loss: 0.5156, Test Loss: 3.6521\n",
      "Epoch: 4513, Train Loss: 0.5138, Test Loss: 3.9894\n",
      "Epoch: 4514, Train Loss: 0.5273, Test Loss: 4.0738\n",
      "Epoch: 4515, Train Loss: 0.4797, Test Loss: 3.6494\n",
      "Epoch: 4516, Train Loss: 0.4452, Test Loss: 3.4784\n",
      "Epoch: 4517, Train Loss: 0.4759, Test Loss: 3.7141\n",
      "Epoch: 4518, Train Loss: 0.4958, Test Loss: 4.6261\n",
      "Epoch: 4519, Train Loss: 0.5220, Test Loss: 4.3667\n",
      "Epoch: 4520, Train Loss: 0.4993, Test Loss: 3.4505\n",
      "Epoch: 4521, Train Loss: 0.5370, Test Loss: 3.3160\n",
      "Epoch: 4522, Train Loss: 0.5504, Test Loss: 3.9418\n",
      "Epoch: 4523, Train Loss: 0.4921, Test Loss: 4.1808\n",
      "Epoch: 4524, Train Loss: 0.5388, Test Loss: 3.7404\n",
      "Epoch: 4525, Train Loss: 0.4794, Test Loss: 3.4993\n",
      "Epoch: 4526, Train Loss: 0.5123, Test Loss: 3.9435\n",
      "Epoch: 4527, Train Loss: 0.4551, Test Loss: 4.2647\n",
      "Epoch: 4528, Train Loss: 0.5307, Test Loss: 4.0092\n",
      "Epoch: 4529, Train Loss: 0.4471, Test Loss: 3.6351\n",
      "Epoch: 4530, Train Loss: 0.5330, Test Loss: 3.9636\n",
      "Epoch: 4531, Train Loss: 0.4673, Test Loss: 4.1162\n",
      "Epoch: 4532, Train Loss: 0.5226, Test Loss: 3.7142\n",
      "Epoch: 4533, Train Loss: 0.4543, Test Loss: 3.5141\n",
      "Epoch: 4534, Train Loss: 0.4978, Test Loss: 3.7992\n",
      "Epoch: 4535, Train Loss: 0.4860, Test Loss: 4.1337\n",
      "Epoch: 4536, Train Loss: 0.4612, Test Loss: 4.1527\n",
      "Epoch: 4537, Train Loss: 0.5142, Test Loss: 3.8428\n",
      "Epoch: 4538, Train Loss: 0.4914, Test Loss: 3.6294\n",
      "Epoch: 4539, Train Loss: 0.4526, Test Loss: 3.7280\n",
      "Epoch: 4540, Train Loss: 0.4844, Test Loss: 3.8852\n",
      "Epoch: 4541, Train Loss: 0.4960, Test Loss: 3.9687\n",
      "Epoch: 4542, Train Loss: 0.4723, Test Loss: 3.8513\n",
      "Epoch: 4543, Train Loss: 0.4596, Test Loss: 3.6200\n",
      "Epoch: 4544, Train Loss: 0.4855, Test Loss: 3.7355\n",
      "Epoch: 4545, Train Loss: 0.4528, Test Loss: 3.8535\n",
      "Epoch: 4546, Train Loss: 0.4861, Test Loss: 3.9934\n",
      "Epoch: 4547, Train Loss: 0.5028, Test Loss: 3.6369\n",
      "Epoch: 4548, Train Loss: 0.5951, Test Loss: 4.0309\n",
      "Epoch: 4549, Train Loss: 0.4765, Test Loss: 4.0198\n",
      "Epoch: 4550, Train Loss: 0.5294, Test Loss: 3.5132\n",
      "Epoch: 4551, Train Loss: 0.4836, Test Loss: 3.4818\n",
      "Epoch: 4552, Train Loss: 0.5216, Test Loss: 3.9679\n",
      "Epoch: 4553, Train Loss: 0.5051, Test Loss: 3.8270\n",
      "Epoch: 4554, Train Loss: 0.4731, Test Loss: 3.9365\n",
      "Epoch: 4555, Train Loss: 0.4646, Test Loss: 4.0128\n",
      "Epoch: 4556, Train Loss: 0.4503, Test Loss: 4.0405\n",
      "Epoch: 4557, Train Loss: 0.4846, Test Loss: 3.8169\n",
      "Epoch: 4558, Train Loss: 0.4770, Test Loss: 3.6367\n",
      "Epoch: 4559, Train Loss: 0.5105, Test Loss: 3.7250\n",
      "Epoch: 4560, Train Loss: 0.4532, Test Loss: 3.9614\n",
      "Epoch: 4561, Train Loss: 0.4791, Test Loss: 3.5486\n",
      "Epoch: 4562, Train Loss: 0.5012, Test Loss: 3.5859\n",
      "Epoch: 4563, Train Loss: 0.4946, Test Loss: 3.8816\n",
      "Epoch: 4564, Train Loss: 0.5045, Test Loss: 4.0495\n",
      "Epoch: 4565, Train Loss: 0.4914, Test Loss: 3.8196\n",
      "Epoch: 4566, Train Loss: 0.5101, Test Loss: 3.7486\n",
      "Epoch: 4567, Train Loss: 0.4710, Test Loss: 3.8565\n",
      "Epoch: 4568, Train Loss: 0.4462, Test Loss: 4.1822\n",
      "Epoch: 4569, Train Loss: 0.5041, Test Loss: 3.8002\n",
      "Epoch: 4570, Train Loss: 0.5064, Test Loss: 3.7500\n",
      "Epoch: 4571, Train Loss: 0.4379, Test Loss: 3.7643\n",
      "Epoch: 4572, Train Loss: 0.4298, Test Loss: 3.7527\n",
      "Epoch: 4573, Train Loss: 0.4530, Test Loss: 3.9205\n",
      "Epoch: 4574, Train Loss: 0.4472, Test Loss: 4.1748\n",
      "Epoch: 4575, Train Loss: 0.4498, Test Loss: 4.1181\n",
      "Epoch: 4576, Train Loss: 0.4759, Test Loss: 3.6929\n",
      "Epoch: 4577, Train Loss: 0.5041, Test Loss: 3.7707\n",
      "Epoch: 4578, Train Loss: 0.4528, Test Loss: 4.2079\n",
      "Epoch: 4579, Train Loss: 0.4788, Test Loss: 4.2883\n",
      "Epoch: 4580, Train Loss: 0.4913, Test Loss: 3.6679\n",
      "Epoch: 4581, Train Loss: 0.5139, Test Loss: 3.4800\n",
      "Epoch: 4582, Train Loss: 0.5339, Test Loss: 4.0645\n",
      "Epoch: 4583, Train Loss: 0.4514, Test Loss: 4.5125\n",
      "Epoch: 4584, Train Loss: 0.5197, Test Loss: 3.9984\n",
      "Epoch: 4585, Train Loss: 0.4674, Test Loss: 3.5546\n",
      "Epoch: 4586, Train Loss: 0.4750, Test Loss: 3.6118\n",
      "Epoch: 4587, Train Loss: 0.4770, Test Loss: 3.7918\n",
      "Epoch: 4588, Train Loss: 0.4351, Test Loss: 4.0706\n",
      "Epoch: 4589, Train Loss: 0.4578, Test Loss: 4.1526\n",
      "Epoch: 4590, Train Loss: 0.4754, Test Loss: 3.7911\n",
      "Epoch: 4591, Train Loss: 0.4737, Test Loss: 3.8007\n",
      "Epoch: 4592, Train Loss: 0.4944, Test Loss: 4.0606\n",
      "Epoch: 4593, Train Loss: 0.4939, Test Loss: 3.9532\n",
      "Epoch: 4594, Train Loss: 0.5063, Test Loss: 3.6473\n",
      "Epoch: 4595, Train Loss: 0.5015, Test Loss: 3.6816\n",
      "Epoch: 4596, Train Loss: 0.4526, Test Loss: 4.0386\n",
      "Epoch: 4597, Train Loss: 0.4734, Test Loss: 4.1619\n",
      "Epoch: 4598, Train Loss: 0.4877, Test Loss: 3.9418\n",
      "Epoch: 4599, Train Loss: 0.5482, Test Loss: 3.9038\n",
      "Epoch: 4600, Train Loss: 0.4647, Test Loss: 4.1817\n",
      "Epoch: 4601, Train Loss: 0.5603, Test Loss: 3.6613\n",
      "Epoch: 4602, Train Loss: 0.4784, Test Loss: 3.2336\n",
      "Epoch: 4603, Train Loss: 0.5790, Test Loss: 3.7018\n",
      "Epoch: 4604, Train Loss: 0.4792, Test Loss: 5.0956\n",
      "Epoch: 4605, Train Loss: 0.5856, Test Loss: 4.8202\n",
      "Epoch: 4606, Train Loss: 0.5729, Test Loss: 3.4490\n",
      "Epoch: 4607, Train Loss: 0.5256, Test Loss: 3.2627\n",
      "Epoch: 4608, Train Loss: 0.5699, Test Loss: 3.7421\n",
      "Epoch: 4609, Train Loss: 0.4964, Test Loss: 4.5532\n",
      "Epoch: 4610, Train Loss: 0.5738, Test Loss: 4.1093\n",
      "Epoch: 4611, Train Loss: 0.4919, Test Loss: 3.3440\n",
      "Epoch: 4612, Train Loss: 0.5548, Test Loss: 3.3441\n",
      "Epoch: 4613, Train Loss: 0.4975, Test Loss: 3.7545\n",
      "Epoch: 4614, Train Loss: 0.4972, Test Loss: 4.6008\n",
      "Epoch: 4615, Train Loss: 0.4989, Test Loss: 4.5746\n",
      "Epoch: 4616, Train Loss: 0.5279, Test Loss: 3.8925\n",
      "Epoch: 4617, Train Loss: 0.4690, Test Loss: 3.4147\n",
      "Epoch: 4618, Train Loss: 0.5514, Test Loss: 3.3557\n",
      "Epoch: 4619, Train Loss: 0.5657, Test Loss: 4.0301\n",
      "Epoch: 4620, Train Loss: 0.5173, Test Loss: 4.4628\n",
      "Epoch: 4621, Train Loss: 0.5168, Test Loss: 3.9498\n",
      "Epoch: 4622, Train Loss: 0.4948, Test Loss: 3.4465\n",
      "Epoch: 4623, Train Loss: 0.5781, Test Loss: 3.6082\n",
      "Epoch: 4624, Train Loss: 0.4553, Test Loss: 4.0033\n",
      "Epoch: 4625, Train Loss: 0.4669, Test Loss: 3.9961\n",
      "Epoch: 4626, Train Loss: 0.4830, Test Loss: 3.6699\n",
      "Epoch: 4627, Train Loss: 0.4760, Test Loss: 3.6025\n",
      "Epoch: 4628, Train Loss: 0.4962, Test Loss: 4.0102\n",
      "Epoch: 4629, Train Loss: 0.4528, Test Loss: 4.3486\n",
      "Epoch: 4630, Train Loss: 0.4876, Test Loss: 3.9845\n",
      "Epoch: 4631, Train Loss: 0.4773, Test Loss: 3.6333\n",
      "Epoch: 4632, Train Loss: 0.5034, Test Loss: 3.9424\n",
      "Epoch: 4633, Train Loss: 0.4712, Test Loss: 3.8398\n",
      "Epoch: 4634, Train Loss: 0.4493, Test Loss: 3.6965\n",
      "Epoch: 4635, Train Loss: 0.4638, Test Loss: 3.9053\n",
      "Epoch: 4636, Train Loss: 0.4542, Test Loss: 4.0760\n",
      "Epoch: 4637, Train Loss: 0.4977, Test Loss: 3.6712\n",
      "Epoch: 4638, Train Loss: 0.5361, Test Loss: 3.9445\n",
      "Epoch: 4639, Train Loss: 0.4627, Test Loss: 3.9967\n",
      "Epoch: 4640, Train Loss: 0.4507, Test Loss: 4.1128\n",
      "Epoch: 4641, Train Loss: 0.4677, Test Loss: 3.8957\n",
      "Epoch: 4642, Train Loss: 0.4633, Test Loss: 3.7798\n",
      "Epoch: 4643, Train Loss: 0.4617, Test Loss: 3.8066\n",
      "Epoch: 4644, Train Loss: 0.4617, Test Loss: 4.1090\n",
      "Epoch: 4645, Train Loss: 0.5023, Test Loss: 3.8202\n",
      "Epoch: 4646, Train Loss: 0.4772, Test Loss: 3.8589\n",
      "Epoch: 4647, Train Loss: 0.4720, Test Loss: 3.9643\n",
      "Epoch: 4648, Train Loss: 0.4705, Test Loss: 3.5679\n",
      "Epoch: 4649, Train Loss: 0.4688, Test Loss: 3.5171\n",
      "Epoch: 4650, Train Loss: 0.4447, Test Loss: 3.7167\n",
      "Epoch: 4651, Train Loss: 0.4555, Test Loss: 4.1624\n",
      "Epoch: 4652, Train Loss: 0.4643, Test Loss: 4.0706\n",
      "Epoch: 4653, Train Loss: 0.4697, Test Loss: 3.9547\n",
      "Epoch: 4654, Train Loss: 0.5003, Test Loss: 4.0003\n",
      "Epoch: 4655, Train Loss: 0.4666, Test Loss: 3.8011\n",
      "Epoch: 4656, Train Loss: 0.4931, Test Loss: 3.9398\n",
      "Epoch: 4657, Train Loss: 0.5255, Test Loss: 3.9680\n",
      "Epoch: 4658, Train Loss: 0.4791, Test Loss: 4.1615\n",
      "Epoch: 4659, Train Loss: 0.4814, Test Loss: 3.8949\n",
      "Epoch: 4660, Train Loss: 0.4579, Test Loss: 3.6773\n",
      "Epoch: 4661, Train Loss: 0.4656, Test Loss: 3.7880\n",
      "Epoch: 4662, Train Loss: 0.4784, Test Loss: 4.3490\n",
      "Epoch: 4663, Train Loss: 0.5351, Test Loss: 4.0643\n",
      "Epoch: 4664, Train Loss: 0.5106, Test Loss: 3.4964\n",
      "Epoch: 4665, Train Loss: 0.5413, Test Loss: 3.4597\n",
      "Epoch: 4666, Train Loss: 0.6071, Test Loss: 4.1364\n",
      "Epoch: 4667, Train Loss: 0.4657, Test Loss: 4.5496\n",
      "Epoch: 4668, Train Loss: 0.5715, Test Loss: 3.7566\n",
      "Epoch: 4669, Train Loss: 0.5199, Test Loss: 3.5112\n",
      "Epoch: 4670, Train Loss: 0.4951, Test Loss: 3.6700\n",
      "Epoch: 4671, Train Loss: 0.4596, Test Loss: 4.2465\n",
      "Epoch: 4672, Train Loss: 0.5622, Test Loss: 4.0456\n",
      "Epoch: 4673, Train Loss: 0.4891, Test Loss: 3.4623\n",
      "Epoch: 4674, Train Loss: 0.5228, Test Loss: 3.5794\n",
      "Epoch: 4675, Train Loss: 0.4682, Test Loss: 4.0203\n",
      "Epoch: 4676, Train Loss: 0.4893, Test Loss: 4.1225\n",
      "Epoch: 4677, Train Loss: 0.4799, Test Loss: 3.7218\n",
      "Epoch: 4678, Train Loss: 0.4510, Test Loss: 3.6725\n",
      "Epoch: 4679, Train Loss: 0.4655, Test Loss: 4.0807\n",
      "Epoch: 4680, Train Loss: 0.4445, Test Loss: 4.2253\n",
      "Epoch: 4681, Train Loss: 0.4597, Test Loss: 3.9732\n",
      "Epoch: 4682, Train Loss: 0.4534, Test Loss: 3.7448\n",
      "Epoch: 4683, Train Loss: 0.4786, Test Loss: 3.9217\n",
      "Epoch: 4684, Train Loss: 0.5404, Test Loss: 4.2968\n",
      "Epoch: 4685, Train Loss: 0.5178, Test Loss: 3.8540\n",
      "Epoch: 4686, Train Loss: 0.4883, Test Loss: 3.3603\n",
      "Epoch: 4687, Train Loss: 0.5399, Test Loss: 3.6903\n",
      "Epoch: 4688, Train Loss: 0.4682, Test Loss: 4.5422\n",
      "Epoch: 4689, Train Loss: 0.4949, Test Loss: 4.5765\n",
      "Epoch: 4690, Train Loss: 0.5723, Test Loss: 3.5594\n",
      "Epoch: 4691, Train Loss: 0.4831, Test Loss: 3.2574\n",
      "Epoch: 4692, Train Loss: 0.5334, Test Loss: 3.6419\n",
      "Epoch: 4693, Train Loss: 0.4194, Test Loss: 4.3485\n",
      "Epoch: 4694, Train Loss: 0.5906, Test Loss: 3.9136\n",
      "Epoch: 4695, Train Loss: 0.4762, Test Loss: 3.4894\n",
      "Epoch: 4696, Train Loss: 0.4823, Test Loss: 3.5786\n",
      "Epoch: 4697, Train Loss: 0.4695, Test Loss: 4.2645\n",
      "Epoch: 4698, Train Loss: 0.5053, Test Loss: 4.1964\n",
      "Epoch: 4699, Train Loss: 0.5229, Test Loss: 3.4189\n",
      "Epoch: 4700, Train Loss: 0.5060, Test Loss: 3.4602\n",
      "Epoch: 4701, Train Loss: 0.4623, Test Loss: 3.9226\n",
      "Epoch: 4702, Train Loss: 0.4783, Test Loss: 4.2645\n",
      "Epoch: 4703, Train Loss: 0.5771, Test Loss: 3.5030\n",
      "Epoch: 4704, Train Loss: 0.4839, Test Loss: 3.3121\n",
      "Epoch: 4705, Train Loss: 0.5330, Test Loss: 3.7053\n",
      "Epoch: 4706, Train Loss: 0.4674, Test Loss: 3.8159\n",
      "Epoch: 4707, Train Loss: 0.4817, Test Loss: 3.9659\n",
      "Epoch: 4708, Train Loss: 0.4437, Test Loss: 4.0087\n",
      "Epoch: 4709, Train Loss: 0.4544, Test Loss: 3.9476\n",
      "Epoch: 4710, Train Loss: 0.4613, Test Loss: 3.8586\n",
      "Epoch: 4711, Train Loss: 0.4455, Test Loss: 4.0726\n",
      "Epoch: 4712, Train Loss: 0.4675, Test Loss: 4.0720\n",
      "Epoch: 4713, Train Loss: 0.4864, Test Loss: 3.7216\n",
      "Epoch: 4714, Train Loss: 0.4371, Test Loss: 3.5882\n",
      "Epoch: 4715, Train Loss: 0.4493, Test Loss: 3.8059\n",
      "Epoch: 4716, Train Loss: 0.4529, Test Loss: 4.3402\n",
      "Epoch: 4717, Train Loss: 0.5122, Test Loss: 4.3116\n",
      "Epoch: 4718, Train Loss: 0.4605, Test Loss: 3.7705\n",
      "Epoch: 4719, Train Loss: 0.4831, Test Loss: 3.5623\n",
      "Epoch: 4720, Train Loss: 0.4670, Test Loss: 3.6992\n",
      "Epoch: 4721, Train Loss: 0.4403, Test Loss: 4.0206\n",
      "Epoch: 4722, Train Loss: 0.4906, Test Loss: 3.9420\n",
      "Epoch: 4723, Train Loss: 0.4439, Test Loss: 3.7208\n",
      "Epoch: 4724, Train Loss: 0.4905, Test Loss: 3.7927\n",
      "Epoch: 4725, Train Loss: 0.5078, Test Loss: 4.2779\n",
      "Epoch: 4726, Train Loss: 0.4682, Test Loss: 4.1531\n",
      "Epoch: 4727, Train Loss: 0.4773, Test Loss: 3.6416\n",
      "Epoch: 4728, Train Loss: 0.4851, Test Loss: 3.7011\n",
      "Epoch: 4729, Train Loss: 0.4691, Test Loss: 4.1532\n",
      "Epoch: 4730, Train Loss: 0.4736, Test Loss: 4.4022\n",
      "Epoch: 4731, Train Loss: 0.4517, Test Loss: 4.0834\n",
      "Epoch: 4732, Train Loss: 0.4770, Test Loss: 3.6642\n",
      "Epoch: 4733, Train Loss: 0.4517, Test Loss: 3.5275\n",
      "Epoch: 4734, Train Loss: 0.4783, Test Loss: 3.9012\n",
      "Epoch: 4735, Train Loss: 0.4624, Test Loss: 4.4638\n",
      "Epoch: 4736, Train Loss: 0.4730, Test Loss: 4.3366\n",
      "Epoch: 4737, Train Loss: 0.5653, Test Loss: 3.3700\n",
      "Epoch: 4738, Train Loss: 0.5222, Test Loss: 3.2120\n",
      "Epoch: 4739, Train Loss: 0.5689, Test Loss: 3.9746\n",
      "Epoch: 4740, Train Loss: 0.5079, Test Loss: 4.5913\n",
      "Epoch: 4741, Train Loss: 0.6203, Test Loss: 3.6511\n",
      "Epoch: 4742, Train Loss: 0.5194, Test Loss: 3.1574\n",
      "Epoch: 4743, Train Loss: 0.5594, Test Loss: 3.4876\n",
      "Epoch: 4744, Train Loss: 0.5075, Test Loss: 4.4820\n",
      "Epoch: 4745, Train Loss: 0.5531, Test Loss: 4.0483\n",
      "Epoch: 4746, Train Loss: 0.4868, Test Loss: 3.2488\n",
      "Epoch: 4747, Train Loss: 0.5566, Test Loss: 3.3174\n",
      "Epoch: 4748, Train Loss: 0.5227, Test Loss: 4.2595\n",
      "Epoch: 4749, Train Loss: 0.5177, Test Loss: 4.4307\n",
      "Epoch: 4750, Train Loss: 0.5164, Test Loss: 3.7359\n",
      "Epoch: 4751, Train Loss: 0.4936, Test Loss: 3.0867\n",
      "Epoch: 4752, Train Loss: 0.6102, Test Loss: 3.3638\n",
      "Epoch: 4753, Train Loss: 0.5160, Test Loss: 4.7263\n",
      "Epoch: 4754, Train Loss: 0.5818, Test Loss: 4.7015\n",
      "Epoch: 4755, Train Loss: 0.6908, Test Loss: 3.1995\n",
      "Epoch: 4756, Train Loss: 0.5979, Test Loss: 3.1207\n",
      "Epoch: 4757, Train Loss: 0.6111, Test Loss: 3.7817\n",
      "Epoch: 4758, Train Loss: 0.5019, Test Loss: 4.9453\n",
      "Epoch: 4759, Train Loss: 0.7199, Test Loss: 4.2013\n",
      "Epoch: 4760, Train Loss: 0.5263, Test Loss: 3.3247\n",
      "Epoch: 4761, Train Loss: 0.5803, Test Loss: 3.2927\n",
      "Epoch: 4762, Train Loss: 0.5446, Test Loss: 3.7298\n",
      "Epoch: 4763, Train Loss: 0.4725, Test Loss: 4.1677\n",
      "Epoch: 4764, Train Loss: 0.5010, Test Loss: 3.9445\n",
      "Epoch: 4765, Train Loss: 0.4747, Test Loss: 3.5621\n",
      "Epoch: 4766, Train Loss: 0.5333, Test Loss: 3.6677\n",
      "Epoch: 4767, Train Loss: 0.4825, Test Loss: 4.0220\n",
      "Epoch: 4768, Train Loss: 0.4864, Test Loss: 3.9143\n",
      "Epoch: 4769, Train Loss: 0.4458, Test Loss: 3.4742\n",
      "Epoch: 4770, Train Loss: 0.5055, Test Loss: 3.3847\n",
      "Epoch: 4771, Train Loss: 0.5490, Test Loss: 4.2479\n",
      "Epoch: 4772, Train Loss: 0.4666, Test Loss: 4.6358\n",
      "Epoch: 4773, Train Loss: 0.5309, Test Loss: 3.8527\n",
      "Epoch: 4774, Train Loss: 0.5128, Test Loss: 3.0200\n",
      "Epoch: 4775, Train Loss: 0.6564, Test Loss: 3.2344\n",
      "Epoch: 4776, Train Loss: 0.5354, Test Loss: 4.6451\n",
      "Epoch: 4777, Train Loss: 0.7797, Test Loss: 4.2855\n",
      "Epoch: 4778, Train Loss: 0.5652, Test Loss: 3.2530\n",
      "Epoch: 4779, Train Loss: 0.4854, Test Loss: 2.9962\n",
      "Epoch: 4780, Train Loss: 0.6809, Test Loss: 3.5452\n",
      "Epoch: 4781, Train Loss: 0.4808, Test Loss: 4.3996\n",
      "Epoch: 4782, Train Loss: 0.5447, Test Loss: 4.2531\n",
      "Epoch: 4783, Train Loss: 0.5610, Test Loss: 3.5052\n",
      "Epoch: 4784, Train Loss: 0.4587, Test Loss: 3.1282\n",
      "Epoch: 4785, Train Loss: 0.6108, Test Loss: 3.4764\n",
      "Epoch: 4786, Train Loss: 0.5080, Test Loss: 3.9623\n",
      "Epoch: 4787, Train Loss: 0.5348, Test Loss: 3.8477\n",
      "Epoch: 4788, Train Loss: 0.5299, Test Loss: 3.3215\n",
      "Epoch: 4789, Train Loss: 0.5567, Test Loss: 3.4446\n",
      "Epoch: 4790, Train Loss: 0.4835, Test Loss: 3.8277\n",
      "Epoch: 4791, Train Loss: 0.4959, Test Loss: 3.8464\n",
      "Epoch: 4792, Train Loss: 0.4981, Test Loss: 3.4817\n",
      "Epoch: 4793, Train Loss: 0.4905, Test Loss: 3.2434\n",
      "Epoch: 4794, Train Loss: 0.5195, Test Loss: 3.3853\n",
      "Epoch: 4795, Train Loss: 0.4662, Test Loss: 3.9885\n",
      "Epoch: 4796, Train Loss: 0.4981, Test Loss: 4.0581\n",
      "Epoch: 4797, Train Loss: 0.4995, Test Loss: 3.6099\n",
      "Epoch: 4798, Train Loss: 0.4581, Test Loss: 3.4150\n",
      "Epoch: 4799, Train Loss: 0.5153, Test Loss: 3.7555\n",
      "Epoch: 4800, Train Loss: 0.4390, Test Loss: 4.0654\n",
      "Epoch: 4801, Train Loss: 0.4628, Test Loss: 3.8768\n",
      "Epoch: 4802, Train Loss: 0.4654, Test Loss: 3.6854\n",
      "Epoch: 4803, Train Loss: 0.4497, Test Loss: 3.6211\n",
      "Epoch: 4804, Train Loss: 0.4371, Test Loss: 3.7593\n",
      "Epoch: 4805, Train Loss: 0.4386, Test Loss: 3.8419\n",
      "Epoch: 4806, Train Loss: 0.4793, Test Loss: 3.4679\n",
      "Epoch: 4807, Train Loss: 0.4872, Test Loss: 3.5737\n",
      "Epoch: 4808, Train Loss: 0.4664, Test Loss: 3.8600\n",
      "Epoch: 4809, Train Loss: 0.4711, Test Loss: 3.7709\n",
      "Epoch: 4810, Train Loss: 0.4492, Test Loss: 3.3397\n",
      "Epoch: 4811, Train Loss: 0.4992, Test Loss: 3.5104\n",
      "Epoch: 4812, Train Loss: 0.4736, Test Loss: 4.0171\n",
      "Epoch: 4813, Train Loss: 0.5045, Test Loss: 3.7336\n",
      "Epoch: 4814, Train Loss: 0.4372, Test Loss: 3.6481\n",
      "Epoch: 4815, Train Loss: 0.4773, Test Loss: 3.6056\n",
      "Epoch: 4816, Train Loss: 0.4788, Test Loss: 4.1524\n",
      "Epoch: 4817, Train Loss: 0.4974, Test Loss: 4.1760\n",
      "Epoch: 4818, Train Loss: 0.4980, Test Loss: 3.5060\n",
      "Epoch: 4819, Train Loss: 0.4979, Test Loss: 3.2010\n",
      "Epoch: 4820, Train Loss: 0.5020, Test Loss: 3.4060\n",
      "Epoch: 4821, Train Loss: 0.4674, Test Loss: 4.1535\n",
      "Epoch: 4822, Train Loss: 0.5202, Test Loss: 4.3748\n",
      "Epoch: 4823, Train Loss: 0.5176, Test Loss: 3.6623\n",
      "Epoch: 4824, Train Loss: 0.4334, Test Loss: 3.1073\n",
      "Epoch: 4825, Train Loss: 0.5921, Test Loss: 3.4970\n",
      "Epoch: 4826, Train Loss: 0.4651, Test Loss: 4.3541\n",
      "Epoch: 4827, Train Loss: 0.4930, Test Loss: 4.4638\n",
      "Epoch: 4828, Train Loss: 0.5366, Test Loss: 3.5351\n",
      "Epoch: 4829, Train Loss: 0.4739, Test Loss: 3.2117\n",
      "Epoch: 4830, Train Loss: 0.5237, Test Loss: 3.4744\n",
      "Epoch: 4831, Train Loss: 0.4726, Test Loss: 4.3061\n",
      "Epoch: 4832, Train Loss: 0.6177, Test Loss: 3.8709\n",
      "Epoch: 4833, Train Loss: 0.4784, Test Loss: 3.2691\n",
      "Epoch: 4834, Train Loss: 0.5171, Test Loss: 3.2356\n",
      "Epoch: 4835, Train Loss: 0.5338, Test Loss: 3.9686\n",
      "Epoch: 4836, Train Loss: 0.4832, Test Loss: 4.2150\n",
      "Epoch: 4837, Train Loss: 0.4999, Test Loss: 3.6940\n",
      "Epoch: 4838, Train Loss: 0.4701, Test Loss: 3.2653\n",
      "Epoch: 4839, Train Loss: 0.4934, Test Loss: 3.2482\n",
      "Epoch: 4840, Train Loss: 0.4774, Test Loss: 3.5401\n",
      "Epoch: 4841, Train Loss: 0.4611, Test Loss: 4.1315\n",
      "Epoch: 4842, Train Loss: 0.4987, Test Loss: 4.0003\n",
      "Epoch: 4843, Train Loss: 0.5236, Test Loss: 3.3227\n",
      "Epoch: 4844, Train Loss: 0.5141, Test Loss: 3.3090\n",
      "Epoch: 4845, Train Loss: 0.5738, Test Loss: 4.2435\n",
      "Epoch: 4846, Train Loss: 0.4893, Test Loss: 4.5420\n",
      "Epoch: 4847, Train Loss: 0.5963, Test Loss: 3.5623\n",
      "Epoch: 4848, Train Loss: 0.4275, Test Loss: 3.1361\n",
      "Epoch: 4849, Train Loss: 0.6919, Test Loss: 3.6688\n",
      "Epoch: 4850, Train Loss: 0.5345, Test Loss: 4.6207\n",
      "Epoch: 4851, Train Loss: 0.5964, Test Loss: 4.1218\n",
      "Epoch: 4852, Train Loss: 0.4337, Test Loss: 3.5647\n",
      "Epoch: 4853, Train Loss: 0.5146, Test Loss: 3.4601\n",
      "Epoch: 4854, Train Loss: 0.4424, Test Loss: 3.6767\n",
      "Epoch: 4855, Train Loss: 0.4793, Test Loss: 4.1203\n",
      "Epoch: 4856, Train Loss: 0.4528, Test Loss: 3.8806\n",
      "Epoch: 4857, Train Loss: 0.4611, Test Loss: 3.5796\n",
      "Epoch: 4858, Train Loss: 0.4536, Test Loss: 3.4955\n",
      "Epoch: 4859, Train Loss: 0.5026, Test Loss: 3.7456\n",
      "Epoch: 4860, Train Loss: 0.5466, Test Loss: 4.4383\n",
      "Epoch: 4861, Train Loss: 0.5695, Test Loss: 4.0265\n",
      "Epoch: 4862, Train Loss: 0.5588, Test Loss: 3.2861\n",
      "Epoch: 4863, Train Loss: 0.5693, Test Loss: 3.5578\n",
      "Epoch: 4864, Train Loss: 0.4986, Test Loss: 4.0287\n",
      "Epoch: 4865, Train Loss: 0.4864, Test Loss: 4.3325\n",
      "Epoch: 4866, Train Loss: 0.5355, Test Loss: 3.8476\n",
      "Epoch: 4867, Train Loss: 0.4657, Test Loss: 3.2691\n",
      "Epoch: 4868, Train Loss: 0.4976, Test Loss: 3.3049\n",
      "Epoch: 4869, Train Loss: 0.5311, Test Loss: 3.9188\n",
      "Epoch: 4870, Train Loss: 0.4698, Test Loss: 4.1331\n",
      "Epoch: 4871, Train Loss: 0.5449, Test Loss: 3.5708\n",
      "Epoch: 4872, Train Loss: 0.4672, Test Loss: 3.2737\n",
      "Epoch: 4873, Train Loss: 0.5686, Test Loss: 3.7685\n",
      "Epoch: 4874, Train Loss: 0.4447, Test Loss: 4.6138\n",
      "Epoch: 4875, Train Loss: 0.5363, Test Loss: 4.2606\n",
      "Epoch: 4876, Train Loss: 0.4975, Test Loss: 3.4569\n",
      "Epoch: 4877, Train Loss: 0.5528, Test Loss: 3.4211\n",
      "Epoch: 4878, Train Loss: 0.4849, Test Loss: 4.0518\n",
      "Epoch: 4879, Train Loss: 0.4736, Test Loss: 4.1654\n",
      "Epoch: 4880, Train Loss: 0.4756, Test Loss: 3.5610\n",
      "Epoch: 4881, Train Loss: 0.4654, Test Loss: 3.2714\n",
      "Epoch: 4882, Train Loss: 0.5210, Test Loss: 3.7045\n",
      "Epoch: 4883, Train Loss: 0.4438, Test Loss: 4.3400\n",
      "Epoch: 4884, Train Loss: 0.5312, Test Loss: 3.9619\n",
      "Epoch: 4885, Train Loss: 0.4833, Test Loss: 3.1147\n",
      "Epoch: 4886, Train Loss: 0.5655, Test Loss: 3.1897\n",
      "Epoch: 4887, Train Loss: 0.5518, Test Loss: 4.1880\n",
      "Epoch: 4888, Train Loss: 0.5371, Test Loss: 4.3907\n",
      "Epoch: 4889, Train Loss: 0.5233, Test Loss: 3.5868\n",
      "Epoch: 4890, Train Loss: 0.4509, Test Loss: 3.2042\n",
      "Epoch: 4891, Train Loss: 0.5351, Test Loss: 3.4326\n",
      "Epoch: 4892, Train Loss: 0.4581, Test Loss: 3.9989\n",
      "Epoch: 4893, Train Loss: 0.4528, Test Loss: 4.0832\n",
      "Epoch: 4894, Train Loss: 0.5109, Test Loss: 3.4255\n",
      "Epoch: 4895, Train Loss: 0.4483, Test Loss: 3.1593\n",
      "Epoch: 4896, Train Loss: 0.4873, Test Loss: 3.4888\n",
      "Epoch: 4897, Train Loss: 0.4535, Test Loss: 4.3551\n",
      "Epoch: 4898, Train Loss: 0.5293, Test Loss: 4.2154\n",
      "Epoch: 4899, Train Loss: 0.5059, Test Loss: 3.4500\n",
      "Epoch: 4900, Train Loss: 0.4958, Test Loss: 3.3149\n",
      "Epoch: 4901, Train Loss: 0.5101, Test Loss: 3.8724\n",
      "Epoch: 4902, Train Loss: 0.4734, Test Loss: 4.1588\n",
      "Epoch: 4903, Train Loss: 0.4772, Test Loss: 3.8260\n",
      "Epoch: 4904, Train Loss: 0.4599, Test Loss: 3.3298\n",
      "Epoch: 4905, Train Loss: 0.5089, Test Loss: 3.4731\n",
      "Epoch: 4906, Train Loss: 0.5040, Test Loss: 4.1880\n",
      "Epoch: 4907, Train Loss: 0.4835, Test Loss: 4.2918\n",
      "Epoch: 4908, Train Loss: 0.5697, Test Loss: 3.5018\n",
      "Epoch: 4909, Train Loss: 0.4707, Test Loss: 3.0849\n",
      "Epoch: 4910, Train Loss: 0.6080, Test Loss: 3.5506\n",
      "Epoch: 4911, Train Loss: 0.4420, Test Loss: 4.2229\n",
      "Epoch: 4912, Train Loss: 0.5052, Test Loss: 3.9568\n",
      "Epoch: 4913, Train Loss: 0.4813, Test Loss: 3.3176\n",
      "Epoch: 4914, Train Loss: 0.4810, Test Loss: 3.2650\n",
      "Epoch: 4915, Train Loss: 0.4882, Test Loss: 3.8695\n",
      "Epoch: 4916, Train Loss: 0.4764, Test Loss: 4.9367\n",
      "Epoch: 4917, Train Loss: 0.5908, Test Loss: 4.1938\n",
      "Epoch: 4918, Train Loss: 0.4813, Test Loss: 3.1796\n",
      "Epoch: 4919, Train Loss: 0.5162, Test Loss: 3.0937\n",
      "Epoch: 4920, Train Loss: 0.6185, Test Loss: 3.9985\n",
      "Epoch: 4921, Train Loss: 0.5035, Test Loss: 4.5813\n",
      "Epoch: 4922, Train Loss: 0.5631, Test Loss: 3.7398\n",
      "Epoch: 4923, Train Loss: 0.4472, Test Loss: 3.2992\n",
      "Epoch: 4924, Train Loss: 0.5597, Test Loss: 3.5636\n",
      "Epoch: 4925, Train Loss: 0.4734, Test Loss: 4.0711\n",
      "Epoch: 4926, Train Loss: 0.5099, Test Loss: 3.8493\n",
      "Epoch: 4927, Train Loss: 0.4444, Test Loss: 3.5285\n",
      "Epoch: 4928, Train Loss: 0.4840, Test Loss: 3.4289\n",
      "Epoch: 4929, Train Loss: 0.4929, Test Loss: 3.6717\n",
      "Epoch: 4930, Train Loss: 0.4254, Test Loss: 3.9326\n",
      "Epoch: 4931, Train Loss: 0.4714, Test Loss: 3.8105\n",
      "Epoch: 4932, Train Loss: 0.4331, Test Loss: 3.6777\n",
      "Epoch: 4933, Train Loss: 0.4324, Test Loss: 3.4726\n",
      "Epoch: 4934, Train Loss: 0.4400, Test Loss: 3.3686\n",
      "Epoch: 4935, Train Loss: 0.4807, Test Loss: 3.8714\n",
      "Epoch: 4936, Train Loss: 0.5129, Test Loss: 3.7601\n",
      "Epoch: 4937, Train Loss: 0.4543, Test Loss: 3.5801\n",
      "Epoch: 4938, Train Loss: 0.4815, Test Loss: 3.6158\n",
      "Epoch: 4939, Train Loss: 0.4689, Test Loss: 3.9974\n",
      "Epoch: 4940, Train Loss: 0.4484, Test Loss: 4.1194\n",
      "Epoch: 4941, Train Loss: 0.4834, Test Loss: 3.7519\n",
      "Epoch: 4942, Train Loss: 0.4956, Test Loss: 3.4044\n",
      "Epoch: 4943, Train Loss: 0.4591, Test Loss: 3.3434\n",
      "Epoch: 4944, Train Loss: 0.5088, Test Loss: 3.9792\n",
      "Epoch: 4945, Train Loss: 0.4373, Test Loss: 4.3373\n",
      "Epoch: 4946, Train Loss: 0.6327, Test Loss: 3.4484\n",
      "Epoch: 4947, Train Loss: 0.5536, Test Loss: 3.4049\n",
      "Epoch: 4948, Train Loss: 0.4920, Test Loss: 3.7834\n",
      "Epoch: 4949, Train Loss: 0.4721, Test Loss: 3.8011\n",
      "Epoch: 4950, Train Loss: 0.4235, Test Loss: 3.8368\n",
      "Epoch: 4951, Train Loss: 0.4696, Test Loss: 3.7032\n",
      "Epoch: 4952, Train Loss: 0.4885, Test Loss: 3.9380\n",
      "Epoch: 4953, Train Loss: 0.4740, Test Loss: 3.7347\n",
      "Epoch: 4954, Train Loss: 0.4617, Test Loss: 3.6859\n",
      "Epoch: 4955, Train Loss: 0.4345, Test Loss: 3.8203\n",
      "Epoch: 4956, Train Loss: 0.4217, Test Loss: 3.8300\n",
      "Epoch: 4957, Train Loss: 0.4221, Test Loss: 3.7184\n",
      "Epoch: 4958, Train Loss: 0.4436, Test Loss: 3.9010\n",
      "Epoch: 4959, Train Loss: 0.4636, Test Loss: 4.1741\n",
      "Epoch: 4960, Train Loss: 0.5195, Test Loss: 3.7837\n",
      "Epoch: 4961, Train Loss: 0.4134, Test Loss: 3.7231\n",
      "Epoch: 4962, Train Loss: 0.4694, Test Loss: 3.8194\n",
      "Epoch: 4963, Train Loss: 0.4766, Test Loss: 3.7007\n",
      "Epoch: 4964, Train Loss: 0.4614, Test Loss: 3.6466\n",
      "Epoch: 4965, Train Loss: 0.4513, Test Loss: 3.7083\n",
      "Epoch: 4966, Train Loss: 0.4301, Test Loss: 3.6596\n",
      "Epoch: 4967, Train Loss: 0.4941, Test Loss: 3.6977\n",
      "Epoch: 4968, Train Loss: 0.4474, Test Loss: 3.8900\n",
      "Epoch: 4969, Train Loss: 0.4614, Test Loss: 3.7191\n",
      "Epoch: 4970, Train Loss: 0.4722, Test Loss: 3.7340\n",
      "Epoch: 4971, Train Loss: 0.4836, Test Loss: 4.0333\n",
      "Epoch: 4972, Train Loss: 0.4856, Test Loss: 4.2053\n",
      "Epoch: 4973, Train Loss: 0.5354, Test Loss: 3.5075\n",
      "Epoch: 4974, Train Loss: 0.5300, Test Loss: 3.2675\n",
      "Epoch: 4975, Train Loss: 0.6645, Test Loss: 4.1770\n",
      "Epoch: 4976, Train Loss: 0.4792, Test Loss: 4.6106\n",
      "Epoch: 4977, Train Loss: 0.5463, Test Loss: 3.8485\n",
      "Epoch: 4978, Train Loss: 0.4715, Test Loss: 3.1788\n",
      "Epoch: 4979, Train Loss: 0.5387, Test Loss: 3.1374\n",
      "Epoch: 4980, Train Loss: 0.6067, Test Loss: 4.0991\n",
      "Epoch: 4981, Train Loss: 0.5470, Test Loss: 4.5129\n",
      "Epoch: 4982, Train Loss: 0.5526, Test Loss: 3.6687\n",
      "Epoch: 4983, Train Loss: 0.4861, Test Loss: 3.0501\n",
      "Epoch: 4984, Train Loss: 0.6293, Test Loss: 3.2905\n",
      "Epoch: 4985, Train Loss: 0.5480, Test Loss: 4.7017\n",
      "Epoch: 4986, Train Loss: 0.6432, Test Loss: 4.6861\n",
      "Epoch: 4987, Train Loss: 0.5899, Test Loss: 3.5917\n",
      "Epoch: 4988, Train Loss: 0.4644, Test Loss: 2.9919\n",
      "Epoch: 4989, Train Loss: 0.6515, Test Loss: 3.3026\n",
      "Epoch: 4990, Train Loss: 0.5341, Test Loss: 4.6497\n",
      "Epoch: 4991, Train Loss: 0.5857, Test Loss: 4.6299\n",
      "Epoch: 4992, Train Loss: 0.6166, Test Loss: 3.3471\n",
      "Epoch: 4993, Train Loss: 0.4533, Test Loss: 2.9230\n",
      "Epoch: 4994, Train Loss: 0.6829, Test Loss: 3.2341\n",
      "Epoch: 4995, Train Loss: 0.4824, Test Loss: 4.5050\n",
      "Epoch: 4996, Train Loss: 0.6112, Test Loss: 4.6458\n",
      "Epoch: 4997, Train Loss: 0.7386, Test Loss: 3.2916\n",
      "Epoch: 4998, Train Loss: 0.4929, Test Loss: 2.9705\n",
      "Epoch: 4999, Train Loss: 0.6298, Test Loss: 3.2523\n",
      "Epoch: 5000, Train Loss: 0.4657, Test Loss: 4.1012\n",
      "Epoch: 5001, Train Loss: 0.4966, Test Loss: 4.3385\n",
      "Epoch: 5002, Train Loss: 0.5281, Test Loss: 3.6896\n",
      "Epoch: 5003, Train Loss: 0.5061, Test Loss: 3.5217\n",
      "Epoch: 5004, Train Loss: 0.5106, Test Loss: 3.6029\n",
      "Epoch: 5005, Train Loss: 0.4712, Test Loss: 3.5285\n",
      "Epoch: 5006, Train Loss: 0.4741, Test Loss: 3.5939\n",
      "Epoch: 5007, Train Loss: 0.4504, Test Loss: 3.7780\n",
      "Epoch: 5008, Train Loss: 0.4548, Test Loss: 3.8686\n",
      "Epoch: 5009, Train Loss: 0.4630, Test Loss: 3.8493\n",
      "Epoch: 5010, Train Loss: 0.4689, Test Loss: 3.6655\n",
      "Epoch: 5011, Train Loss: 0.4468, Test Loss: 3.4697\n",
      "Epoch: 5012, Train Loss: 0.4771, Test Loss: 3.5098\n",
      "Epoch: 5013, Train Loss: 0.4681, Test Loss: 3.8716\n",
      "Epoch: 5014, Train Loss: 0.4562, Test Loss: 3.8196\n",
      "Epoch: 5015, Train Loss: 0.4819, Test Loss: 3.4602\n",
      "Epoch: 5016, Train Loss: 0.5078, Test Loss: 3.5741\n",
      "Epoch: 5017, Train Loss: 0.4734, Test Loss: 3.8703\n",
      "Epoch: 5018, Train Loss: 0.5166, Test Loss: 3.8381\n",
      "Epoch: 5019, Train Loss: 0.4788, Test Loss: 3.5156\n",
      "Epoch: 5020, Train Loss: 0.4865, Test Loss: 3.5148\n",
      "Epoch: 5021, Train Loss: 0.4557, Test Loss: 3.8187\n",
      "Epoch: 5022, Train Loss: 0.4794, Test Loss: 4.1116\n",
      "Epoch: 5023, Train Loss: 0.4935, Test Loss: 3.7899\n",
      "Epoch: 5024, Train Loss: 0.4704, Test Loss: 3.4745\n",
      "Epoch: 5025, Train Loss: 0.4885, Test Loss: 3.4484\n",
      "Epoch: 5026, Train Loss: 0.5245, Test Loss: 3.9858\n",
      "Epoch: 5027, Train Loss: 0.4441, Test Loss: 4.0685\n",
      "Epoch: 5028, Train Loss: 0.4713, Test Loss: 3.8683\n",
      "Epoch: 5029, Train Loss: 0.4562, Test Loss: 3.6599\n",
      "Epoch: 5030, Train Loss: 0.4395, Test Loss: 3.5138\n",
      "Epoch: 5031, Train Loss: 0.4504, Test Loss: 3.6795\n",
      "Epoch: 5032, Train Loss: 0.4533, Test Loss: 3.7229\n",
      "Epoch: 5033, Train Loss: 0.5221, Test Loss: 3.6036\n",
      "Epoch: 5034, Train Loss: 0.4540, Test Loss: 3.4838\n",
      "Epoch: 5035, Train Loss: 0.4451, Test Loss: 3.5767\n",
      "Epoch: 5036, Train Loss: 0.4770, Test Loss: 3.8739\n",
      "Epoch: 5037, Train Loss: 0.4334, Test Loss: 3.7972\n",
      "Epoch: 5038, Train Loss: 0.4243, Test Loss: 3.5035\n",
      "Epoch: 5039, Train Loss: 0.4800, Test Loss: 3.5812\n",
      "Epoch: 5040, Train Loss: 0.4928, Test Loss: 4.1061\n",
      "Epoch: 5041, Train Loss: 0.6008, Test Loss: 3.4836\n",
      "Epoch: 5042, Train Loss: 0.4238, Test Loss: 3.1918\n",
      "Epoch: 5043, Train Loss: 0.5492, Test Loss: 3.5173\n",
      "Epoch: 5044, Train Loss: 0.4889, Test Loss: 4.2372\n",
      "Epoch: 5045, Train Loss: 0.5332, Test Loss: 4.0575\n",
      "Epoch: 5046, Train Loss: 0.5228, Test Loss: 3.2317\n",
      "Epoch: 5047, Train Loss: 0.5160, Test Loss: 3.1401\n",
      "Epoch: 5048, Train Loss: 0.6317, Test Loss: 4.0006\n",
      "Epoch: 5049, Train Loss: 0.4780, Test Loss: 4.3845\n",
      "Epoch: 5050, Train Loss: 0.5637, Test Loss: 3.5462\n",
      "Epoch: 5051, Train Loss: 0.5109, Test Loss: 3.3109\n",
      "Epoch: 5052, Train Loss: 0.4833, Test Loss: 3.1957\n",
      "Epoch: 5053, Train Loss: 0.5067, Test Loss: 3.5006\n",
      "Epoch: 5054, Train Loss: 0.4678, Test Loss: 3.9989\n",
      "Epoch: 5055, Train Loss: 0.4460, Test Loss: 4.0140\n",
      "Epoch: 5056, Train Loss: 0.4815, Test Loss: 3.6668\n",
      "Epoch: 5057, Train Loss: 0.4337, Test Loss: 3.3556\n",
      "Epoch: 5058, Train Loss: 0.4788, Test Loss: 3.5320\n",
      "Epoch: 5059, Train Loss: 0.4485, Test Loss: 3.9167\n",
      "Epoch: 5060, Train Loss: 0.4561, Test Loss: 4.0883\n",
      "Epoch: 5061, Train Loss: 0.4766, Test Loss: 3.6241\n",
      "Epoch: 5062, Train Loss: 0.4742, Test Loss: 3.3739\n",
      "Epoch: 5063, Train Loss: 0.4977, Test Loss: 3.6484\n",
      "Epoch: 5064, Train Loss: 0.4289, Test Loss: 4.1089\n",
      "Epoch: 5065, Train Loss: 0.5319, Test Loss: 3.5543\n",
      "Epoch: 5066, Train Loss: 0.4555, Test Loss: 3.1651\n",
      "Epoch: 5067, Train Loss: 0.5265, Test Loss: 3.5192\n",
      "Epoch: 5068, Train Loss: 0.4581, Test Loss: 4.2904\n",
      "Epoch: 5069, Train Loss: 0.5375, Test Loss: 4.2699\n",
      "Epoch: 5070, Train Loss: 0.4878, Test Loss: 3.6542\n",
      "Epoch: 5071, Train Loss: 0.4595, Test Loss: 3.2883\n",
      "Epoch: 5072, Train Loss: 0.5068, Test Loss: 3.4814\n",
      "Epoch: 5073, Train Loss: 0.4597, Test Loss: 4.1645\n",
      "Epoch: 5074, Train Loss: 0.5138, Test Loss: 3.9654\n",
      "Epoch: 5075, Train Loss: 0.5078, Test Loss: 3.3059\n",
      "Epoch: 5076, Train Loss: 0.4719, Test Loss: 3.2667\n",
      "Epoch: 5077, Train Loss: 0.5093, Test Loss: 3.8323\n",
      "Epoch: 5078, Train Loss: 0.4643, Test Loss: 4.0448\n",
      "Epoch: 5079, Train Loss: 0.4813, Test Loss: 3.6355\n",
      "Epoch: 5080, Train Loss: 0.4691, Test Loss: 3.3348\n",
      "Epoch: 5081, Train Loss: 0.4959, Test Loss: 3.4086\n",
      "Epoch: 5082, Train Loss: 0.4726, Test Loss: 3.8177\n",
      "Epoch: 5083, Train Loss: 0.4501, Test Loss: 3.9970\n",
      "Epoch: 5084, Train Loss: 0.4768, Test Loss: 3.7974\n",
      "Epoch: 5085, Train Loss: 0.5093, Test Loss: 3.3061\n",
      "Epoch: 5086, Train Loss: 0.5570, Test Loss: 3.6634\n",
      "Epoch: 5087, Train Loss: 0.4605, Test Loss: 4.1965\n",
      "Epoch: 5088, Train Loss: 0.5725, Test Loss: 3.9324\n",
      "Epoch: 5089, Train Loss: 0.4520, Test Loss: 3.7041\n",
      "Epoch: 5090, Train Loss: 0.4576, Test Loss: 3.7139\n",
      "Epoch: 5091, Train Loss: 0.5331, Test Loss: 4.1358\n",
      "Epoch: 5092, Train Loss: 0.4670, Test Loss: 4.0479\n",
      "Epoch: 5093, Train Loss: 0.4840, Test Loss: 3.5668\n",
      "Epoch: 5094, Train Loss: 0.4527, Test Loss: 3.3602\n",
      "Epoch: 5095, Train Loss: 0.4496, Test Loss: 3.4872\n",
      "Epoch: 5096, Train Loss: 0.4377, Test Loss: 4.0995\n",
      "Epoch: 5097, Train Loss: 0.4868, Test Loss: 4.2592\n",
      "Epoch: 5098, Train Loss: 0.5105, Test Loss: 3.6316\n",
      "Epoch: 5099, Train Loss: 0.4498, Test Loss: 3.4364\n",
      "Epoch: 5100, Train Loss: 0.4752, Test Loss: 3.6571\n",
      "Epoch: 5101, Train Loss: 0.4326, Test Loss: 4.0163\n",
      "Epoch: 5102, Train Loss: 0.4686, Test Loss: 4.0523\n",
      "Epoch: 5103, Train Loss: 0.4905, Test Loss: 3.5568\n",
      "Epoch: 5104, Train Loss: 0.4398, Test Loss: 3.2220\n",
      "Epoch: 5105, Train Loss: 0.4836, Test Loss: 3.2624\n",
      "Epoch: 5106, Train Loss: 0.5371, Test Loss: 3.9986\n",
      "Epoch: 5107, Train Loss: 0.4582, Test Loss: 4.5897\n",
      "Epoch: 5108, Train Loss: 0.5105, Test Loss: 3.9550\n",
      "Epoch: 5109, Train Loss: 0.4657, Test Loss: 3.5011\n",
      "Epoch: 5110, Train Loss: 0.4424, Test Loss: 3.2736\n",
      "Epoch: 5111, Train Loss: 0.5494, Test Loss: 3.6235\n",
      "Epoch: 5112, Train Loss: 0.4569, Test Loss: 4.0347\n",
      "Epoch: 5113, Train Loss: 0.5177, Test Loss: 3.7989\n",
      "Epoch: 5114, Train Loss: 0.4828, Test Loss: 3.7299\n",
      "Epoch: 5115, Train Loss: 0.4529, Test Loss: 3.6506\n",
      "Epoch: 5116, Train Loss: 0.4536, Test Loss: 3.6009\n",
      "Epoch: 5117, Train Loss: 0.4938, Test Loss: 3.7910\n",
      "Epoch: 5118, Train Loss: 0.4318, Test Loss: 4.0811\n",
      "Epoch: 5119, Train Loss: 0.4430, Test Loss: 3.8228\n",
      "Epoch: 5120, Train Loss: 0.4452, Test Loss: 3.6249\n",
      "Epoch: 5121, Train Loss: 0.4269, Test Loss: 3.4877\n",
      "Epoch: 5122, Train Loss: 0.5000, Test Loss: 3.4118\n",
      "Epoch: 5123, Train Loss: 0.4940, Test Loss: 3.8477\n",
      "Epoch: 5124, Train Loss: 0.4543, Test Loss: 4.2734\n",
      "Epoch: 5125, Train Loss: 0.5126, Test Loss: 4.0757\n",
      "Epoch: 5126, Train Loss: 0.4828, Test Loss: 3.8090\n",
      "Epoch: 5127, Train Loss: 0.4630, Test Loss: 3.6386\n",
      "Epoch: 5128, Train Loss: 0.4451, Test Loss: 3.5460\n",
      "Epoch: 5129, Train Loss: 0.4816, Test Loss: 3.7556\n",
      "Epoch: 5130, Train Loss: 0.4764, Test Loss: 3.9065\n",
      "Epoch: 5131, Train Loss: 0.4545, Test Loss: 3.9806\n",
      "Epoch: 5132, Train Loss: 0.4337, Test Loss: 4.0030\n",
      "Epoch: 5133, Train Loss: 0.4562, Test Loss: 3.7222\n",
      "Epoch: 5134, Train Loss: 0.4522, Test Loss: 3.2962\n",
      "Epoch: 5135, Train Loss: 0.4553, Test Loss: 3.4587\n",
      "Epoch: 5136, Train Loss: 0.4505, Test Loss: 4.1192\n",
      "Epoch: 5137, Train Loss: 0.4560, Test Loss: 4.4145\n",
      "Epoch: 5138, Train Loss: 0.5528, Test Loss: 3.5899\n",
      "Epoch: 5139, Train Loss: 0.4885, Test Loss: 3.3823\n",
      "Epoch: 5140, Train Loss: 0.4959, Test Loss: 3.6656\n",
      "Epoch: 5141, Train Loss: 0.4299, Test Loss: 4.2621\n",
      "Epoch: 5142, Train Loss: 0.4842, Test Loss: 4.3774\n",
      "Epoch: 5143, Train Loss: 0.5568, Test Loss: 3.3995\n",
      "Epoch: 5144, Train Loss: 0.5099, Test Loss: 3.2952\n",
      "Epoch: 5145, Train Loss: 0.5293, Test Loss: 3.6859\n",
      "Epoch: 5146, Train Loss: 0.5198, Test Loss: 4.2370\n",
      "Epoch: 5147, Train Loss: 0.5001, Test Loss: 4.0549\n",
      "Epoch: 5148, Train Loss: 0.4762, Test Loss: 3.6428\n",
      "Epoch: 5149, Train Loss: 0.4576, Test Loss: 3.5449\n",
      "Epoch: 5150, Train Loss: 0.4584, Test Loss: 3.6318\n",
      "Epoch: 5151, Train Loss: 0.5263, Test Loss: 3.9463\n",
      "Epoch: 5152, Train Loss: 0.4511, Test Loss: 4.1479\n",
      "Epoch: 5153, Train Loss: 0.4875, Test Loss: 3.8282\n",
      "Epoch: 5154, Train Loss: 0.4359, Test Loss: 3.4356\n",
      "Epoch: 5155, Train Loss: 0.4453, Test Loss: 3.3802\n",
      "Epoch: 5156, Train Loss: 0.4869, Test Loss: 4.0021\n",
      "Epoch: 5157, Train Loss: 0.4528, Test Loss: 4.3370\n",
      "Epoch: 5158, Train Loss: 0.5110, Test Loss: 4.1531\n",
      "Epoch: 5159, Train Loss: 0.5156, Test Loss: 3.3853\n",
      "Epoch: 5160, Train Loss: 0.4441, Test Loss: 3.1855\n",
      "Epoch: 5161, Train Loss: 0.5202, Test Loss: 3.6575\n",
      "Epoch: 5162, Train Loss: 0.4588, Test Loss: 4.3032\n",
      "Epoch: 5163, Train Loss: 0.5045, Test Loss: 4.0716\n",
      "Epoch: 5164, Train Loss: 0.4632, Test Loss: 3.4494\n",
      "Epoch: 5165, Train Loss: 0.4938, Test Loss: 3.4340\n",
      "Epoch: 5166, Train Loss: 0.4709, Test Loss: 3.8229\n",
      "Epoch: 5167, Train Loss: 0.4858, Test Loss: 4.1126\n",
      "Epoch: 5168, Train Loss: 0.4795, Test Loss: 3.6884\n",
      "Epoch: 5169, Train Loss: 0.4389, Test Loss: 3.2787\n",
      "Epoch: 5170, Train Loss: 0.5585, Test Loss: 3.5543\n",
      "Epoch: 5171, Train Loss: 0.4695, Test Loss: 3.9769\n",
      "Epoch: 5172, Train Loss: 0.5016, Test Loss: 3.6864\n",
      "Epoch: 5173, Train Loss: 0.5095, Test Loss: 3.8281\n",
      "Epoch: 5174, Train Loss: 0.4812, Test Loss: 3.9566\n",
      "Epoch: 5175, Train Loss: 0.4434, Test Loss: 3.6194\n",
      "Epoch: 5176, Train Loss: 0.4714, Test Loss: 3.4262\n",
      "Epoch: 5177, Train Loss: 0.4467, Test Loss: 3.4983\n",
      "Epoch: 5178, Train Loss: 0.4687, Test Loss: 3.7055\n",
      "Epoch: 5179, Train Loss: 0.4587, Test Loss: 3.7117\n",
      "Epoch: 5180, Train Loss: 0.4421, Test Loss: 3.3755\n",
      "Epoch: 5181, Train Loss: 0.4701, Test Loss: 3.4538\n",
      "Epoch: 5182, Train Loss: 0.4514, Test Loss: 3.8848\n",
      "Epoch: 5183, Train Loss: 0.4338, Test Loss: 3.9656\n",
      "Epoch: 5184, Train Loss: 0.4293, Test Loss: 3.7647\n",
      "Epoch: 5185, Train Loss: 0.4754, Test Loss: 3.3683\n",
      "Epoch: 5186, Train Loss: 0.4927, Test Loss: 3.6616\n",
      "Epoch: 5187, Train Loss: 0.4356, Test Loss: 4.1378\n",
      "Epoch: 5188, Train Loss: 0.5169, Test Loss: 3.7048\n",
      "Epoch: 5189, Train Loss: 0.4473, Test Loss: 3.3883\n",
      "Epoch: 5190, Train Loss: 0.4804, Test Loss: 3.5761\n",
      "Epoch: 5191, Train Loss: 0.4676, Test Loss: 4.1094\n",
      "Epoch: 5192, Train Loss: 0.4946, Test Loss: 3.8758\n",
      "Epoch: 5193, Train Loss: 0.4254, Test Loss: 3.5305\n",
      "Epoch: 5194, Train Loss: 0.4464, Test Loss: 3.5864\n",
      "Epoch: 5195, Train Loss: 0.4593, Test Loss: 3.8215\n",
      "Epoch: 5196, Train Loss: 0.4461, Test Loss: 3.8403\n",
      "Epoch: 5197, Train Loss: 0.4614, Test Loss: 3.9340\n",
      "Epoch: 5198, Train Loss: 0.4774, Test Loss: 3.7192\n",
      "Epoch: 5199, Train Loss: 0.4317, Test Loss: 3.5195\n",
      "Epoch: 5200, Train Loss: 0.5115, Test Loss: 3.4867\n",
      "Epoch: 5201, Train Loss: 0.4560, Test Loss: 3.6596\n",
      "Epoch: 5202, Train Loss: 0.4467, Test Loss: 3.6820\n",
      "Epoch: 5203, Train Loss: 0.4512, Test Loss: 3.7172\n",
      "Epoch: 5204, Train Loss: 0.4799, Test Loss: 4.0925\n",
      "Epoch: 5205, Train Loss: 0.4669, Test Loss: 3.9725\n",
      "Epoch: 5206, Train Loss: 0.4552, Test Loss: 3.7677\n",
      "Epoch: 5207, Train Loss: 0.4469, Test Loss: 3.5799\n",
      "Epoch: 5208, Train Loss: 0.4798, Test Loss: 3.8017\n",
      "Epoch: 5209, Train Loss: 0.4486, Test Loss: 4.0491\n",
      "Epoch: 5210, Train Loss: 0.4792, Test Loss: 3.7397\n",
      "Epoch: 5211, Train Loss: 0.4346, Test Loss: 3.3273\n",
      "Epoch: 5212, Train Loss: 0.5107, Test Loss: 3.5816\n",
      "Epoch: 5213, Train Loss: 0.4974, Test Loss: 4.3782\n",
      "Epoch: 5214, Train Loss: 0.5331, Test Loss: 4.1257\n",
      "Epoch: 5215, Train Loss: 0.5272, Test Loss: 3.3682\n",
      "Epoch: 5216, Train Loss: 0.4771, Test Loss: 3.0536\n",
      "Epoch: 5217, Train Loss: 0.5716, Test Loss: 3.4021\n",
      "Epoch: 5218, Train Loss: 0.4910, Test Loss: 4.1991\n",
      "Epoch: 5219, Train Loss: 0.5010, Test Loss: 4.2684\n",
      "Epoch: 5220, Train Loss: 0.4998, Test Loss: 3.6765\n",
      "Epoch: 5221, Train Loss: 0.4596, Test Loss: 3.2772\n",
      "Epoch: 5222, Train Loss: 0.5511, Test Loss: 3.5345\n",
      "Epoch: 5223, Train Loss: 0.4705, Test Loss: 4.0489\n",
      "Epoch: 5224, Train Loss: 0.5633, Test Loss: 3.7372\n",
      "Epoch: 5225, Train Loss: 0.4678, Test Loss: 3.3989\n",
      "Epoch: 5226, Train Loss: 0.4783, Test Loss: 3.3881\n",
      "Epoch: 5227, Train Loss: 0.4682, Test Loss: 3.8463\n",
      "Epoch: 5228, Train Loss: 0.4373, Test Loss: 4.0907\n",
      "Epoch: 5229, Train Loss: 0.5184, Test Loss: 3.7959\n",
      "Epoch: 5230, Train Loss: 0.4451, Test Loss: 3.4027\n",
      "Epoch: 5231, Train Loss: 0.4788, Test Loss: 3.4052\n",
      "Epoch: 5232, Train Loss: 0.4481, Test Loss: 3.8504\n",
      "Epoch: 5233, Train Loss: 0.4619, Test Loss: 3.8990\n",
      "Epoch: 5234, Train Loss: 0.4636, Test Loss: 3.6205\n",
      "Epoch: 5235, Train Loss: 0.4530, Test Loss: 3.2928\n",
      "Epoch: 5236, Train Loss: 0.5075, Test Loss: 3.6525\n",
      "Epoch: 5237, Train Loss: 0.4719, Test Loss: 4.4340\n",
      "Epoch: 5238, Train Loss: 0.4915, Test Loss: 4.3015\n",
      "Epoch: 5239, Train Loss: 0.4911, Test Loss: 3.4004\n",
      "Epoch: 5240, Train Loss: 0.4558, Test Loss: 3.1526\n",
      "Epoch: 5241, Train Loss: 0.5914, Test Loss: 3.7193\n",
      "Epoch: 5242, Train Loss: 0.4841, Test Loss: 4.1720\n",
      "Epoch: 5243, Train Loss: 0.5704, Test Loss: 3.6031\n",
      "Epoch: 5244, Train Loss: 0.4517, Test Loss: 3.2448\n",
      "Epoch: 5245, Train Loss: 0.5250, Test Loss: 3.5210\n",
      "Epoch: 5246, Train Loss: 0.4674, Test Loss: 4.0216\n",
      "Epoch: 5247, Train Loss: 0.4853, Test Loss: 3.9173\n",
      "Epoch: 5248, Train Loss: 0.4515, Test Loss: 3.7080\n",
      "Epoch: 5249, Train Loss: 0.4166, Test Loss: 3.5275\n",
      "Epoch: 5250, Train Loss: 0.4220, Test Loss: 3.5406\n",
      "Epoch: 5251, Train Loss: 0.4402, Test Loss: 3.5942\n",
      "Epoch: 5252, Train Loss: 0.4238, Test Loss: 3.6613\n",
      "Epoch: 5253, Train Loss: 0.4229, Test Loss: 3.5915\n",
      "Epoch: 5254, Train Loss: 0.4311, Test Loss: 3.7730\n",
      "Epoch: 5255, Train Loss: 0.4365, Test Loss: 3.5659\n",
      "Epoch: 5256, Train Loss: 0.4720, Test Loss: 3.9306\n",
      "Epoch: 5257, Train Loss: 0.4279, Test Loss: 4.0450\n",
      "Epoch: 5258, Train Loss: 0.4943, Test Loss: 3.6138\n",
      "Epoch: 5259, Train Loss: 0.4718, Test Loss: 3.3246\n",
      "Epoch: 5260, Train Loss: 0.4615, Test Loss: 3.5270\n",
      "Epoch: 5261, Train Loss: 0.4370, Test Loss: 3.7015\n",
      "Epoch: 5262, Train Loss: 0.4309, Test Loss: 4.0336\n",
      "Epoch: 5263, Train Loss: 0.4899, Test Loss: 3.8119\n",
      "Epoch: 5264, Train Loss: 0.4827, Test Loss: 3.5173\n",
      "Epoch: 5265, Train Loss: 0.4303, Test Loss: 3.5092\n",
      "Epoch: 5266, Train Loss: 0.4152, Test Loss: 3.6954\n",
      "Epoch: 5267, Train Loss: 0.4560, Test Loss: 3.9418\n",
      "Epoch: 5268, Train Loss: 0.5098, Test Loss: 3.7800\n",
      "Epoch: 5269, Train Loss: 0.4608, Test Loss: 3.4499\n",
      "Epoch: 5270, Train Loss: 0.4755, Test Loss: 3.4482\n",
      "Epoch: 5271, Train Loss: 0.4775, Test Loss: 3.7951\n",
      "Epoch: 5272, Train Loss: 0.4451, Test Loss: 4.2606\n",
      "Epoch: 5273, Train Loss: 0.5135, Test Loss: 4.0385\n",
      "Epoch: 5274, Train Loss: 0.4382, Test Loss: 3.6156\n",
      "Epoch: 5275, Train Loss: 0.4597, Test Loss: 3.3375\n",
      "Epoch: 5276, Train Loss: 0.4922, Test Loss: 3.5346\n",
      "Epoch: 5277, Train Loss: 0.4514, Test Loss: 4.1639\n",
      "Epoch: 5278, Train Loss: 0.4664, Test Loss: 4.5656\n",
      "Epoch: 5279, Train Loss: 0.5766, Test Loss: 3.7692\n",
      "Epoch: 5280, Train Loss: 0.4458, Test Loss: 3.4455\n",
      "Epoch: 5281, Train Loss: 0.5141, Test Loss: 3.7780\n",
      "Epoch: 5282, Train Loss: 0.4404, Test Loss: 4.2385\n",
      "Epoch: 5283, Train Loss: 0.4813, Test Loss: 3.8430\n",
      "Epoch: 5284, Train Loss: 0.4488, Test Loss: 3.4736\n",
      "Epoch: 5285, Train Loss: 0.4450, Test Loss: 3.5506\n",
      "Epoch: 5286, Train Loss: 0.4356, Test Loss: 3.9196\n",
      "Epoch: 5287, Train Loss: 0.4913, Test Loss: 3.8927\n",
      "Epoch: 5288, Train Loss: 0.4401, Test Loss: 3.5655\n",
      "Epoch: 5289, Train Loss: 0.4295, Test Loss: 3.4888\n",
      "Epoch: 5290, Train Loss: 0.4656, Test Loss: 3.9459\n",
      "Epoch: 5291, Train Loss: 0.4462, Test Loss: 4.0437\n",
      "Epoch: 5292, Train Loss: 0.4420, Test Loss: 3.7430\n",
      "Epoch: 5293, Train Loss: 0.4587, Test Loss: 3.6378\n",
      "Epoch: 5294, Train Loss: 0.5165, Test Loss: 4.3008\n",
      "Epoch: 5295, Train Loss: 0.4823, Test Loss: 4.1466\n",
      "Epoch: 5296, Train Loss: 0.4520, Test Loss: 3.4989\n",
      "Epoch: 5297, Train Loss: 0.4461, Test Loss: 3.2600\n",
      "Epoch: 5298, Train Loss: 0.5811, Test Loss: 3.7057\n",
      "Epoch: 5299, Train Loss: 0.4502, Test Loss: 4.4046\n",
      "Epoch: 5300, Train Loss: 0.6045, Test Loss: 3.8395\n",
      "Epoch: 5301, Train Loss: 0.5356, Test Loss: 3.1247\n",
      "Epoch: 5302, Train Loss: 0.5397, Test Loss: 3.1884\n",
      "Epoch: 5303, Train Loss: 0.5828, Test Loss: 4.1756\n",
      "Epoch: 5304, Train Loss: 0.5143, Test Loss: 4.5803\n",
      "Epoch: 5305, Train Loss: 0.5540, Test Loss: 3.7787\n",
      "Epoch: 5306, Train Loss: 0.4420, Test Loss: 3.1809\n",
      "Epoch: 5307, Train Loss: 0.5464, Test Loss: 3.3229\n",
      "Epoch: 5308, Train Loss: 0.4853, Test Loss: 4.1652\n",
      "Epoch: 5309, Train Loss: 0.5205, Test Loss: 4.3485\n",
      "Epoch: 5310, Train Loss: 0.5310, Test Loss: 3.6333\n",
      "Epoch: 5311, Train Loss: 0.4470, Test Loss: 3.0537\n",
      "Epoch: 5312, Train Loss: 0.5699, Test Loss: 3.3692\n",
      "Epoch: 5313, Train Loss: 0.4519, Test Loss: 4.2494\n",
      "Epoch: 5314, Train Loss: 0.4911, Test Loss: 4.2658\n",
      "Epoch: 5315, Train Loss: 0.5060, Test Loss: 3.5279\n",
      "Epoch: 5316, Train Loss: 0.4472, Test Loss: 3.1203\n",
      "Epoch: 5317, Train Loss: 0.5860, Test Loss: 3.4022\n",
      "Epoch: 5318, Train Loss: 0.4981, Test Loss: 4.0567\n",
      "Epoch: 5319, Train Loss: 0.5013, Test Loss: 4.0141\n",
      "Epoch: 5320, Train Loss: 0.4782, Test Loss: 3.4600\n",
      "Epoch: 5321, Train Loss: 0.4450, Test Loss: 3.1845\n",
      "Epoch: 5322, Train Loss: 0.5476, Test Loss: 3.2772\n",
      "Epoch: 5323, Train Loss: 0.4943, Test Loss: 3.8076\n",
      "Epoch: 5324, Train Loss: 0.4922, Test Loss: 3.7250\n",
      "Epoch: 5325, Train Loss: 0.4465, Test Loss: 3.5612\n",
      "Epoch: 5326, Train Loss: 0.4110, Test Loss: 3.4080\n",
      "Epoch: 5327, Train Loss: 0.4833, Test Loss: 3.8880\n",
      "Epoch: 5328, Train Loss: 0.4722, Test Loss: 4.0980\n",
      "Epoch: 5329, Train Loss: 0.4671, Test Loss: 3.8952\n",
      "Epoch: 5330, Train Loss: 0.5380, Test Loss: 3.3354\n",
      "Epoch: 5331, Train Loss: 0.5281, Test Loss: 3.4892\n",
      "Epoch: 5332, Train Loss: 0.4248, Test Loss: 3.9790\n",
      "Epoch: 5333, Train Loss: 0.4954, Test Loss: 3.8816\n",
      "Epoch: 5334, Train Loss: 0.4808, Test Loss: 3.3744\n",
      "Epoch: 5335, Train Loss: 0.4410, Test Loss: 3.3092\n",
      "Epoch: 5336, Train Loss: 0.4281, Test Loss: 3.5586\n",
      "Epoch: 5337, Train Loss: 0.4894, Test Loss: 4.0437\n",
      "Epoch: 5338, Train Loss: 0.4775, Test Loss: 3.7681\n",
      "Epoch: 5339, Train Loss: 0.4367, Test Loss: 3.4701\n",
      "Epoch: 5340, Train Loss: 0.4409, Test Loss: 3.5654\n",
      "Epoch: 5341, Train Loss: 0.4405, Test Loss: 3.8063\n",
      "Epoch: 5342, Train Loss: 0.4326, Test Loss: 3.8672\n",
      "Epoch: 5343, Train Loss: 0.4720, Test Loss: 3.6735\n",
      "Epoch: 5344, Train Loss: 0.4148, Test Loss: 3.5689\n",
      "Epoch: 5345, Train Loss: 0.4508, Test Loss: 3.4209\n",
      "Epoch: 5346, Train Loss: 0.4599, Test Loss: 3.5258\n",
      "Epoch: 5347, Train Loss: 0.4617, Test Loss: 3.7578\n",
      "Epoch: 5348, Train Loss: 0.4373, Test Loss: 3.8041\n",
      "Epoch: 5349, Train Loss: 0.4453, Test Loss: 3.4948\n",
      "Epoch: 5350, Train Loss: 0.4369, Test Loss: 3.4360\n",
      "Epoch: 5351, Train Loss: 0.4392, Test Loss: 3.6397\n",
      "Epoch: 5352, Train Loss: 0.4160, Test Loss: 3.8616\n",
      "Epoch: 5353, Train Loss: 0.4425, Test Loss: 3.7233\n",
      "Epoch: 5354, Train Loss: 0.4254, Test Loss: 3.5020\n",
      "Epoch: 5355, Train Loss: 0.4271, Test Loss: 3.3546\n",
      "Epoch: 5356, Train Loss: 0.5045, Test Loss: 3.8753\n",
      "Epoch: 5357, Train Loss: 0.4755, Test Loss: 4.0671\n",
      "Epoch: 5358, Train Loss: 0.4502, Test Loss: 3.7761\n",
      "Epoch: 5359, Train Loss: 0.4327, Test Loss: 3.3976\n",
      "Epoch: 5360, Train Loss: 0.4263, Test Loss: 3.4459\n",
      "Epoch: 5361, Train Loss: 0.4622, Test Loss: 3.9561\n",
      "Epoch: 5362, Train Loss: 0.4332, Test Loss: 4.4106\n",
      "Epoch: 5363, Train Loss: 0.5134, Test Loss: 3.6476\n",
      "Epoch: 5364, Train Loss: 0.4389, Test Loss: 3.1303\n",
      "Epoch: 5365, Train Loss: 0.5345, Test Loss: 3.4425\n",
      "Epoch: 5366, Train Loss: 0.5028, Test Loss: 4.5430\n",
      "Epoch: 5367, Train Loss: 0.6490, Test Loss: 4.0385\n",
      "Epoch: 5368, Train Loss: 0.4856, Test Loss: 3.2450\n",
      "Epoch: 5369, Train Loss: 0.5136, Test Loss: 3.3326\n",
      "Epoch: 5370, Train Loss: 0.4496, Test Loss: 3.7971\n",
      "Epoch: 5371, Train Loss: 0.4835, Test Loss: 3.9035\n",
      "Epoch: 5372, Train Loss: 0.4999, Test Loss: 3.5539\n",
      "Epoch: 5373, Train Loss: 0.4711, Test Loss: 3.4609\n",
      "Epoch: 5374, Train Loss: 0.4582, Test Loss: 3.5266\n",
      "Epoch: 5375, Train Loss: 0.4512, Test Loss: 3.6132\n",
      "Epoch: 5376, Train Loss: 0.4476, Test Loss: 3.9438\n",
      "Epoch: 5377, Train Loss: 0.4722, Test Loss: 3.7575\n",
      "Epoch: 5378, Train Loss: 0.4471, Test Loss: 3.7117\n",
      "Epoch: 5379, Train Loss: 0.4456, Test Loss: 3.6040\n",
      "Epoch: 5380, Train Loss: 0.4126, Test Loss: 3.6375\n",
      "Epoch: 5381, Train Loss: 0.4669, Test Loss: 3.9093\n",
      "Epoch: 5382, Train Loss: 0.4501, Test Loss: 4.0228\n",
      "Epoch: 5383, Train Loss: 0.4484, Test Loss: 3.6368\n",
      "Epoch: 5384, Train Loss: 0.4020, Test Loss: 3.5297\n",
      "Epoch: 5385, Train Loss: 0.4636, Test Loss: 3.7191\n",
      "Epoch: 5386, Train Loss: 0.4442, Test Loss: 3.6898\n",
      "Epoch: 5387, Train Loss: 0.4348, Test Loss: 3.6811\n",
      "Epoch: 5388, Train Loss: 0.4319, Test Loss: 3.6732\n",
      "Epoch: 5389, Train Loss: 0.4133, Test Loss: 3.6916\n",
      "Epoch: 5390, Train Loss: 0.4777, Test Loss: 3.6197\n",
      "Epoch: 5391, Train Loss: 0.4520, Test Loss: 3.4878\n",
      "Epoch: 5392, Train Loss: 0.4454, Test Loss: 3.5514\n",
      "Epoch: 5393, Train Loss: 0.4509, Test Loss: 3.4489\n",
      "Epoch: 5394, Train Loss: 0.4255, Test Loss: 3.6862\n",
      "Epoch: 5395, Train Loss: 0.4573, Test Loss: 3.8989\n",
      "Epoch: 5396, Train Loss: 0.4535, Test Loss: 3.6252\n",
      "Epoch: 5397, Train Loss: 0.4443, Test Loss: 3.4610\n",
      "Epoch: 5398, Train Loss: 0.4464, Test Loss: 3.8327\n",
      "Epoch: 5399, Train Loss: 0.4580, Test Loss: 4.2628\n",
      "Epoch: 5400, Train Loss: 0.4753, Test Loss: 3.8579\n",
      "Epoch: 5401, Train Loss: 0.4767, Test Loss: 3.2080\n",
      "Epoch: 5402, Train Loss: 0.5562, Test Loss: 3.4101\n",
      "Epoch: 5403, Train Loss: 0.4742, Test Loss: 4.2128\n",
      "Epoch: 5404, Train Loss: 0.4964, Test Loss: 4.1224\n",
      "Epoch: 5405, Train Loss: 0.4335, Test Loss: 3.5457\n",
      "Epoch: 5406, Train Loss: 0.4158, Test Loss: 3.2089\n",
      "Epoch: 5407, Train Loss: 0.5487, Test Loss: 3.7361\n",
      "Epoch: 5408, Train Loss: 0.4198, Test Loss: 4.3066\n",
      "Epoch: 5409, Train Loss: 0.4582, Test Loss: 3.9068\n",
      "Epoch: 5410, Train Loss: 0.4538, Test Loss: 3.2928\n",
      "Epoch: 5411, Train Loss: 0.4992, Test Loss: 3.4047\n",
      "Epoch: 5412, Train Loss: 0.4494, Test Loss: 3.8002\n",
      "Epoch: 5413, Train Loss: 0.4353, Test Loss: 3.8746\n",
      "Epoch: 5414, Train Loss: 0.4884, Test Loss: 3.6683\n",
      "Epoch: 5415, Train Loss: 0.4279, Test Loss: 3.4707\n",
      "Epoch: 5416, Train Loss: 0.4924, Test Loss: 3.5760\n",
      "Epoch: 5417, Train Loss: 0.4347, Test Loss: 4.1291\n",
      "Epoch: 5418, Train Loss: 0.4440, Test Loss: 4.0635\n",
      "Epoch: 5419, Train Loss: 0.5159, Test Loss: 3.3822\n",
      "Epoch: 5420, Train Loss: 0.4357, Test Loss: 3.1000\n",
      "Epoch: 5421, Train Loss: 0.5086, Test Loss: 3.4234\n",
      "Epoch: 5422, Train Loss: 0.4317, Test Loss: 4.1347\n",
      "Epoch: 5423, Train Loss: 0.4465, Test Loss: 4.3284\n",
      "Epoch: 5424, Train Loss: 0.5617, Test Loss: 3.4843\n",
      "Epoch: 5425, Train Loss: 0.4183, Test Loss: 3.1275\n",
      "Epoch: 5426, Train Loss: 0.5818, Test Loss: 3.7172\n",
      "Epoch: 5427, Train Loss: 0.4165, Test Loss: 4.3912\n",
      "Epoch: 5428, Train Loss: 0.5637, Test Loss: 3.8121\n",
      "Epoch: 5429, Train Loss: 0.4451, Test Loss: 3.1601\n",
      "Epoch: 5430, Train Loss: 0.5452, Test Loss: 3.2959\n",
      "Epoch: 5431, Train Loss: 0.4960, Test Loss: 3.6718\n",
      "Epoch: 5432, Train Loss: 0.4198, Test Loss: 4.0930\n",
      "Epoch: 5433, Train Loss: 0.4868, Test Loss: 3.7644\n",
      "Epoch: 5434, Train Loss: 0.4363, Test Loss: 3.2861\n",
      "Epoch: 5435, Train Loss: 0.5069, Test Loss: 3.4974\n",
      "Epoch: 5436, Train Loss: 0.4680, Test Loss: 3.7770\n",
      "Epoch: 5437, Train Loss: 0.4184, Test Loss: 4.1923\n",
      "Epoch: 5438, Train Loss: 0.5065, Test Loss: 3.7631\n",
      "Epoch: 5439, Train Loss: 0.4851, Test Loss: 3.1328\n",
      "Epoch: 5440, Train Loss: 0.4776, Test Loss: 3.1261\n",
      "Epoch: 5441, Train Loss: 0.4643, Test Loss: 3.6965\n",
      "Epoch: 5442, Train Loss: 0.4590, Test Loss: 4.3112\n",
      "Epoch: 5443, Train Loss: 0.4968, Test Loss: 3.7904\n",
      "Epoch: 5444, Train Loss: 0.4444, Test Loss: 3.3869\n",
      "Epoch: 5445, Train Loss: 0.4344, Test Loss: 3.4718\n",
      "Epoch: 5446, Train Loss: 0.4648, Test Loss: 3.9852\n",
      "Epoch: 5447, Train Loss: 0.4483, Test Loss: 3.9887\n",
      "Epoch: 5448, Train Loss: 0.4869, Test Loss: 3.5162\n",
      "Epoch: 5449, Train Loss: 0.4265, Test Loss: 3.4389\n",
      "Epoch: 5450, Train Loss: 0.4430, Test Loss: 3.6296\n",
      "Epoch: 5451, Train Loss: 0.4219, Test Loss: 3.9798\n",
      "Epoch: 5452, Train Loss: 0.4749, Test Loss: 3.6668\n",
      "Epoch: 5453, Train Loss: 0.3970, Test Loss: 3.5706\n",
      "Epoch: 5454, Train Loss: 0.4416, Test Loss: 3.6691\n",
      "Epoch: 5455, Train Loss: 0.3955, Test Loss: 3.8683\n",
      "Epoch: 5456, Train Loss: 0.4163, Test Loss: 3.8046\n",
      "Epoch: 5457, Train Loss: 0.4052, Test Loss: 3.6220\n",
      "Epoch: 5458, Train Loss: 0.4564, Test Loss: 3.6548\n",
      "Epoch: 5459, Train Loss: 0.4076, Test Loss: 3.7497\n",
      "Epoch: 5460, Train Loss: 0.4222, Test Loss: 3.5596\n",
      "Epoch: 5461, Train Loss: 0.4213, Test Loss: 3.5161\n",
      "Epoch: 5462, Train Loss: 0.4477, Test Loss: 3.5185\n",
      "Epoch: 5463, Train Loss: 0.4861, Test Loss: 4.0135\n",
      "Epoch: 5464, Train Loss: 0.4451, Test Loss: 4.0748\n",
      "Epoch: 5465, Train Loss: 0.4777, Test Loss: 3.5319\n",
      "Epoch: 5466, Train Loss: 0.4274, Test Loss: 3.5393\n",
      "Epoch: 5467, Train Loss: 0.4452, Test Loss: 3.9460\n",
      "Epoch: 5468, Train Loss: 0.4334, Test Loss: 4.0228\n",
      "Epoch: 5469, Train Loss: 0.4570, Test Loss: 3.6113\n",
      "Epoch: 5470, Train Loss: 0.4588, Test Loss: 3.2543\n",
      "Epoch: 5471, Train Loss: 0.5149, Test Loss: 3.7019\n",
      "Epoch: 5472, Train Loss: 0.4327, Test Loss: 4.2114\n",
      "Epoch: 5473, Train Loss: 0.4397, Test Loss: 4.1199\n",
      "Epoch: 5474, Train Loss: 0.4195, Test Loss: 3.7511\n",
      "Epoch: 5475, Train Loss: 0.4351, Test Loss: 3.3325\n",
      "Epoch: 5476, Train Loss: 0.5058, Test Loss: 3.6061\n",
      "Epoch: 5477, Train Loss: 0.4450, Test Loss: 3.8140\n",
      "Epoch: 5478, Train Loss: 0.4332, Test Loss: 3.7238\n",
      "Epoch: 5479, Train Loss: 0.4277, Test Loss: 3.5786\n",
      "Epoch: 5480, Train Loss: 0.4346, Test Loss: 3.6881\n",
      "Epoch: 5481, Train Loss: 0.4444, Test Loss: 3.7589\n",
      "Epoch: 5482, Train Loss: 0.4486, Test Loss: 3.5755\n",
      "Epoch: 5483, Train Loss: 0.4208, Test Loss: 3.4219\n",
      "Epoch: 5484, Train Loss: 0.4891, Test Loss: 3.6894\n",
      "Epoch: 5485, Train Loss: 0.4321, Test Loss: 3.8727\n",
      "Epoch: 5486, Train Loss: 0.4314, Test Loss: 3.7107\n",
      "Epoch: 5487, Train Loss: 0.4305, Test Loss: 3.6271\n",
      "Epoch: 5488, Train Loss: 0.4166, Test Loss: 3.8054\n",
      "Epoch: 5489, Train Loss: 0.4120, Test Loss: 3.8185\n",
      "Epoch: 5490, Train Loss: 0.4478, Test Loss: 3.6372\n",
      "Epoch: 5491, Train Loss: 0.4530, Test Loss: 3.6248\n",
      "Epoch: 5492, Train Loss: 0.4388, Test Loss: 3.9468\n",
      "Epoch: 5493, Train Loss: 0.4598, Test Loss: 3.8119\n",
      "Epoch: 5494, Train Loss: 0.4168, Test Loss: 3.4576\n",
      "Epoch: 5495, Train Loss: 0.4426, Test Loss: 3.5475\n",
      "Epoch: 5496, Train Loss: 0.4239, Test Loss: 3.8871\n",
      "Epoch: 5497, Train Loss: 0.4434, Test Loss: 3.8089\n",
      "Epoch: 5498, Train Loss: 0.4393, Test Loss: 3.3047\n",
      "Epoch: 5499, Train Loss: 0.4949, Test Loss: 3.3747\n",
      "Epoch: 5500, Train Loss: 0.4334, Test Loss: 3.9251\n",
      "Epoch: 5501, Train Loss: 0.4523, Test Loss: 4.1483\n",
      "Epoch: 5502, Train Loss: 0.4599, Test Loss: 3.6984\n",
      "Epoch: 5503, Train Loss: 0.4304, Test Loss: 3.3668\n",
      "Epoch: 5504, Train Loss: 0.4558, Test Loss: 3.5998\n",
      "Epoch: 5505, Train Loss: 0.4449, Test Loss: 4.3072\n",
      "Epoch: 5506, Train Loss: 0.5092, Test Loss: 3.9610\n",
      "Epoch: 5507, Train Loss: 0.4682, Test Loss: 3.2866\n",
      "Epoch: 5508, Train Loss: 0.5052, Test Loss: 3.4092\n",
      "Epoch: 5509, Train Loss: 0.4547, Test Loss: 3.8641\n",
      "Epoch: 5510, Train Loss: 0.4674, Test Loss: 4.2163\n",
      "Epoch: 5511, Train Loss: 0.5069, Test Loss: 3.6099\n",
      "Epoch: 5512, Train Loss: 0.4547, Test Loss: 3.0467\n",
      "Epoch: 5513, Train Loss: 0.5393, Test Loss: 3.1328\n",
      "Epoch: 5514, Train Loss: 0.5013, Test Loss: 3.9568\n",
      "Epoch: 5515, Train Loss: 0.4486, Test Loss: 4.4515\n",
      "Epoch: 5516, Train Loss: 0.5567, Test Loss: 3.5939\n",
      "Epoch: 5517, Train Loss: 0.4303, Test Loss: 3.0854\n",
      "Epoch: 5518, Train Loss: 0.6543, Test Loss: 3.7421\n",
      "Epoch: 5519, Train Loss: 0.4640, Test Loss: 4.4659\n",
      "Epoch: 5520, Train Loss: 0.5894, Test Loss: 3.6917\n",
      "Epoch: 5521, Train Loss: 0.4598, Test Loss: 3.1901\n",
      "Epoch: 5522, Train Loss: 0.5021, Test Loss: 3.2860\n",
      "Epoch: 5523, Train Loss: 0.4396, Test Loss: 3.9390\n",
      "Epoch: 5524, Train Loss: 0.4067, Test Loss: 4.2434\n",
      "Epoch: 5525, Train Loss: 0.4833, Test Loss: 3.5746\n",
      "Epoch: 5526, Train Loss: 0.4234, Test Loss: 3.1018\n",
      "Epoch: 5527, Train Loss: 0.5743, Test Loss: 3.5347\n",
      "Epoch: 5528, Train Loss: 0.4106, Test Loss: 4.1745\n",
      "Epoch: 5529, Train Loss: 0.5197, Test Loss: 3.9016\n",
      "Epoch: 5530, Train Loss: 0.4250, Test Loss: 3.2635\n",
      "Epoch: 5531, Train Loss: 0.4724, Test Loss: 3.3470\n",
      "Epoch: 5532, Train Loss: 0.4615, Test Loss: 3.8931\n",
      "Epoch: 5533, Train Loss: 0.5324, Test Loss: 3.8015\n",
      "Epoch: 5534, Train Loss: 0.4623, Test Loss: 3.5981\n",
      "Epoch: 5535, Train Loss: 0.4272, Test Loss: 3.4646\n",
      "Epoch: 5536, Train Loss: 0.4556, Test Loss: 3.4335\n",
      "Epoch: 5537, Train Loss: 0.4216, Test Loss: 3.5199\n",
      "Epoch: 5538, Train Loss: 0.4163, Test Loss: 3.7167\n",
      "Epoch: 5539, Train Loss: 0.4481, Test Loss: 3.6162\n",
      "Epoch: 5540, Train Loss: 0.4265, Test Loss: 3.4507\n",
      "Epoch: 5541, Train Loss: 0.4226, Test Loss: 3.5483\n",
      "Epoch: 5542, Train Loss: 0.4904, Test Loss: 3.9193\n",
      "Epoch: 5543, Train Loss: 0.4892, Test Loss: 3.6160\n",
      "Epoch: 5544, Train Loss: 0.4380, Test Loss: 3.1523\n",
      "Epoch: 5545, Train Loss: 0.5359, Test Loss: 3.6139\n",
      "Epoch: 5546, Train Loss: 0.4353, Test Loss: 4.0418\n",
      "Epoch: 5547, Train Loss: 0.4945, Test Loss: 3.6534\n",
      "Epoch: 5548, Train Loss: 0.4278, Test Loss: 3.2907\n",
      "Epoch: 5549, Train Loss: 0.4772, Test Loss: 3.3770\n",
      "Epoch: 5550, Train Loss: 0.4486, Test Loss: 3.5907\n",
      "Epoch: 5551, Train Loss: 0.3972, Test Loss: 3.6676\n",
      "Epoch: 5552, Train Loss: 0.4559, Test Loss: 3.5449\n",
      "Epoch: 5553, Train Loss: 0.4610, Test Loss: 3.4314\n",
      "Epoch: 5554, Train Loss: 0.4406, Test Loss: 3.7553\n",
      "Epoch: 5555, Train Loss: 0.4379, Test Loss: 3.7853\n",
      "Epoch: 5556, Train Loss: 0.4386, Test Loss: 3.5929\n",
      "Epoch: 5557, Train Loss: 0.4354, Test Loss: 3.3204\n",
      "Epoch: 5558, Train Loss: 0.4543, Test Loss: 3.4822\n",
      "Epoch: 5559, Train Loss: 0.4175, Test Loss: 3.7217\n",
      "Epoch: 5560, Train Loss: 0.4534, Test Loss: 3.6732\n",
      "Epoch: 5561, Train Loss: 0.4440, Test Loss: 3.4248\n",
      "Epoch: 5562, Train Loss: 0.4320, Test Loss: 3.5538\n",
      "Epoch: 5563, Train Loss: 0.4028, Test Loss: 4.0091\n",
      "Epoch: 5564, Train Loss: 0.4536, Test Loss: 3.8195\n",
      "Epoch: 5565, Train Loss: 0.4108, Test Loss: 3.5775\n",
      "Epoch: 5566, Train Loss: 0.4217, Test Loss: 3.6288\n",
      "Epoch: 5567, Train Loss: 0.4038, Test Loss: 3.6092\n",
      "Epoch: 5568, Train Loss: 0.4196, Test Loss: 3.6375\n",
      "Epoch: 5569, Train Loss: 0.4937, Test Loss: 3.3668\n",
      "Epoch: 5570, Train Loss: 0.4354, Test Loss: 3.6352\n",
      "Epoch: 5571, Train Loss: 0.3988, Test Loss: 3.9157\n",
      "Epoch: 5572, Train Loss: 0.4146, Test Loss: 3.9450\n",
      "Epoch: 5573, Train Loss: 0.4629, Test Loss: 3.2948\n",
      "Epoch: 5574, Train Loss: 0.4918, Test Loss: 3.3918\n",
      "Epoch: 5575, Train Loss: 0.4376, Test Loss: 4.0355\n",
      "Epoch: 5576, Train Loss: 0.4260, Test Loss: 4.4672\n",
      "Epoch: 5577, Train Loss: 0.4808, Test Loss: 3.8001\n",
      "Epoch: 5578, Train Loss: 0.4048, Test Loss: 3.2534\n",
      "Epoch: 5579, Train Loss: 0.4919, Test Loss: 3.4128\n",
      "Epoch: 5580, Train Loss: 0.4654, Test Loss: 3.8869\n",
      "Epoch: 5581, Train Loss: 0.4603, Test Loss: 4.1341\n",
      "Epoch: 5582, Train Loss: 0.4769, Test Loss: 3.6999\n",
      "Epoch: 5583, Train Loss: 0.4213, Test Loss: 3.2473\n",
      "Epoch: 5584, Train Loss: 0.5219, Test Loss: 3.5948\n",
      "Epoch: 5585, Train Loss: 0.4400, Test Loss: 4.0303\n",
      "Epoch: 5586, Train Loss: 0.4660, Test Loss: 4.0600\n",
      "Epoch: 5587, Train Loss: 0.4660, Test Loss: 3.6236\n",
      "Epoch: 5588, Train Loss: 0.4723, Test Loss: 3.4984\n",
      "Epoch: 5589, Train Loss: 0.4702, Test Loss: 3.8924\n",
      "Epoch: 5590, Train Loss: 0.4620, Test Loss: 3.8521\n",
      "Epoch: 5591, Train Loss: 0.4634, Test Loss: 3.5053\n",
      "Epoch: 5592, Train Loss: 0.4345, Test Loss: 3.6311\n",
      "Epoch: 5593, Train Loss: 0.4422, Test Loss: 3.7453\n",
      "Epoch: 5594, Train Loss: 0.4792, Test Loss: 3.8476\n",
      "Epoch: 5595, Train Loss: 0.4251, Test Loss: 3.7204\n",
      "Epoch: 5596, Train Loss: 0.4178, Test Loss: 3.5321\n",
      "Epoch: 5597, Train Loss: 0.4408, Test Loss: 3.7262\n",
      "Epoch: 5598, Train Loss: 0.4471, Test Loss: 3.7769\n",
      "Epoch: 5599, Train Loss: 0.4038, Test Loss: 3.7433\n",
      "Epoch: 5600, Train Loss: 0.4359, Test Loss: 3.6483\n",
      "Epoch: 5601, Train Loss: 0.4066, Test Loss: 3.6918\n",
      "Epoch: 5602, Train Loss: 0.4585, Test Loss: 3.9579\n",
      "Epoch: 5603, Train Loss: 0.4827, Test Loss: 3.7375\n",
      "Epoch: 5604, Train Loss: 0.3951, Test Loss: 3.6731\n",
      "Epoch: 5605, Train Loss: 0.4142, Test Loss: 3.6801\n",
      "Epoch: 5606, Train Loss: 0.4098, Test Loss: 3.7592\n",
      "Epoch: 5607, Train Loss: 0.4675, Test Loss: 3.4187\n",
      "Epoch: 5608, Train Loss: 0.4033, Test Loss: 3.3065\n",
      "Epoch: 5609, Train Loss: 0.5158, Test Loss: 3.8960\n",
      "Epoch: 5610, Train Loss: 0.4199, Test Loss: 4.4341\n",
      "Epoch: 5611, Train Loss: 0.4854, Test Loss: 3.8596\n",
      "Epoch: 5612, Train Loss: 0.4608, Test Loss: 3.0609\n",
      "Epoch: 5613, Train Loss: 0.5480, Test Loss: 3.1558\n",
      "Epoch: 5614, Train Loss: 0.5595, Test Loss: 4.1958\n",
      "Epoch: 5615, Train Loss: 0.4992, Test Loss: 4.6953\n",
      "Epoch: 5616, Train Loss: 0.6520, Test Loss: 3.4982\n",
      "Epoch: 5617, Train Loss: 0.4003, Test Loss: 2.9339\n",
      "Epoch: 5618, Train Loss: 0.6784, Test Loss: 3.2636\n",
      "Epoch: 5619, Train Loss: 0.4482, Test Loss: 4.1250\n",
      "Epoch: 5620, Train Loss: 0.5036, Test Loss: 4.1350\n",
      "Epoch: 5621, Train Loss: 0.4620, Test Loss: 3.5711\n",
      "Epoch: 5622, Train Loss: 0.4003, Test Loss: 3.1265\n",
      "Epoch: 5623, Train Loss: 0.5923, Test Loss: 3.5288\n",
      "Epoch: 5624, Train Loss: 0.4170, Test Loss: 4.1017\n",
      "Epoch: 5625, Train Loss: 0.4985, Test Loss: 3.9226\n",
      "Epoch: 5626, Train Loss: 0.4650, Test Loss: 3.3215\n",
      "Epoch: 5627, Train Loss: 0.4100, Test Loss: 3.1650\n",
      "Epoch: 5628, Train Loss: 0.5062, Test Loss: 3.6066\n",
      "Epoch: 5629, Train Loss: 0.4398, Test Loss: 4.2139\n",
      "Epoch: 5630, Train Loss: 0.4763, Test Loss: 3.9812\n",
      "Epoch: 5631, Train Loss: 0.4590, Test Loss: 3.4547\n",
      "Epoch: 5632, Train Loss: 0.4350, Test Loss: 3.3364\n",
      "Epoch: 5633, Train Loss: 0.4617, Test Loss: 3.7338\n",
      "Epoch: 5634, Train Loss: 0.4319, Test Loss: 3.9000\n",
      "Epoch: 5635, Train Loss: 0.4346, Test Loss: 3.7462\n",
      "Epoch: 5636, Train Loss: 0.4520, Test Loss: 3.2888\n",
      "Epoch: 5637, Train Loss: 0.4746, Test Loss: 3.3726\n",
      "Epoch: 5638, Train Loss: 0.4501, Test Loss: 3.9433\n",
      "Epoch: 5639, Train Loss: 0.4554, Test Loss: 3.8527\n",
      "Epoch: 5640, Train Loss: 0.4110, Test Loss: 3.5796\n",
      "Epoch: 5641, Train Loss: 0.4308, Test Loss: 3.5887\n",
      "Epoch: 5642, Train Loss: 0.4149, Test Loss: 3.8872\n",
      "Epoch: 5643, Train Loss: 0.4339, Test Loss: 4.0276\n",
      "Epoch: 5644, Train Loss: 0.4619, Test Loss: 3.5417\n",
      "Epoch: 5645, Train Loss: 0.4081, Test Loss: 3.3746\n",
      "Epoch: 5646, Train Loss: 0.4563, Test Loss: 3.6835\n",
      "Epoch: 5647, Train Loss: 0.4364, Test Loss: 4.0303\n",
      "Epoch: 5648, Train Loss: 0.4358, Test Loss: 3.9072\n",
      "Epoch: 5649, Train Loss: 0.4136, Test Loss: 3.3674\n",
      "Epoch: 5650, Train Loss: 0.4311, Test Loss: 3.2144\n",
      "Epoch: 5651, Train Loss: 0.4589, Test Loss: 3.5324\n",
      "Epoch: 5652, Train Loss: 0.4260, Test Loss: 3.9099\n",
      "Epoch: 5653, Train Loss: 0.4600, Test Loss: 3.8671\n",
      "Epoch: 5654, Train Loss: 0.4346, Test Loss: 3.4564\n",
      "Epoch: 5655, Train Loss: 0.4611, Test Loss: 3.2007\n",
      "Epoch: 5656, Train Loss: 0.4963, Test Loss: 3.6301\n",
      "Epoch: 5657, Train Loss: 0.4276, Test Loss: 4.4672\n",
      "Epoch: 5658, Train Loss: 0.5247, Test Loss: 3.9165\n",
      "Epoch: 5659, Train Loss: 0.4910, Test Loss: 3.2219\n",
      "Epoch: 5660, Train Loss: 0.4669, Test Loss: 3.2213\n",
      "Epoch: 5661, Train Loss: 0.4464, Test Loss: 3.6504\n",
      "Epoch: 5662, Train Loss: 0.4603, Test Loss: 4.4277\n",
      "Epoch: 5663, Train Loss: 0.5996, Test Loss: 3.7961\n",
      "Epoch: 5664, Train Loss: 0.4398, Test Loss: 3.1132\n",
      "Epoch: 5665, Train Loss: 0.6124, Test Loss: 3.4288\n",
      "Epoch: 5666, Train Loss: 0.4526, Test Loss: 4.4052\n",
      "Epoch: 5667, Train Loss: 0.5553, Test Loss: 4.2302\n",
      "Epoch: 5668, Train Loss: 0.5363, Test Loss: 3.1961\n",
      "Epoch: 5669, Train Loss: 0.4952, Test Loss: 3.0313\n",
      "Epoch: 5670, Train Loss: 0.5401, Test Loss: 3.4968\n",
      "Epoch: 5671, Train Loss: 0.4412, Test Loss: 4.0040\n",
      "Epoch: 5672, Train Loss: 0.5305, Test Loss: 3.5277\n",
      "Epoch: 5673, Train Loss: 0.4427, Test Loss: 3.1933\n",
      "Epoch: 5674, Train Loss: 0.5318, Test Loss: 3.5491\n",
      "Epoch: 5675, Train Loss: 0.4626, Test Loss: 3.7348\n",
      "Epoch: 5676, Train Loss: 0.4461, Test Loss: 3.7533\n",
      "Epoch: 5677, Train Loss: 0.4687, Test Loss: 3.2929\n",
      "Epoch: 5678, Train Loss: 0.4830, Test Loss: 3.4632\n",
      "Epoch: 5679, Train Loss: 0.4482, Test Loss: 4.0031\n",
      "Epoch: 5680, Train Loss: 0.4570, Test Loss: 3.9744\n",
      "Epoch: 5681, Train Loss: 0.4543, Test Loss: 3.7056\n",
      "Epoch: 5682, Train Loss: 0.4138, Test Loss: 3.5405\n",
      "Epoch: 5683, Train Loss: 0.4611, Test Loss: 3.5623\n",
      "Epoch: 5684, Train Loss: 0.4088, Test Loss: 3.5105\n",
      "Epoch: 5685, Train Loss: 0.5044, Test Loss: 3.2130\n",
      "Epoch: 5686, Train Loss: 0.5208, Test Loss: 3.5550\n",
      "Epoch: 5687, Train Loss: 0.4513, Test Loss: 3.8227\n",
      "Epoch: 5688, Train Loss: 0.4637, Test Loss: 3.6386\n",
      "Epoch: 5689, Train Loss: 0.4327, Test Loss: 3.3669\n",
      "Epoch: 5690, Train Loss: 0.4456, Test Loss: 3.5576\n",
      "Epoch: 5691, Train Loss: 0.4459, Test Loss: 3.5962\n",
      "Epoch: 5692, Train Loss: 0.4410, Test Loss: 3.5333\n",
      "Epoch: 5693, Train Loss: 0.4204, Test Loss: 3.4647\n",
      "Epoch: 5694, Train Loss: 0.4537, Test Loss: 3.4816\n",
      "Epoch: 5695, Train Loss: 0.4526, Test Loss: 3.4969\n",
      "Epoch: 5696, Train Loss: 0.4169, Test Loss: 3.4215\n",
      "Epoch: 5697, Train Loss: 0.4322, Test Loss: 3.6762\n",
      "Epoch: 5698, Train Loss: 0.4183, Test Loss: 3.8222\n",
      "Epoch: 5699, Train Loss: 0.4243, Test Loss: 3.7120\n",
      "Epoch: 5700, Train Loss: 0.3946, Test Loss: 3.3457\n",
      "Epoch: 5701, Train Loss: 0.4134, Test Loss: 3.1358\n",
      "Epoch: 5702, Train Loss: 0.4447, Test Loss: 3.4668\n",
      "Epoch: 5703, Train Loss: 0.4470, Test Loss: 4.0160\n",
      "Epoch: 5704, Train Loss: 0.4464, Test Loss: 3.8670\n",
      "Epoch: 5705, Train Loss: 0.4334, Test Loss: 3.3716\n",
      "Epoch: 5706, Train Loss: 0.4192, Test Loss: 3.4120\n",
      "Epoch: 5707, Train Loss: 0.4314, Test Loss: 3.8949\n",
      "Epoch: 5708, Train Loss: 0.4347, Test Loss: 3.7222\n",
      "Epoch: 5709, Train Loss: 0.4247, Test Loss: 3.2660\n",
      "Epoch: 5710, Train Loss: 0.4821, Test Loss: 3.5220\n",
      "Epoch: 5711, Train Loss: 0.4217, Test Loss: 3.8452\n",
      "Epoch: 5712, Train Loss: 0.4280, Test Loss: 3.8861\n",
      "Epoch: 5713, Train Loss: 0.4184, Test Loss: 3.5529\n",
      "Epoch: 5714, Train Loss: 0.4084, Test Loss: 3.3642\n",
      "Epoch: 5715, Train Loss: 0.4562, Test Loss: 3.8273\n",
      "Epoch: 5716, Train Loss: 0.4477, Test Loss: 3.8881\n",
      "Epoch: 5717, Train Loss: 0.4470, Test Loss: 3.4486\n",
      "Epoch: 5718, Train Loss: 0.4200, Test Loss: 3.3260\n",
      "Epoch: 5719, Train Loss: 0.4427, Test Loss: 3.7609\n",
      "Epoch: 5720, Train Loss: 0.4555, Test Loss: 3.9587\n",
      "Epoch: 5721, Train Loss: 0.4404, Test Loss: 3.4903\n",
      "Epoch: 5722, Train Loss: 0.4956, Test Loss: 3.6656\n",
      "Epoch: 5723, Train Loss: 0.4384, Test Loss: 3.6349\n",
      "Epoch: 5724, Train Loss: 0.4057, Test Loss: 3.4877\n",
      "Epoch: 5725, Train Loss: 0.4022, Test Loss: 3.5510\n",
      "Epoch: 5726, Train Loss: 0.4159, Test Loss: 3.7700\n",
      "Epoch: 5727, Train Loss: 0.3910, Test Loss: 3.7558\n",
      "Epoch: 5728, Train Loss: 0.4177, Test Loss: 3.5728\n",
      "Epoch: 5729, Train Loss: 0.4429, Test Loss: 3.3212\n",
      "Epoch: 5730, Train Loss: 0.4301, Test Loss: 3.3875\n",
      "Epoch: 5731, Train Loss: 0.4448, Test Loss: 3.5335\n",
      "Epoch: 5732, Train Loss: 0.4205, Test Loss: 3.6499\n",
      "Epoch: 5733, Train Loss: 0.4365, Test Loss: 3.6051\n",
      "Epoch: 5734, Train Loss: 0.4588, Test Loss: 3.6898\n",
      "Epoch: 5735, Train Loss: 0.4407, Test Loss: 3.8723\n",
      "Epoch: 5736, Train Loss: 0.4558, Test Loss: 3.4351\n",
      "Epoch: 5737, Train Loss: 0.4553, Test Loss: 3.1575\n",
      "Epoch: 5738, Train Loss: 0.4874, Test Loss: 3.5190\n",
      "Epoch: 5739, Train Loss: 0.4457, Test Loss: 4.5507\n",
      "Epoch: 5740, Train Loss: 0.5680, Test Loss: 4.1693\n",
      "Epoch: 5741, Train Loss: 0.4877, Test Loss: 3.1925\n",
      "Epoch: 5742, Train Loss: 0.4692, Test Loss: 3.1128\n",
      "Epoch: 5743, Train Loss: 0.5180, Test Loss: 3.7092\n",
      "Epoch: 5744, Train Loss: 0.4368, Test Loss: 3.9737\n",
      "Epoch: 5745, Train Loss: 0.4196, Test Loss: 3.7901\n",
      "Epoch: 5746, Train Loss: 0.4552, Test Loss: 3.3521\n",
      "Epoch: 5747, Train Loss: 0.4596, Test Loss: 3.2472\n",
      "Epoch: 5748, Train Loss: 0.4692, Test Loss: 3.6446\n",
      "Epoch: 5749, Train Loss: 0.4473, Test Loss: 3.9699\n",
      "Epoch: 5750, Train Loss: 0.4479, Test Loss: 3.8182\n",
      "Epoch: 5751, Train Loss: 0.4399, Test Loss: 3.2984\n",
      "Epoch: 5752, Train Loss: 0.4712, Test Loss: 3.2277\n",
      "Epoch: 5753, Train Loss: 0.4586, Test Loss: 3.6052\n",
      "Epoch: 5754, Train Loss: 0.4171, Test Loss: 4.0047\n",
      "Epoch: 5755, Train Loss: 0.4431, Test Loss: 3.9705\n",
      "Epoch: 5756, Train Loss: 0.4497, Test Loss: 3.3245\n",
      "Epoch: 5757, Train Loss: 0.4862, Test Loss: 3.5276\n",
      "Epoch: 5758, Train Loss: 0.4293, Test Loss: 3.7113\n",
      "Epoch: 5759, Train Loss: 0.4806, Test Loss: 3.6352\n",
      "Epoch: 5760, Train Loss: 0.4131, Test Loss: 3.7363\n",
      "Epoch: 5761, Train Loss: 0.4410, Test Loss: 3.7408\n",
      "Epoch: 5762, Train Loss: 0.4335, Test Loss: 3.6616\n",
      "Epoch: 5763, Train Loss: 0.4635, Test Loss: 3.7212\n",
      "Epoch: 5764, Train Loss: 0.4399, Test Loss: 3.8340\n",
      "Epoch: 5765, Train Loss: 0.4028, Test Loss: 3.9574\n",
      "Epoch: 5766, Train Loss: 0.4684, Test Loss: 3.6022\n",
      "Epoch: 5767, Train Loss: 0.4902, Test Loss: 3.4374\n",
      "Epoch: 5768, Train Loss: 0.4345, Test Loss: 3.5730\n",
      "Epoch: 5769, Train Loss: 0.4155, Test Loss: 3.7706\n",
      "Epoch: 5770, Train Loss: 0.4426, Test Loss: 3.7474\n",
      "Epoch: 5771, Train Loss: 0.4224, Test Loss: 3.9374\n",
      "Epoch: 5772, Train Loss: 0.4434, Test Loss: 3.8987\n",
      "Epoch: 5773, Train Loss: 0.4339, Test Loss: 3.7021\n",
      "Epoch: 5774, Train Loss: 0.4587, Test Loss: 3.4471\n",
      "Epoch: 5775, Train Loss: 0.4080, Test Loss: 3.2347\n",
      "Epoch: 5776, Train Loss: 0.4759, Test Loss: 3.5012\n",
      "Epoch: 5777, Train Loss: 0.4109, Test Loss: 3.7738\n",
      "Epoch: 5778, Train Loss: 0.4106, Test Loss: 3.7832\n",
      "Epoch: 5779, Train Loss: 0.4298, Test Loss: 3.2738\n",
      "Epoch: 5780, Train Loss: 0.4372, Test Loss: 3.1161\n",
      "Epoch: 5781, Train Loss: 0.6238, Test Loss: 3.9958\n",
      "Epoch: 5782, Train Loss: 0.5024, Test Loss: 4.2390\n",
      "Epoch: 5783, Train Loss: 0.5011, Test Loss: 3.6583\n",
      "Epoch: 5784, Train Loss: 0.4216, Test Loss: 3.1310\n",
      "Epoch: 5785, Train Loss: 0.4375, Test Loss: 3.1597\n",
      "Epoch: 5786, Train Loss: 0.5092, Test Loss: 3.9610\n",
      "Epoch: 5787, Train Loss: 0.4376, Test Loss: 4.1794\n",
      "Epoch: 5788, Train Loss: 0.5377, Test Loss: 3.2993\n",
      "Epoch: 5789, Train Loss: 0.4750, Test Loss: 3.1626\n",
      "Epoch: 5790, Train Loss: 0.4558, Test Loss: 3.5392\n",
      "Epoch: 5791, Train Loss: 0.4486, Test Loss: 3.8807\n",
      "Epoch: 5792, Train Loss: 0.4768, Test Loss: 3.5166\n",
      "Epoch: 5793, Train Loss: 0.4347, Test Loss: 3.0942\n",
      "Epoch: 5794, Train Loss: 0.4415, Test Loss: 3.1500\n",
      "Epoch: 5795, Train Loss: 0.4574, Test Loss: 3.8474\n",
      "Epoch: 5796, Train Loss: 0.4432, Test Loss: 4.0189\n",
      "Epoch: 5797, Train Loss: 0.4385, Test Loss: 3.6929\n",
      "Epoch: 5798, Train Loss: 0.4461, Test Loss: 3.3391\n",
      "Epoch: 5799, Train Loss: 0.4457, Test Loss: 3.3608\n",
      "Epoch: 5800, Train Loss: 0.4409, Test Loss: 3.4871\n",
      "Epoch: 5801, Train Loss: 0.4274, Test Loss: 3.4510\n",
      "Epoch: 5802, Train Loss: 0.4368, Test Loss: 3.6657\n",
      "Epoch: 5803, Train Loss: 0.4676, Test Loss: 3.3621\n",
      "Epoch: 5804, Train Loss: 0.3982, Test Loss: 3.2840\n",
      "Epoch: 5805, Train Loss: 0.5103, Test Loss: 3.9961\n",
      "Epoch: 5806, Train Loss: 0.4247, Test Loss: 4.2057\n",
      "Epoch: 5807, Train Loss: 0.4666, Test Loss: 3.6075\n",
      "Epoch: 5808, Train Loss: 0.4495, Test Loss: 3.1352\n",
      "Epoch: 5809, Train Loss: 0.5044, Test Loss: 3.3515\n",
      "Epoch: 5810, Train Loss: 0.4082, Test Loss: 3.7002\n",
      "Epoch: 5811, Train Loss: 0.4344, Test Loss: 3.8271\n",
      "Epoch: 5812, Train Loss: 0.4335, Test Loss: 3.5813\n",
      "Epoch: 5813, Train Loss: 0.4057, Test Loss: 3.3445\n",
      "Epoch: 5814, Train Loss: 0.4319, Test Loss: 3.3631\n",
      "Epoch: 5815, Train Loss: 0.4401, Test Loss: 3.7186\n",
      "Epoch: 5816, Train Loss: 0.4086, Test Loss: 3.8564\n",
      "Epoch: 5817, Train Loss: 0.4667, Test Loss: 3.3191\n",
      "Epoch: 5818, Train Loss: 0.4215, Test Loss: 3.2349\n",
      "Epoch: 5819, Train Loss: 0.4381, Test Loss: 3.5184\n",
      "Epoch: 5820, Train Loss: 0.4222, Test Loss: 4.0084\n",
      "Epoch: 5821, Train Loss: 0.4252, Test Loss: 3.9800\n",
      "Epoch: 5822, Train Loss: 0.4750, Test Loss: 3.2756\n",
      "Epoch: 5823, Train Loss: 0.4412, Test Loss: 3.1997\n",
      "Epoch: 5824, Train Loss: 0.4369, Test Loss: 3.6842\n",
      "Epoch: 5825, Train Loss: 0.4005, Test Loss: 4.1579\n",
      "Epoch: 5826, Train Loss: 0.5323, Test Loss: 3.7947\n",
      "Epoch: 5827, Train Loss: 0.4395, Test Loss: 3.2340\n",
      "Epoch: 5828, Train Loss: 0.4694, Test Loss: 3.5161\n",
      "Epoch: 5829, Train Loss: 0.4027, Test Loss: 3.9492\n",
      "Epoch: 5830, Train Loss: 0.4453, Test Loss: 3.7774\n",
      "Epoch: 5831, Train Loss: 0.4234, Test Loss: 3.5681\n",
      "Epoch: 5832, Train Loss: 0.4139, Test Loss: 3.3475\n",
      "Epoch: 5833, Train Loss: 0.4950, Test Loss: 3.7571\n",
      "Epoch: 5834, Train Loss: 0.4352, Test Loss: 3.7843\n",
      "Epoch: 5835, Train Loss: 0.4503, Test Loss: 3.2971\n",
      "Epoch: 5836, Train Loss: 0.4054, Test Loss: 3.1301\n",
      "Epoch: 5837, Train Loss: 0.4799, Test Loss: 3.5286\n",
      "Epoch: 5838, Train Loss: 0.4017, Test Loss: 3.9860\n",
      "Epoch: 5839, Train Loss: 0.4536, Test Loss: 3.9502\n",
      "Epoch: 5840, Train Loss: 0.4282, Test Loss: 3.4772\n",
      "Epoch: 5841, Train Loss: 0.4408, Test Loss: 3.4036\n",
      "Epoch: 5842, Train Loss: 0.4139, Test Loss: 3.8557\n",
      "Epoch: 5843, Train Loss: 0.4155, Test Loss: 3.9129\n",
      "Epoch: 5844, Train Loss: 0.3800, Test Loss: 3.6997\n",
      "Epoch: 5845, Train Loss: 0.4293, Test Loss: 3.4182\n",
      "Epoch: 5846, Train Loss: 0.4485, Test Loss: 3.5417\n",
      "Epoch: 5847, Train Loss: 0.3811, Test Loss: 3.7457\n",
      "Epoch: 5848, Train Loss: 0.3982, Test Loss: 3.8848\n",
      "Epoch: 5849, Train Loss: 0.4215, Test Loss: 3.5779\n",
      "Epoch: 5850, Train Loss: 0.4271, Test Loss: 3.4351\n",
      "Epoch: 5851, Train Loss: 0.4069, Test Loss: 3.3517\n",
      "Epoch: 5852, Train Loss: 0.4679, Test Loss: 3.7750\n",
      "Epoch: 5853, Train Loss: 0.4318, Test Loss: 3.7622\n",
      "Epoch: 5854, Train Loss: 0.4055, Test Loss: 3.6683\n",
      "Epoch: 5855, Train Loss: 0.3991, Test Loss: 3.4054\n",
      "Epoch: 5856, Train Loss: 0.4583, Test Loss: 3.6351\n",
      "Epoch: 5857, Train Loss: 0.4099, Test Loss: 3.9259\n",
      "Epoch: 5858, Train Loss: 0.4458, Test Loss: 3.6135\n",
      "Epoch: 5859, Train Loss: 0.4198, Test Loss: 3.2429\n",
      "Epoch: 5860, Train Loss: 0.4347, Test Loss: 3.4503\n",
      "Epoch: 5861, Train Loss: 0.4657, Test Loss: 4.1415\n",
      "Epoch: 5862, Train Loss: 0.4820, Test Loss: 3.8452\n",
      "Epoch: 5863, Train Loss: 0.4295, Test Loss: 3.3138\n",
      "Epoch: 5864, Train Loss: 0.4726, Test Loss: 3.1091\n",
      "Epoch: 5865, Train Loss: 0.5246, Test Loss: 3.8374\n",
      "Epoch: 5866, Train Loss: 0.4556, Test Loss: 4.1846\n",
      "Epoch: 5867, Train Loss: 0.4869, Test Loss: 3.5973\n",
      "Epoch: 5868, Train Loss: 0.4144, Test Loss: 3.1475\n",
      "Epoch: 5869, Train Loss: 0.5085, Test Loss: 3.3506\n",
      "Epoch: 5870, Train Loss: 0.3921, Test Loss: 3.7786\n",
      "Epoch: 5871, Train Loss: 0.4600, Test Loss: 4.0705\n",
      "Epoch: 5872, Train Loss: 0.4882, Test Loss: 3.5112\n",
      "Epoch: 5873, Train Loss: 0.4272, Test Loss: 3.1906\n",
      "Epoch: 5874, Train Loss: 0.4820, Test Loss: 3.6385\n",
      "Epoch: 5875, Train Loss: 0.4128, Test Loss: 4.0355\n",
      "Epoch: 5876, Train Loss: 0.4647, Test Loss: 3.6057\n",
      "Epoch: 5877, Train Loss: 0.4012, Test Loss: 3.5041\n",
      "Epoch: 5878, Train Loss: 0.4290, Test Loss: 3.3805\n",
      "Epoch: 5879, Train Loss: 0.4230, Test Loss: 3.6136\n",
      "Epoch: 5880, Train Loss: 0.4280, Test Loss: 3.8354\n",
      "Epoch: 5881, Train Loss: 0.4559, Test Loss: 3.5402\n",
      "Epoch: 5882, Train Loss: 0.4177, Test Loss: 3.3332\n",
      "Epoch: 5883, Train Loss: 0.4638, Test Loss: 3.4897\n",
      "Epoch: 5884, Train Loss: 0.3915, Test Loss: 3.8041\n",
      "Epoch: 5885, Train Loss: 0.4198, Test Loss: 3.7040\n",
      "Epoch: 5886, Train Loss: 0.4319, Test Loss: 3.5695\n",
      "Epoch: 5887, Train Loss: 0.4085, Test Loss: 3.3233\n",
      "Epoch: 5888, Train Loss: 0.4386, Test Loss: 3.5234\n",
      "Epoch: 5889, Train Loss: 0.4223, Test Loss: 3.4827\n",
      "Epoch: 5890, Train Loss: 0.4157, Test Loss: 3.4166\n",
      "Epoch: 5891, Train Loss: 0.4109, Test Loss: 3.5197\n",
      "Epoch: 5892, Train Loss: 0.4062, Test Loss: 3.4858\n",
      "Epoch: 5893, Train Loss: 0.4500, Test Loss: 3.4793\n",
      "Epoch: 5894, Train Loss: 0.4328, Test Loss: 3.7997\n",
      "Epoch: 5895, Train Loss: 0.4448, Test Loss: 3.7162\n",
      "Epoch: 5896, Train Loss: 0.4492, Test Loss: 3.3357\n",
      "Epoch: 5897, Train Loss: 0.4589, Test Loss: 3.2290\n",
      "Epoch: 5898, Train Loss: 0.4694, Test Loss: 3.6858\n",
      "Epoch: 5899, Train Loss: 0.4534, Test Loss: 3.9650\n",
      "Epoch: 5900, Train Loss: 0.4917, Test Loss: 3.4503\n",
      "Epoch: 5901, Train Loss: 0.4299, Test Loss: 3.3187\n",
      "Epoch: 5902, Train Loss: 0.5045, Test Loss: 3.8576\n",
      "Epoch: 5903, Train Loss: 0.4479, Test Loss: 3.9965\n",
      "Epoch: 5904, Train Loss: 0.5940, Test Loss: 3.2882\n",
      "Epoch: 5905, Train Loss: 0.4510, Test Loss: 3.1570\n",
      "Epoch: 5906, Train Loss: 0.4931, Test Loss: 3.5054\n",
      "Epoch: 5907, Train Loss: 0.4328, Test Loss: 3.5873\n",
      "Epoch: 5908, Train Loss: 0.4194, Test Loss: 3.5698\n",
      "Epoch: 5909, Train Loss: 0.3914, Test Loss: 3.4634\n",
      "Epoch: 5910, Train Loss: 0.4665, Test Loss: 3.5878\n",
      "Epoch: 5911, Train Loss: 0.4426, Test Loss: 3.6525\n",
      "Epoch: 5912, Train Loss: 0.4149, Test Loss: 3.7177\n",
      "Epoch: 5913, Train Loss: 0.4344, Test Loss: 3.3802\n",
      "Epoch: 5914, Train Loss: 0.4869, Test Loss: 3.5173\n",
      "Epoch: 5915, Train Loss: 0.4045, Test Loss: 3.7355\n",
      "Epoch: 5916, Train Loss: 0.4639, Test Loss: 3.7521\n",
      "Epoch: 5917, Train Loss: 0.4265, Test Loss: 3.5036\n",
      "Epoch: 5918, Train Loss: 0.4253, Test Loss: 3.3254\n",
      "Epoch: 5919, Train Loss: 0.4770, Test Loss: 3.6582\n",
      "Epoch: 5920, Train Loss: 0.4532, Test Loss: 3.6948\n",
      "Epoch: 5921, Train Loss: 0.4655, Test Loss: 3.5633\n",
      "Epoch: 5922, Train Loss: 0.4446, Test Loss: 3.4941\n",
      "Epoch: 5923, Train Loss: 0.4251, Test Loss: 3.6721\n",
      "Epoch: 5924, Train Loss: 0.4753, Test Loss: 3.5469\n",
      "Epoch: 5925, Train Loss: 0.4523, Test Loss: 3.3170\n",
      "Epoch: 5926, Train Loss: 0.4443, Test Loss: 3.5796\n",
      "Epoch: 5927, Train Loss: 0.4567, Test Loss: 3.6930\n",
      "Epoch: 5928, Train Loss: 0.4191, Test Loss: 3.4151\n",
      "Epoch: 5929, Train Loss: 0.4176, Test Loss: 3.1454\n",
      "Epoch: 5930, Train Loss: 0.4502, Test Loss: 3.2921\n",
      "Epoch: 5931, Train Loss: 0.4473, Test Loss: 3.9776\n",
      "Epoch: 5932, Train Loss: 0.4943, Test Loss: 3.8299\n",
      "Epoch: 5933, Train Loss: 0.4299, Test Loss: 3.2513\n",
      "Epoch: 5934, Train Loss: 0.4161, Test Loss: 3.0463\n",
      "Epoch: 5935, Train Loss: 0.4734, Test Loss: 3.3563\n",
      "Epoch: 5936, Train Loss: 0.4279, Test Loss: 3.9637\n",
      "Epoch: 5937, Train Loss: 0.4423, Test Loss: 4.0229\n",
      "Epoch: 5938, Train Loss: 0.4924, Test Loss: 3.2138\n",
      "Epoch: 5939, Train Loss: 0.4560, Test Loss: 3.1682\n",
      "Epoch: 5940, Train Loss: 0.4422, Test Loss: 3.7005\n",
      "Epoch: 5941, Train Loss: 0.4157, Test Loss: 4.0297\n",
      "Epoch: 5942, Train Loss: 0.5155, Test Loss: 3.4070\n",
      "Epoch: 5943, Train Loss: 0.4197, Test Loss: 3.2833\n",
      "Epoch: 5944, Train Loss: 0.4433, Test Loss: 3.7699\n",
      "Epoch: 5945, Train Loss: 0.4177, Test Loss: 4.1620\n",
      "Epoch: 5946, Train Loss: 0.4789, Test Loss: 3.5704\n",
      "Epoch: 5947, Train Loss: 0.4249, Test Loss: 3.0676\n",
      "Epoch: 5948, Train Loss: 0.4824, Test Loss: 3.2550\n",
      "Epoch: 5949, Train Loss: 0.4538, Test Loss: 3.9438\n",
      "Epoch: 5950, Train Loss: 0.4672, Test Loss: 4.0129\n",
      "Epoch: 5951, Train Loss: 0.4592, Test Loss: 3.5472\n",
      "Epoch: 5952, Train Loss: 0.4264, Test Loss: 3.2319\n",
      "Epoch: 5953, Train Loss: 0.4650, Test Loss: 3.4931\n",
      "Epoch: 5954, Train Loss: 0.4248, Test Loss: 3.7790\n",
      "Epoch: 5955, Train Loss: 0.4697, Test Loss: 3.5826\n",
      "Epoch: 5956, Train Loss: 0.4311, Test Loss: 3.5893\n",
      "Epoch: 5957, Train Loss: 0.4308, Test Loss: 3.4356\n",
      "Epoch: 5958, Train Loss: 0.4688, Test Loss: 3.2867\n",
      "Epoch: 5959, Train Loss: 0.4694, Test Loss: 3.4063\n",
      "Epoch: 5960, Train Loss: 0.4391, Test Loss: 3.5659\n",
      "Epoch: 5961, Train Loss: 0.4661, Test Loss: 4.0195\n",
      "Epoch: 5962, Train Loss: 0.5044, Test Loss: 3.6615\n",
      "Epoch: 5963, Train Loss: 0.3914, Test Loss: 3.3438\n",
      "Epoch: 5964, Train Loss: 0.4286, Test Loss: 3.2717\n",
      "Epoch: 5965, Train Loss: 0.4465, Test Loss: 3.7404\n",
      "Epoch: 5966, Train Loss: 0.4953, Test Loss: 3.5604\n",
      "Epoch: 5967, Train Loss: 0.4086, Test Loss: 3.3543\n",
      "Epoch: 5968, Train Loss: 0.4566, Test Loss: 3.5747\n",
      "Epoch: 5969, Train Loss: 0.4308, Test Loss: 3.8580\n",
      "Epoch: 5970, Train Loss: 0.4745, Test Loss: 3.8284\n",
      "Epoch: 5971, Train Loss: 0.4951, Test Loss: 3.2232\n",
      "Epoch: 5972, Train Loss: 0.4293, Test Loss: 3.1222\n",
      "Epoch: 5973, Train Loss: 0.4374, Test Loss: 3.4620\n",
      "Epoch: 5974, Train Loss: 0.4494, Test Loss: 4.0728\n",
      "Epoch: 5975, Train Loss: 0.5028, Test Loss: 3.8742\n",
      "Epoch: 5976, Train Loss: 0.4603, Test Loss: 3.1392\n",
      "Epoch: 5977, Train Loss: 0.4841, Test Loss: 3.1112\n",
      "Epoch: 5978, Train Loss: 0.4697, Test Loss: 3.7709\n",
      "Epoch: 5979, Train Loss: 0.4428, Test Loss: 3.8854\n",
      "Epoch: 5980, Train Loss: 0.4387, Test Loss: 3.4902\n",
      "Epoch: 5981, Train Loss: 0.4353, Test Loss: 3.0473\n",
      "Epoch: 5982, Train Loss: 0.5380, Test Loss: 3.6007\n",
      "Epoch: 5983, Train Loss: 0.4438, Test Loss: 3.8716\n",
      "Epoch: 5984, Train Loss: 0.4740, Test Loss: 3.3547\n",
      "Epoch: 5985, Train Loss: 0.4014, Test Loss: 3.0966\n",
      "Epoch: 5986, Train Loss: 0.5162, Test Loss: 3.6969\n",
      "Epoch: 5987, Train Loss: 0.4293, Test Loss: 3.7428\n",
      "Epoch: 5988, Train Loss: 0.4505, Test Loss: 3.3084\n",
      "Epoch: 5989, Train Loss: 0.4172, Test Loss: 3.0859\n",
      "Epoch: 5990, Train Loss: 0.5186, Test Loss: 3.5422\n",
      "Epoch: 5991, Train Loss: 0.4167, Test Loss: 3.7925\n",
      "Epoch: 5992, Train Loss: 0.4591, Test Loss: 3.5427\n",
      "Epoch: 5993, Train Loss: 0.4441, Test Loss: 3.4867\n",
      "Epoch: 5994, Train Loss: 0.4246, Test Loss: 3.4491\n",
      "Epoch: 5995, Train Loss: 0.3990, Test Loss: 3.4825\n",
      "Epoch: 5996, Train Loss: 0.4131, Test Loss: 3.5260\n",
      "Epoch: 5997, Train Loss: 0.4024, Test Loss: 3.4350\n",
      "Epoch: 5998, Train Loss: 0.4364, Test Loss: 3.3370\n",
      "Epoch: 5999, Train Loss: 0.4110, Test Loss: 3.4039\n",
      "Epoch: 6000, Train Loss: 0.4108, Test Loss: 3.5141\n",
      "Epoch: 6001, Train Loss: 0.4137, Test Loss: 3.7128\n",
      "Epoch: 6002, Train Loss: 0.4070, Test Loss: 3.6838\n",
      "Epoch: 6003, Train Loss: 0.4057, Test Loss: 3.3189\n",
      "Epoch: 6004, Train Loss: 0.4647, Test Loss: 3.5254\n",
      "Epoch: 6005, Train Loss: 0.4326, Test Loss: 3.6961\n",
      "Epoch: 6006, Train Loss: 0.3977, Test Loss: 3.6683\n",
      "Epoch: 6007, Train Loss: 0.4116, Test Loss: 3.6765\n",
      "Epoch: 6008, Train Loss: 0.4455, Test Loss: 3.8038\n",
      "Epoch: 6009, Train Loss: 0.4171, Test Loss: 3.6283\n",
      "Epoch: 6010, Train Loss: 0.4202, Test Loss: 3.2793\n",
      "Epoch: 6011, Train Loss: 0.4796, Test Loss: 3.3021\n",
      "Epoch: 6012, Train Loss: 0.4553, Test Loss: 3.6341\n",
      "Epoch: 6013, Train Loss: 0.4313, Test Loss: 3.8633\n",
      "Epoch: 6014, Train Loss: 0.4934, Test Loss: 3.4181\n",
      "Epoch: 6015, Train Loss: 0.4877, Test Loss: 3.4254\n",
      "Epoch: 6016, Train Loss: 0.5222, Test Loss: 4.2185\n",
      "Epoch: 6017, Train Loss: 0.4929, Test Loss: 3.9790\n",
      "Epoch: 6018, Train Loss: 0.4887, Test Loss: 3.2248\n",
      "Epoch: 6019, Train Loss: 0.4864, Test Loss: 3.2573\n",
      "Epoch: 6020, Train Loss: 0.4646, Test Loss: 3.9279\n",
      "Epoch: 6021, Train Loss: 0.4060, Test Loss: 4.5719\n",
      "Epoch: 6022, Train Loss: 0.6112, Test Loss: 3.5755\n",
      "Epoch: 6023, Train Loss: 0.4005, Test Loss: 3.0586\n",
      "Epoch: 6024, Train Loss: 0.4661, Test Loss: 3.1721\n",
      "Epoch: 6025, Train Loss: 0.5587, Test Loss: 3.8773\n",
      "Epoch: 6026, Train Loss: 0.4777, Test Loss: 3.9913\n",
      "Epoch: 6027, Train Loss: 0.5016, Test Loss: 3.2579\n",
      "Epoch: 6028, Train Loss: 0.4129, Test Loss: 3.0445\n",
      "Epoch: 6029, Train Loss: 0.4696, Test Loss: 3.3617\n",
      "Epoch: 6030, Train Loss: 0.4423, Test Loss: 4.2117\n",
      "Epoch: 6031, Train Loss: 0.5630, Test Loss: 3.7084\n",
      "Epoch: 6032, Train Loss: 0.4038, Test Loss: 3.2019\n",
      "Epoch: 6033, Train Loss: 0.4214, Test Loss: 3.1154\n",
      "Epoch: 6034, Train Loss: 0.4745, Test Loss: 3.6474\n",
      "Epoch: 6035, Train Loss: 0.4484, Test Loss: 3.8229\n",
      "Epoch: 6036, Train Loss: 0.4519, Test Loss: 3.6231\n",
      "Epoch: 6037, Train Loss: 0.4045, Test Loss: 3.3991\n",
      "Epoch: 6038, Train Loss: 0.4357, Test Loss: 3.3155\n",
      "Epoch: 6039, Train Loss: 0.4198, Test Loss: 3.6798\n",
      "Epoch: 6040, Train Loss: 0.4158, Test Loss: 3.9671\n",
      "Epoch: 6041, Train Loss: 0.4907, Test Loss: 3.4024\n",
      "Epoch: 6042, Train Loss: 0.4286, Test Loss: 3.0396\n",
      "Epoch: 6043, Train Loss: 0.4878, Test Loss: 3.4255\n",
      "Epoch: 6044, Train Loss: 0.4341, Test Loss: 3.9311\n",
      "Epoch: 6045, Train Loss: 0.5008, Test Loss: 3.4654\n",
      "Epoch: 6046, Train Loss: 0.4584, Test Loss: 3.0247\n",
      "Epoch: 6047, Train Loss: 0.4708, Test Loss: 3.1423\n",
      "Epoch: 6048, Train Loss: 0.4664, Test Loss: 3.7519\n",
      "Epoch: 6049, Train Loss: 0.4284, Test Loss: 4.0190\n",
      "Epoch: 6050, Train Loss: 0.4758, Test Loss: 3.4765\n",
      "Epoch: 6051, Train Loss: 0.4502, Test Loss: 3.0159\n",
      "Epoch: 6052, Train Loss: 0.5010, Test Loss: 3.0956\n",
      "Epoch: 6053, Train Loss: 0.4543, Test Loss: 3.6615\n",
      "Epoch: 6054, Train Loss: 0.4295, Test Loss: 3.8568\n",
      "Epoch: 6055, Train Loss: 0.5156, Test Loss: 3.1943\n",
      "Epoch: 6056, Train Loss: 0.4517, Test Loss: 3.1328\n",
      "Epoch: 6057, Train Loss: 0.4644, Test Loss: 3.6851\n",
      "Epoch: 6058, Train Loss: 0.4086, Test Loss: 4.0740\n",
      "Epoch: 6059, Train Loss: 0.4895, Test Loss: 3.5236\n",
      "Epoch: 6060, Train Loss: 0.4220, Test Loss: 3.1304\n",
      "Epoch: 6061, Train Loss: 0.4468, Test Loss: 3.2601\n",
      "Epoch: 6062, Train Loss: 0.4093, Test Loss: 3.7843\n",
      "Epoch: 6063, Train Loss: 0.4436, Test Loss: 3.8529\n",
      "Epoch: 6064, Train Loss: 0.5027, Test Loss: 3.3198\n",
      "Epoch: 6065, Train Loss: 0.4099, Test Loss: 3.2216\n",
      "Epoch: 6066, Train Loss: 0.4192, Test Loss: 3.5178\n",
      "Epoch: 6067, Train Loss: 0.4158, Test Loss: 3.5488\n",
      "Epoch: 6068, Train Loss: 0.4007, Test Loss: 3.4021\n",
      "Epoch: 6069, Train Loss: 0.3929, Test Loss: 3.2204\n",
      "Epoch: 6070, Train Loss: 0.4087, Test Loss: 3.3864\n",
      "Epoch: 6071, Train Loss: 0.4272, Test Loss: 3.7903\n",
      "Epoch: 6072, Train Loss: 0.4305, Test Loss: 3.6340\n",
      "Epoch: 6073, Train Loss: 0.4050, Test Loss: 3.4035\n",
      "Epoch: 6074, Train Loss: 0.4260, Test Loss: 3.2140\n",
      "Epoch: 6075, Train Loss: 0.4417, Test Loss: 3.4137\n",
      "Epoch: 6076, Train Loss: 0.4028, Test Loss: 3.6551\n",
      "Epoch: 6077, Train Loss: 0.3886, Test Loss: 3.9643\n",
      "Epoch: 6078, Train Loss: 0.4922, Test Loss: 3.3058\n",
      "Epoch: 6079, Train Loss: 0.4485, Test Loss: 3.1152\n",
      "Epoch: 6080, Train Loss: 0.4466, Test Loss: 3.3896\n",
      "Epoch: 6081, Train Loss: 0.4228, Test Loss: 3.8534\n",
      "Epoch: 6082, Train Loss: 0.4290, Test Loss: 3.7242\n",
      "Epoch: 6083, Train Loss: 0.4401, Test Loss: 3.2726\n",
      "Epoch: 6084, Train Loss: 0.4334, Test Loss: 3.2824\n",
      "Epoch: 6085, Train Loss: 0.4373, Test Loss: 3.7421\n",
      "Epoch: 6086, Train Loss: 0.4644, Test Loss: 3.8785\n",
      "Epoch: 6087, Train Loss: 0.4240, Test Loss: 3.4186\n",
      "Epoch: 6088, Train Loss: 0.4015, Test Loss: 3.1650\n",
      "Epoch: 6089, Train Loss: 0.5381, Test Loss: 3.6185\n",
      "Epoch: 6090, Train Loss: 0.4264, Test Loss: 3.8466\n",
      "Epoch: 6091, Train Loss: 0.4216, Test Loss: 3.3908\n",
      "Epoch: 6092, Train Loss: 0.4559, Test Loss: 3.2505\n",
      "Epoch: 6093, Train Loss: 0.4289, Test Loss: 3.5633\n",
      "Epoch: 6094, Train Loss: 0.4043, Test Loss: 3.8069\n",
      "Epoch: 6095, Train Loss: 0.4317, Test Loss: 3.5769\n",
      "Epoch: 6096, Train Loss: 0.4470, Test Loss: 3.1280\n",
      "Epoch: 6097, Train Loss: 0.5338, Test Loss: 3.3918\n",
      "Epoch: 6098, Train Loss: 0.4081, Test Loss: 3.8824\n",
      "Epoch: 6099, Train Loss: 0.4396, Test Loss: 3.9607\n",
      "Epoch: 6100, Train Loss: 0.4256, Test Loss: 3.5219\n",
      "Epoch: 6101, Train Loss: 0.4065, Test Loss: 3.0861\n",
      "Epoch: 6102, Train Loss: 0.4446, Test Loss: 3.3078\n",
      "Epoch: 6103, Train Loss: 0.4347, Test Loss: 3.8580\n",
      "Epoch: 6104, Train Loss: 0.4372, Test Loss: 3.8471\n",
      "Epoch: 6105, Train Loss: 0.4615, Test Loss: 3.4328\n",
      "Epoch: 6106, Train Loss: 0.4054, Test Loss: 3.3162\n",
      "Epoch: 6107, Train Loss: 0.4956, Test Loss: 3.9425\n",
      "Epoch: 6108, Train Loss: 0.4192, Test Loss: 3.9597\n",
      "Epoch: 6109, Train Loss: 0.4770, Test Loss: 3.1941\n",
      "Epoch: 6110, Train Loss: 0.4538, Test Loss: 3.1327\n",
      "Epoch: 6111, Train Loss: 0.4434, Test Loss: 3.5313\n",
      "Epoch: 6112, Train Loss: 0.4398, Test Loss: 3.8115\n",
      "Epoch: 6113, Train Loss: 0.4131, Test Loss: 3.6032\n",
      "Epoch: 6114, Train Loss: 0.4321, Test Loss: 3.3387\n",
      "Epoch: 6115, Train Loss: 0.4244, Test Loss: 3.3851\n",
      "Epoch: 6116, Train Loss: 0.3914, Test Loss: 3.6119\n",
      "Epoch: 6117, Train Loss: 0.4018, Test Loss: 3.6998\n",
      "Epoch: 6118, Train Loss: 0.4316, Test Loss: 3.3957\n",
      "Epoch: 6119, Train Loss: 0.4005, Test Loss: 3.3328\n",
      "Epoch: 6120, Train Loss: 0.4457, Test Loss: 3.5269\n",
      "Epoch: 6121, Train Loss: 0.4489, Test Loss: 4.0623\n",
      "Epoch: 6122, Train Loss: 0.4760, Test Loss: 3.9302\n",
      "Epoch: 6123, Train Loss: 0.4372, Test Loss: 3.2569\n",
      "Epoch: 6124, Train Loss: 0.4300, Test Loss: 3.0429\n",
      "Epoch: 6125, Train Loss: 0.4668, Test Loss: 3.4717\n",
      "Epoch: 6126, Train Loss: 0.3829, Test Loss: 3.8596\n",
      "Epoch: 6127, Train Loss: 0.4607, Test Loss: 3.4952\n",
      "Epoch: 6128, Train Loss: 0.3976, Test Loss: 3.1066\n",
      "Epoch: 6129, Train Loss: 0.4485, Test Loss: 3.1965\n",
      "Epoch: 6130, Train Loss: 0.4491, Test Loss: 3.5011\n",
      "Epoch: 6131, Train Loss: 0.4412, Test Loss: 3.9276\n",
      "Epoch: 6132, Train Loss: 0.4042, Test Loss: 3.7662\n",
      "Epoch: 6133, Train Loss: 0.4126, Test Loss: 3.2917\n",
      "Epoch: 6134, Train Loss: 0.4554, Test Loss: 3.2649\n",
      "Epoch: 6135, Train Loss: 0.4247, Test Loss: 3.5565\n",
      "Epoch: 6136, Train Loss: 0.4221, Test Loss: 3.8382\n",
      "Epoch: 6137, Train Loss: 0.4645, Test Loss: 3.9103\n",
      "Epoch: 6138, Train Loss: 0.4456, Test Loss: 3.3610\n",
      "Epoch: 6139, Train Loss: 0.4461, Test Loss: 3.3761\n",
      "Epoch: 6140, Train Loss: 0.3921, Test Loss: 3.3846\n",
      "Epoch: 6141, Train Loss: 0.3912, Test Loss: 3.5858\n",
      "Epoch: 6142, Train Loss: 0.4228, Test Loss: 3.5725\n",
      "Epoch: 6143, Train Loss: 0.4129, Test Loss: 3.5512\n",
      "Epoch: 6144, Train Loss: 0.4153, Test Loss: 3.6381\n",
      "Epoch: 6145, Train Loss: 0.3974, Test Loss: 3.6286\n",
      "Epoch: 6146, Train Loss: 0.4314, Test Loss: 3.3914\n",
      "Epoch: 6147, Train Loss: 0.4057, Test Loss: 3.2622\n",
      "Epoch: 6148, Train Loss: 0.4191, Test Loss: 3.4710\n",
      "Epoch: 6149, Train Loss: 0.4179, Test Loss: 3.7523\n",
      "Epoch: 6150, Train Loss: 0.4455, Test Loss: 3.7222\n",
      "Epoch: 6151, Train Loss: 0.3948, Test Loss: 3.6240\n",
      "Epoch: 6152, Train Loss: 0.4270, Test Loss: 3.9137\n",
      "Epoch: 6153, Train Loss: 0.4541, Test Loss: 3.5785\n",
      "Epoch: 6154, Train Loss: 0.3833, Test Loss: 3.3604\n",
      "Epoch: 6155, Train Loss: 0.4383, Test Loss: 3.5184\n",
      "Epoch: 6156, Train Loss: 0.4176, Test Loss: 3.9721\n",
      "Epoch: 6157, Train Loss: 0.4151, Test Loss: 4.0079\n",
      "Epoch: 6158, Train Loss: 0.5016, Test Loss: 3.2909\n",
      "Epoch: 6159, Train Loss: 0.4346, Test Loss: 3.0712\n",
      "Epoch: 6160, Train Loss: 0.5962, Test Loss: 3.9275\n",
      "Epoch: 6161, Train Loss: 0.4599, Test Loss: 4.2232\n",
      "Epoch: 6162, Train Loss: 0.5419, Test Loss: 3.2684\n",
      "Epoch: 6163, Train Loss: 0.4563, Test Loss: 3.0112\n",
      "Epoch: 6164, Train Loss: 0.5197, Test Loss: 3.4053\n",
      "Epoch: 6165, Train Loss: 0.4140, Test Loss: 3.8618\n",
      "Epoch: 6166, Train Loss: 0.4481, Test Loss: 3.7405\n",
      "Epoch: 6167, Train Loss: 0.4418, Test Loss: 3.2460\n",
      "Epoch: 6168, Train Loss: 0.4343, Test Loss: 3.2995\n",
      "Epoch: 6169, Train Loss: 0.4264, Test Loss: 3.8140\n",
      "Epoch: 6170, Train Loss: 0.4196, Test Loss: 3.7893\n",
      "Epoch: 6171, Train Loss: 0.4311, Test Loss: 3.5563\n",
      "Epoch: 6172, Train Loss: 0.4211, Test Loss: 3.3652\n",
      "Epoch: 6173, Train Loss: 0.4229, Test Loss: 3.2552\n",
      "Epoch: 6174, Train Loss: 0.5012, Test Loss: 3.8661\n",
      "Epoch: 6175, Train Loss: 0.4301, Test Loss: 4.2041\n",
      "Epoch: 6176, Train Loss: 0.4803, Test Loss: 3.5081\n",
      "Epoch: 6177, Train Loss: 0.4041, Test Loss: 3.0272\n",
      "Epoch: 6178, Train Loss: 0.5297, Test Loss: 3.3936\n",
      "Epoch: 6179, Train Loss: 0.4467, Test Loss: 4.0350\n",
      "Epoch: 6180, Train Loss: 0.4963, Test Loss: 3.8695\n",
      "Epoch: 6181, Train Loss: 0.4962, Test Loss: 3.1880\n",
      "Epoch: 6182, Train Loss: 0.4964, Test Loss: 3.2822\n",
      "Epoch: 6183, Train Loss: 0.4808, Test Loss: 4.0004\n",
      "Epoch: 6184, Train Loss: 0.4626, Test Loss: 4.0004\n",
      "Epoch: 6185, Train Loss: 0.4320, Test Loss: 3.4548\n",
      "Epoch: 6186, Train Loss: 0.4504, Test Loss: 3.1674\n",
      "Epoch: 6187, Train Loss: 0.4537, Test Loss: 3.2759\n",
      "Epoch: 6188, Train Loss: 0.4364, Test Loss: 3.7130\n",
      "Epoch: 6189, Train Loss: 0.4190, Test Loss: 3.7995\n",
      "Epoch: 6190, Train Loss: 0.4490, Test Loss: 3.6533\n",
      "Epoch: 6191, Train Loss: 0.4495, Test Loss: 3.5854\n",
      "Epoch: 6192, Train Loss: 0.4280, Test Loss: 3.5284\n",
      "Epoch: 6193, Train Loss: 0.3734, Test Loss: 3.4674\n",
      "Epoch: 6194, Train Loss: 0.4045, Test Loss: 3.5174\n",
      "Epoch: 6195, Train Loss: 0.4129, Test Loss: 3.5587\n",
      "Epoch: 6196, Train Loss: 0.4060, Test Loss: 3.3918\n",
      "Epoch: 6197, Train Loss: 0.3979, Test Loss: 3.4680\n",
      "Epoch: 6198, Train Loss: 0.4654, Test Loss: 3.8551\n",
      "Epoch: 6199, Train Loss: 0.4242, Test Loss: 3.6967\n",
      "Epoch: 6200, Train Loss: 0.4851, Test Loss: 3.1392\n",
      "Epoch: 6201, Train Loss: 0.4452, Test Loss: 3.1268\n",
      "Epoch: 6202, Train Loss: 0.4090, Test Loss: 3.3737\n",
      "Epoch: 6203, Train Loss: 0.4093, Test Loss: 3.7026\n",
      "Epoch: 6204, Train Loss: 0.4386, Test Loss: 3.7121\n",
      "Epoch: 6205, Train Loss: 0.4264, Test Loss: 3.3509\n",
      "Epoch: 6206, Train Loss: 0.4649, Test Loss: 3.4511\n",
      "Epoch: 6207, Train Loss: 0.4005, Test Loss: 3.7109\n",
      "Epoch: 6208, Train Loss: 0.4457, Test Loss: 3.6148\n",
      "Epoch: 6209, Train Loss: 0.4521, Test Loss: 3.7684\n",
      "Epoch: 6210, Train Loss: 0.4069, Test Loss: 3.5097\n",
      "Epoch: 6211, Train Loss: 0.4094, Test Loss: 3.4573\n",
      "Epoch: 6212, Train Loss: 0.3785, Test Loss: 3.4076\n",
      "Epoch: 6213, Train Loss: 0.4272, Test Loss: 3.5388\n",
      "Epoch: 6214, Train Loss: 0.4107, Test Loss: 3.8029\n",
      "Epoch: 6215, Train Loss: 0.4198, Test Loss: 3.6743\n",
      "Epoch: 6216, Train Loss: 0.4914, Test Loss: 3.0799\n",
      "Epoch: 6217, Train Loss: 0.4506, Test Loss: 3.1357\n",
      "Epoch: 6218, Train Loss: 0.4572, Test Loss: 3.8877\n",
      "Epoch: 6219, Train Loss: 0.4502, Test Loss: 3.9521\n",
      "Epoch: 6220, Train Loss: 0.4440, Test Loss: 3.3264\n",
      "Epoch: 6221, Train Loss: 0.4153, Test Loss: 3.2344\n",
      "Epoch: 6222, Train Loss: 0.4786, Test Loss: 3.8092\n",
      "Epoch: 6223, Train Loss: 0.4189, Test Loss: 4.1036\n",
      "Epoch: 6224, Train Loss: 0.5368, Test Loss: 3.3916\n",
      "Epoch: 6225, Train Loss: 0.4026, Test Loss: 3.0287\n",
      "Epoch: 6226, Train Loss: 0.5626, Test Loss: 3.5788\n",
      "Epoch: 6227, Train Loss: 0.3934, Test Loss: 3.9986\n",
      "Epoch: 6228, Train Loss: 0.4854, Test Loss: 3.5857\n",
      "Epoch: 6229, Train Loss: 0.4225, Test Loss: 2.9947\n",
      "Epoch: 6230, Train Loss: 0.4942, Test Loss: 3.2246\n",
      "Epoch: 6231, Train Loss: 0.4440, Test Loss: 3.9571\n",
      "Epoch: 6232, Train Loss: 0.4636, Test Loss: 4.1035\n",
      "Epoch: 6233, Train Loss: 0.4196, Test Loss: 3.5792\n",
      "Epoch: 6234, Train Loss: 0.3744, Test Loss: 3.2114\n",
      "Epoch: 6235, Train Loss: 0.4097, Test Loss: 3.1918\n",
      "Epoch: 6236, Train Loss: 0.4269, Test Loss: 3.6036\n",
      "Epoch: 6237, Train Loss: 0.4189, Test Loss: 3.7732\n",
      "Epoch: 6238, Train Loss: 0.4045, Test Loss: 3.6895\n",
      "Epoch: 6239, Train Loss: 0.4146, Test Loss: 3.2748\n",
      "Epoch: 6240, Train Loss: 0.4256, Test Loss: 3.2618\n",
      "Epoch: 6241, Train Loss: 0.5013, Test Loss: 4.1308\n",
      "Epoch: 6242, Train Loss: 0.4652, Test Loss: 4.0841\n",
      "Epoch: 6243, Train Loss: 0.4584, Test Loss: 3.4420\n",
      "Epoch: 6244, Train Loss: 0.4337, Test Loss: 2.9802\n",
      "Epoch: 6245, Train Loss: 0.6082, Test Loss: 3.4908\n",
      "Epoch: 6246, Train Loss: 0.3840, Test Loss: 4.2473\n",
      "Epoch: 6247, Train Loss: 0.4949, Test Loss: 4.0355\n",
      "Epoch: 6248, Train Loss: 0.5334, Test Loss: 3.1169\n",
      "Epoch: 6249, Train Loss: 0.4997, Test Loss: 3.0836\n",
      "Epoch: 6250, Train Loss: 0.4973, Test Loss: 3.6338\n",
      "Epoch: 6251, Train Loss: 0.3943, Test Loss: 4.0742\n",
      "Epoch: 6252, Train Loss: 0.4251, Test Loss: 3.8395\n",
      "Epoch: 6253, Train Loss: 0.4619, Test Loss: 3.2917\n",
      "Epoch: 6254, Train Loss: 0.4800, Test Loss: 3.0633\n",
      "Epoch: 6255, Train Loss: 0.5001, Test Loss: 3.4405\n",
      "Epoch: 6256, Train Loss: 0.4518, Test Loss: 3.8442\n",
      "Epoch: 6257, Train Loss: 0.4365, Test Loss: 3.7988\n",
      "Epoch: 6258, Train Loss: 0.4390, Test Loss: 3.3846\n",
      "Epoch: 6259, Train Loss: 0.4285, Test Loss: 3.4104\n",
      "Epoch: 6260, Train Loss: 0.4415, Test Loss: 3.9533\n",
      "Epoch: 6261, Train Loss: 0.4565, Test Loss: 3.8339\n",
      "Epoch: 6262, Train Loss: 0.3941, Test Loss: 3.4847\n",
      "Epoch: 6263, Train Loss: 0.3999, Test Loss: 3.2855\n",
      "Epoch: 6264, Train Loss: 0.4428, Test Loss: 3.6497\n",
      "Epoch: 6265, Train Loss: 0.4485, Test Loss: 3.6727\n",
      "Epoch: 6266, Train Loss: 0.4173, Test Loss: 3.3941\n",
      "Epoch: 6267, Train Loss: 0.4146, Test Loss: 3.3737\n",
      "Epoch: 6268, Train Loss: 0.4120, Test Loss: 3.4814\n",
      "Epoch: 6269, Train Loss: 0.4269, Test Loss: 3.6646\n",
      "Epoch: 6270, Train Loss: 0.3949, Test Loss: 3.5800\n",
      "Epoch: 6271, Train Loss: 0.4046, Test Loss: 3.3376\n",
      "Epoch: 6272, Train Loss: 0.4645, Test Loss: 3.5545\n",
      "Epoch: 6273, Train Loss: 0.3804, Test Loss: 4.0118\n",
      "Epoch: 6274, Train Loss: 0.4102, Test Loss: 3.9568\n",
      "Epoch: 6275, Train Loss: 0.4447, Test Loss: 3.3466\n",
      "Epoch: 6276, Train Loss: 0.4042, Test Loss: 3.0550\n",
      "Epoch: 6277, Train Loss: 0.4770, Test Loss: 3.4829\n",
      "Epoch: 6278, Train Loss: 0.4142, Test Loss: 4.0140\n",
      "Epoch: 6279, Train Loss: 0.5006, Test Loss: 3.5398\n",
      "Epoch: 6280, Train Loss: 0.3984, Test Loss: 3.2224\n",
      "Epoch: 6281, Train Loss: 0.4080, Test Loss: 3.2534\n",
      "Epoch: 6282, Train Loss: 0.4374, Test Loss: 3.6552\n",
      "Epoch: 6283, Train Loss: 0.3954, Test Loss: 3.8825\n",
      "Epoch: 6284, Train Loss: 0.4698, Test Loss: 3.3935\n",
      "Epoch: 6285, Train Loss: 0.4260, Test Loss: 3.1433\n",
      "Epoch: 6286, Train Loss: 0.4552, Test Loss: 3.2630\n",
      "Epoch: 6287, Train Loss: 0.4240, Test Loss: 3.6214\n",
      "Epoch: 6288, Train Loss: 0.4147, Test Loss: 3.5815\n",
      "Epoch: 6289, Train Loss: 0.4259, Test Loss: 3.4738\n",
      "Epoch: 6290, Train Loss: 0.4496, Test Loss: 3.2327\n",
      "Epoch: 6291, Train Loss: 0.4352, Test Loss: 3.4893\n",
      "Epoch: 6292, Train Loss: 0.4201, Test Loss: 3.7431\n",
      "Epoch: 6293, Train Loss: 0.4620, Test Loss: 3.3623\n",
      "Epoch: 6294, Train Loss: 0.3979, Test Loss: 3.4016\n",
      "Epoch: 6295, Train Loss: 0.4231, Test Loss: 3.4670\n",
      "Epoch: 6296, Train Loss: 0.4391, Test Loss: 3.4394\n",
      "Epoch: 6297, Train Loss: 0.4111, Test Loss: 3.6135\n",
      "Epoch: 6298, Train Loss: 0.4411, Test Loss: 3.5480\n",
      "Epoch: 6299, Train Loss: 0.4321, Test Loss: 3.2020\n",
      "Epoch: 6300, Train Loss: 0.4590, Test Loss: 3.3365\n",
      "Epoch: 6301, Train Loss: 0.4576, Test Loss: 3.8820\n",
      "Epoch: 6302, Train Loss: 0.4633, Test Loss: 3.8714\n",
      "Epoch: 6303, Train Loss: 0.4309, Test Loss: 3.4726\n",
      "Epoch: 6304, Train Loss: 0.4060, Test Loss: 3.2860\n",
      "Epoch: 6305, Train Loss: 0.4417, Test Loss: 3.7742\n",
      "Epoch: 6306, Train Loss: 0.4838, Test Loss: 3.7997\n",
      "Epoch: 6307, Train Loss: 0.3926, Test Loss: 3.5457\n",
      "Epoch: 6308, Train Loss: 0.4174, Test Loss: 3.1278\n",
      "Epoch: 6309, Train Loss: 0.4671, Test Loss: 3.2902\n",
      "Epoch: 6310, Train Loss: 0.4303, Test Loss: 3.5571\n",
      "Epoch: 6311, Train Loss: 0.4129, Test Loss: 3.7510\n",
      "Epoch: 6312, Train Loss: 0.3890, Test Loss: 3.7499\n",
      "Epoch: 6313, Train Loss: 0.4233, Test Loss: 3.4396\n",
      "Epoch: 6314, Train Loss: 0.4237, Test Loss: 3.2686\n",
      "Epoch: 6315, Train Loss: 0.4298, Test Loss: 3.2923\n",
      "Epoch: 6316, Train Loss: 0.4459, Test Loss: 3.6412\n",
      "Epoch: 6317, Train Loss: 0.4347, Test Loss: 4.1127\n",
      "Epoch: 6318, Train Loss: 0.4580, Test Loss: 3.7629\n",
      "Epoch: 6319, Train Loss: 0.4388, Test Loss: 3.4883\n",
      "Epoch: 6320, Train Loss: 0.4143, Test Loss: 3.2398\n",
      "Epoch: 6321, Train Loss: 0.4633, Test Loss: 3.5077\n",
      "Epoch: 6322, Train Loss: 0.3671, Test Loss: 3.7515\n",
      "Epoch: 6323, Train Loss: 0.4465, Test Loss: 3.6109\n",
      "Epoch: 6324, Train Loss: 0.4114, Test Loss: 3.3539\n",
      "Epoch: 6325, Train Loss: 0.4218, Test Loss: 3.3172\n",
      "Epoch: 6326, Train Loss: 0.4268, Test Loss: 3.8217\n",
      "Epoch: 6327, Train Loss: 0.4277, Test Loss: 3.9690\n",
      "Epoch: 6328, Train Loss: 0.4596, Test Loss: 3.3258\n",
      "Epoch: 6329, Train Loss: 0.4198, Test Loss: 3.0448\n",
      "Epoch: 6330, Train Loss: 0.4619, Test Loss: 3.1313\n",
      "Epoch: 6331, Train Loss: 0.4758, Test Loss: 3.8144\n",
      "Epoch: 6332, Train Loss: 0.4789, Test Loss: 3.6146\n",
      "Epoch: 6333, Train Loss: 0.4242, Test Loss: 3.2467\n",
      "Epoch: 6334, Train Loss: 0.4412, Test Loss: 3.4274\n",
      "Epoch: 6335, Train Loss: 0.4345, Test Loss: 3.8753\n",
      "Epoch: 6336, Train Loss: 0.4596, Test Loss: 3.7194\n",
      "Epoch: 6337, Train Loss: 0.3903, Test Loss: 3.3464\n",
      "Epoch: 6338, Train Loss: 0.3951, Test Loss: 3.3322\n",
      "Epoch: 6339, Train Loss: 0.4332, Test Loss: 3.6612\n",
      "Epoch: 6340, Train Loss: 0.4064, Test Loss: 3.7100\n",
      "Epoch: 6341, Train Loss: 0.4199, Test Loss: 3.3657\n",
      "Epoch: 6342, Train Loss: 0.4013, Test Loss: 3.3128\n",
      "Epoch: 6343, Train Loss: 0.3901, Test Loss: 3.5723\n",
      "Epoch: 6344, Train Loss: 0.4045, Test Loss: 3.5332\n",
      "Epoch: 6345, Train Loss: 0.3790, Test Loss: 3.3912\n",
      "Epoch: 6346, Train Loss: 0.3885, Test Loss: 3.3741\n",
      "Epoch: 6347, Train Loss: 0.4163, Test Loss: 3.6385\n",
      "Epoch: 6348, Train Loss: 0.4470, Test Loss: 3.5091\n",
      "Epoch: 6349, Train Loss: 0.4036, Test Loss: 3.4176\n",
      "Epoch: 6350, Train Loss: 0.3784, Test Loss: 3.3144\n",
      "Epoch: 6351, Train Loss: 0.4080, Test Loss: 3.3852\n",
      "Epoch: 6352, Train Loss: 0.4092, Test Loss: 3.6629\n",
      "Epoch: 6353, Train Loss: 0.4158, Test Loss: 3.7068\n",
      "Epoch: 6354, Train Loss: 0.3781, Test Loss: 3.6348\n",
      "Epoch: 6355, Train Loss: 0.4445, Test Loss: 3.4944\n",
      "Epoch: 6356, Train Loss: 0.3844, Test Loss: 3.4742\n",
      "Epoch: 6357, Train Loss: 0.4075, Test Loss: 3.6277\n",
      "Epoch: 6358, Train Loss: 0.3999, Test Loss: 3.7051\n",
      "Epoch: 6359, Train Loss: 0.3976, Test Loss: 3.4578\n",
      "Epoch: 6360, Train Loss: 0.3938, Test Loss: 3.4440\n",
      "Epoch: 6361, Train Loss: 0.4042, Test Loss: 3.6300\n",
      "Epoch: 6362, Train Loss: 0.3928, Test Loss: 3.7161\n",
      "Epoch: 6363, Train Loss: 0.3668, Test Loss: 3.6876\n",
      "Epoch: 6364, Train Loss: 0.4111, Test Loss: 3.4054\n",
      "Epoch: 6365, Train Loss: 0.4325, Test Loss: 3.8176\n",
      "Epoch: 6366, Train Loss: 0.4323, Test Loss: 3.9116\n",
      "Epoch: 6367, Train Loss: 0.4133, Test Loss: 3.6276\n",
      "Epoch: 6368, Train Loss: 0.3880, Test Loss: 3.5965\n",
      "Epoch: 6369, Train Loss: 0.4084, Test Loss: 3.5469\n",
      "Epoch: 6370, Train Loss: 0.3850, Test Loss: 3.6005\n",
      "Epoch: 6371, Train Loss: 0.3984, Test Loss: 3.6924\n",
      "Epoch: 6372, Train Loss: 0.4404, Test Loss: 4.2368\n",
      "Epoch: 6373, Train Loss: 0.5246, Test Loss: 3.5175\n",
      "Epoch: 6374, Train Loss: 0.4277, Test Loss: 3.1585\n",
      "Epoch: 6375, Train Loss: 0.4621, Test Loss: 3.6068\n",
      "Epoch: 6376, Train Loss: 0.3917, Test Loss: 3.9725\n",
      "Epoch: 6377, Train Loss: 0.4460, Test Loss: 3.6442\n",
      "Epoch: 6378, Train Loss: 0.4189, Test Loss: 3.1960\n",
      "Epoch: 6379, Train Loss: 0.4325, Test Loss: 3.3814\n",
      "Epoch: 6380, Train Loss: 0.3951, Test Loss: 3.8618\n",
      "Epoch: 6381, Train Loss: 0.4347, Test Loss: 3.8161\n",
      "Epoch: 6382, Train Loss: 0.4469, Test Loss: 3.2546\n",
      "Epoch: 6383, Train Loss: 0.4460, Test Loss: 3.3032\n",
      "Epoch: 6384, Train Loss: 0.4362, Test Loss: 3.9669\n",
      "Epoch: 6385, Train Loss: 0.4268, Test Loss: 4.1516\n",
      "Epoch: 6386, Train Loss: 0.5415, Test Loss: 3.2826\n",
      "Epoch: 6387, Train Loss: 0.4775, Test Loss: 2.9834\n",
      "Epoch: 6388, Train Loss: 0.5614, Test Loss: 3.5512\n",
      "Epoch: 6389, Train Loss: 0.4126, Test Loss: 4.2740\n",
      "Epoch: 6390, Train Loss: 0.4774, Test Loss: 3.9235\n",
      "Epoch: 6391, Train Loss: 0.4623, Test Loss: 3.0297\n",
      "Epoch: 6392, Train Loss: 0.4814, Test Loss: 3.0073\n",
      "Epoch: 6393, Train Loss: 0.5321, Test Loss: 4.0195\n",
      "Epoch: 6394, Train Loss: 0.4435, Test Loss: 4.3582\n",
      "Epoch: 6395, Train Loss: 0.5658, Test Loss: 3.3274\n",
      "Epoch: 6396, Train Loss: 0.3955, Test Loss: 2.8961\n",
      "Epoch: 6397, Train Loss: 0.5886, Test Loss: 3.3681\n",
      "Epoch: 6398, Train Loss: 0.4157, Test Loss: 4.2292\n",
      "Epoch: 6399, Train Loss: 0.5168, Test Loss: 3.9627\n",
      "Epoch: 6400, Train Loss: 0.5149, Test Loss: 3.0643\n",
      "Epoch: 6401, Train Loss: 0.4928, Test Loss: 2.9121\n",
      "Epoch: 6402, Train Loss: 0.6509, Test Loss: 3.7183\n",
      "Epoch: 6403, Train Loss: 0.3854, Test Loss: 4.4921\n",
      "Epoch: 6404, Train Loss: 0.6049, Test Loss: 3.5986\n",
      "Epoch: 6405, Train Loss: 0.3987, Test Loss: 2.8785\n",
      "Epoch: 6406, Train Loss: 0.6123, Test Loss: 2.9932\n",
      "Epoch: 6407, Train Loss: 0.4913, Test Loss: 4.0253\n",
      "Epoch: 6408, Train Loss: 0.5145, Test Loss: 4.1846\n",
      "Epoch: 6409, Train Loss: 0.6243, Test Loss: 3.0828\n",
      "Epoch: 6410, Train Loss: 0.4147, Test Loss: 2.8347\n",
      "Epoch: 6411, Train Loss: 0.6083, Test Loss: 3.1247\n",
      "Epoch: 6412, Train Loss: 0.4172, Test Loss: 3.6007\n",
      "Epoch: 6413, Train Loss: 0.4103, Test Loss: 3.6691\n",
      "Epoch: 6414, Train Loss: 0.4219, Test Loss: 3.2570\n",
      "Epoch: 6415, Train Loss: 0.3938, Test Loss: 2.9523\n",
      "Epoch: 6416, Train Loss: 0.4751, Test Loss: 3.1967\n",
      "Epoch: 6417, Train Loss: 0.4602, Test Loss: 3.7726\n",
      "Epoch: 6418, Train Loss: 0.4751, Test Loss: 3.6626\n",
      "Epoch: 6419, Train Loss: 0.4115, Test Loss: 3.2150\n",
      "Epoch: 6420, Train Loss: 0.4336, Test Loss: 3.0421\n",
      "Epoch: 6421, Train Loss: 0.4691, Test Loss: 3.4419\n",
      "Epoch: 6422, Train Loss: 0.4135, Test Loss: 3.9144\n",
      "Epoch: 6423, Train Loss: 0.5042, Test Loss: 3.4938\n",
      "Epoch: 6424, Train Loss: 0.4450, Test Loss: 2.9646\n",
      "Epoch: 6425, Train Loss: 0.5046, Test Loss: 2.9755\n",
      "Epoch: 6426, Train Loss: 0.5299, Test Loss: 3.6775\n",
      "Epoch: 6427, Train Loss: 0.5026, Test Loss: 3.7303\n",
      "Epoch: 6428, Train Loss: 0.4661, Test Loss: 3.4014\n",
      "Epoch: 6429, Train Loss: 0.3781, Test Loss: 3.1659\n",
      "Epoch: 6430, Train Loss: 0.3977, Test Loss: 3.1725\n",
      "Epoch: 6431, Train Loss: 0.4421, Test Loss: 3.4122\n",
      "Epoch: 6432, Train Loss: 0.4359, Test Loss: 3.5783\n",
      "Epoch: 6433, Train Loss: 0.4732, Test Loss: 3.2631\n",
      "Epoch: 6434, Train Loss: 0.4025, Test Loss: 3.0135\n",
      "Epoch: 6435, Train Loss: 0.4519, Test Loss: 3.2039\n",
      "Epoch: 6436, Train Loss: 0.4312, Test Loss: 3.5207\n",
      "Epoch: 6437, Train Loss: 0.4096, Test Loss: 3.7884\n",
      "Epoch: 6438, Train Loss: 0.4342, Test Loss: 3.4810\n",
      "Epoch: 6439, Train Loss: 0.4343, Test Loss: 3.0748\n",
      "Epoch: 6440, Train Loss: 0.4865, Test Loss: 3.2293\n",
      "Epoch: 6441, Train Loss: 0.4027, Test Loss: 3.5870\n",
      "Epoch: 6442, Train Loss: 0.4359, Test Loss: 3.4325\n",
      "Epoch: 6443, Train Loss: 0.3836, Test Loss: 3.2226\n",
      "Epoch: 6444, Train Loss: 0.4073, Test Loss: 3.2880\n",
      "Epoch: 6445, Train Loss: 0.4417, Test Loss: 3.2553\n",
      "Epoch: 6446, Train Loss: 0.4063, Test Loss: 3.1594\n",
      "Epoch: 6447, Train Loss: 0.4151, Test Loss: 3.2597\n",
      "Epoch: 6448, Train Loss: 0.4435, Test Loss: 3.6105\n",
      "Epoch: 6449, Train Loss: 0.4394, Test Loss: 3.3935\n",
      "Epoch: 6450, Train Loss: 0.3918, Test Loss: 3.1719\n",
      "Epoch: 6451, Train Loss: 0.4371, Test Loss: 3.3033\n",
      "Epoch: 6452, Train Loss: 0.4035, Test Loss: 3.5186\n",
      "Epoch: 6453, Train Loss: 0.4242, Test Loss: 3.4866\n",
      "Epoch: 6454, Train Loss: 0.4471, Test Loss: 3.0900\n",
      "Epoch: 6455, Train Loss: 0.4117, Test Loss: 3.1134\n",
      "Epoch: 6456, Train Loss: 0.4095, Test Loss: 3.4183\n",
      "Epoch: 6457, Train Loss: 0.4262, Test Loss: 3.6227\n",
      "Epoch: 6458, Train Loss: 0.4073, Test Loss: 3.6343\n",
      "Epoch: 6459, Train Loss: 0.4531, Test Loss: 3.3658\n",
      "Epoch: 6460, Train Loss: 0.3706, Test Loss: 3.2525\n",
      "Epoch: 6461, Train Loss: 0.3984, Test Loss: 3.2909\n",
      "Epoch: 6462, Train Loss: 0.3885, Test Loss: 3.5258\n",
      "Epoch: 6463, Train Loss: 0.4082, Test Loss: 3.6789\n",
      "Epoch: 6464, Train Loss: 0.4500, Test Loss: 3.2212\n",
      "Epoch: 6465, Train Loss: 0.4502, Test Loss: 3.2594\n",
      "Epoch: 6466, Train Loss: 0.3980, Test Loss: 3.3110\n",
      "Epoch: 6467, Train Loss: 0.4055, Test Loss: 3.4461\n",
      "Epoch: 6468, Train Loss: 0.3933, Test Loss: 3.4657\n",
      "Epoch: 6469, Train Loss: 0.4139, Test Loss: 3.5118\n",
      "Epoch: 6470, Train Loss: 0.4047, Test Loss: 3.3573\n",
      "Epoch: 6471, Train Loss: 0.3966, Test Loss: 3.3258\n",
      "Epoch: 6472, Train Loss: 0.4091, Test Loss: 3.5370\n",
      "Epoch: 6473, Train Loss: 0.4173, Test Loss: 3.6635\n",
      "Epoch: 6474, Train Loss: 0.4643, Test Loss: 3.2442\n",
      "Epoch: 6475, Train Loss: 0.4383, Test Loss: 3.1195\n",
      "Epoch: 6476, Train Loss: 0.4430, Test Loss: 3.6302\n",
      "Epoch: 6477, Train Loss: 0.4066, Test Loss: 3.9822\n",
      "Epoch: 6478, Train Loss: 0.5071, Test Loss: 3.5632\n",
      "Epoch: 6479, Train Loss: 0.4385, Test Loss: 3.2181\n",
      "Epoch: 6480, Train Loss: 0.4722, Test Loss: 3.6265\n",
      "Epoch: 6481, Train Loss: 0.4276, Test Loss: 3.7872\n",
      "Epoch: 6482, Train Loss: 0.4166, Test Loss: 3.5953\n",
      "Epoch: 6483, Train Loss: 0.4436, Test Loss: 3.1541\n",
      "Epoch: 6484, Train Loss: 0.4542, Test Loss: 3.1711\n",
      "Epoch: 6485, Train Loss: 0.4375, Test Loss: 4.0259\n",
      "Epoch: 6486, Train Loss: 0.5732, Test Loss: 3.6779\n",
      "Epoch: 6487, Train Loss: 0.4275, Test Loss: 3.0926\n",
      "Epoch: 6488, Train Loss: 0.4336, Test Loss: 3.0546\n",
      "Epoch: 6489, Train Loss: 0.4979, Test Loss: 3.4228\n",
      "Epoch: 6490, Train Loss: 0.4019, Test Loss: 3.8699\n",
      "Epoch: 6491, Train Loss: 0.4698, Test Loss: 3.8314\n",
      "Epoch: 6492, Train Loss: 0.5183, Test Loss: 3.2103\n",
      "Epoch: 6493, Train Loss: 0.4079, Test Loss: 3.0538\n",
      "Epoch: 6494, Train Loss: 0.4548, Test Loss: 3.4962\n",
      "Epoch: 6495, Train Loss: 0.4591, Test Loss: 3.6985\n",
      "Epoch: 6496, Train Loss: 0.4537, Test Loss: 3.4183\n",
      "Epoch: 6497, Train Loss: 0.4088, Test Loss: 3.2827\n",
      "Epoch: 6498, Train Loss: 0.4486, Test Loss: 3.4841\n",
      "Epoch: 6499, Train Loss: 0.4166, Test Loss: 3.6601\n",
      "Epoch: 6500, Train Loss: 0.4063, Test Loss: 3.8100\n",
      "Epoch: 6501, Train Loss: 0.4173, Test Loss: 3.7183\n",
      "Epoch: 6502, Train Loss: 0.4401, Test Loss: 3.2749\n",
      "Epoch: 6503, Train Loss: 0.4022, Test Loss: 3.0780\n",
      "Epoch: 6504, Train Loss: 0.4610, Test Loss: 3.4705\n",
      "Epoch: 6505, Train Loss: 0.4185, Test Loss: 3.9634\n",
      "Epoch: 6506, Train Loss: 0.4845, Test Loss: 3.4878\n",
      "Epoch: 6507, Train Loss: 0.3807, Test Loss: 3.1509\n",
      "Epoch: 6508, Train Loss: 0.5002, Test Loss: 3.5778\n",
      "Epoch: 6509, Train Loss: 0.4326, Test Loss: 3.7278\n",
      "Epoch: 6510, Train Loss: 0.4352, Test Loss: 3.2666\n",
      "Epoch: 6511, Train Loss: 0.3996, Test Loss: 3.1119\n",
      "Epoch: 6512, Train Loss: 0.4127, Test Loss: 3.2293\n",
      "Epoch: 6513, Train Loss: 0.4329, Test Loss: 3.7242\n",
      "Epoch: 6514, Train Loss: 0.4772, Test Loss: 3.5109\n",
      "Epoch: 6515, Train Loss: 0.4079, Test Loss: 3.2151\n",
      "Epoch: 6516, Train Loss: 0.4455, Test Loss: 3.3370\n",
      "Epoch: 6517, Train Loss: 0.4268, Test Loss: 3.7604\n",
      "Epoch: 6518, Train Loss: 0.4716, Test Loss: 3.5318\n",
      "Epoch: 6519, Train Loss: 0.4115, Test Loss: 3.2237\n",
      "Epoch: 6520, Train Loss: 0.4127, Test Loss: 3.2070\n",
      "Epoch: 6521, Train Loss: 0.4096, Test Loss: 3.4796\n",
      "Epoch: 6522, Train Loss: 0.3931, Test Loss: 3.5858\n",
      "Epoch: 6523, Train Loss: 0.4152, Test Loss: 3.6963\n",
      "Epoch: 6524, Train Loss: 0.4251, Test Loss: 3.3202\n",
      "Epoch: 6525, Train Loss: 0.4141, Test Loss: 3.1268\n",
      "Epoch: 6526, Train Loss: 0.4331, Test Loss: 3.2191\n",
      "Epoch: 6527, Train Loss: 0.3971, Test Loss: 3.5344\n",
      "Epoch: 6528, Train Loss: 0.3817, Test Loss: 3.6906\n",
      "Epoch: 6529, Train Loss: 0.4318, Test Loss: 3.4740\n",
      "Epoch: 6530, Train Loss: 0.4001, Test Loss: 3.3337\n",
      "Epoch: 6531, Train Loss: 0.3837, Test Loss: 3.1612\n",
      "Epoch: 6532, Train Loss: 0.4049, Test Loss: 3.2492\n",
      "Epoch: 6533, Train Loss: 0.4193, Test Loss: 3.2149\n",
      "Epoch: 6534, Train Loss: 0.4114, Test Loss: 3.3931\n",
      "Epoch: 6535, Train Loss: 0.4079, Test Loss: 3.5482\n",
      "Epoch: 6536, Train Loss: 0.4994, Test Loss: 3.3612\n",
      "Epoch: 6537, Train Loss: 0.4098, Test Loss: 3.3695\n",
      "Epoch: 6538, Train Loss: 0.3826, Test Loss: 3.4721\n",
      "Epoch: 6539, Train Loss: 0.3852, Test Loss: 3.4279\n",
      "Epoch: 6540, Train Loss: 0.3863, Test Loss: 3.4328\n",
      "Epoch: 6541, Train Loss: 0.4144, Test Loss: 3.2859\n",
      "Epoch: 6542, Train Loss: 0.4226, Test Loss: 3.2364\n",
      "Epoch: 6543, Train Loss: 0.4217, Test Loss: 3.4347\n",
      "Epoch: 6544, Train Loss: 0.4048, Test Loss: 3.6832\n",
      "Epoch: 6545, Train Loss: 0.3937, Test Loss: 3.5035\n",
      "Epoch: 6546, Train Loss: 0.4012, Test Loss: 3.3368\n",
      "Epoch: 6547, Train Loss: 0.3834, Test Loss: 3.2056\n",
      "Epoch: 6548, Train Loss: 0.4242, Test Loss: 3.3002\n",
      "Epoch: 6549, Train Loss: 0.4156, Test Loss: 3.5194\n",
      "Epoch: 6550, Train Loss: 0.3895, Test Loss: 3.6708\n",
      "Epoch: 6551, Train Loss: 0.3838, Test Loss: 3.4141\n",
      "Epoch: 6552, Train Loss: 0.3961, Test Loss: 3.3007\n",
      "Epoch: 6553, Train Loss: 0.3791, Test Loss: 3.2565\n",
      "Epoch: 6554, Train Loss: 0.4378, Test Loss: 3.4495\n",
      "Epoch: 6555, Train Loss: 0.3963, Test Loss: 3.7901\n",
      "Epoch: 6556, Train Loss: 0.4060, Test Loss: 3.6786\n",
      "Epoch: 6557, Train Loss: 0.4202, Test Loss: 3.1013\n",
      "Epoch: 6558, Train Loss: 0.4246, Test Loss: 3.0421\n",
      "Epoch: 6559, Train Loss: 0.4315, Test Loss: 3.5778\n",
      "Epoch: 6560, Train Loss: 0.4177, Test Loss: 4.1214\n",
      "Epoch: 6561, Train Loss: 0.4985, Test Loss: 3.4825\n",
      "Epoch: 6562, Train Loss: 0.4133, Test Loss: 3.1217\n",
      "Epoch: 6563, Train Loss: 0.4325, Test Loss: 3.1964\n",
      "Epoch: 6564, Train Loss: 0.4048, Test Loss: 3.7853\n",
      "Epoch: 6565, Train Loss: 0.4729, Test Loss: 3.6283\n",
      "Epoch: 6566, Train Loss: 0.4287, Test Loss: 3.0850\n",
      "Epoch: 6567, Train Loss: 0.4377, Test Loss: 3.1334\n",
      "Epoch: 6568, Train Loss: 0.3763, Test Loss: 3.4690\n",
      "Epoch: 6569, Train Loss: 0.3996, Test Loss: 3.7012\n",
      "Epoch: 6570, Train Loss: 0.3948, Test Loss: 3.7175\n",
      "Epoch: 6571, Train Loss: 0.4248, Test Loss: 3.1983\n",
      "Epoch: 6572, Train Loss: 0.4101, Test Loss: 3.0952\n",
      "Epoch: 6573, Train Loss: 0.4197, Test Loss: 3.4966\n",
      "Epoch: 6574, Train Loss: 0.3917, Test Loss: 3.6360\n",
      "Epoch: 6575, Train Loss: 0.4014, Test Loss: 3.5344\n",
      "Epoch: 6576, Train Loss: 0.3980, Test Loss: 3.2751\n",
      "Epoch: 6577, Train Loss: 0.3877, Test Loss: 3.0962\n",
      "Epoch: 6578, Train Loss: 0.4094, Test Loss: 3.2266\n",
      "Epoch: 6579, Train Loss: 0.4166, Test Loss: 3.5084\n",
      "Epoch: 6580, Train Loss: 0.3998, Test Loss: 3.6050\n",
      "Epoch: 6581, Train Loss: 0.4232, Test Loss: 3.2861\n",
      "Epoch: 6582, Train Loss: 0.4026, Test Loss: 3.1149\n",
      "Epoch: 6583, Train Loss: 0.3911, Test Loss: 3.1709\n",
      "Epoch: 6584, Train Loss: 0.4013, Test Loss: 3.4945\n",
      "Epoch: 6585, Train Loss: 0.4498, Test Loss: 3.3530\n",
      "Epoch: 6586, Train Loss: 0.3986, Test Loss: 3.2013\n",
      "Epoch: 6587, Train Loss: 0.3822, Test Loss: 3.2531\n",
      "Epoch: 6588, Train Loss: 0.3986, Test Loss: 3.5156\n",
      "Epoch: 6589, Train Loss: 0.4001, Test Loss: 3.4128\n",
      "Epoch: 6590, Train Loss: 0.3847, Test Loss: 3.1473\n",
      "Epoch: 6591, Train Loss: 0.4241, Test Loss: 3.2427\n",
      "Epoch: 6592, Train Loss: 0.4225, Test Loss: 3.7006\n",
      "Epoch: 6593, Train Loss: 0.4082, Test Loss: 3.7923\n",
      "Epoch: 6594, Train Loss: 0.4776, Test Loss: 3.1731\n",
      "Epoch: 6595, Train Loss: 0.3900, Test Loss: 3.0597\n",
      "Epoch: 6596, Train Loss: 0.3973, Test Loss: 3.2909\n",
      "Epoch: 6597, Train Loss: 0.3854, Test Loss: 3.7976\n",
      "Epoch: 6598, Train Loss: 0.4853, Test Loss: 3.4556\n",
      "Epoch: 6599, Train Loss: 0.4002, Test Loss: 3.1766\n",
      "Epoch: 6600, Train Loss: 0.4299, Test Loss: 3.2640\n",
      "Epoch: 6601, Train Loss: 0.4255, Test Loss: 3.8664\n",
      "Epoch: 6602, Train Loss: 0.4905, Test Loss: 3.4481\n",
      "Epoch: 6603, Train Loss: 0.3965, Test Loss: 2.9336\n",
      "Epoch: 6604, Train Loss: 0.5358, Test Loss: 3.1789\n",
      "Epoch: 6605, Train Loss: 0.4348, Test Loss: 3.9849\n",
      "Epoch: 6606, Train Loss: 0.4963, Test Loss: 3.8475\n",
      "Epoch: 6607, Train Loss: 0.4320, Test Loss: 3.1164\n",
      "Epoch: 6608, Train Loss: 0.4068, Test Loss: 2.9851\n",
      "Epoch: 6609, Train Loss: 0.4852, Test Loss: 3.6029\n",
      "Epoch: 6610, Train Loss: 0.3920, Test Loss: 4.0170\n",
      "Epoch: 6611, Train Loss: 0.4413, Test Loss: 3.5997\n",
      "Epoch: 6612, Train Loss: 0.3935, Test Loss: 3.0018\n",
      "Epoch: 6613, Train Loss: 0.4876, Test Loss: 3.1267\n",
      "Epoch: 6614, Train Loss: 0.4040, Test Loss: 3.5463\n",
      "Epoch: 6615, Train Loss: 0.4142, Test Loss: 3.7063\n",
      "Epoch: 6616, Train Loss: 0.4065, Test Loss: 3.4168\n",
      "Epoch: 6617, Train Loss: 0.4006, Test Loss: 3.1745\n",
      "Epoch: 6618, Train Loss: 0.3990, Test Loss: 3.2143\n",
      "Epoch: 6619, Train Loss: 0.4011, Test Loss: 3.4816\n",
      "Epoch: 6620, Train Loss: 0.3803, Test Loss: 3.6169\n",
      "Epoch: 6621, Train Loss: 0.3805, Test Loss: 3.5349\n",
      "Epoch: 6622, Train Loss: 0.3796, Test Loss: 3.2455\n",
      "Epoch: 6623, Train Loss: 0.3688, Test Loss: 3.1703\n",
      "Epoch: 6624, Train Loss: 0.3980, Test Loss: 3.2613\n",
      "Epoch: 6625, Train Loss: 0.3975, Test Loss: 3.6206\n",
      "Epoch: 6626, Train Loss: 0.4065, Test Loss: 3.6511\n",
      "Epoch: 6627, Train Loss: 0.3841, Test Loss: 3.2197\n",
      "Epoch: 6628, Train Loss: 0.3967, Test Loss: 3.1180\n",
      "Epoch: 6629, Train Loss: 0.3858, Test Loss: 3.3099\n",
      "Epoch: 6630, Train Loss: 0.4039, Test Loss: 3.4299\n",
      "Epoch: 6631, Train Loss: 0.4199, Test Loss: 3.2051\n",
      "Epoch: 6632, Train Loss: 0.4193, Test Loss: 3.2852\n",
      "Epoch: 6633, Train Loss: 0.3845, Test Loss: 3.3780\n",
      "Epoch: 6634, Train Loss: 0.4321, Test Loss: 3.2715\n",
      "Epoch: 6635, Train Loss: 0.3898, Test Loss: 3.1765\n",
      "Epoch: 6636, Train Loss: 0.4099, Test Loss: 3.3820\n",
      "Epoch: 6637, Train Loss: 0.3888, Test Loss: 3.3853\n",
      "Epoch: 6638, Train Loss: 0.3888, Test Loss: 3.3304\n",
      "Epoch: 6639, Train Loss: 0.3900, Test Loss: 3.3233\n",
      "Epoch: 6640, Train Loss: 0.4234, Test Loss: 3.2914\n",
      "Epoch: 6641, Train Loss: 0.4054, Test Loss: 3.1503\n",
      "Epoch: 6642, Train Loss: 0.4131, Test Loss: 3.3332\n",
      "Epoch: 6643, Train Loss: 0.3929, Test Loss: 3.5931\n",
      "Epoch: 6644, Train Loss: 0.3781, Test Loss: 3.5610\n",
      "Epoch: 6645, Train Loss: 0.3879, Test Loss: 3.4431\n",
      "Epoch: 6646, Train Loss: 0.3989, Test Loss: 3.3582\n",
      "Epoch: 6647, Train Loss: 0.3615, Test Loss: 3.2662\n",
      "Epoch: 6648, Train Loss: 0.4105, Test Loss: 3.3893\n",
      "Epoch: 6649, Train Loss: 0.3945, Test Loss: 3.4575\n",
      "Epoch: 6650, Train Loss: 0.3876, Test Loss: 3.3281\n",
      "Epoch: 6651, Train Loss: 0.3890, Test Loss: 3.0474\n",
      "Epoch: 6652, Train Loss: 0.4026, Test Loss: 2.9880\n",
      "Epoch: 6653, Train Loss: 0.4342, Test Loss: 3.5739\n",
      "Epoch: 6654, Train Loss: 0.4303, Test Loss: 3.8489\n",
      "Epoch: 6655, Train Loss: 0.4270, Test Loss: 3.3993\n",
      "Epoch: 6656, Train Loss: 0.3979, Test Loss: 2.9541\n",
      "Epoch: 6657, Train Loss: 0.5474, Test Loss: 3.3727\n",
      "Epoch: 6658, Train Loss: 0.3939, Test Loss: 3.8173\n",
      "Epoch: 6659, Train Loss: 0.4594, Test Loss: 3.6028\n",
      "Epoch: 6660, Train Loss: 0.4294, Test Loss: 3.0044\n",
      "Epoch: 6661, Train Loss: 0.4657, Test Loss: 3.0910\n",
      "Epoch: 6662, Train Loss: 0.4430, Test Loss: 3.6571\n",
      "Epoch: 6663, Train Loss: 0.4137, Test Loss: 3.8037\n",
      "Epoch: 6664, Train Loss: 0.4365, Test Loss: 3.5032\n",
      "Epoch: 6665, Train Loss: 0.3848, Test Loss: 3.1660\n",
      "Epoch: 6666, Train Loss: 0.4046, Test Loss: 3.0155\n",
      "Epoch: 6667, Train Loss: 0.4727, Test Loss: 3.3728\n",
      "Epoch: 6668, Train Loss: 0.4032, Test Loss: 3.8101\n",
      "Epoch: 6669, Train Loss: 0.4571, Test Loss: 3.5532\n",
      "Epoch: 6670, Train Loss: 0.4071, Test Loss: 3.0344\n",
      "Epoch: 6671, Train Loss: 0.4399, Test Loss: 3.0799\n",
      "Epoch: 6672, Train Loss: 0.4492, Test Loss: 3.4473\n",
      "Epoch: 6673, Train Loss: 0.4232, Test Loss: 3.9863\n",
      "Epoch: 6674, Train Loss: 0.4658, Test Loss: 3.4775\n",
      "Epoch: 6675, Train Loss: 0.4157, Test Loss: 3.1139\n",
      "Epoch: 6676, Train Loss: 0.4338, Test Loss: 3.2464\n",
      "Epoch: 6677, Train Loss: 0.3915, Test Loss: 3.5863\n",
      "Epoch: 6678, Train Loss: 0.4189, Test Loss: 3.5524\n",
      "Epoch: 6679, Train Loss: 0.3955, Test Loss: 3.3297\n",
      "Epoch: 6680, Train Loss: 0.4279, Test Loss: 3.1949\n",
      "Epoch: 6681, Train Loss: 0.4527, Test Loss: 3.5868\n",
      "Epoch: 6682, Train Loss: 0.4435, Test Loss: 3.5302\n",
      "Epoch: 6683, Train Loss: 0.4113, Test Loss: 3.3843\n",
      "Epoch: 6684, Train Loss: 0.3831, Test Loss: 3.2031\n",
      "Epoch: 6685, Train Loss: 0.4215, Test Loss: 3.3850\n",
      "Epoch: 6686, Train Loss: 0.3913, Test Loss: 3.7615\n",
      "Epoch: 6687, Train Loss: 0.4289, Test Loss: 3.4303\n",
      "Epoch: 6688, Train Loss: 0.3944, Test Loss: 3.2449\n",
      "Epoch: 6689, Train Loss: 0.4364, Test Loss: 3.4468\n",
      "Epoch: 6690, Train Loss: 0.3826, Test Loss: 3.5986\n",
      "Epoch: 6691, Train Loss: 0.3864, Test Loss: 3.4349\n",
      "Epoch: 6692, Train Loss: 0.3691, Test Loss: 3.2382\n",
      "Epoch: 6693, Train Loss: 0.3849, Test Loss: 3.2415\n",
      "Epoch: 6694, Train Loss: 0.4216, Test Loss: 3.5263\n",
      "Epoch: 6695, Train Loss: 0.3967, Test Loss: 4.0870\n",
      "Epoch: 6696, Train Loss: 0.4146, Test Loss: 3.8774\n",
      "Epoch: 6697, Train Loss: 0.4307, Test Loss: 3.1924\n",
      "Epoch: 6698, Train Loss: 0.4138, Test Loss: 3.1645\n",
      "Epoch: 6699, Train Loss: 0.4122, Test Loss: 3.6275\n",
      "Epoch: 6700, Train Loss: 0.3819, Test Loss: 3.9720\n",
      "Epoch: 6701, Train Loss: 0.5705, Test Loss: 3.1328\n",
      "Epoch: 6702, Train Loss: 0.3971, Test Loss: 2.8937\n",
      "Epoch: 6703, Train Loss: 0.4597, Test Loss: 3.1128\n",
      "Epoch: 6704, Train Loss: 0.4960, Test Loss: 3.9796\n",
      "Epoch: 6705, Train Loss: 0.4825, Test Loss: 3.9490\n",
      "Epoch: 6706, Train Loss: 0.5141, Test Loss: 3.0704\n",
      "Epoch: 6707, Train Loss: 0.4500, Test Loss: 2.8737\n",
      "Epoch: 6708, Train Loss: 0.5383, Test Loss: 3.3373\n",
      "Epoch: 6709, Train Loss: 0.4112, Test Loss: 4.0168\n",
      "Epoch: 6710, Train Loss: 0.5338, Test Loss: 3.5146\n",
      "Epoch: 6711, Train Loss: 0.4160, Test Loss: 2.9805\n",
      "Epoch: 6712, Train Loss: 0.4436, Test Loss: 2.9783\n",
      "Epoch: 6713, Train Loss: 0.4492, Test Loss: 3.4648\n",
      "Epoch: 6714, Train Loss: 0.4040, Test Loss: 3.6787\n",
      "Epoch: 6715, Train Loss: 0.4423, Test Loss: 3.3119\n",
      "Epoch: 6716, Train Loss: 0.4036, Test Loss: 3.1209\n",
      "Epoch: 6717, Train Loss: 0.4209, Test Loss: 3.1406\n",
      "Epoch: 6718, Train Loss: 0.4234, Test Loss: 3.5631\n",
      "Epoch: 6719, Train Loss: 0.4012, Test Loss: 3.5747\n",
      "Epoch: 6720, Train Loss: 0.4075, Test Loss: 3.5103\n",
      "Epoch: 6721, Train Loss: 0.4242, Test Loss: 3.4338\n",
      "Epoch: 6722, Train Loss: 0.4105, Test Loss: 3.3777\n",
      "Epoch: 6723, Train Loss: 0.3730, Test Loss: 3.2633\n",
      "Epoch: 6724, Train Loss: 0.4084, Test Loss: 3.3714\n",
      "Epoch: 6725, Train Loss: 0.4013, Test Loss: 3.6201\n",
      "Epoch: 6726, Train Loss: 0.3993, Test Loss: 3.5541\n",
      "Epoch: 6727, Train Loss: 0.4214, Test Loss: 3.2462\n",
      "Epoch: 6728, Train Loss: 0.4043, Test Loss: 3.1962\n",
      "Epoch: 6729, Train Loss: 0.4059, Test Loss: 3.3989\n",
      "Epoch: 6730, Train Loss: 0.3819, Test Loss: 3.9521\n",
      "Epoch: 6731, Train Loss: 0.4323, Test Loss: 4.0399\n",
      "Epoch: 6732, Train Loss: 0.4617, Test Loss: 3.3603\n",
      "Epoch: 6733, Train Loss: 0.3866, Test Loss: 3.0617\n",
      "Epoch: 6734, Train Loss: 0.4311, Test Loss: 3.1316\n",
      "Epoch: 6735, Train Loss: 0.4335, Test Loss: 3.6606\n",
      "Epoch: 6736, Train Loss: 0.4631, Test Loss: 3.6558\n",
      "Epoch: 6737, Train Loss: 0.4401, Test Loss: 3.2066\n",
      "Epoch: 6738, Train Loss: 0.3736, Test Loss: 3.0480\n",
      "Epoch: 6739, Train Loss: 0.4247, Test Loss: 3.3818\n",
      "Epoch: 6740, Train Loss: 0.3684, Test Loss: 3.7358\n",
      "Epoch: 6741, Train Loss: 0.4332, Test Loss: 3.3867\n",
      "Epoch: 6742, Train Loss: 0.3911, Test Loss: 3.1386\n",
      "Epoch: 6743, Train Loss: 0.4217, Test Loss: 3.4229\n",
      "Epoch: 6744, Train Loss: 0.3784, Test Loss: 3.5841\n",
      "Epoch: 6745, Train Loss: 0.3823, Test Loss: 3.7121\n",
      "Epoch: 6746, Train Loss: 0.4176, Test Loss: 3.3456\n",
      "Epoch: 6747, Train Loss: 0.4150, Test Loss: 3.0829\n",
      "Epoch: 6748, Train Loss: 0.4355, Test Loss: 3.4578\n",
      "Epoch: 6749, Train Loss: 0.3678, Test Loss: 3.7550\n",
      "Epoch: 6750, Train Loss: 0.4681, Test Loss: 3.3234\n",
      "Epoch: 6751, Train Loss: 0.4327, Test Loss: 3.0356\n",
      "Epoch: 6752, Train Loss: 0.4470, Test Loss: 3.2716\n",
      "Epoch: 6753, Train Loss: 0.3978, Test Loss: 3.4699\n",
      "Epoch: 6754, Train Loss: 0.3924, Test Loss: 3.4960\n",
      "Epoch: 6755, Train Loss: 0.3774, Test Loss: 3.5027\n",
      "Epoch: 6756, Train Loss: 0.4164, Test Loss: 3.1298\n",
      "Epoch: 6757, Train Loss: 0.3865, Test Loss: 3.0418\n",
      "Epoch: 6758, Train Loss: 0.4480, Test Loss: 3.5459\n",
      "Epoch: 6759, Train Loss: 0.4400, Test Loss: 4.0353\n",
      "Epoch: 6760, Train Loss: 0.5054, Test Loss: 3.4163\n",
      "Epoch: 6761, Train Loss: 0.3845, Test Loss: 3.0122\n",
      "Epoch: 6762, Train Loss: 0.4522, Test Loss: 3.3032\n",
      "Epoch: 6763, Train Loss: 0.4527, Test Loss: 3.8555\n",
      "Epoch: 6764, Train Loss: 0.4544, Test Loss: 3.5385\n",
      "Epoch: 6765, Train Loss: 0.4778, Test Loss: 3.0687\n",
      "Epoch: 6766, Train Loss: 0.4499, Test Loss: 2.9892\n",
      "Epoch: 6767, Train Loss: 0.4425, Test Loss: 3.4896\n",
      "Epoch: 6768, Train Loss: 0.3903, Test Loss: 3.7609\n",
      "Epoch: 6769, Train Loss: 0.4386, Test Loss: 3.3932\n",
      "Epoch: 6770, Train Loss: 0.4266, Test Loss: 3.2929\n",
      "Epoch: 6771, Train Loss: 0.4224, Test Loss: 3.3949\n",
      "Epoch: 6772, Train Loss: 0.4177, Test Loss: 3.5233\n",
      "Epoch: 6773, Train Loss: 0.3881, Test Loss: 3.7849\n",
      "Epoch: 6774, Train Loss: 0.4246, Test Loss: 3.4706\n",
      "Epoch: 6775, Train Loss: 0.4351, Test Loss: 2.9985\n",
      "Epoch: 6776, Train Loss: 0.4925, Test Loss: 3.1573\n",
      "Epoch: 6777, Train Loss: 0.3855, Test Loss: 3.5426\n",
      "Epoch: 6778, Train Loss: 0.4549, Test Loss: 3.4653\n",
      "Epoch: 6779, Train Loss: 0.4265, Test Loss: 3.0774\n",
      "Epoch: 6780, Train Loss: 0.4492, Test Loss: 3.2292\n",
      "Epoch: 6781, Train Loss: 0.4190, Test Loss: 3.6088\n",
      "Epoch: 6782, Train Loss: 0.4117, Test Loss: 3.7354\n",
      "Epoch: 6783, Train Loss: 0.4360, Test Loss: 3.3077\n",
      "Epoch: 6784, Train Loss: 0.4117, Test Loss: 3.2384\n",
      "Epoch: 6785, Train Loss: 0.4282, Test Loss: 3.6468\n",
      "Epoch: 6786, Train Loss: 0.4070, Test Loss: 3.6416\n",
      "Epoch: 6787, Train Loss: 0.4150, Test Loss: 3.2726\n",
      "Epoch: 6788, Train Loss: 0.3756, Test Loss: 3.0822\n",
      "Epoch: 6789, Train Loss: 0.4338, Test Loss: 3.4616\n",
      "Epoch: 6790, Train Loss: 0.4245, Test Loss: 3.8551\n",
      "Epoch: 6791, Train Loss: 0.4443, Test Loss: 3.4869\n",
      "Epoch: 6792, Train Loss: 0.3927, Test Loss: 3.0921\n",
      "Epoch: 6793, Train Loss: 0.3971, Test Loss: 3.0825\n",
      "Epoch: 6794, Train Loss: 0.4529, Test Loss: 3.6526\n",
      "Epoch: 6795, Train Loss: 0.3875, Test Loss: 4.1395\n",
      "Epoch: 6796, Train Loss: 0.5135, Test Loss: 3.3656\n",
      "Epoch: 6797, Train Loss: 0.3833, Test Loss: 2.9350\n",
      "Epoch: 6798, Train Loss: 0.5310, Test Loss: 3.3299\n",
      "Epoch: 6799, Train Loss: 0.3961, Test Loss: 3.8462\n",
      "Epoch: 6800, Train Loss: 0.4499, Test Loss: 3.4220\n",
      "Epoch: 6801, Train Loss: 0.4210, Test Loss: 2.8538\n",
      "Epoch: 6802, Train Loss: 0.4727, Test Loss: 2.9394\n",
      "Epoch: 6803, Train Loss: 0.4535, Test Loss: 3.6420\n",
      "Epoch: 6804, Train Loss: 0.4420, Test Loss: 3.8760\n",
      "Epoch: 6805, Train Loss: 0.5113, Test Loss: 3.1094\n",
      "Epoch: 6806, Train Loss: 0.4105, Test Loss: 2.8375\n",
      "Epoch: 6807, Train Loss: 0.4832, Test Loss: 3.0404\n",
      "Epoch: 6808, Train Loss: 0.3825, Test Loss: 3.6382\n",
      "Epoch: 6809, Train Loss: 0.4226, Test Loss: 3.7861\n",
      "Epoch: 6810, Train Loss: 0.4673, Test Loss: 3.1053\n",
      "Epoch: 6811, Train Loss: 0.3924, Test Loss: 2.8446\n",
      "Epoch: 6812, Train Loss: 0.4440, Test Loss: 2.9524\n",
      "Epoch: 6813, Train Loss: 0.4025, Test Loss: 3.3408\n",
      "Epoch: 6814, Train Loss: 0.3853, Test Loss: 3.7203\n",
      "Epoch: 6815, Train Loss: 0.4400, Test Loss: 3.3139\n",
      "Epoch: 6816, Train Loss: 0.3673, Test Loss: 2.9741\n",
      "Epoch: 6817, Train Loss: 0.4022, Test Loss: 3.0101\n",
      "Epoch: 6818, Train Loss: 0.4136, Test Loss: 3.5008\n",
      "Epoch: 6819, Train Loss: 0.4482, Test Loss: 3.3136\n",
      "Epoch: 6820, Train Loss: 0.4093, Test Loss: 3.0020\n",
      "Epoch: 6821, Train Loss: 0.4873, Test Loss: 3.2874\n",
      "Epoch: 6822, Train Loss: 0.4161, Test Loss: 3.9404\n",
      "Epoch: 6823, Train Loss: 0.4846, Test Loss: 3.5996\n",
      "Epoch: 6824, Train Loss: 0.4411, Test Loss: 2.9410\n",
      "Epoch: 6825, Train Loss: 0.4547, Test Loss: 2.8673\n",
      "Epoch: 6826, Train Loss: 0.5143, Test Loss: 3.0323\n",
      "Epoch: 6827, Train Loss: 0.3933, Test Loss: 3.4673\n",
      "Epoch: 6828, Train Loss: 0.4293, Test Loss: 3.4394\n",
      "Epoch: 6829, Train Loss: 0.3623, Test Loss: 3.1674\n",
      "Epoch: 6830, Train Loss: 0.4024, Test Loss: 3.1014\n",
      "Epoch: 6831, Train Loss: 0.4514, Test Loss: 3.3951\n",
      "Epoch: 6832, Train Loss: 0.3724, Test Loss: 3.5678\n",
      "Epoch: 6833, Train Loss: 0.4273, Test Loss: 3.1881\n",
      "Epoch: 6834, Train Loss: 0.4092, Test Loss: 3.0532\n",
      "Epoch: 6835, Train Loss: 0.4383, Test Loss: 3.3660\n",
      "Epoch: 6836, Train Loss: 0.3968, Test Loss: 3.8347\n",
      "Epoch: 6837, Train Loss: 0.4221, Test Loss: 3.6281\n",
      "Epoch: 6838, Train Loss: 0.4062, Test Loss: 3.0981\n",
      "Epoch: 6839, Train Loss: 0.4197, Test Loss: 3.0520\n",
      "Epoch: 6840, Train Loss: 0.3905, Test Loss: 3.2872\n",
      "Epoch: 6841, Train Loss: 0.3919, Test Loss: 3.6032\n",
      "Epoch: 6842, Train Loss: 0.4396, Test Loss: 3.2349\n",
      "Epoch: 6843, Train Loss: 0.3948, Test Loss: 3.0620\n",
      "Epoch: 6844, Train Loss: 0.4138, Test Loss: 3.2934\n",
      "Epoch: 6845, Train Loss: 0.3696, Test Loss: 3.4705\n",
      "Epoch: 6846, Train Loss: 0.3980, Test Loss: 3.5532\n",
      "Epoch: 6847, Train Loss: 0.3889, Test Loss: 3.3639\n",
      "Epoch: 6848, Train Loss: 0.3836, Test Loss: 3.1267\n",
      "Epoch: 6849, Train Loss: 0.3951, Test Loss: 3.0880\n",
      "Epoch: 6850, Train Loss: 0.4221, Test Loss: 3.5402\n",
      "Epoch: 6851, Train Loss: 0.4018, Test Loss: 3.6951\n",
      "Epoch: 6852, Train Loss: 0.4255, Test Loss: 3.2365\n",
      "Epoch: 6853, Train Loss: 0.3749, Test Loss: 2.9401\n",
      "Epoch: 6854, Train Loss: 0.4542, Test Loss: 3.3494\n",
      "Epoch: 6855, Train Loss: 0.3847, Test Loss: 3.8039\n",
      "Epoch: 6856, Train Loss: 0.4965, Test Loss: 3.3262\n",
      "Epoch: 6857, Train Loss: 0.3910, Test Loss: 3.0008\n",
      "Epoch: 6858, Train Loss: 0.4375, Test Loss: 3.3819\n",
      "Epoch: 6859, Train Loss: 0.3751, Test Loss: 3.5734\n",
      "Epoch: 6860, Train Loss: 0.3914, Test Loss: 3.5258\n",
      "Epoch: 6861, Train Loss: 0.3821, Test Loss: 3.2195\n",
      "Epoch: 6862, Train Loss: 0.4076, Test Loss: 3.0829\n",
      "Epoch: 6863, Train Loss: 0.3778, Test Loss: 3.2326\n",
      "Epoch: 6864, Train Loss: 0.3943, Test Loss: 3.5939\n",
      "Epoch: 6865, Train Loss: 0.3979, Test Loss: 3.4866\n",
      "Epoch: 6866, Train Loss: 0.4449, Test Loss: 3.0496\n",
      "Epoch: 6867, Train Loss: 0.4444, Test Loss: 3.1311\n",
      "Epoch: 6868, Train Loss: 0.3779, Test Loss: 3.4496\n",
      "Epoch: 6869, Train Loss: 0.4158, Test Loss: 3.6713\n",
      "Epoch: 6870, Train Loss: 0.4636, Test Loss: 3.2342\n",
      "Epoch: 6871, Train Loss: 0.3792, Test Loss: 2.9724\n",
      "Epoch: 6872, Train Loss: 0.4544, Test Loss: 3.2241\n",
      "Epoch: 6873, Train Loss: 0.3917, Test Loss: 3.9666\n",
      "Epoch: 6874, Train Loss: 0.4521, Test Loss: 3.9949\n",
      "Epoch: 6875, Train Loss: 0.4941, Test Loss: 3.0599\n",
      "Epoch: 6876, Train Loss: 0.4133, Test Loss: 2.8012\n",
      "Epoch: 6877, Train Loss: 0.5316, Test Loss: 3.3004\n",
      "Epoch: 6878, Train Loss: 0.3882, Test Loss: 3.8182\n",
      "Epoch: 6879, Train Loss: 0.4864, Test Loss: 3.4787\n",
      "Epoch: 6880, Train Loss: 0.4213, Test Loss: 2.9441\n",
      "Epoch: 6881, Train Loss: 0.4735, Test Loss: 3.1704\n",
      "Epoch: 6882, Train Loss: 0.3810, Test Loss: 3.8306\n",
      "Epoch: 6883, Train Loss: 0.4672, Test Loss: 3.8342\n",
      "Epoch: 6884, Train Loss: 0.4893, Test Loss: 3.0244\n",
      "Epoch: 6885, Train Loss: 0.4472, Test Loss: 2.9647\n",
      "Epoch: 6886, Train Loss: 0.4277, Test Loss: 3.3201\n",
      "Epoch: 6887, Train Loss: 0.3779, Test Loss: 3.8832\n",
      "Epoch: 6888, Train Loss: 0.5399, Test Loss: 3.3593\n",
      "Epoch: 6889, Train Loss: 0.3599, Test Loss: 2.9404\n",
      "Epoch: 6890, Train Loss: 0.4909, Test Loss: 3.1566\n",
      "Epoch: 6891, Train Loss: 0.3755, Test Loss: 3.6150\n",
      "Epoch: 6892, Train Loss: 0.4272, Test Loss: 3.6382\n",
      "Epoch: 6893, Train Loss: 0.4120, Test Loss: 3.1853\n",
      "Epoch: 6894, Train Loss: 0.4142, Test Loss: 3.1221\n",
      "Epoch: 6895, Train Loss: 0.4078, Test Loss: 3.3258\n",
      "Epoch: 6896, Train Loss: 0.3943, Test Loss: 3.4101\n",
      "Epoch: 6897, Train Loss: 0.3903, Test Loss: 3.1570\n",
      "Epoch: 6898, Train Loss: 0.3859, Test Loss: 3.0859\n",
      "Epoch: 6899, Train Loss: 0.4110, Test Loss: 3.2486\n",
      "Epoch: 6900, Train Loss: 0.3969, Test Loss: 3.6673\n",
      "Epoch: 6901, Train Loss: 0.4113, Test Loss: 3.5783\n",
      "Epoch: 6902, Train Loss: 0.3942, Test Loss: 3.1627\n",
      "Epoch: 6903, Train Loss: 0.3660, Test Loss: 2.9159\n",
      "Epoch: 6904, Train Loss: 0.4061, Test Loss: 3.1049\n",
      "Epoch: 6905, Train Loss: 0.3974, Test Loss: 3.7860\n",
      "Epoch: 6906, Train Loss: 0.4625, Test Loss: 3.5799\n",
      "Epoch: 6907, Train Loss: 0.3641, Test Loss: 3.2031\n",
      "Epoch: 6908, Train Loss: 0.4445, Test Loss: 3.4422\n",
      "Epoch: 6909, Train Loss: 0.4051, Test Loss: 3.4125\n",
      "Epoch: 6910, Train Loss: 0.3969, Test Loss: 3.2712\n",
      "Epoch: 6911, Train Loss: 0.4048, Test Loss: 3.4747\n",
      "Epoch: 6912, Train Loss: 0.3796, Test Loss: 3.4951\n",
      "Epoch: 6913, Train Loss: 0.3995, Test Loss: 3.2911\n",
      "Epoch: 6914, Train Loss: 0.4085, Test Loss: 3.1031\n",
      "Epoch: 6915, Train Loss: 0.4357, Test Loss: 3.3791\n",
      "Epoch: 6916, Train Loss: 0.3833, Test Loss: 3.8116\n",
      "Epoch: 6917, Train Loss: 0.4467, Test Loss: 3.5408\n",
      "Epoch: 6918, Train Loss: 0.3995, Test Loss: 3.2428\n",
      "Epoch: 6919, Train Loss: 0.3994, Test Loss: 3.1740\n",
      "Epoch: 6920, Train Loss: 0.3640, Test Loss: 3.3564\n",
      "Epoch: 6921, Train Loss: 0.3966, Test Loss: 3.5682\n",
      "Epoch: 6922, Train Loss: 0.3637, Test Loss: 3.5678\n",
      "Epoch: 6923, Train Loss: 0.4315, Test Loss: 3.1334\n",
      "Epoch: 6924, Train Loss: 0.3799, Test Loss: 3.0008\n",
      "Epoch: 6925, Train Loss: 0.4088, Test Loss: 3.3256\n",
      "Epoch: 6926, Train Loss: 0.3555, Test Loss: 3.7435\n",
      "Epoch: 6927, Train Loss: 0.4340, Test Loss: 3.4351\n",
      "Epoch: 6928, Train Loss: 0.3625, Test Loss: 3.1369\n",
      "Epoch: 6929, Train Loss: 0.4225, Test Loss: 3.2585\n",
      "Epoch: 6930, Train Loss: 0.3930, Test Loss: 3.6642\n",
      "Epoch: 6931, Train Loss: 0.4054, Test Loss: 3.6317\n",
      "Epoch: 6932, Train Loss: 0.4336, Test Loss: 3.1818\n",
      "Epoch: 6933, Train Loss: 0.3815, Test Loss: 3.0681\n",
      "Epoch: 6934, Train Loss: 0.4284, Test Loss: 3.3308\n",
      "Epoch: 6935, Train Loss: 0.3987, Test Loss: 3.6784\n",
      "Epoch: 6936, Train Loss: 0.4379, Test Loss: 3.3762\n",
      "Epoch: 6937, Train Loss: 0.3760, Test Loss: 3.0481\n",
      "Epoch: 6938, Train Loss: 0.4012, Test Loss: 2.9480\n",
      "Epoch: 6939, Train Loss: 0.4566, Test Loss: 3.4005\n",
      "Epoch: 6940, Train Loss: 0.3998, Test Loss: 3.7702\n",
      "Epoch: 6941, Train Loss: 0.4618, Test Loss: 3.2298\n",
      "Epoch: 6942, Train Loss: 0.3614, Test Loss: 2.9223\n",
      "Epoch: 6943, Train Loss: 0.5852, Test Loss: 3.5992\n",
      "Epoch: 6944, Train Loss: 0.3890, Test Loss: 4.2934\n",
      "Epoch: 6945, Train Loss: 0.5452, Test Loss: 3.5028\n",
      "Epoch: 6946, Train Loss: 0.3997, Test Loss: 2.8824\n",
      "Epoch: 6947, Train Loss: 0.4325, Test Loss: 2.8371\n",
      "Epoch: 6948, Train Loss: 0.4821, Test Loss: 3.4624\n",
      "Epoch: 6949, Train Loss: 0.4210, Test Loss: 3.9739\n",
      "Epoch: 6950, Train Loss: 0.5931, Test Loss: 3.1639\n",
      "Epoch: 6951, Train Loss: 0.4138, Test Loss: 2.8016\n",
      "Epoch: 6952, Train Loss: 0.5680, Test Loss: 3.1312\n",
      "Epoch: 6953, Train Loss: 0.3815, Test Loss: 3.6580\n",
      "Epoch: 6954, Train Loss: 0.4766, Test Loss: 3.5611\n",
      "Epoch: 6955, Train Loss: 0.4796, Test Loss: 2.9855\n",
      "Epoch: 6956, Train Loss: 0.3939, Test Loss: 2.8117\n",
      "Epoch: 6957, Train Loss: 0.5085, Test Loss: 3.2420\n",
      "Epoch: 6958, Train Loss: 0.4205, Test Loss: 4.1820\n",
      "Epoch: 6959, Train Loss: 0.5596, Test Loss: 3.7083\n",
      "Epoch: 6960, Train Loss: 0.5073, Test Loss: 2.7874\n",
      "Epoch: 6961, Train Loss: 0.5024, Test Loss: 2.7243\n",
      "Epoch: 6962, Train Loss: 0.5945, Test Loss: 3.2496\n",
      "Epoch: 6963, Train Loss: 0.3933, Test Loss: 4.3061\n",
      "Epoch: 6964, Train Loss: 0.7524, Test Loss: 3.4453\n",
      "Epoch: 6965, Train Loss: 0.4074, Test Loss: 2.8446\n",
      "Epoch: 6966, Train Loss: 0.5541, Test Loss: 2.8497\n",
      "Epoch: 6967, Train Loss: 0.4620, Test Loss: 3.3702\n",
      "Epoch: 6968, Train Loss: 0.4158, Test Loss: 3.7806\n",
      "Epoch: 6969, Train Loss: 0.5092, Test Loss: 3.3217\n",
      "Epoch: 6970, Train Loss: 0.3946, Test Loss: 2.8675\n",
      "Epoch: 6971, Train Loss: 0.3893, Test Loss: 2.7380\n",
      "Epoch: 6972, Train Loss: 0.5730, Test Loss: 3.2561\n",
      "Epoch: 6973, Train Loss: 0.3733, Test Loss: 3.8119\n",
      "Epoch: 6974, Train Loss: 0.4682, Test Loss: 3.5152\n",
      "Epoch: 6975, Train Loss: 0.4198, Test Loss: 2.9060\n",
      "Epoch: 6976, Train Loss: 0.4312, Test Loss: 2.8901\n",
      "Epoch: 6977, Train Loss: 0.4998, Test Loss: 3.5204\n",
      "Epoch: 6978, Train Loss: 0.4167, Test Loss: 3.8232\n",
      "Epoch: 6979, Train Loss: 0.5115, Test Loss: 3.2450\n",
      "Epoch: 6980, Train Loss: 0.3989, Test Loss: 2.8627\n",
      "Epoch: 6981, Train Loss: 0.5105, Test Loss: 2.9682\n",
      "Epoch: 6982, Train Loss: 0.3901, Test Loss: 3.2639\n",
      "Epoch: 6983, Train Loss: 0.3944, Test Loss: 3.4692\n",
      "Epoch: 6984, Train Loss: 0.4607, Test Loss: 3.1240\n",
      "Epoch: 6985, Train Loss: 0.4256, Test Loss: 2.9955\n",
      "Epoch: 6986, Train Loss: 0.4331, Test Loss: 2.8908\n",
      "Epoch: 6987, Train Loss: 0.4339, Test Loss: 3.1263\n",
      "Epoch: 6988, Train Loss: 0.4042, Test Loss: 3.7179\n",
      "Epoch: 6989, Train Loss: 0.4926, Test Loss: 3.6049\n",
      "Epoch: 6990, Train Loss: 0.4403, Test Loss: 3.0794\n",
      "Epoch: 6991, Train Loss: 0.3919, Test Loss: 2.8625\n",
      "Epoch: 6992, Train Loss: 0.4960, Test Loss: 3.1414\n",
      "Epoch: 6993, Train Loss: 0.3702, Test Loss: 3.5068\n",
      "Epoch: 6994, Train Loss: 0.4426, Test Loss: 3.3286\n",
      "Epoch: 6995, Train Loss: 0.3897, Test Loss: 3.1671\n",
      "Epoch: 6996, Train Loss: 0.3813, Test Loss: 3.2772\n",
      "Epoch: 6997, Train Loss: 0.4110, Test Loss: 3.2891\n",
      "Epoch: 6998, Train Loss: 0.4455, Test Loss: 3.1349\n",
      "Epoch: 6999, Train Loss: 0.3609, Test Loss: 3.0959\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_graph=data_graph.to(device)\n",
    "data_graph_test=data_graph_test.to(device)\n",
    "model = GraphNetwork(num_features=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data_graph)\n",
    "    loss = criterion(out, data_graph.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data_graph_test)\n",
    "        test_loss = criterion(out, data_graph_test.y).item()\n",
    "    return test_loss\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(7000):  # Número de épocas\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 23.4643, Test Loss: 12.2964\n",
      "Epoch: 1, Train Loss: 12.1058, Test Loss: 15.6045\n",
      "Epoch: 2, Train Loss: 16.0948, Test Loss: 14.0782\n",
      "Epoch: 3, Train Loss: 14.2273, Test Loss: 11.0065\n",
      "Epoch: 4, Train Loss: 10.9557, Test Loss: 9.6320\n",
      "Epoch: 5, Train Loss: 9.4200, Test Loss: 9.4145\n",
      "Epoch: 6, Train Loss: 9.3847, Test Loss: 9.4375\n",
      "Epoch: 7, Train Loss: 9.6868, Test Loss: 9.3600\n",
      "Epoch: 8, Train Loss: 8.7764, Test Loss: 8.9768\n",
      "Epoch: 9, Train Loss: 8.7951, Test Loss: 8.2709\n",
      "Epoch: 10, Train Loss: 8.1806, Test Loss: 7.5600\n",
      "Epoch: 11, Train Loss: 7.2242, Test Loss: 7.4075\n",
      "Epoch: 12, Train Loss: 6.7311, Test Loss: 7.6866\n",
      "Epoch: 13, Train Loss: 7.7026, Test Loss: 7.8031\n",
      "Epoch: 14, Train Loss: 7.2618, Test Loss: 7.5470\n",
      "Epoch: 15, Train Loss: 7.2686, Test Loss: 7.1344\n",
      "Epoch: 16, Train Loss: 6.8351, Test Loss: 6.8314\n",
      "Epoch: 17, Train Loss: 6.1225, Test Loss: 6.9413\n",
      "Epoch: 18, Train Loss: 6.2308, Test Loss: 7.3317\n",
      "Epoch: 19, Train Loss: 6.6308, Test Loss: 7.3921\n",
      "Epoch: 20, Train Loss: 6.7648, Test Loss: 7.0407\n",
      "Epoch: 21, Train Loss: 6.4167, Test Loss: 6.6109\n",
      "Epoch: 22, Train Loss: 6.0300, Test Loss: 6.3859\n",
      "Epoch: 23, Train Loss: 6.0779, Test Loss: 6.3928\n",
      "Epoch: 24, Train Loss: 5.9783, Test Loss: 6.4987\n",
      "Epoch: 25, Train Loss: 6.5772, Test Loss: 6.4546\n",
      "Epoch: 26, Train Loss: 6.4936, Test Loss: 6.2841\n",
      "Epoch: 27, Train Loss: 6.1451, Test Loss: 6.1871\n",
      "Epoch: 28, Train Loss: 5.5404, Test Loss: 6.2507\n",
      "Epoch: 29, Train Loss: 5.9839, Test Loss: 6.3349\n",
      "Epoch: 30, Train Loss: 5.5992, Test Loss: 6.3350\n",
      "Epoch: 31, Train Loss: 5.8310, Test Loss: 6.2602\n",
      "Epoch: 32, Train Loss: 5.4430, Test Loss: 6.1234\n",
      "Epoch: 33, Train Loss: 5.6803, Test Loss: 5.9828\n",
      "Epoch: 34, Train Loss: 5.3748, Test Loss: 5.9373\n",
      "Epoch: 35, Train Loss: 5.3804, Test Loss: 5.9441\n",
      "Epoch: 36, Train Loss: 5.2735, Test Loss: 5.9176\n",
      "Epoch: 37, Train Loss: 5.4732, Test Loss: 5.8824\n",
      "Epoch: 38, Train Loss: 4.9673, Test Loss: 5.9048\n",
      "Epoch: 39, Train Loss: 5.2886, Test Loss: 5.9293\n",
      "Epoch: 40, Train Loss: 5.3738, Test Loss: 5.9490\n",
      "Epoch: 41, Train Loss: 5.5266, Test Loss: 5.9765\n",
      "Epoch: 42, Train Loss: 5.3906, Test Loss: 5.9408\n",
      "Epoch: 43, Train Loss: 5.2379, Test Loss: 5.8912\n",
      "Epoch: 44, Train Loss: 5.2726, Test Loss: 5.8717\n",
      "Epoch: 45, Train Loss: 5.3949, Test Loss: 5.8286\n",
      "Epoch: 46, Train Loss: 5.0609, Test Loss: 5.8013\n",
      "Epoch: 47, Train Loss: 5.0801, Test Loss: 5.8277\n",
      "Epoch: 48, Train Loss: 5.0488, Test Loss: 5.8087\n",
      "Epoch: 49, Train Loss: 5.0947, Test Loss: 5.9123\n",
      "Epoch: 50, Train Loss: 4.9983, Test Loss: 6.0221\n",
      "Epoch: 51, Train Loss: 4.9649, Test Loss: 5.9783\n",
      "Epoch: 52, Train Loss: 4.9415, Test Loss: 5.9463\n",
      "Epoch: 53, Train Loss: 5.1541, Test Loss: 5.9154\n",
      "Epoch: 54, Train Loss: 4.8234, Test Loss: 5.8627\n",
      "Epoch: 55, Train Loss: 4.9097, Test Loss: 5.9129\n",
      "Epoch: 56, Train Loss: 4.6949, Test Loss: 5.8790\n",
      "Epoch: 57, Train Loss: 5.0739, Test Loss: 5.8144\n",
      "Epoch: 58, Train Loss: 4.7241, Test Loss: 5.8077\n",
      "Epoch: 59, Train Loss: 4.7810, Test Loss: 5.8340\n",
      "Epoch: 60, Train Loss: 4.8184, Test Loss: 5.9144\n",
      "Epoch: 61, Train Loss: 4.6794, Test Loss: 6.0040\n",
      "Epoch: 62, Train Loss: 4.7654, Test Loss: 5.8169\n",
      "Epoch: 63, Train Loss: 4.8417, Test Loss: 5.6359\n",
      "Epoch: 64, Train Loss: 4.8201, Test Loss: 5.6142\n",
      "Epoch: 65, Train Loss: 4.7810, Test Loss: 5.6918\n",
      "Epoch: 66, Train Loss: 4.7890, Test Loss: 5.8031\n",
      "Epoch: 67, Train Loss: 4.5577, Test Loss: 5.7455\n",
      "Epoch: 68, Train Loss: 4.4369, Test Loss: 5.5970\n",
      "Epoch: 69, Train Loss: 4.8349, Test Loss: 5.5943\n",
      "Epoch: 70, Train Loss: 4.7547, Test Loss: 5.7227\n",
      "Epoch: 71, Train Loss: 4.4911, Test Loss: 5.8045\n",
      "Epoch: 72, Train Loss: 4.3981, Test Loss: 5.7516\n",
      "Epoch: 73, Train Loss: 4.7739, Test Loss: 5.5526\n",
      "Epoch: 74, Train Loss: 4.2231, Test Loss: 5.4712\n",
      "Epoch: 75, Train Loss: 4.8397, Test Loss: 5.5588\n",
      "Epoch: 76, Train Loss: 4.4338, Test Loss: 5.8175\n",
      "Epoch: 77, Train Loss: 4.3380, Test Loss: 5.7744\n",
      "Epoch: 78, Train Loss: 4.4369, Test Loss: 5.5406\n",
      "Epoch: 79, Train Loss: 4.2931, Test Loss: 5.4783\n",
      "Epoch: 80, Train Loss: 4.4866, Test Loss: 5.5652\n",
      "Epoch: 81, Train Loss: 4.3934, Test Loss: 5.6381\n",
      "Epoch: 82, Train Loss: 4.2613, Test Loss: 5.4764\n",
      "Epoch: 83, Train Loss: 4.2100, Test Loss: 5.4028\n",
      "Epoch: 84, Train Loss: 4.1891, Test Loss: 5.5153\n",
      "Epoch: 85, Train Loss: 4.3421, Test Loss: 5.6530\n",
      "Epoch: 86, Train Loss: 4.1039, Test Loss: 5.6909\n",
      "Epoch: 87, Train Loss: 4.3148, Test Loss: 5.5316\n",
      "Epoch: 88, Train Loss: 4.1288, Test Loss: 5.4606\n",
      "Epoch: 89, Train Loss: 4.4640, Test Loss: 5.5355\n",
      "Epoch: 90, Train Loss: 4.3016, Test Loss: 5.8203\n",
      "Epoch: 91, Train Loss: 4.2768, Test Loss: 5.8267\n",
      "Epoch: 92, Train Loss: 4.2904, Test Loss: 5.5532\n",
      "Epoch: 93, Train Loss: 4.1790, Test Loss: 5.4199\n",
      "Epoch: 94, Train Loss: 4.1407, Test Loss: 5.5952\n",
      "Epoch: 95, Train Loss: 4.0633, Test Loss: 5.5799\n",
      "Epoch: 96, Train Loss: 4.1455, Test Loss: 5.6235\n",
      "Epoch: 97, Train Loss: 3.9585, Test Loss: 5.8674\n",
      "Epoch: 98, Train Loss: 4.1713, Test Loss: 5.9885\n",
      "Epoch: 99, Train Loss: 4.3603, Test Loss: 5.4383\n",
      "Epoch: 100, Train Loss: 4.4600, Test Loss: 5.4105\n",
      "Epoch: 101, Train Loss: 4.0823, Test Loss: 5.8671\n",
      "Epoch: 102, Train Loss: 3.9667, Test Loss: 5.8068\n",
      "Epoch: 103, Train Loss: 4.3350, Test Loss: 5.4682\n",
      "Epoch: 104, Train Loss: 3.9787, Test Loss: 5.2934\n",
      "Epoch: 105, Train Loss: 4.1612, Test Loss: 5.6540\n",
      "Epoch: 106, Train Loss: 3.9191, Test Loss: 5.9346\n",
      "Epoch: 107, Train Loss: 4.0901, Test Loss: 5.6279\n",
      "Epoch: 108, Train Loss: 3.9594, Test Loss: 5.4399\n",
      "Epoch: 109, Train Loss: 3.8433, Test Loss: 5.6142\n",
      "Epoch: 110, Train Loss: 3.9159, Test Loss: 5.6219\n",
      "Epoch: 111, Train Loss: 4.0849, Test Loss: 5.6781\n",
      "Epoch: 112, Train Loss: 3.8118, Test Loss: 5.6405\n",
      "Epoch: 113, Train Loss: 3.8402, Test Loss: 5.6711\n",
      "Epoch: 114, Train Loss: 3.8971, Test Loss: 5.6406\n",
      "Epoch: 115, Train Loss: 3.8419, Test Loss: 5.4430\n",
      "Epoch: 116, Train Loss: 4.0804, Test Loss: 5.4388\n",
      "Epoch: 117, Train Loss: 4.0901, Test Loss: 5.6382\n",
      "Epoch: 118, Train Loss: 3.8156, Test Loss: 5.7441\n",
      "Epoch: 119, Train Loss: 3.7747, Test Loss: 5.6532\n",
      "Epoch: 120, Train Loss: 3.8768, Test Loss: 5.2953\n",
      "Epoch: 121, Train Loss: 3.7037, Test Loss: 5.4108\n",
      "Epoch: 122, Train Loss: 3.8168, Test Loss: 5.7896\n",
      "Epoch: 123, Train Loss: 3.8996, Test Loss: 5.6353\n",
      "Epoch: 124, Train Loss: 3.8141, Test Loss: 5.3358\n",
      "Epoch: 125, Train Loss: 3.9518, Test Loss: 5.5535\n",
      "Epoch: 126, Train Loss: 3.7719, Test Loss: 5.6548\n",
      "Epoch: 127, Train Loss: 3.9536, Test Loss: 5.8026\n",
      "Epoch: 128, Train Loss: 3.6457, Test Loss: 5.6227\n",
      "Epoch: 129, Train Loss: 3.6067, Test Loss: 5.7541\n",
      "Epoch: 130, Train Loss: 3.7872, Test Loss: 5.6672\n",
      "Epoch: 131, Train Loss: 3.7377, Test Loss: 5.7403\n",
      "Epoch: 132, Train Loss: 3.5701, Test Loss: 5.5948\n",
      "Epoch: 133, Train Loss: 3.8048, Test Loss: 5.6547\n",
      "Epoch: 134, Train Loss: 3.7742, Test Loss: 5.8120\n",
      "Epoch: 135, Train Loss: 3.4732, Test Loss: 6.1155\n",
      "Epoch: 136, Train Loss: 3.4955, Test Loss: 5.7922\n",
      "Epoch: 137, Train Loss: 3.5215, Test Loss: 5.6125\n",
      "Epoch: 138, Train Loss: 3.7523, Test Loss: 6.4345\n",
      "Epoch: 139, Train Loss: 3.5126, Test Loss: 6.3516\n",
      "Epoch: 140, Train Loss: 3.3279, Test Loss: 5.4870\n",
      "Epoch: 141, Train Loss: 3.7462, Test Loss: 6.2860\n",
      "Epoch: 142, Train Loss: 3.7358, Test Loss: 6.6819\n",
      "Epoch: 143, Train Loss: 3.4706, Test Loss: 5.7854\n",
      "Epoch: 144, Train Loss: 3.7906, Test Loss: 5.7376\n",
      "Epoch: 145, Train Loss: 3.4276, Test Loss: 6.4369\n",
      "Epoch: 146, Train Loss: 3.4031, Test Loss: 6.5463\n",
      "Epoch: 147, Train Loss: 3.4512, Test Loss: 5.7716\n",
      "Epoch: 148, Train Loss: 3.5002, Test Loss: 6.1571\n",
      "Epoch: 149, Train Loss: 3.5199, Test Loss: 6.6376\n",
      "Epoch: 150, Train Loss: 3.6455, Test Loss: 6.2256\n",
      "Epoch: 151, Train Loss: 3.5458, Test Loss: 5.6616\n",
      "Epoch: 152, Train Loss: 3.3006, Test Loss: 6.7790\n",
      "Epoch: 153, Train Loss: 3.6281, Test Loss: 6.3664\n",
      "Epoch: 154, Train Loss: 3.5093, Test Loss: 5.7504\n",
      "Epoch: 155, Train Loss: 3.3676, Test Loss: 6.5542\n",
      "Epoch: 156, Train Loss: 3.2839, Test Loss: 7.0204\n",
      "Epoch: 157, Train Loss: 3.9098, Test Loss: 6.2948\n",
      "Epoch: 158, Train Loss: 3.4855, Test Loss: 6.2453\n",
      "Epoch: 159, Train Loss: 3.3169, Test Loss: 6.9928\n",
      "Epoch: 160, Train Loss: 3.4117, Test Loss: 6.4324\n",
      "Epoch: 161, Train Loss: 3.2845, Test Loss: 6.0024\n",
      "Epoch: 162, Train Loss: 3.6402, Test Loss: 6.7164\n",
      "Epoch: 163, Train Loss: 3.4733, Test Loss: 6.7786\n",
      "Epoch: 164, Train Loss: 3.5102, Test Loss: 5.8243\n",
      "Epoch: 165, Train Loss: 3.3548, Test Loss: 6.4593\n",
      "Epoch: 166, Train Loss: 3.3727, Test Loss: 6.8172\n",
      "Epoch: 167, Train Loss: 3.6160, Test Loss: 5.7376\n",
      "Epoch: 168, Train Loss: 3.6829, Test Loss: 5.7009\n",
      "Epoch: 169, Train Loss: 3.3785, Test Loss: 6.7077\n",
      "Epoch: 170, Train Loss: 3.5840, Test Loss: 6.7782\n",
      "Epoch: 171, Train Loss: 3.3497, Test Loss: 5.5947\n",
      "Epoch: 172, Train Loss: 3.5031, Test Loss: 6.1315\n",
      "Epoch: 173, Train Loss: 3.3256, Test Loss: 6.2268\n",
      "Epoch: 174, Train Loss: 3.2997, Test Loss: 6.2117\n",
      "Epoch: 175, Train Loss: 3.2421, Test Loss: 5.8789\n",
      "Epoch: 176, Train Loss: 3.2999, Test Loss: 6.0526\n",
      "Epoch: 177, Train Loss: 3.5012, Test Loss: 6.9146\n",
      "Epoch: 178, Train Loss: 3.3902, Test Loss: 6.5389\n",
      "Epoch: 179, Train Loss: 3.4214, Test Loss: 5.3989\n",
      "Epoch: 180, Train Loss: 3.4460, Test Loss: 6.0717\n",
      "Epoch: 181, Train Loss: 3.3640, Test Loss: 7.0904\n",
      "Epoch: 182, Train Loss: 3.3498, Test Loss: 6.0880\n",
      "Epoch: 183, Train Loss: 3.3849, Test Loss: 5.5537\n",
      "Epoch: 184, Train Loss: 3.4211, Test Loss: 6.5826\n",
      "Epoch: 185, Train Loss: 3.3226, Test Loss: 6.8626\n",
      "Epoch: 186, Train Loss: 3.4313, Test Loss: 5.9496\n",
      "Epoch: 187, Train Loss: 3.5038, Test Loss: 5.9863\n",
      "Epoch: 188, Train Loss: 3.4528, Test Loss: 6.4979\n",
      "Epoch: 189, Train Loss: 3.2043, Test Loss: 6.2037\n",
      "Epoch: 190, Train Loss: 3.4542, Test Loss: 5.8098\n",
      "Epoch: 191, Train Loss: 3.3837, Test Loss: 5.8328\n",
      "Epoch: 192, Train Loss: 3.1753, Test Loss: 6.6098\n",
      "Epoch: 193, Train Loss: 3.4120, Test Loss: 6.1113\n",
      "Epoch: 194, Train Loss: 3.1042, Test Loss: 5.8143\n",
      "Epoch: 195, Train Loss: 3.1110, Test Loss: 6.0894\n",
      "Epoch: 196, Train Loss: 3.2212, Test Loss: 6.4085\n",
      "Epoch: 197, Train Loss: 3.1647, Test Loss: 6.0879\n",
      "Epoch: 198, Train Loss: 3.3251, Test Loss: 6.3634\n",
      "Epoch: 199, Train Loss: 3.1890, Test Loss: 6.1226\n",
      "Epoch: 200, Train Loss: 3.1009, Test Loss: 6.1944\n",
      "Epoch: 201, Train Loss: 3.4623, Test Loss: 6.4628\n",
      "Epoch: 202, Train Loss: 3.3694, Test Loss: 6.7932\n",
      "Epoch: 203, Train Loss: 3.3001, Test Loss: 6.2789\n",
      "Epoch: 204, Train Loss: 3.0887, Test Loss: 6.1931\n",
      "Epoch: 205, Train Loss: 3.2613, Test Loss: 6.5655\n",
      "Epoch: 206, Train Loss: 3.4114, Test Loss: 6.7697\n",
      "Epoch: 207, Train Loss: 3.4782, Test Loss: 5.6988\n",
      "Epoch: 208, Train Loss: 3.2147, Test Loss: 6.5251\n",
      "Epoch: 209, Train Loss: 3.0297, Test Loss: 7.1476\n",
      "Epoch: 210, Train Loss: 3.2953, Test Loss: 5.9757\n",
      "Epoch: 211, Train Loss: 3.2661, Test Loss: 5.5055\n",
      "Epoch: 212, Train Loss: 3.5316, Test Loss: 7.0182\n",
      "Epoch: 213, Train Loss: 3.2351, Test Loss: 6.7709\n",
      "Epoch: 214, Train Loss: 3.2070, Test Loss: 6.0205\n",
      "Epoch: 215, Train Loss: 3.4146, Test Loss: 6.1264\n",
      "Epoch: 216, Train Loss: 3.3697, Test Loss: 6.9843\n",
      "Epoch: 217, Train Loss: 3.4306, Test Loss: 6.0817\n",
      "Epoch: 218, Train Loss: 3.2862, Test Loss: 6.6093\n",
      "Epoch: 219, Train Loss: 3.2318, Test Loss: 6.7010\n",
      "Epoch: 220, Train Loss: 3.0848, Test Loss: 6.4597\n",
      "Epoch: 221, Train Loss: 3.1667, Test Loss: 7.4880\n",
      "Epoch: 222, Train Loss: 3.1706, Test Loss: 6.5884\n",
      "Epoch: 223, Train Loss: 3.4180, Test Loss: 6.4341\n",
      "Epoch: 224, Train Loss: 3.3812, Test Loss: 7.3273\n",
      "Epoch: 225, Train Loss: 3.1637, Test Loss: 6.4463\n",
      "Epoch: 226, Train Loss: 2.9829, Test Loss: 6.1988\n",
      "Epoch: 227, Train Loss: 3.0600, Test Loss: 7.3138\n",
      "Epoch: 228, Train Loss: 3.2223, Test Loss: 6.8083\n",
      "Epoch: 229, Train Loss: 3.0999, Test Loss: 5.8811\n",
      "Epoch: 230, Train Loss: 3.2123, Test Loss: 6.3225\n",
      "Epoch: 231, Train Loss: 3.1136, Test Loss: 8.0373\n",
      "Epoch: 232, Train Loss: 3.3360, Test Loss: 6.6293\n",
      "Epoch: 233, Train Loss: 3.2039, Test Loss: 5.4939\n",
      "Epoch: 234, Train Loss: 3.2932, Test Loss: 7.4928\n",
      "Epoch: 235, Train Loss: 3.1767, Test Loss: 7.8527\n",
      "Epoch: 236, Train Loss: 2.9978, Test Loss: 5.6907\n",
      "Epoch: 237, Train Loss: 3.4863, Test Loss: 6.4759\n",
      "Epoch: 238, Train Loss: 3.1919, Test Loss: 8.1405\n",
      "Epoch: 239, Train Loss: 3.5038, Test Loss: 6.3839\n",
      "Epoch: 240, Train Loss: 3.1773, Test Loss: 5.9497\n",
      "Epoch: 241, Train Loss: 3.3033, Test Loss: 7.5891\n",
      "Epoch: 242, Train Loss: 3.3768, Test Loss: 6.8479\n",
      "Epoch: 243, Train Loss: 3.0230, Test Loss: 5.3049\n",
      "Epoch: 244, Train Loss: 3.3604, Test Loss: 6.4715\n",
      "Epoch: 245, Train Loss: 2.9835, Test Loss: 8.0634\n",
      "Epoch: 246, Train Loss: 3.3616, Test Loss: 6.0636\n",
      "Epoch: 247, Train Loss: 3.1857, Test Loss: 5.7195\n",
      "Epoch: 248, Train Loss: 3.4638, Test Loss: 7.6146\n",
      "Epoch: 249, Train Loss: 3.0256, Test Loss: 7.6091\n",
      "Epoch: 250, Train Loss: 3.1762, Test Loss: 6.2097\n",
      "Epoch: 251, Train Loss: 3.1857, Test Loss: 6.1839\n",
      "Epoch: 252, Train Loss: 3.1928, Test Loss: 7.3791\n",
      "Epoch: 253, Train Loss: 3.0817, Test Loss: 7.3668\n",
      "Epoch: 254, Train Loss: 3.2132, Test Loss: 6.3151\n",
      "Epoch: 255, Train Loss: 3.2969, Test Loss: 6.3593\n",
      "Epoch: 256, Train Loss: 2.8694, Test Loss: 7.5267\n",
      "Epoch: 257, Train Loss: 3.2632, Test Loss: 7.8448\n",
      "Epoch: 258, Train Loss: 3.1070, Test Loss: 6.6526\n",
      "Epoch: 259, Train Loss: 3.1796, Test Loss: 7.0480\n",
      "Epoch: 260, Train Loss: 2.7739, Test Loss: 7.7351\n",
      "Epoch: 261, Train Loss: 3.0756, Test Loss: 6.7235\n",
      "Epoch: 262, Train Loss: 3.1046, Test Loss: 6.3249\n",
      "Epoch: 263, Train Loss: 3.0411, Test Loss: 7.5651\n",
      "Epoch: 264, Train Loss: 3.0442, Test Loss: 7.3274\n",
      "Epoch: 265, Train Loss: 3.1918, Test Loss: 6.4875\n",
      "Epoch: 266, Train Loss: 2.9063, Test Loss: 6.9402\n",
      "Epoch: 267, Train Loss: 3.0923, Test Loss: 8.2022\n",
      "Epoch: 268, Train Loss: 3.3265, Test Loss: 7.5474\n",
      "Epoch: 269, Train Loss: 3.2173, Test Loss: 5.9838\n",
      "Epoch: 270, Train Loss: 3.3751, Test Loss: 7.5640\n",
      "Epoch: 271, Train Loss: 2.9070, Test Loss: 8.8411\n",
      "Epoch: 272, Train Loss: 3.4470, Test Loss: 6.4169\n",
      "Epoch: 273, Train Loss: 3.0858, Test Loss: 6.0598\n",
      "Epoch: 274, Train Loss: 2.9888, Test Loss: 7.9873\n",
      "Epoch: 275, Train Loss: 3.1375, Test Loss: 8.5570\n",
      "Epoch: 276, Train Loss: 3.0585, Test Loss: 6.5309\n",
      "Epoch: 277, Train Loss: 2.9637, Test Loss: 6.5162\n",
      "Epoch: 278, Train Loss: 2.8332, Test Loss: 8.4716\n",
      "Epoch: 279, Train Loss: 3.1315, Test Loss: 8.8922\n",
      "Epoch: 280, Train Loss: 3.0213, Test Loss: 6.4052\n",
      "Epoch: 281, Train Loss: 3.2678, Test Loss: 6.8673\n",
      "Epoch: 282, Train Loss: 2.9158, Test Loss: 8.0257\n",
      "Epoch: 283, Train Loss: 3.0757, Test Loss: 7.2942\n",
      "Epoch: 284, Train Loss: 3.0907, Test Loss: 6.5715\n",
      "Epoch: 285, Train Loss: 2.9350, Test Loss: 6.8991\n",
      "Epoch: 286, Train Loss: 3.1248, Test Loss: 7.9028\n",
      "Epoch: 287, Train Loss: 2.9133, Test Loss: 7.5498\n",
      "Epoch: 288, Train Loss: 2.8833, Test Loss: 7.0185\n",
      "Epoch: 289, Train Loss: 2.9213, Test Loss: 7.0136\n",
      "Epoch: 290, Train Loss: 2.9125, Test Loss: 7.7575\n",
      "Epoch: 291, Train Loss: 2.7647, Test Loss: 7.0545\n",
      "Epoch: 292, Train Loss: 2.9237, Test Loss: 7.1194\n",
      "Epoch: 293, Train Loss: 2.8867, Test Loss: 7.4286\n",
      "Epoch: 294, Train Loss: 2.8125, Test Loss: 8.1011\n",
      "Epoch: 295, Train Loss: 2.8741, Test Loss: 7.8278\n",
      "Epoch: 296, Train Loss: 3.1270, Test Loss: 6.6470\n",
      "Epoch: 297, Train Loss: 3.0358, Test Loss: 7.8793\n",
      "Epoch: 298, Train Loss: 2.9083, Test Loss: 7.8592\n",
      "Epoch: 299, Train Loss: 2.8741, Test Loss: 7.3506\n",
      "Epoch: 300, Train Loss: 3.1111, Test Loss: 7.6420\n",
      "Epoch: 301, Train Loss: 3.0051, Test Loss: 7.4853\n",
      "Epoch: 302, Train Loss: 2.9139, Test Loss: 6.8977\n",
      "Epoch: 303, Train Loss: 3.0140, Test Loss: 8.0349\n",
      "Epoch: 304, Train Loss: 2.8007, Test Loss: 8.5932\n",
      "Epoch: 305, Train Loss: 2.9676, Test Loss: 6.9339\n",
      "Epoch: 306, Train Loss: 2.9401, Test Loss: 7.1215\n",
      "Epoch: 307, Train Loss: 2.8911, Test Loss: 9.1996\n",
      "Epoch: 308, Train Loss: 2.8859, Test Loss: 7.6825\n",
      "Epoch: 309, Train Loss: 2.9484, Test Loss: 7.1366\n",
      "Epoch: 310, Train Loss: 2.9113, Test Loss: 8.0179\n",
      "Epoch: 311, Train Loss: 2.6701, Test Loss: 8.7135\n",
      "Epoch: 312, Train Loss: 3.1103, Test Loss: 7.7410\n",
      "Epoch: 313, Train Loss: 3.0149, Test Loss: 7.6719\n",
      "Epoch: 314, Train Loss: 2.9488, Test Loss: 7.6550\n",
      "Epoch: 315, Train Loss: 2.8103, Test Loss: 8.5551\n",
      "Epoch: 316, Train Loss: 2.9248, Test Loss: 7.6965\n",
      "Epoch: 317, Train Loss: 2.9490, Test Loss: 7.0960\n",
      "Epoch: 318, Train Loss: 2.9997, Test Loss: 7.2265\n",
      "Epoch: 319, Train Loss: 2.7196, Test Loss: 8.5743\n",
      "Epoch: 320, Train Loss: 2.7311, Test Loss: 7.7978\n",
      "Epoch: 321, Train Loss: 2.6516, Test Loss: 6.9595\n",
      "Epoch: 322, Train Loss: 2.9218, Test Loss: 8.1952\n",
      "Epoch: 323, Train Loss: 2.9987, Test Loss: 8.8598\n",
      "Epoch: 324, Train Loss: 2.8553, Test Loss: 8.2296\n",
      "Epoch: 325, Train Loss: 2.9471, Test Loss: 6.8329\n",
      "Epoch: 326, Train Loss: 3.1377, Test Loss: 7.7087\n",
      "Epoch: 327, Train Loss: 2.8193, Test Loss: 8.0595\n",
      "Epoch: 328, Train Loss: 2.7076, Test Loss: 7.4834\n",
      "Epoch: 329, Train Loss: 2.8532, Test Loss: 7.1406\n",
      "Epoch: 330, Train Loss: 2.7366, Test Loss: 7.9873\n",
      "Epoch: 331, Train Loss: 2.5810, Test Loss: 9.0924\n",
      "Epoch: 332, Train Loss: 2.9050, Test Loss: 7.3490\n",
      "Epoch: 333, Train Loss: 3.0329, Test Loss: 7.3177\n",
      "Epoch: 334, Train Loss: 3.0554, Test Loss: 8.7103\n",
      "Epoch: 335, Train Loss: 3.1264, Test Loss: 7.5223\n",
      "Epoch: 336, Train Loss: 2.8962, Test Loss: 6.7018\n",
      "Epoch: 337, Train Loss: 2.8770, Test Loss: 8.5838\n",
      "Epoch: 338, Train Loss: 3.0238, Test Loss: 8.5265\n",
      "Epoch: 339, Train Loss: 3.0973, Test Loss: 6.4776\n",
      "Epoch: 340, Train Loss: 2.9923, Test Loss: 7.7991\n",
      "Epoch: 341, Train Loss: 2.6931, Test Loss: 9.2777\n",
      "Epoch: 342, Train Loss: 3.3195, Test Loss: 6.9436\n",
      "Epoch: 343, Train Loss: 2.8377, Test Loss: 6.1676\n",
      "Epoch: 344, Train Loss: 2.9046, Test Loss: 8.5554\n",
      "Epoch: 345, Train Loss: 2.7454, Test Loss: 9.6827\n",
      "Epoch: 346, Train Loss: 3.1720, Test Loss: 6.1144\n",
      "Epoch: 347, Train Loss: 2.7976, Test Loss: 6.6569\n",
      "Epoch: 348, Train Loss: 3.0031, Test Loss: 8.9541\n",
      "Epoch: 349, Train Loss: 2.9155, Test Loss: 8.2070\n",
      "Epoch: 350, Train Loss: 2.7341, Test Loss: 6.4131\n",
      "Epoch: 351, Train Loss: 2.8287, Test Loss: 6.5943\n",
      "Epoch: 352, Train Loss: 2.7897, Test Loss: 7.9857\n",
      "Epoch: 353, Train Loss: 2.8512, Test Loss: 7.7401\n",
      "Epoch: 354, Train Loss: 2.6929, Test Loss: 6.3276\n",
      "Epoch: 355, Train Loss: 3.0987, Test Loss: 7.3701\n",
      "Epoch: 356, Train Loss: 2.7692, Test Loss: 8.5667\n",
      "Epoch: 357, Train Loss: 2.9266, Test Loss: 6.5281\n",
      "Epoch: 358, Train Loss: 2.8716, Test Loss: 7.2237\n",
      "Epoch: 359, Train Loss: 2.7373, Test Loss: 9.0136\n",
      "Epoch: 360, Train Loss: 3.0335, Test Loss: 7.1838\n",
      "Epoch: 361, Train Loss: 2.7750, Test Loss: 7.0643\n",
      "Epoch: 362, Train Loss: 2.9875, Test Loss: 8.4269\n",
      "Epoch: 363, Train Loss: 2.8451, Test Loss: 7.8243\n",
      "Epoch: 364, Train Loss: 2.7735, Test Loss: 6.7617\n",
      "Epoch: 365, Train Loss: 2.6700, Test Loss: 7.5035\n",
      "Epoch: 366, Train Loss: 2.7514, Test Loss: 8.2683\n",
      "Epoch: 367, Train Loss: 2.7196, Test Loss: 7.7401\n",
      "Epoch: 368, Train Loss: 2.7194, Test Loss: 7.5138\n",
      "Epoch: 369, Train Loss: 2.7027, Test Loss: 7.9979\n",
      "Epoch: 370, Train Loss: 2.6938, Test Loss: 7.9173\n",
      "Epoch: 371, Train Loss: 2.4467, Test Loss: 7.3065\n",
      "Epoch: 372, Train Loss: 2.7400, Test Loss: 8.0182\n",
      "Epoch: 373, Train Loss: 2.7070, Test Loss: 8.7498\n",
      "Epoch: 374, Train Loss: 2.6918, Test Loss: 7.9918\n",
      "Epoch: 375, Train Loss: 2.6051, Test Loss: 7.5504\n",
      "Epoch: 376, Train Loss: 2.8412, Test Loss: 8.3112\n",
      "Epoch: 377, Train Loss: 2.7834, Test Loss: 8.4115\n",
      "Epoch: 378, Train Loss: 2.6840, Test Loss: 7.2659\n",
      "Epoch: 379, Train Loss: 2.8516, Test Loss: 9.2841\n",
      "Epoch: 380, Train Loss: 2.8317, Test Loss: 7.7984\n",
      "Epoch: 381, Train Loss: 2.5765, Test Loss: 6.8564\n",
      "Epoch: 382, Train Loss: 3.0386, Test Loss: 9.6790\n",
      "Epoch: 383, Train Loss: 2.7147, Test Loss: 8.9147\n",
      "Epoch: 384, Train Loss: 3.0700, Test Loss: 6.1888\n",
      "Epoch: 385, Train Loss: 3.1213, Test Loss: 8.1288\n",
      "Epoch: 386, Train Loss: 2.7008, Test Loss: 10.5935\n",
      "Epoch: 387, Train Loss: 2.8742, Test Loss: 7.8426\n",
      "Epoch: 388, Train Loss: 2.6985, Test Loss: 6.0001\n",
      "Epoch: 389, Train Loss: 3.0448, Test Loss: 8.6981\n",
      "Epoch: 390, Train Loss: 2.5695, Test Loss: 10.4802\n",
      "Epoch: 391, Train Loss: 2.9607, Test Loss: 7.9063\n",
      "Epoch: 392, Train Loss: 2.6794, Test Loss: 6.4007\n",
      "Epoch: 393, Train Loss: 2.9750, Test Loss: 8.4103\n",
      "Epoch: 394, Train Loss: 2.5591, Test Loss: 9.3459\n",
      "Epoch: 395, Train Loss: 2.7696, Test Loss: 7.6461\n",
      "Epoch: 396, Train Loss: 2.5969, Test Loss: 7.3823\n",
      "Epoch: 397, Train Loss: 2.6166, Test Loss: 7.9272\n",
      "Epoch: 398, Train Loss: 2.5619, Test Loss: 8.3056\n",
      "Epoch: 399, Train Loss: 2.9344, Test Loss: 7.8199\n",
      "Epoch: 400, Train Loss: 2.8909, Test Loss: 7.6286\n",
      "Epoch: 401, Train Loss: 2.5717, Test Loss: 7.7172\n",
      "Epoch: 402, Train Loss: 2.6601, Test Loss: 7.9637\n",
      "Epoch: 403, Train Loss: 2.5203, Test Loss: 7.3324\n",
      "Epoch: 404, Train Loss: 2.6311, Test Loss: 7.7980\n",
      "Epoch: 405, Train Loss: 2.6818, Test Loss: 8.4555\n",
      "Epoch: 406, Train Loss: 2.7224, Test Loss: 8.2106\n",
      "Epoch: 407, Train Loss: 2.6011, Test Loss: 7.9750\n",
      "Epoch: 408, Train Loss: 2.3561, Test Loss: 8.3597\n",
      "Epoch: 409, Train Loss: 2.7360, Test Loss: 8.6647\n",
      "Epoch: 410, Train Loss: 2.6785, Test Loss: 8.3228\n",
      "Epoch: 411, Train Loss: 2.6385, Test Loss: 6.7324\n",
      "Epoch: 412, Train Loss: 3.0184, Test Loss: 9.4948\n",
      "Epoch: 413, Train Loss: 2.8815, Test Loss: 9.8717\n",
      "Epoch: 414, Train Loss: 2.7599, Test Loss: 6.6049\n",
      "Epoch: 415, Train Loss: 2.6987, Test Loss: 7.1714\n",
      "Epoch: 416, Train Loss: 2.6634, Test Loss: 9.5227\n",
      "Epoch: 417, Train Loss: 2.7506, Test Loss: 8.5775\n",
      "Epoch: 418, Train Loss: 2.5762, Test Loss: 7.0209\n",
      "Epoch: 419, Train Loss: 2.6963, Test Loss: 6.7212\n",
      "Epoch: 420, Train Loss: 2.6711, Test Loss: 8.2876\n",
      "Epoch: 421, Train Loss: 2.8655, Test Loss: 9.2941\n",
      "Epoch: 422, Train Loss: 2.7601, Test Loss: 7.4968\n",
      "Epoch: 423, Train Loss: 2.4938, Test Loss: 6.9009\n",
      "Epoch: 424, Train Loss: 2.5603, Test Loss: 9.3204\n",
      "Epoch: 425, Train Loss: 2.5004, Test Loss: 9.2046\n",
      "Epoch: 426, Train Loss: 2.4410, Test Loss: 6.9648\n",
      "Epoch: 427, Train Loss: 2.6325, Test Loss: 7.0103\n",
      "Epoch: 428, Train Loss: 2.7034, Test Loss: 9.0204\n",
      "Epoch: 429, Train Loss: 2.3933, Test Loss: 8.6009\n",
      "Epoch: 430, Train Loss: 2.6843, Test Loss: 6.9916\n",
      "Epoch: 431, Train Loss: 2.6784, Test Loss: 7.8750\n",
      "Epoch: 432, Train Loss: 2.7131, Test Loss: 9.3602\n",
      "Epoch: 433, Train Loss: 2.8232, Test Loss: 7.4919\n",
      "Epoch: 434, Train Loss: 2.6118, Test Loss: 6.0992\n",
      "Epoch: 435, Train Loss: 2.6300, Test Loss: 8.4668\n",
      "Epoch: 436, Train Loss: 2.7684, Test Loss: 9.7097\n",
      "Epoch: 437, Train Loss: 2.8260, Test Loss: 6.9588\n",
      "Epoch: 438, Train Loss: 2.5032, Test Loss: 6.5374\n",
      "Epoch: 439, Train Loss: 2.8753, Test Loss: 9.4184\n",
      "Epoch: 440, Train Loss: 2.7007, Test Loss: 9.0717\n",
      "Epoch: 441, Train Loss: 2.5368, Test Loss: 6.4566\n",
      "Epoch: 442, Train Loss: 2.8397, Test Loss: 6.8056\n",
      "Epoch: 443, Train Loss: 2.5145, Test Loss: 9.3105\n",
      "Epoch: 444, Train Loss: 2.8133, Test Loss: 8.7287\n",
      "Epoch: 445, Train Loss: 2.5840, Test Loss: 6.5251\n",
      "Epoch: 446, Train Loss: 2.6196, Test Loss: 7.4502\n",
      "Epoch: 447, Train Loss: 2.4161, Test Loss: 9.0829\n",
      "Epoch: 448, Train Loss: 2.4770, Test Loss: 8.2949\n",
      "Epoch: 449, Train Loss: 2.3970, Test Loss: 7.2050\n",
      "Epoch: 450, Train Loss: 2.4594, Test Loss: 7.9922\n",
      "Epoch: 451, Train Loss: 2.5225, Test Loss: 9.5472\n",
      "Epoch: 452, Train Loss: 2.4676, Test Loss: 8.2330\n",
      "Epoch: 453, Train Loss: 2.4145, Test Loss: 7.9829\n",
      "Epoch: 454, Train Loss: 2.6596, Test Loss: 8.4096\n",
      "Epoch: 455, Train Loss: 2.6329, Test Loss: 8.3440\n",
      "Epoch: 456, Train Loss: 2.3730, Test Loss: 8.3960\n",
      "Epoch: 457, Train Loss: 2.4953, Test Loss: 8.1853\n",
      "Epoch: 458, Train Loss: 2.4136, Test Loss: 7.9738\n",
      "Epoch: 459, Train Loss: 2.8086, Test Loss: 8.6113\n",
      "Epoch: 460, Train Loss: 2.4523, Test Loss: 9.3458\n",
      "Epoch: 461, Train Loss: 2.7110, Test Loss: 8.3381\n",
      "Epoch: 462, Train Loss: 2.5781, Test Loss: 7.1905\n",
      "Epoch: 463, Train Loss: 2.5842, Test Loss: 8.2405\n",
      "Epoch: 464, Train Loss: 2.5449, Test Loss: 8.8092\n",
      "Epoch: 465, Train Loss: 2.6417, Test Loss: 7.1018\n",
      "Epoch: 466, Train Loss: 2.5093, Test Loss: 7.0996\n",
      "Epoch: 467, Train Loss: 2.3052, Test Loss: 8.2751\n",
      "Epoch: 468, Train Loss: 2.7167, Test Loss: 9.0923\n",
      "Epoch: 469, Train Loss: 2.3442, Test Loss: 7.6171\n",
      "Epoch: 470, Train Loss: 2.6664, Test Loss: 7.2211\n",
      "Epoch: 471, Train Loss: 2.6221, Test Loss: 9.2711\n",
      "Epoch: 472, Train Loss: 2.4864, Test Loss: 9.0356\n",
      "Epoch: 473, Train Loss: 2.6205, Test Loss: 8.4702\n",
      "Epoch: 474, Train Loss: 2.5060, Test Loss: 7.8378\n",
      "Epoch: 475, Train Loss: 2.7044, Test Loss: 9.7539\n",
      "Epoch: 476, Train Loss: 2.5469, Test Loss: 8.1311\n",
      "Epoch: 477, Train Loss: 2.6147, Test Loss: 6.9718\n",
      "Epoch: 478, Train Loss: 2.6152, Test Loss: 9.4288\n",
      "Epoch: 479, Train Loss: 2.4331, Test Loss: 9.0010\n",
      "Epoch: 480, Train Loss: 2.8886, Test Loss: 6.5560\n",
      "Epoch: 481, Train Loss: 2.8209, Test Loss: 7.7744\n",
      "Epoch: 482, Train Loss: 2.5657, Test Loss: 9.0245\n",
      "Epoch: 483, Train Loss: 2.3631, Test Loss: 8.0320\n",
      "Epoch: 484, Train Loss: 2.5348, Test Loss: 7.6279\n",
      "Epoch: 485, Train Loss: 2.4596, Test Loss: 8.1789\n",
      "Epoch: 486, Train Loss: 2.4294, Test Loss: 8.8434\n",
      "Epoch: 487, Train Loss: 2.4821, Test Loss: 7.7943\n",
      "Epoch: 488, Train Loss: 2.5237, Test Loss: 8.5465\n",
      "Epoch: 489, Train Loss: 2.2666, Test Loss: 8.7987\n",
      "Epoch: 490, Train Loss: 2.4134, Test Loss: 7.9101\n",
      "Epoch: 491, Train Loss: 2.4478, Test Loss: 8.4379\n",
      "Epoch: 492, Train Loss: 2.4333, Test Loss: 8.5897\n",
      "Epoch: 493, Train Loss: 2.2913, Test Loss: 7.7405\n",
      "Epoch: 494, Train Loss: 2.4909, Test Loss: 8.9137\n",
      "Epoch: 495, Train Loss: 2.3373, Test Loss: 9.6119\n",
      "Epoch: 496, Train Loss: 2.6431, Test Loss: 7.4545\n",
      "Epoch: 497, Train Loss: 2.1754, Test Loss: 7.5854\n",
      "Epoch: 498, Train Loss: 2.4956, Test Loss: 9.5006\n",
      "Epoch: 499, Train Loss: 2.3910, Test Loss: 9.4904\n",
      "Epoch: 500, Train Loss: 2.6443, Test Loss: 6.8270\n",
      "Epoch: 501, Train Loss: 2.3500, Test Loss: 7.4230\n",
      "Epoch: 502, Train Loss: 2.8859, Test Loss: 12.2643\n",
      "Epoch: 503, Train Loss: 3.5268, Test Loss: 8.1084\n",
      "Epoch: 504, Train Loss: 2.3498, Test Loss: 6.3855\n",
      "Epoch: 505, Train Loss: 2.7805, Test Loss: 9.2565\n",
      "Epoch: 506, Train Loss: 2.4209, Test Loss: 10.2019\n",
      "Epoch: 507, Train Loss: 2.6353, Test Loss: 6.7813\n",
      "Epoch: 508, Train Loss: 2.5971, Test Loss: 6.1263\n",
      "Epoch: 509, Train Loss: 2.9878, Test Loss: 9.8432\n",
      "Epoch: 510, Train Loss: 2.4579, Test Loss: 10.2915\n",
      "Epoch: 511, Train Loss: 2.8095, Test Loss: 6.0513\n",
      "Epoch: 512, Train Loss: 2.7003, Test Loss: 6.0488\n",
      "Epoch: 513, Train Loss: 2.7928, Test Loss: 9.7519\n",
      "Epoch: 514, Train Loss: 2.5952, Test Loss: 10.0145\n",
      "Epoch: 515, Train Loss: 2.5981, Test Loss: 6.1648\n",
      "Epoch: 516, Train Loss: 2.6805, Test Loss: 5.4588\n",
      "Epoch: 517, Train Loss: 3.0576, Test Loss: 9.6766\n",
      "Epoch: 518, Train Loss: 2.5194, Test Loss: 10.7767\n",
      "Epoch: 519, Train Loss: 2.9213, Test Loss: 6.2254\n",
      "Epoch: 520, Train Loss: 2.4334, Test Loss: 5.4934\n",
      "Epoch: 521, Train Loss: 2.8985, Test Loss: 8.3825\n",
      "Epoch: 522, Train Loss: 2.3110, Test Loss: 10.6862\n",
      "Epoch: 523, Train Loss: 2.8169, Test Loss: 7.6620\n",
      "Epoch: 524, Train Loss: 2.3824, Test Loss: 5.6236\n",
      "Epoch: 525, Train Loss: 3.1173, Test Loss: 7.5990\n",
      "Epoch: 526, Train Loss: 2.7291, Test Loss: 9.4304\n",
      "Epoch: 527, Train Loss: 2.6515, Test Loss: 7.5699\n",
      "Epoch: 528, Train Loss: 2.0697, Test Loss: 6.4254\n",
      "Epoch: 529, Train Loss: 2.3661, Test Loss: 6.9682\n",
      "Epoch: 530, Train Loss: 2.2724, Test Loss: 8.6478\n",
      "Epoch: 531, Train Loss: 2.3856, Test Loss: 8.8429\n",
      "Epoch: 532, Train Loss: 2.4233, Test Loss: 6.8125\n",
      "Epoch: 533, Train Loss: 2.4101, Test Loss: 6.0065\n",
      "Epoch: 534, Train Loss: 2.7039, Test Loss: 7.9068\n",
      "Epoch: 535, Train Loss: 2.1739, Test Loss: 9.8068\n",
      "Epoch: 536, Train Loss: 2.8440, Test Loss: 7.7900\n",
      "Epoch: 537, Train Loss: 2.2581, Test Loss: 6.2854\n",
      "Epoch: 538, Train Loss: 2.5475, Test Loss: 6.5788\n",
      "Epoch: 539, Train Loss: 2.5534, Test Loss: 9.2210\n",
      "Epoch: 540, Train Loss: 2.4996, Test Loss: 8.6294\n",
      "Epoch: 541, Train Loss: 2.4688, Test Loss: 7.0164\n",
      "Epoch: 542, Train Loss: 2.5622, Test Loss: 6.6820\n",
      "Epoch: 543, Train Loss: 2.2546, Test Loss: 7.8070\n",
      "Epoch: 544, Train Loss: 2.3820, Test Loss: 8.2934\n",
      "Epoch: 545, Train Loss: 2.4214, Test Loss: 7.4618\n",
      "Epoch: 546, Train Loss: 2.3171, Test Loss: 6.4418\n",
      "Epoch: 547, Train Loss: 2.4321, Test Loss: 7.4602\n",
      "Epoch: 548, Train Loss: 2.4610, Test Loss: 8.5410\n",
      "Epoch: 549, Train Loss: 2.2839, Test Loss: 7.4777\n",
      "Epoch: 550, Train Loss: 2.2497, Test Loss: 6.1996\n",
      "Epoch: 551, Train Loss: 2.3582, Test Loss: 7.4691\n",
      "Epoch: 552, Train Loss: 2.5710, Test Loss: 8.4832\n",
      "Epoch: 553, Train Loss: 2.4115, Test Loss: 8.3980\n",
      "Epoch: 554, Train Loss: 2.6039, Test Loss: 7.0940\n",
      "Epoch: 555, Train Loss: 2.2393, Test Loss: 6.9425\n",
      "Epoch: 556, Train Loss: 2.3133, Test Loss: 7.3594\n",
      "Epoch: 557, Train Loss: 2.5583, Test Loss: 8.8378\n",
      "Epoch: 558, Train Loss: 2.5796, Test Loss: 8.7540\n",
      "Epoch: 559, Train Loss: 2.4378, Test Loss: 6.9608\n",
      "Epoch: 560, Train Loss: 2.4840, Test Loss: 7.2394\n",
      "Epoch: 561, Train Loss: 2.2976, Test Loss: 8.6693\n",
      "Epoch: 562, Train Loss: 2.7206, Test Loss: 8.2515\n",
      "Epoch: 563, Train Loss: 2.3120, Test Loss: 7.4339\n",
      "Epoch: 564, Train Loss: 2.2838, Test Loss: 7.3487\n",
      "Epoch: 565, Train Loss: 2.4306, Test Loss: 8.4997\n",
      "Epoch: 566, Train Loss: 2.4465, Test Loss: 8.7071\n",
      "Epoch: 567, Train Loss: 2.2397, Test Loss: 8.4084\n",
      "Epoch: 568, Train Loss: 2.2714, Test Loss: 7.4369\n",
      "Epoch: 569, Train Loss: 2.2213, Test Loss: 7.8828\n",
      "Epoch: 570, Train Loss: 2.3168, Test Loss: 9.2653\n",
      "Epoch: 571, Train Loss: 2.3937, Test Loss: 8.0056\n",
      "Epoch: 572, Train Loss: 2.0820, Test Loss: 7.5640\n",
      "Epoch: 573, Train Loss: 2.4958, Test Loss: 8.4487\n",
      "Epoch: 574, Train Loss: 2.3416, Test Loss: 9.4689\n",
      "Epoch: 575, Train Loss: 2.4271, Test Loss: 8.8683\n",
      "Epoch: 576, Train Loss: 2.1560, Test Loss: 7.9749\n",
      "Epoch: 577, Train Loss: 2.2484, Test Loss: 9.0615\n",
      "Epoch: 578, Train Loss: 2.4830, Test Loss: 9.0003\n",
      "Epoch: 579, Train Loss: 2.3322, Test Loss: 8.3970\n",
      "Epoch: 580, Train Loss: 2.2767, Test Loss: 8.6892\n",
      "Epoch: 581, Train Loss: 2.3250, Test Loss: 9.1619\n",
      "Epoch: 582, Train Loss: 2.1771, Test Loss: 9.3857\n",
      "Epoch: 583, Train Loss: 2.4297, Test Loss: 8.7118\n",
      "Epoch: 584, Train Loss: 2.3700, Test Loss: 8.2854\n",
      "Epoch: 585, Train Loss: 2.3374, Test Loss: 8.3576\n",
      "Epoch: 586, Train Loss: 2.3125, Test Loss: 10.4911\n",
      "Epoch: 587, Train Loss: 2.2156, Test Loss: 8.8790\n",
      "Epoch: 588, Train Loss: 2.2316, Test Loss: 7.8153\n",
      "Epoch: 589, Train Loss: 2.2596, Test Loss: 9.4614\n",
      "Epoch: 590, Train Loss: 2.3963, Test Loss: 9.7603\n",
      "Epoch: 591, Train Loss: 2.4543, Test Loss: 8.0748\n",
      "Epoch: 592, Train Loss: 2.2258, Test Loss: 7.8915\n",
      "Epoch: 593, Train Loss: 2.0810, Test Loss: 10.1997\n",
      "Epoch: 594, Train Loss: 2.3463, Test Loss: 8.9512\n",
      "Epoch: 595, Train Loss: 2.2263, Test Loss: 6.8486\n",
      "Epoch: 596, Train Loss: 2.2762, Test Loss: 8.1897\n",
      "Epoch: 597, Train Loss: 2.4837, Test Loss: 11.1737\n",
      "Epoch: 598, Train Loss: 2.4328, Test Loss: 8.8757\n",
      "Epoch: 599, Train Loss: 2.0958, Test Loss: 6.1989\n",
      "Epoch: 600, Train Loss: 2.7077, Test Loss: 8.4365\n",
      "Epoch: 601, Train Loss: 2.2395, Test Loss: 11.1544\n",
      "Epoch: 602, Train Loss: 2.7529, Test Loss: 7.8129\n",
      "Epoch: 603, Train Loss: 2.1762, Test Loss: 6.2616\n",
      "Epoch: 604, Train Loss: 2.4001, Test Loss: 8.5710\n",
      "Epoch: 605, Train Loss: 2.1679, Test Loss: 10.5796\n",
      "Epoch: 606, Train Loss: 2.3448, Test Loss: 7.9216\n",
      "Epoch: 607, Train Loss: 2.2027, Test Loss: 6.4354\n",
      "Epoch: 608, Train Loss: 2.4770, Test Loss: 9.4241\n",
      "Epoch: 609, Train Loss: 2.3171, Test Loss: 10.7955\n",
      "Epoch: 610, Train Loss: 2.6278, Test Loss: 7.3015\n",
      "Epoch: 611, Train Loss: 2.1810, Test Loss: 5.6011\n",
      "Epoch: 612, Train Loss: 3.2932, Test Loss: 10.3431\n",
      "Epoch: 613, Train Loss: 2.4984, Test Loss: 11.9714\n",
      "Epoch: 614, Train Loss: 2.9699, Test Loss: 7.3019\n",
      "Epoch: 615, Train Loss: 2.0721, Test Loss: 5.5214\n",
      "Epoch: 616, Train Loss: 3.2311, Test Loss: 8.6278\n",
      "Epoch: 617, Train Loss: 2.3020, Test Loss: 10.7896\n",
      "Epoch: 618, Train Loss: 2.7738, Test Loss: 7.9301\n",
      "Epoch: 619, Train Loss: 2.0137, Test Loss: 5.7863\n",
      "Epoch: 620, Train Loss: 2.7294, Test Loss: 7.4695\n",
      "Epoch: 621, Train Loss: 2.0908, Test Loss: 9.6719\n",
      "Epoch: 622, Train Loss: 2.3079, Test Loss: 8.4748\n",
      "Epoch: 623, Train Loss: 2.2186, Test Loss: 6.8141\n",
      "Epoch: 624, Train Loss: 2.1232, Test Loss: 6.5114\n",
      "Epoch: 625, Train Loss: 2.2487, Test Loss: 8.2442\n",
      "Epoch: 626, Train Loss: 2.2118, Test Loss: 8.8040\n",
      "Epoch: 627, Train Loss: 2.3590, Test Loss: 7.5297\n",
      "Epoch: 628, Train Loss: 2.3814, Test Loss: 7.4037\n",
      "Epoch: 629, Train Loss: 2.1199, Test Loss: 7.7873\n",
      "Epoch: 630, Train Loss: 2.1772, Test Loss: 8.4516\n",
      "Epoch: 631, Train Loss: 1.9949, Test Loss: 8.3726\n",
      "Epoch: 632, Train Loss: 2.3771, Test Loss: 7.6854\n",
      "Epoch: 633, Train Loss: 2.2035, Test Loss: 7.1130\n",
      "Epoch: 634, Train Loss: 2.3956, Test Loss: 8.5999\n",
      "Epoch: 635, Train Loss: 2.1513, Test Loss: 9.1449\n",
      "Epoch: 636, Train Loss: 2.0744, Test Loss: 7.7595\n",
      "Epoch: 637, Train Loss: 2.2136, Test Loss: 7.6836\n",
      "Epoch: 638, Train Loss: 2.2636, Test Loss: 9.6922\n",
      "Epoch: 639, Train Loss: 2.2638, Test Loss: 9.7741\n",
      "Epoch: 640, Train Loss: 2.3718, Test Loss: 7.1561\n",
      "Epoch: 641, Train Loss: 2.4738, Test Loss: 7.8732\n",
      "Epoch: 642, Train Loss: 2.2895, Test Loss: 10.4666\n",
      "Epoch: 643, Train Loss: 2.2254, Test Loss: 9.3475\n",
      "Epoch: 644, Train Loss: 2.0614, Test Loss: 6.2113\n",
      "Epoch: 645, Train Loss: 2.4334, Test Loss: 7.0489\n",
      "Epoch: 646, Train Loss: 2.0311, Test Loss: 10.2303\n",
      "Epoch: 647, Train Loss: 2.1411, Test Loss: 9.3996\n",
      "Epoch: 648, Train Loss: 2.1877, Test Loss: 7.8339\n",
      "Epoch: 649, Train Loss: 2.1686, Test Loss: 7.3981\n",
      "Epoch: 650, Train Loss: 2.2528, Test Loss: 8.9740\n",
      "Epoch: 651, Train Loss: 2.3096, Test Loss: 9.7261\n",
      "Epoch: 652, Train Loss: 2.5109, Test Loss: 7.4013\n",
      "Epoch: 653, Train Loss: 2.2971, Test Loss: 7.3652\n",
      "Epoch: 654, Train Loss: 2.1637, Test Loss: 9.2276\n",
      "Epoch: 655, Train Loss: 2.2654, Test Loss: 9.9890\n",
      "Epoch: 656, Train Loss: 2.2533, Test Loss: 7.5223\n",
      "Epoch: 657, Train Loss: 2.2720, Test Loss: 6.9278\n",
      "Epoch: 658, Train Loss: 2.4169, Test Loss: 9.1944\n",
      "Epoch: 659, Train Loss: 2.1135, Test Loss: 9.7614\n",
      "Epoch: 660, Train Loss: 2.1038, Test Loss: 7.4182\n",
      "Epoch: 661, Train Loss: 2.2667, Test Loss: 6.9774\n",
      "Epoch: 662, Train Loss: 2.1971, Test Loss: 8.9936\n",
      "Epoch: 663, Train Loss: 2.1985, Test Loss: 9.1063\n",
      "Epoch: 664, Train Loss: 2.0172, Test Loss: 7.7228\n",
      "Epoch: 665, Train Loss: 2.3192, Test Loss: 7.6044\n",
      "Epoch: 666, Train Loss: 2.1046, Test Loss: 8.1272\n",
      "Epoch: 667, Train Loss: 2.0425, Test Loss: 8.6978\n",
      "Epoch: 668, Train Loss: 2.0121, Test Loss: 8.6047\n",
      "Epoch: 669, Train Loss: 2.0721, Test Loss: 7.6844\n",
      "Epoch: 670, Train Loss: 2.0153, Test Loss: 7.8350\n",
      "Epoch: 671, Train Loss: 2.1157, Test Loss: 8.6981\n",
      "Epoch: 672, Train Loss: 2.0210, Test Loss: 8.5352\n",
      "Epoch: 673, Train Loss: 2.0977, Test Loss: 7.9331\n",
      "Epoch: 674, Train Loss: 1.9624, Test Loss: 8.4867\n",
      "Epoch: 675, Train Loss: 2.2880, Test Loss: 9.6594\n",
      "Epoch: 676, Train Loss: 2.2958, Test Loss: 8.0620\n",
      "Epoch: 677, Train Loss: 2.1768, Test Loss: 6.9085\n",
      "Epoch: 678, Train Loss: 2.2798, Test Loss: 9.5433\n",
      "Epoch: 679, Train Loss: 2.0441, Test Loss: 9.7672\n",
      "Epoch: 680, Train Loss: 2.2368, Test Loss: 6.8062\n",
      "Epoch: 681, Train Loss: 2.0839, Test Loss: 6.7710\n",
      "Epoch: 682, Train Loss: 2.3387, Test Loss: 9.6780\n",
      "Epoch: 683, Train Loss: 2.2221, Test Loss: 9.7946\n",
      "Epoch: 684, Train Loss: 2.3646, Test Loss: 6.6954\n",
      "Epoch: 685, Train Loss: 2.1180, Test Loss: 6.6168\n",
      "Epoch: 686, Train Loss: 2.1753, Test Loss: 9.7154\n",
      "Epoch: 687, Train Loss: 2.2031, Test Loss: 10.9706\n",
      "Epoch: 688, Train Loss: 2.2334, Test Loss: 7.9362\n",
      "Epoch: 689, Train Loss: 2.0208, Test Loss: 6.0799\n",
      "Epoch: 690, Train Loss: 2.6166, Test Loss: 8.7352\n",
      "Epoch: 691, Train Loss: 2.0800, Test Loss: 12.0864\n",
      "Epoch: 692, Train Loss: 2.9402, Test Loss: 7.7063\n",
      "Epoch: 693, Train Loss: 1.9245, Test Loss: 5.4839\n",
      "Epoch: 694, Train Loss: 3.1371, Test Loss: 9.1831\n",
      "Epoch: 695, Train Loss: 2.0973, Test Loss: 11.9604\n",
      "Epoch: 696, Train Loss: 2.7648, Test Loss: 7.9352\n",
      "Epoch: 697, Train Loss: 2.0998, Test Loss: 5.4244\n",
      "Epoch: 698, Train Loss: 3.0665, Test Loss: 7.3599\n",
      "Epoch: 699, Train Loss: 2.2727, Test Loss: 11.6460\n",
      "Epoch: 700, Train Loss: 2.8373, Test Loss: 9.4957\n",
      "Epoch: 701, Train Loss: 2.3316, Test Loss: 5.4185\n",
      "Epoch: 702, Train Loss: 2.9021, Test Loss: 6.1841\n",
      "Epoch: 703, Train Loss: 2.4761, Test Loss: 9.7190\n",
      "Epoch: 704, Train Loss: 2.2692, Test Loss: 9.9002\n",
      "Epoch: 705, Train Loss: 2.4070, Test Loss: 6.8972\n",
      "Epoch: 706, Train Loss: 2.2963, Test Loss: 5.6809\n",
      "Epoch: 707, Train Loss: 2.6145, Test Loss: 7.8083\n",
      "Epoch: 708, Train Loss: 2.0894, Test Loss: 10.1376\n",
      "Epoch: 709, Train Loss: 2.2607, Test Loss: 8.6059\n",
      "Epoch: 710, Train Loss: 2.2607, Test Loss: 6.3282\n",
      "Epoch: 711, Train Loss: 2.2972, Test Loss: 6.2125\n",
      "Epoch: 712, Train Loss: 2.3609, Test Loss: 8.3733\n",
      "Epoch: 713, Train Loss: 1.9883, Test Loss: 9.4514\n",
      "Epoch: 714, Train Loss: 2.3034, Test Loss: 7.4119\n",
      "Epoch: 715, Train Loss: 2.0306, Test Loss: 6.5507\n",
      "Epoch: 716, Train Loss: 2.1653, Test Loss: 6.7931\n",
      "Epoch: 717, Train Loss: 2.2351, Test Loss: 8.3964\n",
      "Epoch: 718, Train Loss: 2.1221, Test Loss: 9.1523\n",
      "Epoch: 719, Train Loss: 2.0994, Test Loss: 7.7573\n",
      "Epoch: 720, Train Loss: 1.9576, Test Loss: 6.6785\n",
      "Epoch: 721, Train Loss: 2.3519, Test Loss: 7.4151\n",
      "Epoch: 722, Train Loss: 1.9821, Test Loss: 8.6303\n",
      "Epoch: 723, Train Loss: 2.0034, Test Loss: 8.8851\n",
      "Epoch: 724, Train Loss: 1.9920, Test Loss: 7.2258\n",
      "Epoch: 725, Train Loss: 2.0237, Test Loss: 6.8138\n",
      "Epoch: 726, Train Loss: 2.1542, Test Loss: 8.5873\n",
      "Epoch: 727, Train Loss: 1.9914, Test Loss: 8.7504\n",
      "Epoch: 728, Train Loss: 2.1732, Test Loss: 7.9463\n",
      "Epoch: 729, Train Loss: 2.0635, Test Loss: 7.2022\n",
      "Epoch: 730, Train Loss: 1.9828, Test Loss: 7.3164\n",
      "Epoch: 731, Train Loss: 2.0013, Test Loss: 8.8948\n",
      "Epoch: 732, Train Loss: 2.1524, Test Loss: 8.9532\n",
      "Epoch: 733, Train Loss: 1.9303, Test Loss: 7.4085\n",
      "Epoch: 734, Train Loss: 2.0850, Test Loss: 7.4231\n",
      "Epoch: 735, Train Loss: 2.1054, Test Loss: 7.9698\n",
      "Epoch: 736, Train Loss: 1.9954, Test Loss: 7.8723\n",
      "Epoch: 737, Train Loss: 1.9403, Test Loss: 8.1907\n",
      "Epoch: 738, Train Loss: 2.0468, Test Loss: 7.3269\n",
      "Epoch: 739, Train Loss: 1.9417, Test Loss: 7.8687\n",
      "Epoch: 740, Train Loss: 1.9461, Test Loss: 8.4420\n",
      "Epoch: 741, Train Loss: 2.0738, Test Loss: 8.7216\n",
      "Epoch: 742, Train Loss: 1.9732, Test Loss: 7.6287\n",
      "Epoch: 743, Train Loss: 2.0677, Test Loss: 7.8351\n",
      "Epoch: 744, Train Loss: 1.9919, Test Loss: 8.6972\n",
      "Epoch: 745, Train Loss: 1.9439, Test Loss: 7.5851\n",
      "Epoch: 746, Train Loss: 1.9156, Test Loss: 7.7915\n",
      "Epoch: 747, Train Loss: 1.9975, Test Loss: 9.1273\n",
      "Epoch: 748, Train Loss: 1.9594, Test Loss: 9.4614\n",
      "Epoch: 749, Train Loss: 1.9150, Test Loss: 8.9176\n",
      "Epoch: 750, Train Loss: 2.1096, Test Loss: 7.5641\n",
      "Epoch: 751, Train Loss: 2.1331, Test Loss: 8.0000\n",
      "Epoch: 752, Train Loss: 1.8535, Test Loss: 9.6559\n",
      "Epoch: 753, Train Loss: 1.8821, Test Loss: 9.7162\n",
      "Epoch: 754, Train Loss: 1.9424, Test Loss: 8.5208\n",
      "Epoch: 755, Train Loss: 2.0987, Test Loss: 7.4778\n",
      "Epoch: 756, Train Loss: 1.9619, Test Loss: 8.4891\n",
      "Epoch: 757, Train Loss: 2.1734, Test Loss: 10.1495\n",
      "Epoch: 758, Train Loss: 2.1752, Test Loss: 8.9600\n",
      "Epoch: 759, Train Loss: 1.9548, Test Loss: 6.7988\n",
      "Epoch: 760, Train Loss: 2.3354, Test Loss: 8.1041\n",
      "Epoch: 761, Train Loss: 1.8916, Test Loss: 9.4919\n",
      "Epoch: 762, Train Loss: 2.0303, Test Loss: 9.0709\n",
      "Epoch: 763, Train Loss: 2.0241, Test Loss: 8.0321\n",
      "Epoch: 764, Train Loss: 1.7725, Test Loss: 7.8977\n",
      "Epoch: 765, Train Loss: 2.0603, Test Loss: 9.8253\n",
      "Epoch: 766, Train Loss: 1.9591, Test Loss: 9.6363\n",
      "Epoch: 767, Train Loss: 1.8727, Test Loss: 7.8709\n",
      "Epoch: 768, Train Loss: 2.1357, Test Loss: 7.3064\n",
      "Epoch: 769, Train Loss: 1.8756, Test Loss: 9.5264\n",
      "Epoch: 770, Train Loss: 1.7274, Test Loss: 10.3426\n",
      "Epoch: 771, Train Loss: 2.2121, Test Loss: 8.4164\n",
      "Epoch: 772, Train Loss: 1.8330, Test Loss: 7.3668\n",
      "Epoch: 773, Train Loss: 1.8344, Test Loss: 8.6784\n",
      "Epoch: 774, Train Loss: 2.0685, Test Loss: 12.0058\n",
      "Epoch: 775, Train Loss: 2.3331, Test Loss: 9.1647\n",
      "Epoch: 776, Train Loss: 1.7933, Test Loss: 6.3452\n",
      "Epoch: 777, Train Loss: 2.4404, Test Loss: 9.4419\n",
      "Epoch: 778, Train Loss: 1.8824, Test Loss: 9.7419\n",
      "Epoch: 779, Train Loss: 2.0214, Test Loss: 7.3075\n",
      "Epoch: 780, Train Loss: 1.9841, Test Loss: 7.7563\n",
      "Epoch: 781, Train Loss: 1.8547, Test Loss: 8.8903\n",
      "Epoch: 782, Train Loss: 1.9922, Test Loss: 9.5991\n",
      "Epoch: 783, Train Loss: 1.9631, Test Loss: 7.7726\n",
      "Epoch: 784, Train Loss: 1.8396, Test Loss: 7.1457\n",
      "Epoch: 785, Train Loss: 2.0941, Test Loss: 8.4370\n",
      "Epoch: 786, Train Loss: 1.9977, Test Loss: 9.3230\n",
      "Epoch: 787, Train Loss: 2.0676, Test Loss: 8.1983\n",
      "Epoch: 788, Train Loss: 2.0571, Test Loss: 7.5000\n",
      "Epoch: 789, Train Loss: 2.0524, Test Loss: 7.9334\n",
      "Epoch: 790, Train Loss: 1.8905, Test Loss: 9.5507\n",
      "Epoch: 791, Train Loss: 1.9672, Test Loss: 9.2443\n",
      "Epoch: 792, Train Loss: 2.1162, Test Loss: 7.5692\n",
      "Epoch: 793, Train Loss: 1.8435, Test Loss: 8.2726\n",
      "Epoch: 794, Train Loss: 1.8977, Test Loss: 8.8708\n",
      "Epoch: 795, Train Loss: 1.7973, Test Loss: 9.3469\n",
      "Epoch: 796, Train Loss: 1.8547, Test Loss: 7.5103\n",
      "Epoch: 797, Train Loss: 1.9564, Test Loss: 8.0485\n",
      "Epoch: 798, Train Loss: 2.0475, Test Loss: 9.3262\n",
      "Epoch: 799, Train Loss: 1.8668, Test Loss: 8.9906\n",
      "Epoch: 800, Train Loss: 1.8892, Test Loss: 7.1439\n",
      "Epoch: 801, Train Loss: 1.8583, Test Loss: 8.0486\n",
      "Epoch: 802, Train Loss: 1.7728, Test Loss: 9.7694\n",
      "Epoch: 803, Train Loss: 2.0296, Test Loss: 7.8415\n",
      "Epoch: 804, Train Loss: 1.9097, Test Loss: 7.2369\n",
      "Epoch: 805, Train Loss: 2.1789, Test Loss: 10.0890\n",
      "Epoch: 806, Train Loss: 2.1646, Test Loss: 9.2937\n",
      "Epoch: 807, Train Loss: 2.1406, Test Loss: 6.2407\n",
      "Epoch: 808, Train Loss: 2.1912, Test Loss: 7.5367\n",
      "Epoch: 809, Train Loss: 1.8506, Test Loss: 11.3321\n",
      "Epoch: 810, Train Loss: 2.4663, Test Loss: 8.4027\n",
      "Epoch: 811, Train Loss: 1.8641, Test Loss: 6.4282\n",
      "Epoch: 812, Train Loss: 2.0604, Test Loss: 7.4960\n",
      "Epoch: 813, Train Loss: 1.9686, Test Loss: 10.6226\n",
      "Epoch: 814, Train Loss: 2.1957, Test Loss: 8.9430\n",
      "Epoch: 815, Train Loss: 1.6956, Test Loss: 6.0988\n",
      "Epoch: 816, Train Loss: 2.4170, Test Loss: 7.5680\n",
      "Epoch: 817, Train Loss: 2.0807, Test Loss: 10.8853\n",
      "Epoch: 818, Train Loss: 2.1959, Test Loss: 9.3225\n",
      "Epoch: 819, Train Loss: 1.9802, Test Loss: 6.5683\n",
      "Epoch: 820, Train Loss: 1.9541, Test Loss: 6.6167\n",
      "Epoch: 821, Train Loss: 2.0153, Test Loss: 9.9129\n",
      "Epoch: 822, Train Loss: 1.8752, Test Loss: 9.4187\n",
      "Epoch: 823, Train Loss: 2.0338, Test Loss: 6.4370\n",
      "Epoch: 824, Train Loss: 1.9282, Test Loss: 7.0399\n",
      "Epoch: 825, Train Loss: 1.9688, Test Loss: 9.4901\n",
      "Epoch: 826, Train Loss: 1.8948, Test Loss: 9.2567\n",
      "Epoch: 827, Train Loss: 2.0192, Test Loss: 6.5284\n",
      "Epoch: 828, Train Loss: 2.0227, Test Loss: 6.9668\n",
      "Epoch: 829, Train Loss: 1.9384, Test Loss: 9.6106\n",
      "Epoch: 830, Train Loss: 1.9104, Test Loss: 9.3599\n",
      "Epoch: 831, Train Loss: 1.9553, Test Loss: 6.5280\n",
      "Epoch: 832, Train Loss: 1.9470, Test Loss: 6.8762\n",
      "Epoch: 833, Train Loss: 1.9313, Test Loss: 8.6005\n",
      "Epoch: 834, Train Loss: 1.9063, Test Loss: 8.7975\n",
      "Epoch: 835, Train Loss: 1.9575, Test Loss: 7.2994\n",
      "Epoch: 836, Train Loss: 1.7459, Test Loss: 6.9180\n",
      "Epoch: 837, Train Loss: 2.0410, Test Loss: 7.5198\n",
      "Epoch: 838, Train Loss: 1.8166, Test Loss: 9.1438\n",
      "Epoch: 839, Train Loss: 1.8145, Test Loss: 8.7764\n",
      "Epoch: 840, Train Loss: 1.9526, Test Loss: 7.2562\n",
      "Epoch: 841, Train Loss: 1.8358, Test Loss: 8.2091\n",
      "Epoch: 842, Train Loss: 1.7196, Test Loss: 8.3452\n",
      "Epoch: 843, Train Loss: 1.5916, Test Loss: 8.1479\n",
      "Epoch: 844, Train Loss: 1.6894, Test Loss: 8.4025\n",
      "Epoch: 845, Train Loss: 1.7327, Test Loss: 9.0288\n",
      "Epoch: 846, Train Loss: 1.9486, Test Loss: 8.7866\n",
      "Epoch: 847, Train Loss: 1.7517, Test Loss: 8.0310\n",
      "Epoch: 848, Train Loss: 1.7397, Test Loss: 8.0647\n",
      "Epoch: 849, Train Loss: 1.8602, Test Loss: 9.9367\n",
      "Epoch: 850, Train Loss: 1.8834, Test Loss: 9.4654\n",
      "Epoch: 851, Train Loss: 1.6908, Test Loss: 7.9626\n",
      "Epoch: 852, Train Loss: 1.7795, Test Loss: 7.8623\n",
      "Epoch: 853, Train Loss: 1.9756, Test Loss: 8.8106\n",
      "Epoch: 854, Train Loss: 1.8179, Test Loss: 9.2683\n",
      "Epoch: 855, Train Loss: 1.8551, Test Loss: 9.0172\n",
      "Epoch: 856, Train Loss: 1.8474, Test Loss: 8.4302\n",
      "Epoch: 857, Train Loss: 1.8210, Test Loss: 8.6662\n",
      "Epoch: 858, Train Loss: 1.6474, Test Loss: 9.3000\n",
      "Epoch: 859, Train Loss: 1.7506, Test Loss: 9.7505\n",
      "Epoch: 860, Train Loss: 1.9780, Test Loss: 8.8724\n",
      "Epoch: 861, Train Loss: 1.8261, Test Loss: 7.9806\n",
      "Epoch: 862, Train Loss: 1.7953, Test Loss: 9.2387\n",
      "Epoch: 863, Train Loss: 1.7036, Test Loss: 9.2427\n",
      "Epoch: 864, Train Loss: 1.7571, Test Loss: 8.8293\n",
      "Epoch: 865, Train Loss: 1.7251, Test Loss: 8.0745\n",
      "Epoch: 866, Train Loss: 1.7863, Test Loss: 9.1154\n",
      "Epoch: 867, Train Loss: 1.7879, Test Loss: 10.7761\n",
      "Epoch: 868, Train Loss: 2.1411, Test Loss: 7.3644\n",
      "Epoch: 869, Train Loss: 1.9849, Test Loss: 8.3167\n",
      "Epoch: 870, Train Loss: 1.6728, Test Loss: 10.2445\n",
      "Epoch: 871, Train Loss: 1.9752, Test Loss: 9.0443\n",
      "Epoch: 872, Train Loss: 1.8017, Test Loss: 8.9386\n",
      "Epoch: 873, Train Loss: 1.8418, Test Loss: 9.2319\n",
      "Epoch: 874, Train Loss: 1.8679, Test Loss: 8.6780\n",
      "Epoch: 875, Train Loss: 1.7888, Test Loss: 8.4537\n",
      "Epoch: 876, Train Loss: 1.8284, Test Loss: 9.5151\n",
      "Epoch: 877, Train Loss: 1.9838, Test Loss: 7.8753\n",
      "Epoch: 878, Train Loss: 1.8507, Test Loss: 7.7762\n",
      "Epoch: 879, Train Loss: 1.6851, Test Loss: 9.3569\n",
      "Epoch: 880, Train Loss: 1.7513, Test Loss: 10.8361\n",
      "Epoch: 881, Train Loss: 1.6969, Test Loss: 8.1694\n",
      "Epoch: 882, Train Loss: 1.8850, Test Loss: 7.5363\n",
      "Epoch: 883, Train Loss: 1.9125, Test Loss: 11.4368\n",
      "Epoch: 884, Train Loss: 2.1986, Test Loss: 8.7436\n",
      "Epoch: 885, Train Loss: 1.7358, Test Loss: 7.0218\n",
      "Epoch: 886, Train Loss: 1.7976, Test Loss: 9.3884\n",
      "Epoch: 887, Train Loss: 1.7310, Test Loss: 10.5480\n",
      "Epoch: 888, Train Loss: 1.8036, Test Loss: 8.3377\n",
      "Epoch: 889, Train Loss: 1.7989, Test Loss: 8.2555\n",
      "Epoch: 890, Train Loss: 1.7582, Test Loss: 9.1840\n",
      "Epoch: 891, Train Loss: 1.8792, Test Loss: 9.0901\n",
      "Epoch: 892, Train Loss: 1.8223, Test Loss: 8.0771\n",
      "Epoch: 893, Train Loss: 1.8580, Test Loss: 8.4988\n",
      "Epoch: 894, Train Loss: 1.7217, Test Loss: 9.0996\n",
      "Epoch: 895, Train Loss: 1.7312, Test Loss: 8.7791\n",
      "Epoch: 896, Train Loss: 1.6934, Test Loss: 8.0136\n",
      "Epoch: 897, Train Loss: 1.7126, Test Loss: 7.8406\n",
      "Epoch: 898, Train Loss: 1.6801, Test Loss: 8.6616\n",
      "Epoch: 899, Train Loss: 1.8319, Test Loss: 9.0218\n",
      "Epoch: 900, Train Loss: 1.8585, Test Loss: 8.4994\n",
      "Epoch: 901, Train Loss: 1.6834, Test Loss: 7.5641\n",
      "Epoch: 902, Train Loss: 1.8192, Test Loss: 9.2707\n",
      "Epoch: 903, Train Loss: 1.7787, Test Loss: 8.0695\n",
      "Epoch: 904, Train Loss: 1.6770, Test Loss: 7.8044\n",
      "Epoch: 905, Train Loss: 1.7198, Test Loss: 8.4944\n",
      "Epoch: 906, Train Loss: 1.5610, Test Loss: 8.2612\n",
      "Epoch: 907, Train Loss: 1.9309, Test Loss: 9.1620\n",
      "Epoch: 908, Train Loss: 1.5638, Test Loss: 8.8075\n",
      "Epoch: 909, Train Loss: 1.7410, Test Loss: 8.3874\n",
      "Epoch: 910, Train Loss: 1.8599, Test Loss: 8.1230\n",
      "Epoch: 911, Train Loss: 1.6369, Test Loss: 8.6731\n",
      "Epoch: 912, Train Loss: 1.9207, Test Loss: 9.0448\n",
      "Epoch: 913, Train Loss: 1.8057, Test Loss: 8.7569\n",
      "Epoch: 914, Train Loss: 1.7647, Test Loss: 8.9133\n",
      "Epoch: 915, Train Loss: 1.7952, Test Loss: 8.1343\n",
      "Epoch: 916, Train Loss: 1.6376, Test Loss: 7.6336\n",
      "Epoch: 917, Train Loss: 1.8399, Test Loss: 9.8074\n",
      "Epoch: 918, Train Loss: 1.7764, Test Loss: 8.7137\n",
      "Epoch: 919, Train Loss: 1.6985, Test Loss: 7.2769\n",
      "Epoch: 920, Train Loss: 1.7350, Test Loss: 8.6969\n",
      "Epoch: 921, Train Loss: 1.8043, Test Loss: 9.2421\n",
      "Epoch: 922, Train Loss: 1.6386, Test Loss: 8.3804\n",
      "Epoch: 923, Train Loss: 1.6236, Test Loss: 7.8142\n",
      "Epoch: 924, Train Loss: 1.6659, Test Loss: 9.2050\n",
      "Epoch: 925, Train Loss: 1.7832, Test Loss: 8.9193\n",
      "Epoch: 926, Train Loss: 1.6648, Test Loss: 9.8450\n",
      "Epoch: 927, Train Loss: 1.7390, Test Loss: 8.8091\n",
      "Epoch: 928, Train Loss: 1.7533, Test Loss: 7.0353\n",
      "Epoch: 929, Train Loss: 2.0351, Test Loss: 10.6599\n",
      "Epoch: 930, Train Loss: 1.8870, Test Loss: 9.0980\n",
      "Epoch: 931, Train Loss: 1.6814, Test Loss: 6.4965\n",
      "Epoch: 932, Train Loss: 2.0222, Test Loss: 9.3304\n",
      "Epoch: 933, Train Loss: 1.8088, Test Loss: 9.8631\n",
      "Epoch: 934, Train Loss: 1.6451, Test Loss: 7.8702\n",
      "Epoch: 935, Train Loss: 1.5981, Test Loss: 6.7961\n",
      "Epoch: 936, Train Loss: 1.7637, Test Loss: 9.9506\n",
      "Epoch: 937, Train Loss: 1.8139, Test Loss: 9.0468\n",
      "Epoch: 938, Train Loss: 1.7857, Test Loss: 6.5790\n",
      "Epoch: 939, Train Loss: 1.8669, Test Loss: 8.1519\n",
      "Epoch: 940, Train Loss: 1.6284, Test Loss: 10.3694\n",
      "Epoch: 941, Train Loss: 1.9273, Test Loss: 7.8492\n",
      "Epoch: 942, Train Loss: 1.6115, Test Loss: 5.9271\n",
      "Epoch: 943, Train Loss: 2.1012, Test Loss: 10.4596\n",
      "Epoch: 944, Train Loss: 1.9294, Test Loss: 10.1415\n",
      "Epoch: 945, Train Loss: 1.9683, Test Loss: 5.7098\n",
      "Epoch: 946, Train Loss: 2.2540, Test Loss: 7.3271\n",
      "Epoch: 947, Train Loss: 1.6579, Test Loss: 11.7966\n",
      "Epoch: 948, Train Loss: 2.5261, Test Loss: 7.4447\n",
      "Epoch: 949, Train Loss: 1.6713, Test Loss: 5.6365\n",
      "Epoch: 950, Train Loss: 2.1197, Test Loss: 8.7202\n",
      "Epoch: 951, Train Loss: 1.6328, Test Loss: 10.4585\n",
      "Epoch: 952, Train Loss: 1.9817, Test Loss: 6.9031\n",
      "Epoch: 953, Train Loss: 1.7372, Test Loss: 6.4790\n",
      "Epoch: 954, Train Loss: 1.7923, Test Loss: 8.9312\n",
      "Epoch: 955, Train Loss: 1.9253, Test Loss: 9.0908\n",
      "Epoch: 956, Train Loss: 1.8144, Test Loss: 7.6054\n",
      "Epoch: 957, Train Loss: 1.5841, Test Loss: 6.4155\n",
      "Epoch: 958, Train Loss: 1.7092, Test Loss: 7.7190\n",
      "Epoch: 959, Train Loss: 1.5891, Test Loss: 8.3415\n",
      "Epoch: 960, Train Loss: 1.6542, Test Loss: 6.9550\n",
      "Epoch: 961, Train Loss: 1.6818, Test Loss: 7.4325\n",
      "Epoch: 962, Train Loss: 1.7084, Test Loss: 8.4441\n",
      "Epoch: 963, Train Loss: 1.6893, Test Loss: 8.1626\n",
      "Epoch: 964, Train Loss: 1.6583, Test Loss: 7.6881\n",
      "Epoch: 965, Train Loss: 1.5059, Test Loss: 7.4680\n",
      "Epoch: 966, Train Loss: 1.6735, Test Loss: 7.5282\n",
      "Epoch: 967, Train Loss: 1.6156, Test Loss: 8.4958\n",
      "Epoch: 968, Train Loss: 1.6646, Test Loss: 8.7538\n",
      "Epoch: 969, Train Loss: 1.5847, Test Loss: 7.8910\n",
      "Epoch: 970, Train Loss: 1.6632, Test Loss: 7.2472\n",
      "Epoch: 971, Train Loss: 1.7813, Test Loss: 9.3315\n",
      "Epoch: 972, Train Loss: 1.6816, Test Loss: 9.5635\n",
      "Epoch: 973, Train Loss: 1.6619, Test Loss: 6.2660\n",
      "Epoch: 974, Train Loss: 1.7537, Test Loss: 7.1717\n",
      "Epoch: 975, Train Loss: 1.9137, Test Loss: 11.8585\n",
      "Epoch: 976, Train Loss: 1.9529, Test Loss: 8.7766\n",
      "Epoch: 977, Train Loss: 1.5447, Test Loss: 6.1339\n",
      "Epoch: 978, Train Loss: 1.9290, Test Loss: 7.6244\n",
      "Epoch: 979, Train Loss: 1.4611, Test Loss: 9.8957\n",
      "Epoch: 980, Train Loss: 1.6737, Test Loss: 8.1753\n",
      "Epoch: 981, Train Loss: 1.7147, Test Loss: 5.7633\n",
      "Epoch: 982, Train Loss: 2.1564, Test Loss: 8.7088\n",
      "Epoch: 983, Train Loss: 1.6153, Test Loss: 11.0993\n",
      "Epoch: 984, Train Loss: 2.1651, Test Loss: 6.7726\n",
      "Epoch: 985, Train Loss: 1.6907, Test Loss: 5.6724\n",
      "Epoch: 986, Train Loss: 2.0237, Test Loss: 9.7837\n",
      "Epoch: 987, Train Loss: 1.7396, Test Loss: 10.0577\n",
      "Epoch: 988, Train Loss: 1.8044, Test Loss: 6.5030\n",
      "Epoch: 989, Train Loss: 1.7230, Test Loss: 5.9670\n",
      "Epoch: 990, Train Loss: 1.8047, Test Loss: 9.6754\n",
      "Epoch: 991, Train Loss: 1.9258, Test Loss: 9.1694\n",
      "Epoch: 992, Train Loss: 1.6055, Test Loss: 6.9484\n",
      "Epoch: 993, Train Loss: 1.6838, Test Loss: 6.0771\n",
      "Epoch: 994, Train Loss: 1.8653, Test Loss: 8.5162\n",
      "Epoch: 995, Train Loss: 1.6524, Test Loss: 9.7342\n",
      "Epoch: 996, Train Loss: 1.8461, Test Loss: 6.7964\n",
      "Epoch: 997, Train Loss: 1.5968, Test Loss: 6.1328\n",
      "Epoch: 998, Train Loss: 1.7140, Test Loss: 7.9553\n",
      "Epoch: 999, Train Loss: 1.5096, Test Loss: 8.0256\n",
      "Epoch: 1000, Train Loss: 1.5590, Test Loss: 7.2616\n",
      "Epoch: 1001, Train Loss: 1.5173, Test Loss: 7.5024\n",
      "Epoch: 1002, Train Loss: 1.5129, Test Loss: 7.8211\n",
      "Epoch: 1003, Train Loss: 1.5234, Test Loss: 8.6738\n",
      "Epoch: 1004, Train Loss: 1.7609, Test Loss: 6.7191\n",
      "Epoch: 1005, Train Loss: 1.6666, Test Loss: 6.9402\n",
      "Epoch: 1006, Train Loss: 1.5158, Test Loss: 9.1600\n",
      "Epoch: 1007, Train Loss: 1.7006, Test Loss: 7.7954\n",
      "Epoch: 1008, Train Loss: 1.5763, Test Loss: 6.7652\n",
      "Epoch: 1009, Train Loss: 1.5523, Test Loss: 7.2714\n",
      "Epoch: 1010, Train Loss: 1.5460, Test Loss: 9.8233\n",
      "Epoch: 1011, Train Loss: 1.8383, Test Loss: 7.4032\n",
      "Epoch: 1012, Train Loss: 1.5289, Test Loss: 6.0079\n",
      "Epoch: 1013, Train Loss: 1.8481, Test Loss: 9.0059\n",
      "Epoch: 1014, Train Loss: 1.6322, Test Loss: 9.2642\n",
      "Epoch: 1015, Train Loss: 1.6255, Test Loss: 6.6589\n",
      "Epoch: 1016, Train Loss: 1.8052, Test Loss: 7.0684\n",
      "Epoch: 1017, Train Loss: 1.6579, Test Loss: 10.1585\n",
      "Epoch: 1018, Train Loss: 1.6471, Test Loss: 8.9492\n",
      "Epoch: 1019, Train Loss: 1.8180, Test Loss: 5.2365\n",
      "Epoch: 1020, Train Loss: 2.0435, Test Loss: 6.8389\n",
      "Epoch: 1021, Train Loss: 1.5593, Test Loss: 12.5534\n",
      "Epoch: 1022, Train Loss: 2.3201, Test Loss: 9.1339\n",
      "Epoch: 1023, Train Loss: 1.5999, Test Loss: 4.6287\n",
      "Epoch: 1024, Train Loss: 2.5237, Test Loss: 6.2084\n",
      "Epoch: 1025, Train Loss: 1.7076, Test Loss: 11.7212\n",
      "Epoch: 1026, Train Loss: 2.3442, Test Loss: 9.4047\n",
      "Epoch: 1027, Train Loss: 1.9005, Test Loss: 4.9496\n",
      "Epoch: 1028, Train Loss: 2.1851, Test Loss: 5.5730\n",
      "Epoch: 1029, Train Loss: 1.8260, Test Loss: 10.4468\n",
      "Epoch: 1030, Train Loss: 2.2508, Test Loss: 10.1964\n",
      "Epoch: 1031, Train Loss: 2.1676, Test Loss: 5.4992\n",
      "Epoch: 1032, Train Loss: 1.7050, Test Loss: 4.7313\n",
      "Epoch: 1033, Train Loss: 2.2784, Test Loss: 7.2459\n",
      "Epoch: 1034, Train Loss: 1.5907, Test Loss: 9.6110\n",
      "Epoch: 1035, Train Loss: 2.0981, Test Loss: 7.6034\n",
      "Epoch: 1036, Train Loss: 1.5183, Test Loss: 5.6576\n",
      "Epoch: 1037, Train Loss: 1.8613, Test Loss: 5.8863\n",
      "Epoch: 1038, Train Loss: 1.6433, Test Loss: 7.1052\n",
      "Epoch: 1039, Train Loss: 1.4925, Test Loss: 8.3513\n",
      "Epoch: 1040, Train Loss: 1.9224, Test Loss: 6.7532\n",
      "Epoch: 1041, Train Loss: 1.6744, Test Loss: 6.2223\n",
      "Epoch: 1042, Train Loss: 1.6322, Test Loss: 7.5497\n",
      "Epoch: 1043, Train Loss: 1.8335, Test Loss: 7.5532\n",
      "Epoch: 1044, Train Loss: 1.6082, Test Loss: 6.4440\n",
      "Epoch: 1045, Train Loss: 1.5861, Test Loss: 6.7807\n",
      "Epoch: 1046, Train Loss: 1.5158, Test Loss: 8.3118\n",
      "Epoch: 1047, Train Loss: 1.6475, Test Loss: 7.6347\n",
      "Epoch: 1048, Train Loss: 1.3686, Test Loss: 6.5778\n",
      "Epoch: 1049, Train Loss: 1.6390, Test Loss: 7.1154\n",
      "Epoch: 1050, Train Loss: 1.5036, Test Loss: 8.3112\n",
      "Epoch: 1051, Train Loss: 1.5405, Test Loss: 7.8163\n",
      "Epoch: 1052, Train Loss: 1.5670, Test Loss: 6.8171\n",
      "Epoch: 1053, Train Loss: 1.5870, Test Loss: 6.9937\n",
      "Epoch: 1054, Train Loss: 1.5136, Test Loss: 8.4263\n",
      "Epoch: 1055, Train Loss: 1.4173, Test Loss: 7.6639\n",
      "Epoch: 1056, Train Loss: 1.5793, Test Loss: 6.3404\n",
      "Epoch: 1057, Train Loss: 1.6564, Test Loss: 7.8086\n",
      "Epoch: 1058, Train Loss: 1.5910, Test Loss: 8.6926\n",
      "Epoch: 1059, Train Loss: 1.5572, Test Loss: 7.4997\n",
      "Epoch: 1060, Train Loss: 1.4896, Test Loss: 6.8682\n",
      "Epoch: 1061, Train Loss: 1.5138, Test Loss: 7.9420\n",
      "Epoch: 1062, Train Loss: 1.5407, Test Loss: 7.2813\n",
      "Epoch: 1063, Train Loss: 1.4496, Test Loss: 7.6809\n",
      "Epoch: 1064, Train Loss: 1.6880, Test Loss: 7.3534\n",
      "Epoch: 1065, Train Loss: 1.5684, Test Loss: 7.7896\n",
      "Epoch: 1066, Train Loss: 1.5738, Test Loss: 7.9873\n",
      "Epoch: 1067, Train Loss: 1.5674, Test Loss: 8.5281\n",
      "Epoch: 1068, Train Loss: 1.4910, Test Loss: 7.7157\n",
      "Epoch: 1069, Train Loss: 1.4299, Test Loss: 7.7163\n",
      "Epoch: 1070, Train Loss: 1.5214, Test Loss: 8.0701\n",
      "Epoch: 1071, Train Loss: 1.5169, Test Loss: 8.4286\n",
      "Epoch: 1072, Train Loss: 1.4990, Test Loss: 7.9231\n",
      "Epoch: 1073, Train Loss: 1.5171, Test Loss: 7.1135\n",
      "Epoch: 1074, Train Loss: 1.5264, Test Loss: 7.9312\n",
      "Epoch: 1075, Train Loss: 1.5476, Test Loss: 9.6653\n",
      "Epoch: 1076, Train Loss: 1.6006, Test Loss: 8.2395\n",
      "Epoch: 1077, Train Loss: 1.3601, Test Loss: 7.1779\n",
      "Epoch: 1078, Train Loss: 1.4719, Test Loss: 7.2422\n",
      "Epoch: 1079, Train Loss: 1.5908, Test Loss: 9.4197\n",
      "Epoch: 1080, Train Loss: 1.5118, Test Loss: 8.9819\n",
      "Epoch: 1081, Train Loss: 1.5456, Test Loss: 7.2407\n",
      "Epoch: 1082, Train Loss: 1.5165, Test Loss: 6.5680\n",
      "Epoch: 1083, Train Loss: 1.4765, Test Loss: 7.8380\n",
      "Epoch: 1084, Train Loss: 1.4322, Test Loss: 9.6044\n",
      "Epoch: 1085, Train Loss: 1.6637, Test Loss: 7.1615\n",
      "Epoch: 1086, Train Loss: 1.6701, Test Loss: 7.1055\n",
      "Epoch: 1087, Train Loss: 1.5855, Test Loss: 8.2747\n",
      "Epoch: 1088, Train Loss: 1.4370, Test Loss: 8.0504\n",
      "Epoch: 1089, Train Loss: 1.6831, Test Loss: 6.7390\n",
      "Epoch: 1090, Train Loss: 1.6834, Test Loss: 7.7438\n",
      "Epoch: 1091, Train Loss: 1.5384, Test Loss: 7.9466\n",
      "Epoch: 1092, Train Loss: 1.3777, Test Loss: 7.9586\n",
      "Epoch: 1093, Train Loss: 1.4639, Test Loss: 7.2455\n",
      "Epoch: 1094, Train Loss: 1.5294, Test Loss: 7.9024\n",
      "Epoch: 1095, Train Loss: 1.4774, Test Loss: 8.9331\n",
      "Epoch: 1096, Train Loss: 1.6056, Test Loss: 7.8888\n",
      "Epoch: 1097, Train Loss: 1.4825, Test Loss: 6.9212\n",
      "Epoch: 1098, Train Loss: 1.4885, Test Loss: 6.6511\n",
      "Epoch: 1099, Train Loss: 1.5300, Test Loss: 8.2226\n",
      "Epoch: 1100, Train Loss: 1.4607, Test Loss: 9.8490\n",
      "Epoch: 1101, Train Loss: 1.8467, Test Loss: 6.4119\n",
      "Epoch: 1102, Train Loss: 1.5533, Test Loss: 5.6127\n",
      "Epoch: 1103, Train Loss: 1.7444, Test Loss: 9.5096\n",
      "Epoch: 1104, Train Loss: 1.7014, Test Loss: 8.5824\n",
      "Epoch: 1105, Train Loss: 1.5784, Test Loss: 5.5682\n",
      "Epoch: 1106, Train Loss: 1.6406, Test Loss: 6.2562\n",
      "Epoch: 1107, Train Loss: 1.5187, Test Loss: 9.5688\n",
      "Epoch: 1108, Train Loss: 1.8042, Test Loss: 7.7700\n",
      "Epoch: 1109, Train Loss: 1.3322, Test Loss: 5.6828\n",
      "Epoch: 1110, Train Loss: 1.7647, Test Loss: 7.1812\n",
      "Epoch: 1111, Train Loss: 1.4805, Test Loss: 8.0846\n",
      "Epoch: 1112, Train Loss: 1.5061, Test Loss: 7.3458\n",
      "Epoch: 1113, Train Loss: 1.5488, Test Loss: 6.2069\n",
      "Epoch: 1114, Train Loss: 1.7344, Test Loss: 6.9365\n",
      "Epoch: 1115, Train Loss: 1.4348, Test Loss: 8.2044\n",
      "Epoch: 1116, Train Loss: 1.4844, Test Loss: 8.1780\n",
      "Epoch: 1117, Train Loss: 1.6846, Test Loss: 6.4209\n",
      "Epoch: 1118, Train Loss: 1.4289, Test Loss: 5.7016\n",
      "Epoch: 1119, Train Loss: 1.6746, Test Loss: 7.6416\n",
      "Epoch: 1120, Train Loss: 1.5280, Test Loss: 8.3399\n",
      "Epoch: 1121, Train Loss: 1.4179, Test Loss: 7.1110\n",
      "Epoch: 1122, Train Loss: 1.3152, Test Loss: 5.9264\n",
      "Epoch: 1123, Train Loss: 1.6842, Test Loss: 8.0434\n",
      "Epoch: 1124, Train Loss: 1.5861, Test Loss: 8.2070\n",
      "Epoch: 1125, Train Loss: 1.6092, Test Loss: 6.7413\n",
      "Epoch: 1126, Train Loss: 1.3812, Test Loss: 5.7921\n",
      "Epoch: 1127, Train Loss: 1.5996, Test Loss: 8.4676\n",
      "Epoch: 1128, Train Loss: 1.4750, Test Loss: 8.2154\n",
      "Epoch: 1129, Train Loss: 1.4504, Test Loss: 6.0226\n",
      "Epoch: 1130, Train Loss: 1.4844, Test Loss: 6.3214\n",
      "Epoch: 1131, Train Loss: 1.4318, Test Loss: 7.8240\n",
      "Epoch: 1132, Train Loss: 1.5447, Test Loss: 7.0600\n",
      "Epoch: 1133, Train Loss: 1.3834, Test Loss: 6.0520\n",
      "Epoch: 1134, Train Loss: 1.5347, Test Loss: 6.9037\n",
      "Epoch: 1135, Train Loss: 1.6398, Test Loss: 8.7482\n",
      "Epoch: 1136, Train Loss: 1.6734, Test Loss: 6.8693\n",
      "Epoch: 1137, Train Loss: 1.3932, Test Loss: 6.4219\n",
      "Epoch: 1138, Train Loss: 1.6845, Test Loss: 7.8073\n",
      "Epoch: 1139, Train Loss: 1.5311, Test Loss: 8.7287\n",
      "Epoch: 1140, Train Loss: 1.6726, Test Loss: 5.8372\n",
      "Epoch: 1141, Train Loss: 1.6287, Test Loss: 6.1900\n",
      "Epoch: 1142, Train Loss: 1.5651, Test Loss: 11.1358\n",
      "Epoch: 1143, Train Loss: 2.0411, Test Loss: 8.5765\n",
      "Epoch: 1144, Train Loss: 1.5214, Test Loss: 4.9534\n",
      "Epoch: 1145, Train Loss: 1.8457, Test Loss: 5.6410\n",
      "Epoch: 1146, Train Loss: 1.5355, Test Loss: 8.3322\n",
      "Epoch: 1147, Train Loss: 1.4877, Test Loss: 8.7784\n",
      "Epoch: 1148, Train Loss: 1.7180, Test Loss: 5.8192\n",
      "Epoch: 1149, Train Loss: 1.4927, Test Loss: 5.3489\n",
      "Epoch: 1150, Train Loss: 1.5356, Test Loss: 7.4496\n",
      "Epoch: 1151, Train Loss: 1.4728, Test Loss: 8.7121\n",
      "Epoch: 1152, Train Loss: 1.8909, Test Loss: 5.7870\n",
      "Epoch: 1153, Train Loss: 1.7108, Test Loss: 6.1992\n",
      "Epoch: 1154, Train Loss: 1.3807, Test Loss: 6.7044\n",
      "Epoch: 1155, Train Loss: 1.3898, Test Loss: 7.3675\n",
      "Epoch: 1156, Train Loss: 1.4204, Test Loss: 6.8638\n",
      "Epoch: 1157, Train Loss: 1.5065, Test Loss: 6.5960\n",
      "Epoch: 1158, Train Loss: 1.4175, Test Loss: 6.0925\n",
      "Epoch: 1159, Train Loss: 1.4067, Test Loss: 6.6146\n",
      "Epoch: 1160, Train Loss: 1.2295, Test Loss: 7.8576\n",
      "Epoch: 1161, Train Loss: 1.4953, Test Loss: 6.9581\n",
      "Epoch: 1162, Train Loss: 1.3668, Test Loss: 5.8810\n",
      "Epoch: 1163, Train Loss: 1.4798, Test Loss: 6.9291\n",
      "Epoch: 1164, Train Loss: 1.5073, Test Loss: 7.6848\n",
      "Epoch: 1165, Train Loss: 1.4392, Test Loss: 6.4799\n",
      "Epoch: 1166, Train Loss: 1.3556, Test Loss: 6.4002\n",
      "Epoch: 1167, Train Loss: 1.3433, Test Loss: 7.3310\n",
      "Epoch: 1168, Train Loss: 1.4847, Test Loss: 7.2596\n",
      "Epoch: 1169, Train Loss: 1.3706, Test Loss: 6.6320\n",
      "Epoch: 1170, Train Loss: 1.4149, Test Loss: 6.9235\n",
      "Epoch: 1171, Train Loss: 1.3110, Test Loss: 7.4784\n",
      "Epoch: 1172, Train Loss: 1.2847, Test Loss: 6.9129\n",
      "Epoch: 1173, Train Loss: 1.4352, Test Loss: 6.0177\n",
      "Epoch: 1174, Train Loss: 1.5076, Test Loss: 8.2021\n",
      "Epoch: 1175, Train Loss: 1.3699, Test Loss: 10.3874\n",
      "Epoch: 1176, Train Loss: 1.6954, Test Loss: 6.7146\n",
      "Epoch: 1177, Train Loss: 1.3039, Test Loss: 5.4109\n",
      "Epoch: 1178, Train Loss: 1.5822, Test Loss: 7.6126\n",
      "Epoch: 1179, Train Loss: 1.3537, Test Loss: 9.6870\n",
      "Epoch: 1180, Train Loss: 1.6701, Test Loss: 6.7795\n",
      "Epoch: 1181, Train Loss: 1.1663, Test Loss: 5.4162\n",
      "Epoch: 1182, Train Loss: 1.5071, Test Loss: 6.6755\n",
      "Epoch: 1183, Train Loss: 1.4135, Test Loss: 8.0401\n",
      "Epoch: 1184, Train Loss: 1.4898, Test Loss: 6.6530\n",
      "Epoch: 1185, Train Loss: 1.2065, Test Loss: 6.1390\n",
      "Epoch: 1186, Train Loss: 1.3939, Test Loss: 7.2236\n",
      "Epoch: 1187, Train Loss: 1.4046, Test Loss: 7.6212\n",
      "Epoch: 1188, Train Loss: 1.3869, Test Loss: 6.8845\n",
      "Epoch: 1189, Train Loss: 1.2634, Test Loss: 5.5990\n",
      "Epoch: 1190, Train Loss: 1.5200, Test Loss: 7.8608\n",
      "Epoch: 1191, Train Loss: 1.5112, Test Loss: 9.1329\n",
      "Epoch: 1192, Train Loss: 1.5601, Test Loss: 6.5586\n",
      "Epoch: 1193, Train Loss: 1.3191, Test Loss: 5.6182\n",
      "Epoch: 1194, Train Loss: 1.5190, Test Loss: 7.1162\n",
      "Epoch: 1195, Train Loss: 1.3711, Test Loss: 7.5811\n",
      "Epoch: 1196, Train Loss: 1.4957, Test Loss: 6.3052\n",
      "Epoch: 1197, Train Loss: 1.4696, Test Loss: 5.6967\n",
      "Epoch: 1198, Train Loss: 1.3350, Test Loss: 7.0957\n",
      "Epoch: 1199, Train Loss: 1.2121, Test Loss: 8.1789\n",
      "Epoch: 1200, Train Loss: 1.3863, Test Loss: 6.4487\n",
      "Epoch: 1201, Train Loss: 1.2364, Test Loss: 5.6328\n",
      "Epoch: 1202, Train Loss: 1.5917, Test Loss: 7.7243\n",
      "Epoch: 1203, Train Loss: 1.3360, Test Loss: 7.8626\n",
      "Epoch: 1204, Train Loss: 1.4643, Test Loss: 5.8857\n",
      "Epoch: 1205, Train Loss: 1.4164, Test Loss: 5.6198\n",
      "Epoch: 1206, Train Loss: 1.6620, Test Loss: 7.3597\n",
      "Epoch: 1207, Train Loss: 1.2784, Test Loss: 8.1260\n",
      "Epoch: 1208, Train Loss: 1.4769, Test Loss: 7.7391\n",
      "Epoch: 1209, Train Loss: 1.3894, Test Loss: 5.6645\n",
      "Epoch: 1210, Train Loss: 1.4643, Test Loss: 6.3712\n",
      "Epoch: 1211, Train Loss: 1.4452, Test Loss: 9.1291\n",
      "Epoch: 1212, Train Loss: 1.5929, Test Loss: 7.0040\n",
      "Epoch: 1213, Train Loss: 1.5061, Test Loss: 5.4333\n",
      "Epoch: 1214, Train Loss: 1.7056, Test Loss: 8.0342\n",
      "Epoch: 1215, Train Loss: 1.2048, Test Loss: 8.9089\n",
      "Epoch: 1216, Train Loss: 1.8681, Test Loss: 5.5789\n",
      "Epoch: 1217, Train Loss: 1.4582, Test Loss: 4.9638\n",
      "Epoch: 1218, Train Loss: 1.6614, Test Loss: 7.5117\n",
      "Epoch: 1219, Train Loss: 1.4741, Test Loss: 8.7063\n",
      "Epoch: 1220, Train Loss: 1.6650, Test Loss: 6.0533\n",
      "Epoch: 1221, Train Loss: 1.2862, Test Loss: 5.8206\n",
      "Epoch: 1222, Train Loss: 1.2925, Test Loss: 6.6095\n",
      "Epoch: 1223, Train Loss: 1.4233, Test Loss: 7.2155\n",
      "Epoch: 1224, Train Loss: 1.4398, Test Loss: 7.0721\n",
      "Epoch: 1225, Train Loss: 1.2584, Test Loss: 6.3189\n",
      "Epoch: 1226, Train Loss: 1.3426, Test Loss: 6.5170\n",
      "Epoch: 1227, Train Loss: 1.2514, Test Loss: 6.6600\n",
      "Epoch: 1228, Train Loss: 1.3234, Test Loss: 8.1156\n",
      "Epoch: 1229, Train Loss: 1.5584, Test Loss: 6.5956\n",
      "Epoch: 1230, Train Loss: 1.3161, Test Loss: 5.9084\n",
      "Epoch: 1231, Train Loss: 1.4333, Test Loss: 7.2012\n",
      "Epoch: 1232, Train Loss: 1.3313, Test Loss: 7.8554\n",
      "Epoch: 1233, Train Loss: 1.4127, Test Loss: 6.2658\n",
      "Epoch: 1234, Train Loss: 1.3723, Test Loss: 5.8325\n",
      "Epoch: 1235, Train Loss: 1.2969, Test Loss: 6.8498\n",
      "Epoch: 1236, Train Loss: 1.2506, Test Loss: 7.5650\n",
      "Epoch: 1237, Train Loss: 1.4142, Test Loss: 6.4586\n",
      "Epoch: 1238, Train Loss: 1.3487, Test Loss: 6.1171\n",
      "Epoch: 1239, Train Loss: 1.3904, Test Loss: 7.6827\n",
      "Epoch: 1240, Train Loss: 1.4456, Test Loss: 7.2518\n",
      "Epoch: 1241, Train Loss: 1.3291, Test Loss: 5.8858\n",
      "Epoch: 1242, Train Loss: 1.3349, Test Loss: 6.4078\n",
      "Epoch: 1243, Train Loss: 1.4622, Test Loss: 9.0944\n",
      "Epoch: 1244, Train Loss: 1.5150, Test Loss: 7.3084\n",
      "Epoch: 1245, Train Loss: 1.3244, Test Loss: 5.3937\n",
      "Epoch: 1246, Train Loss: 1.5962, Test Loss: 6.8601\n",
      "Epoch: 1247, Train Loss: 1.2693, Test Loss: 8.7661\n",
      "Epoch: 1248, Train Loss: 1.5557, Test Loss: 6.7978\n",
      "Epoch: 1249, Train Loss: 1.2696, Test Loss: 5.4987\n",
      "Epoch: 1250, Train Loss: 1.5114, Test Loss: 7.4518\n",
      "Epoch: 1251, Train Loss: 1.3417, Test Loss: 9.4816\n",
      "Epoch: 1252, Train Loss: 1.9071, Test Loss: 5.6793\n",
      "Epoch: 1253, Train Loss: 1.5111, Test Loss: 4.8792\n",
      "Epoch: 1254, Train Loss: 1.6544, Test Loss: 7.1024\n",
      "Epoch: 1255, Train Loss: 1.3297, Test Loss: 8.1668\n",
      "Epoch: 1256, Train Loss: 1.4579, Test Loss: 6.3517\n",
      "Epoch: 1257, Train Loss: 1.2344, Test Loss: 4.7842\n",
      "Epoch: 1258, Train Loss: 1.7344, Test Loss: 6.5971\n",
      "Epoch: 1259, Train Loss: 1.2246, Test Loss: 8.9491\n",
      "Epoch: 1260, Train Loss: 1.7681, Test Loss: 6.0871\n",
      "Epoch: 1261, Train Loss: 1.3756, Test Loss: 4.4535\n",
      "Epoch: 1262, Train Loss: 2.0120, Test Loss: 6.8275\n",
      "Epoch: 1263, Train Loss: 1.2460, Test Loss: 8.9439\n",
      "Epoch: 1264, Train Loss: 1.8091, Test Loss: 6.5055\n",
      "Epoch: 1265, Train Loss: 1.2116, Test Loss: 4.4793\n",
      "Epoch: 1266, Train Loss: 1.5073, Test Loss: 5.4178\n",
      "Epoch: 1267, Train Loss: 1.4322, Test Loss: 8.2736\n",
      "Epoch: 1268, Train Loss: 1.6401, Test Loss: 7.0400\n",
      "Epoch: 1269, Train Loss: 1.2784, Test Loss: 4.9933\n",
      "Epoch: 1270, Train Loss: 1.3764, Test Loss: 5.0443\n",
      "Epoch: 1271, Train Loss: 1.4359, Test Loss: 7.1556\n",
      "Epoch: 1272, Train Loss: 1.4834, Test Loss: 7.2019\n",
      "Epoch: 1273, Train Loss: 1.3304, Test Loss: 5.8589\n",
      "Epoch: 1274, Train Loss: 1.2601, Test Loss: 5.1540\n",
      "Epoch: 1275, Train Loss: 1.3093, Test Loss: 6.0664\n",
      "Epoch: 1276, Train Loss: 1.3493, Test Loss: 6.4988\n",
      "Epoch: 1277, Train Loss: 1.2510, Test Loss: 6.2119\n",
      "Epoch: 1278, Train Loss: 1.3341, Test Loss: 6.9320\n",
      "Epoch: 1279, Train Loss: 1.2666, Test Loss: 6.6048\n",
      "Epoch: 1280, Train Loss: 1.1936, Test Loss: 6.0175\n",
      "Epoch: 1281, Train Loss: 1.2795, Test Loss: 6.2784\n",
      "Epoch: 1282, Train Loss: 1.3662, Test Loss: 7.3375\n",
      "Epoch: 1283, Train Loss: 1.2942, Test Loss: 7.3700\n",
      "Epoch: 1284, Train Loss: 1.2693, Test Loss: 5.9987\n",
      "Epoch: 1285, Train Loss: 1.2691, Test Loss: 6.1413\n",
      "Epoch: 1286, Train Loss: 1.4077, Test Loss: 7.8941\n",
      "Epoch: 1287, Train Loss: 1.3140, Test Loss: 7.9076\n",
      "Epoch: 1288, Train Loss: 1.2994, Test Loss: 6.3093\n",
      "Epoch: 1289, Train Loss: 1.3136, Test Loss: 5.6222\n",
      "Epoch: 1290, Train Loss: 1.4391, Test Loss: 6.6561\n",
      "Epoch: 1291, Train Loss: 1.2983, Test Loss: 7.9733\n",
      "Epoch: 1292, Train Loss: 1.4062, Test Loss: 5.8442\n",
      "Epoch: 1293, Train Loss: 1.2970, Test Loss: 5.6512\n",
      "Epoch: 1294, Train Loss: 1.3340, Test Loss: 6.9645\n",
      "Epoch: 1295, Train Loss: 1.1300, Test Loss: 8.5187\n",
      "Epoch: 1296, Train Loss: 1.4465, Test Loss: 6.8255\n",
      "Epoch: 1297, Train Loss: 1.3355, Test Loss: 6.0444\n",
      "Epoch: 1298, Train Loss: 1.3496, Test Loss: 6.6918\n",
      "Epoch: 1299, Train Loss: 1.0919, Test Loss: 6.7966\n",
      "Epoch: 1300, Train Loss: 1.2039, Test Loss: 6.9744\n",
      "Epoch: 1301, Train Loss: 1.2819, Test Loss: 6.4665\n",
      "Epoch: 1302, Train Loss: 1.2413, Test Loss: 5.8573\n",
      "Epoch: 1303, Train Loss: 1.2871, Test Loss: 6.8495\n",
      "Epoch: 1304, Train Loss: 1.2433, Test Loss: 8.1414\n",
      "Epoch: 1305, Train Loss: 1.3235, Test Loss: 6.6875\n",
      "Epoch: 1306, Train Loss: 1.2069, Test Loss: 5.2727\n",
      "Epoch: 1307, Train Loss: 1.4708, Test Loss: 7.4841\n",
      "Epoch: 1308, Train Loss: 1.2559, Test Loss: 8.0146\n",
      "Epoch: 1309, Train Loss: 1.3353, Test Loss: 5.8324\n",
      "Epoch: 1310, Train Loss: 1.2756, Test Loss: 5.2995\n",
      "Epoch: 1311, Train Loss: 1.4026, Test Loss: 6.8504\n",
      "Epoch: 1312, Train Loss: 1.2520, Test Loss: 7.2645\n",
      "Epoch: 1313, Train Loss: 1.2074, Test Loss: 6.5776\n",
      "Epoch: 1314, Train Loss: 1.2140, Test Loss: 6.1865\n",
      "Epoch: 1315, Train Loss: 1.2180, Test Loss: 6.6207\n",
      "Epoch: 1316, Train Loss: 1.2216, Test Loss: 7.0001\n",
      "Epoch: 1317, Train Loss: 1.1727, Test Loss: 7.2793\n",
      "Epoch: 1318, Train Loss: 1.1616, Test Loss: 6.9317\n",
      "Epoch: 1319, Train Loss: 1.2727, Test Loss: 5.4626\n",
      "Epoch: 1320, Train Loss: 1.3999, Test Loss: 6.9815\n",
      "Epoch: 1321, Train Loss: 1.2373, Test Loss: 8.3921\n",
      "Epoch: 1322, Train Loss: 1.2698, Test Loss: 6.7378\n",
      "Epoch: 1323, Train Loss: 1.2093, Test Loss: 5.4243\n",
      "Epoch: 1324, Train Loss: 1.4748, Test Loss: 7.1906\n",
      "Epoch: 1325, Train Loss: 1.2641, Test Loss: 9.0585\n",
      "Epoch: 1326, Train Loss: 1.5489, Test Loss: 6.4050\n",
      "Epoch: 1327, Train Loss: 1.0817, Test Loss: 4.9222\n",
      "Epoch: 1328, Train Loss: 1.6415, Test Loss: 6.5849\n",
      "Epoch: 1329, Train Loss: 1.2563, Test Loss: 8.3823\n",
      "Epoch: 1330, Train Loss: 1.3749, Test Loss: 6.7825\n",
      "Epoch: 1331, Train Loss: 1.2816, Test Loss: 5.3666\n",
      "Epoch: 1332, Train Loss: 1.2962, Test Loss: 6.4548\n",
      "Epoch: 1333, Train Loss: 1.1958, Test Loss: 8.4084\n",
      "Epoch: 1334, Train Loss: 1.4156, Test Loss: 6.9118\n",
      "Epoch: 1335, Train Loss: 1.2938, Test Loss: 4.9393\n",
      "Epoch: 1336, Train Loss: 1.3665, Test Loss: 5.6130\n",
      "Epoch: 1337, Train Loss: 1.3556, Test Loss: 9.1903\n",
      "Epoch: 1338, Train Loss: 1.5815, Test Loss: 7.0759\n",
      "Epoch: 1339, Train Loss: 1.3713, Test Loss: 4.3648\n",
      "Epoch: 1340, Train Loss: 1.6763, Test Loss: 5.3056\n",
      "Epoch: 1341, Train Loss: 1.3977, Test Loss: 9.7808\n",
      "Epoch: 1342, Train Loss: 1.7976, Test Loss: 8.0433\n",
      "Epoch: 1343, Train Loss: 1.4187, Test Loss: 4.4165\n",
      "Epoch: 1344, Train Loss: 1.8692, Test Loss: 4.9136\n",
      "Epoch: 1345, Train Loss: 1.2700, Test Loss: 7.8996\n",
      "Epoch: 1346, Train Loss: 1.3538, Test Loss: 7.9019\n",
      "Epoch: 1347, Train Loss: 1.4355, Test Loss: 5.6666\n",
      "Epoch: 1348, Train Loss: 1.2803, Test Loss: 4.9023\n",
      "Epoch: 1349, Train Loss: 1.4068, Test Loss: 6.0965\n",
      "Epoch: 1350, Train Loss: 1.2042, Test Loss: 8.1169\n",
      "Epoch: 1351, Train Loss: 1.3701, Test Loss: 7.2483\n",
      "Epoch: 1352, Train Loss: 1.3117, Test Loss: 5.1184\n",
      "Epoch: 1353, Train Loss: 1.3607, Test Loss: 4.6040\n",
      "Epoch: 1354, Train Loss: 1.7524, Test Loss: 7.2725\n",
      "Epoch: 1355, Train Loss: 1.5457, Test Loss: 7.5392\n",
      "Epoch: 1356, Train Loss: 1.3744, Test Loss: 5.1570\n",
      "Epoch: 1357, Train Loss: 1.3998, Test Loss: 4.5644\n",
      "Epoch: 1358, Train Loss: 1.3829, Test Loss: 5.6161\n",
      "Epoch: 1359, Train Loss: 1.2701, Test Loss: 9.2429\n",
      "Epoch: 1360, Train Loss: 1.7948, Test Loss: 7.1140\n",
      "Epoch: 1361, Train Loss: 1.2050, Test Loss: 4.5246\n",
      "Epoch: 1362, Train Loss: 1.5916, Test Loss: 5.0009\n",
      "Epoch: 1363, Train Loss: 1.3401, Test Loss: 7.6796\n",
      "Epoch: 1364, Train Loss: 1.2314, Test Loss: 8.0786\n",
      "Epoch: 1365, Train Loss: 1.4552, Test Loss: 5.3583\n",
      "Epoch: 1366, Train Loss: 1.4694, Test Loss: 5.2999\n",
      "Epoch: 1367, Train Loss: 1.2967, Test Loss: 6.4897\n",
      "Epoch: 1368, Train Loss: 1.2010, Test Loss: 6.6829\n",
      "Epoch: 1369, Train Loss: 1.0696, Test Loss: 6.0203\n",
      "Epoch: 1370, Train Loss: 1.3334, Test Loss: 5.2748\n",
      "Epoch: 1371, Train Loss: 1.1634, Test Loss: 5.4622\n",
      "Epoch: 1372, Train Loss: 1.2665, Test Loss: 6.2658\n",
      "Epoch: 1373, Train Loss: 1.1385, Test Loss: 6.8700\n",
      "Epoch: 1374, Train Loss: 1.1743, Test Loss: 6.2661\n",
      "Epoch: 1375, Train Loss: 1.2418, Test Loss: 6.1825\n",
      "Epoch: 1376, Train Loss: 1.1018, Test Loss: 6.3057\n",
      "Epoch: 1377, Train Loss: 1.2900, Test Loss: 5.9597\n",
      "Epoch: 1378, Train Loss: 1.1526, Test Loss: 7.0457\n",
      "Epoch: 1379, Train Loss: 1.2275, Test Loss: 7.0853\n",
      "Epoch: 1380, Train Loss: 1.2671, Test Loss: 5.8067\n",
      "Epoch: 1381, Train Loss: 1.2370, Test Loss: 5.4741\n",
      "Epoch: 1382, Train Loss: 1.2653, Test Loss: 6.4228\n",
      "Epoch: 1383, Train Loss: 1.2867, Test Loss: 7.1119\n",
      "Epoch: 1384, Train Loss: 1.2398, Test Loss: 6.7808\n",
      "Epoch: 1385, Train Loss: 1.2030, Test Loss: 6.2428\n",
      "Epoch: 1386, Train Loss: 1.2309, Test Loss: 6.4463\n",
      "Epoch: 1387, Train Loss: 1.0947, Test Loss: 6.6658\n",
      "Epoch: 1388, Train Loss: 1.2097, Test Loss: 6.8876\n",
      "Epoch: 1389, Train Loss: 1.2185, Test Loss: 6.4325\n",
      "Epoch: 1390, Train Loss: 1.2946, Test Loss: 5.7388\n",
      "Epoch: 1391, Train Loss: 1.2904, Test Loss: 6.1229\n",
      "Epoch: 1392, Train Loss: 1.2551, Test Loss: 7.1314\n",
      "Epoch: 1393, Train Loss: 1.1621, Test Loss: 6.6851\n",
      "Epoch: 1394, Train Loss: 1.2317, Test Loss: 6.4632\n",
      "Epoch: 1395, Train Loss: 1.1525, Test Loss: 6.8257\n",
      "Epoch: 1396, Train Loss: 1.2226, Test Loss: 6.7816\n",
      "Epoch: 1397, Train Loss: 1.1106, Test Loss: 6.3800\n",
      "Epoch: 1398, Train Loss: 1.1611, Test Loss: 6.8131\n",
      "Epoch: 1399, Train Loss: 1.0743, Test Loss: 6.6420\n",
      "Epoch: 1400, Train Loss: 1.2156, Test Loss: 6.3294\n",
      "Epoch: 1401, Train Loss: 1.1298, Test Loss: 6.5344\n",
      "Epoch: 1402, Train Loss: 1.1933, Test Loss: 6.7197\n",
      "Epoch: 1403, Train Loss: 1.3468, Test Loss: 6.8045\n",
      "Epoch: 1404, Train Loss: 1.1390, Test Loss: 6.6501\n",
      "Epoch: 1405, Train Loss: 1.1039, Test Loss: 6.4196\n",
      "Epoch: 1406, Train Loss: 1.0927, Test Loss: 6.5736\n",
      "Epoch: 1407, Train Loss: 1.1584, Test Loss: 6.2726\n",
      "Epoch: 1408, Train Loss: 1.1661, Test Loss: 6.7612\n",
      "Epoch: 1409, Train Loss: 1.0718, Test Loss: 6.6809\n",
      "Epoch: 1410, Train Loss: 1.2057, Test Loss: 7.3272\n",
      "Epoch: 1411, Train Loss: 1.2968, Test Loss: 5.9517\n",
      "Epoch: 1412, Train Loss: 1.1693, Test Loss: 5.5017\n",
      "Epoch: 1413, Train Loss: 1.1162, Test Loss: 6.0861\n",
      "Epoch: 1414, Train Loss: 1.2889, Test Loss: 7.1725\n",
      "Epoch: 1415, Train Loss: 1.2383, Test Loss: 6.8075\n",
      "Epoch: 1416, Train Loss: 1.2039, Test Loss: 6.1861\n",
      "Epoch: 1417, Train Loss: 1.2241, Test Loss: 5.3841\n",
      "Epoch: 1418, Train Loss: 1.2788, Test Loss: 6.4468\n",
      "Epoch: 1419, Train Loss: 1.2430, Test Loss: 6.8428\n",
      "Epoch: 1420, Train Loss: 1.2288, Test Loss: 5.6706\n",
      "Epoch: 1421, Train Loss: 1.0857, Test Loss: 5.3153\n",
      "Epoch: 1422, Train Loss: 1.2372, Test Loss: 6.3107\n",
      "Epoch: 1423, Train Loss: 0.9848, Test Loss: 7.5053\n",
      "Epoch: 1424, Train Loss: 1.2809, Test Loss: 6.5292\n",
      "Epoch: 1425, Train Loss: 1.1209, Test Loss: 6.0192\n",
      "Epoch: 1426, Train Loss: 1.1931, Test Loss: 5.4768\n",
      "Epoch: 1427, Train Loss: 1.2728, Test Loss: 6.4234\n",
      "Epoch: 1428, Train Loss: 1.0666, Test Loss: 7.2282\n",
      "Epoch: 1429, Train Loss: 1.2020, Test Loss: 6.8838\n",
      "Epoch: 1430, Train Loss: 1.2257, Test Loss: 5.2399\n",
      "Epoch: 1431, Train Loss: 1.0877, Test Loss: 5.3328\n",
      "Epoch: 1432, Train Loss: 1.2095, Test Loss: 7.2611\n",
      "Epoch: 1433, Train Loss: 1.0565, Test Loss: 8.5000\n",
      "Epoch: 1434, Train Loss: 1.3329, Test Loss: 5.4443\n",
      "Epoch: 1435, Train Loss: 1.2582, Test Loss: 5.0500\n",
      "Epoch: 1436, Train Loss: 1.3209, Test Loss: 8.0197\n",
      "Epoch: 1437, Train Loss: 1.2916, Test Loss: 7.9581\n",
      "Epoch: 1438, Train Loss: 1.2026, Test Loss: 5.0365\n",
      "Epoch: 1439, Train Loss: 1.4979, Test Loss: 6.0920\n",
      "Epoch: 1440, Train Loss: 1.0613, Test Loss: 7.9188\n",
      "Epoch: 1441, Train Loss: 1.3526, Test Loss: 6.3946\n",
      "Epoch: 1442, Train Loss: 1.0158, Test Loss: 4.9882\n",
      "Epoch: 1443, Train Loss: 1.4006, Test Loss: 5.8991\n",
      "Epoch: 1444, Train Loss: 1.1392, Test Loss: 6.4723\n",
      "Epoch: 1445, Train Loss: 1.1527, Test Loss: 5.7739\n",
      "Epoch: 1446, Train Loss: 1.1225, Test Loss: 6.3121\n",
      "Epoch: 1447, Train Loss: 1.1009, Test Loss: 6.2805\n",
      "Epoch: 1448, Train Loss: 1.1133, Test Loss: 5.6254\n",
      "Epoch: 1449, Train Loss: 1.0688, Test Loss: 5.8466\n",
      "Epoch: 1450, Train Loss: 1.0782, Test Loss: 6.9736\n",
      "Epoch: 1451, Train Loss: 1.2497, Test Loss: 6.0788\n",
      "Epoch: 1452, Train Loss: 1.1290, Test Loss: 5.2150\n",
      "Epoch: 1453, Train Loss: 1.2461, Test Loss: 6.6961\n",
      "Epoch: 1454, Train Loss: 1.0523, Test Loss: 7.3451\n",
      "Epoch: 1455, Train Loss: 1.3716, Test Loss: 5.0225\n",
      "Epoch: 1456, Train Loss: 1.2515, Test Loss: 5.1932\n",
      "Epoch: 1457, Train Loss: 1.1335, Test Loss: 6.6477\n",
      "Epoch: 1458, Train Loss: 0.9666, Test Loss: 7.5645\n",
      "Epoch: 1459, Train Loss: 1.1744, Test Loss: 5.6281\n",
      "Epoch: 1460, Train Loss: 1.0940, Test Loss: 5.4200\n",
      "Epoch: 1461, Train Loss: 1.2558, Test Loss: 7.0535\n",
      "Epoch: 1462, Train Loss: 1.1133, Test Loss: 7.4675\n",
      "Epoch: 1463, Train Loss: 1.1601, Test Loss: 5.3840\n",
      "Epoch: 1464, Train Loss: 1.2158, Test Loss: 5.6695\n",
      "Epoch: 1465, Train Loss: 1.1405, Test Loss: 7.5863\n",
      "Epoch: 1466, Train Loss: 1.2386, Test Loss: 6.6492\n",
      "Epoch: 1467, Train Loss: 1.1376, Test Loss: 5.3260\n",
      "Epoch: 1468, Train Loss: 1.2808, Test Loss: 6.3311\n",
      "Epoch: 1469, Train Loss: 1.0235, Test Loss: 7.4682\n",
      "Epoch: 1470, Train Loss: 1.1481, Test Loss: 6.4721\n",
      "Epoch: 1471, Train Loss: 1.2469, Test Loss: 4.4545\n",
      "Epoch: 1472, Train Loss: 1.4134, Test Loss: 5.8206\n",
      "Epoch: 1473, Train Loss: 1.1507, Test Loss: 8.5155\n",
      "Epoch: 1474, Train Loss: 1.3258, Test Loss: 6.8336\n",
      "Epoch: 1475, Train Loss: 1.0840, Test Loss: 4.9850\n",
      "Epoch: 1476, Train Loss: 1.2449, Test Loss: 5.8731\n",
      "Epoch: 1477, Train Loss: 1.1002, Test Loss: 7.6948\n",
      "Epoch: 1478, Train Loss: 1.1184, Test Loss: 7.1139\n",
      "Epoch: 1479, Train Loss: 1.1753, Test Loss: 4.4792\n",
      "Epoch: 1480, Train Loss: 1.2169, Test Loss: 4.2596\n",
      "Epoch: 1481, Train Loss: 1.6477, Test Loss: 8.4576\n",
      "Epoch: 1482, Train Loss: 1.5195, Test Loss: 8.5331\n",
      "Epoch: 1483, Train Loss: 1.8257, Test Loss: 4.2469\n",
      "Epoch: 1484, Train Loss: 1.3906, Test Loss: 4.0541\n",
      "Epoch: 1485, Train Loss: 1.5980, Test Loss: 7.3833\n",
      "Epoch: 1486, Train Loss: 1.3005, Test Loss: 8.0705\n",
      "Epoch: 1487, Train Loss: 1.4614, Test Loss: 5.0414\n",
      "Epoch: 1488, Train Loss: 1.1283, Test Loss: 3.9767\n",
      "Epoch: 1489, Train Loss: 1.4839, Test Loss: 5.3670\n",
      "Epoch: 1490, Train Loss: 1.1982, Test Loss: 6.7526\n",
      "Epoch: 1491, Train Loss: 1.1708, Test Loss: 6.6044\n",
      "Epoch: 1492, Train Loss: 1.2241, Test Loss: 4.8608\n",
      "Epoch: 1493, Train Loss: 1.1575, Test Loss: 4.7620\n",
      "Epoch: 1494, Train Loss: 1.1847, Test Loss: 5.8534\n",
      "Epoch: 1495, Train Loss: 1.1191, Test Loss: 6.4208\n",
      "Epoch: 1496, Train Loss: 1.0742, Test Loss: 6.2365\n",
      "Epoch: 1497, Train Loss: 1.0692, Test Loss: 5.4439\n",
      "Epoch: 1498, Train Loss: 1.1104, Test Loss: 5.3193\n",
      "Epoch: 1499, Train Loss: 1.1344, Test Loss: 5.9123\n",
      "Epoch: 1500, Train Loss: 1.0179, Test Loss: 6.9006\n",
      "Epoch: 1501, Train Loss: 1.1219, Test Loss: 5.5827\n",
      "Epoch: 1502, Train Loss: 1.0561, Test Loss: 5.4519\n",
      "Epoch: 1503, Train Loss: 1.1859, Test Loss: 6.6956\n",
      "Epoch: 1504, Train Loss: 1.0945, Test Loss: 6.9266\n",
      "Epoch: 1505, Train Loss: 1.1089, Test Loss: 5.6939\n",
      "Epoch: 1506, Train Loss: 1.0741, Test Loss: 5.2551\n",
      "Epoch: 1507, Train Loss: 1.2157, Test Loss: 6.1742\n",
      "Epoch: 1508, Train Loss: 1.0790, Test Loss: 6.6609\n",
      "Epoch: 1509, Train Loss: 1.1262, Test Loss: 5.6362\n",
      "Epoch: 1510, Train Loss: 0.9885, Test Loss: 5.3341\n",
      "Epoch: 1511, Train Loss: 1.2510, Test Loss: 6.4807\n",
      "Epoch: 1512, Train Loss: 1.0961, Test Loss: 7.3163\n",
      "Epoch: 1513, Train Loss: 1.2338, Test Loss: 5.2433\n",
      "Epoch: 1514, Train Loss: 1.1207, Test Loss: 4.8840\n",
      "Epoch: 1515, Train Loss: 1.2464, Test Loss: 6.4587\n",
      "Epoch: 1516, Train Loss: 1.1175, Test Loss: 7.0592\n",
      "Epoch: 1517, Train Loss: 1.1776, Test Loss: 5.4490\n",
      "Epoch: 1518, Train Loss: 1.1410, Test Loss: 5.1531\n",
      "Epoch: 1519, Train Loss: 1.1098, Test Loss: 5.6486\n",
      "Epoch: 1520, Train Loss: 1.0199, Test Loss: 6.3819\n",
      "Epoch: 1521, Train Loss: 1.0419, Test Loss: 6.8006\n",
      "Epoch: 1522, Train Loss: 1.1500, Test Loss: 5.6022\n",
      "Epoch: 1523, Train Loss: 1.0068, Test Loss: 5.2425\n",
      "Epoch: 1524, Train Loss: 1.2033, Test Loss: 5.6579\n",
      "Epoch: 1525, Train Loss: 1.0393, Test Loss: 6.2275\n",
      "Epoch: 1526, Train Loss: 1.1439, Test Loss: 5.7503\n",
      "Epoch: 1527, Train Loss: 1.0756, Test Loss: 5.3864\n",
      "Epoch: 1528, Train Loss: 1.1521, Test Loss: 6.1362\n",
      "Epoch: 1529, Train Loss: 1.0482, Test Loss: 5.7622\n",
      "Epoch: 1530, Train Loss: 1.0297, Test Loss: 6.0113\n",
      "Epoch: 1531, Train Loss: 1.0338, Test Loss: 6.5977\n",
      "Epoch: 1532, Train Loss: 1.0796, Test Loss: 6.0311\n",
      "Epoch: 1533, Train Loss: 0.9845, Test Loss: 5.4795\n",
      "Epoch: 1534, Train Loss: 1.2126, Test Loss: 6.3404\n",
      "Epoch: 1535, Train Loss: 1.1052, Test Loss: 6.6475\n",
      "Epoch: 1536, Train Loss: 1.0542, Test Loss: 5.6725\n",
      "Epoch: 1537, Train Loss: 0.9792, Test Loss: 5.7486\n",
      "Epoch: 1538, Train Loss: 1.0503, Test Loss: 6.0532\n",
      "Epoch: 1539, Train Loss: 1.0931, Test Loss: 5.7313\n",
      "Epoch: 1540, Train Loss: 1.0428, Test Loss: 6.1209\n",
      "Epoch: 1541, Train Loss: 1.0450, Test Loss: 5.7053\n",
      "Epoch: 1542, Train Loss: 1.1102, Test Loss: 5.6557\n",
      "Epoch: 1543, Train Loss: 1.0725, Test Loss: 6.7565\n",
      "Epoch: 1544, Train Loss: 1.1634, Test Loss: 5.5199\n",
      "Epoch: 1545, Train Loss: 1.0998, Test Loss: 5.6366\n",
      "Epoch: 1546, Train Loss: 1.0613, Test Loss: 6.5774\n",
      "Epoch: 1547, Train Loss: 1.0120, Test Loss: 6.2985\n",
      "Epoch: 1548, Train Loss: 1.0174, Test Loss: 5.2087\n",
      "Epoch: 1549, Train Loss: 1.0343, Test Loss: 5.2217\n",
      "Epoch: 1550, Train Loss: 1.0953, Test Loss: 7.4558\n",
      "Epoch: 1551, Train Loss: 1.3030, Test Loss: 5.8835\n",
      "Epoch: 1552, Train Loss: 0.9634, Test Loss: 5.2572\n",
      "Epoch: 1553, Train Loss: 1.0217, Test Loss: 6.1457\n",
      "Epoch: 1554, Train Loss: 1.0876, Test Loss: 6.2600\n",
      "Epoch: 1555, Train Loss: 1.0394, Test Loss: 5.4872\n",
      "Epoch: 1556, Train Loss: 1.0816, Test Loss: 5.4118\n",
      "Epoch: 1557, Train Loss: 1.0097, Test Loss: 5.5542\n",
      "Epoch: 1558, Train Loss: 1.0478, Test Loss: 6.2606\n",
      "Epoch: 1559, Train Loss: 1.0189, Test Loss: 5.9796\n",
      "Epoch: 1560, Train Loss: 1.0659, Test Loss: 6.0090\n",
      "Epoch: 1561, Train Loss: 0.9982, Test Loss: 5.6123\n",
      "Epoch: 1562, Train Loss: 0.9358, Test Loss: 5.6017\n",
      "Epoch: 1563, Train Loss: 1.0587, Test Loss: 5.0583\n",
      "Epoch: 1564, Train Loss: 1.0271, Test Loss: 5.5454\n",
      "Epoch: 1565, Train Loss: 1.0589, Test Loss: 6.4977\n",
      "Epoch: 1566, Train Loss: 0.9958, Test Loss: 6.7416\n",
      "Epoch: 1567, Train Loss: 1.0703, Test Loss: 5.1443\n",
      "Epoch: 1568, Train Loss: 1.0406, Test Loss: 5.1865\n",
      "Epoch: 1569, Train Loss: 1.0545, Test Loss: 6.7186\n",
      "Epoch: 1570, Train Loss: 1.1097, Test Loss: 6.2319\n",
      "Epoch: 1571, Train Loss: 1.1707, Test Loss: 4.7302\n",
      "Epoch: 1572, Train Loss: 1.0449, Test Loss: 5.0913\n",
      "Epoch: 1573, Train Loss: 1.1039, Test Loss: 6.8061\n",
      "Epoch: 1574, Train Loss: 1.0849, Test Loss: 5.9631\n",
      "Epoch: 1575, Train Loss: 1.0147, Test Loss: 4.9210\n",
      "Epoch: 1576, Train Loss: 1.1169, Test Loss: 5.2773\n",
      "Epoch: 1577, Train Loss: 1.0829, Test Loss: 5.7935\n",
      "Epoch: 1578, Train Loss: 1.0281, Test Loss: 5.8102\n",
      "Epoch: 1579, Train Loss: 0.9702, Test Loss: 5.3211\n",
      "Epoch: 1580, Train Loss: 1.0278, Test Loss: 5.7851\n",
      "Epoch: 1581, Train Loss: 1.0218, Test Loss: 6.1162\n",
      "Epoch: 1582, Train Loss: 0.9957, Test Loss: 5.5932\n",
      "Epoch: 1583, Train Loss: 1.0166, Test Loss: 5.4056\n",
      "Epoch: 1584, Train Loss: 1.0121, Test Loss: 6.3575\n",
      "Epoch: 1585, Train Loss: 1.0772, Test Loss: 5.9790\n",
      "Epoch: 1586, Train Loss: 1.0102, Test Loss: 5.7005\n",
      "Epoch: 1587, Train Loss: 0.9261, Test Loss: 5.4290\n",
      "Epoch: 1588, Train Loss: 1.0843, Test Loss: 5.5195\n",
      "Epoch: 1589, Train Loss: 1.0228, Test Loss: 5.4796\n",
      "Epoch: 1590, Train Loss: 1.0214, Test Loss: 5.6690\n",
      "Epoch: 1591, Train Loss: 1.0276, Test Loss: 6.3180\n",
      "Epoch: 1592, Train Loss: 0.9873, Test Loss: 5.8615\n",
      "Epoch: 1593, Train Loss: 1.0327, Test Loss: 5.6304\n",
      "Epoch: 1594, Train Loss: 0.9929, Test Loss: 6.1748\n",
      "Epoch: 1595, Train Loss: 1.1290, Test Loss: 6.3462\n",
      "Epoch: 1596, Train Loss: 1.0455, Test Loss: 5.4181\n",
      "Epoch: 1597, Train Loss: 1.0914, Test Loss: 5.7375\n",
      "Epoch: 1598, Train Loss: 1.0287, Test Loss: 6.2707\n",
      "Epoch: 1599, Train Loss: 1.0112, Test Loss: 7.1309\n",
      "Epoch: 1600, Train Loss: 1.3052, Test Loss: 4.3002\n",
      "Epoch: 1601, Train Loss: 1.3543, Test Loss: 5.2547\n",
      "Epoch: 1602, Train Loss: 1.0625, Test Loss: 8.3486\n",
      "Epoch: 1603, Train Loss: 1.3761, Test Loss: 6.2975\n",
      "Epoch: 1604, Train Loss: 0.9838, Test Loss: 4.6479\n",
      "Epoch: 1605, Train Loss: 1.1440, Test Loss: 5.5998\n",
      "Epoch: 1606, Train Loss: 0.9452, Test Loss: 7.5679\n",
      "Epoch: 1607, Train Loss: 1.0881, Test Loss: 6.9945\n",
      "Epoch: 1608, Train Loss: 1.0697, Test Loss: 5.0025\n",
      "Epoch: 1609, Train Loss: 1.1360, Test Loss: 5.1736\n",
      "Epoch: 1610, Train Loss: 1.0056, Test Loss: 5.9208\n",
      "Epoch: 1611, Train Loss: 0.9240, Test Loss: 5.9955\n",
      "Epoch: 1612, Train Loss: 0.9595, Test Loss: 6.1473\n",
      "Epoch: 1613, Train Loss: 1.0946, Test Loss: 5.8543\n",
      "Epoch: 1614, Train Loss: 1.0280, Test Loss: 5.0328\n",
      "Epoch: 1615, Train Loss: 1.0320, Test Loss: 6.1601\n",
      "Epoch: 1616, Train Loss: 0.9856, Test Loss: 6.5039\n",
      "Epoch: 1617, Train Loss: 1.1186, Test Loss: 5.5184\n",
      "Epoch: 1618, Train Loss: 1.0803, Test Loss: 5.2032\n",
      "Epoch: 1619, Train Loss: 1.0302, Test Loss: 6.2248\n",
      "Epoch: 1620, Train Loss: 1.1007, Test Loss: 5.8401\n",
      "Epoch: 1621, Train Loss: 0.9787, Test Loss: 5.0653\n",
      "Epoch: 1622, Train Loss: 0.9653, Test Loss: 5.6598\n",
      "Epoch: 1623, Train Loss: 0.9756, Test Loss: 6.5992\n",
      "Epoch: 1624, Train Loss: 0.9408, Test Loss: 6.2483\n",
      "Epoch: 1625, Train Loss: 0.9874, Test Loss: 5.2303\n",
      "Epoch: 1626, Train Loss: 1.0371, Test Loss: 5.5796\n",
      "Epoch: 1627, Train Loss: 1.0815, Test Loss: 7.3189\n",
      "Epoch: 1628, Train Loss: 1.1272, Test Loss: 5.7990\n",
      "Epoch: 1629, Train Loss: 1.0180, Test Loss: 4.3219\n",
      "Epoch: 1630, Train Loss: 1.1278, Test Loss: 4.9159\n",
      "Epoch: 1631, Train Loss: 0.9044, Test Loss: 6.5157\n",
      "Epoch: 1632, Train Loss: 0.9898, Test Loss: 6.3183\n",
      "Epoch: 1633, Train Loss: 0.9357, Test Loss: 5.1250\n",
      "Epoch: 1634, Train Loss: 0.9344, Test Loss: 4.8897\n",
      "Epoch: 1635, Train Loss: 0.9605, Test Loss: 5.7616\n",
      "Epoch: 1636, Train Loss: 1.0348, Test Loss: 6.9546\n",
      "Epoch: 1637, Train Loss: 1.0942, Test Loss: 5.3044\n",
      "Epoch: 1638, Train Loss: 0.9465, Test Loss: 4.8081\n",
      "Epoch: 1639, Train Loss: 1.0412, Test Loss: 6.0188\n",
      "Epoch: 1640, Train Loss: 1.0327, Test Loss: 5.4543\n",
      "Epoch: 1641, Train Loss: 0.9613, Test Loss: 5.7238\n",
      "Epoch: 1642, Train Loss: 0.9608, Test Loss: 5.7889\n",
      "Epoch: 1643, Train Loss: 0.9145, Test Loss: 5.4169\n",
      "Epoch: 1644, Train Loss: 0.9400, Test Loss: 6.7411\n",
      "Epoch: 1645, Train Loss: 1.0663, Test Loss: 4.9377\n",
      "Epoch: 1646, Train Loss: 1.0395, Test Loss: 4.9644\n",
      "Epoch: 1647, Train Loss: 1.1319, Test Loss: 6.2493\n",
      "Epoch: 1648, Train Loss: 1.0252, Test Loss: 5.5906\n",
      "Epoch: 1649, Train Loss: 0.9516, Test Loss: 5.8620\n",
      "Epoch: 1650, Train Loss: 0.9800, Test Loss: 5.6033\n",
      "Epoch: 1651, Train Loss: 0.8759, Test Loss: 5.6264\n",
      "Epoch: 1652, Train Loss: 0.9163, Test Loss: 5.2901\n",
      "Epoch: 1653, Train Loss: 1.0517, Test Loss: 5.2650\n",
      "Epoch: 1654, Train Loss: 0.9971, Test Loss: 5.4575\n",
      "Epoch: 1655, Train Loss: 0.9433, Test Loss: 5.6210\n",
      "Epoch: 1656, Train Loss: 0.9491, Test Loss: 5.1118\n",
      "Epoch: 1657, Train Loss: 0.9778, Test Loss: 5.3929\n",
      "Epoch: 1658, Train Loss: 0.9921, Test Loss: 6.0490\n",
      "Epoch: 1659, Train Loss: 0.8935, Test Loss: 5.9178\n",
      "Epoch: 1660, Train Loss: 0.9150, Test Loss: 5.0367\n",
      "Epoch: 1661, Train Loss: 1.0087, Test Loss: 6.2648\n",
      "Epoch: 1662, Train Loss: 0.9564, Test Loss: 6.2318\n",
      "Epoch: 1663, Train Loss: 0.8843, Test Loss: 5.2113\n",
      "Epoch: 1664, Train Loss: 1.0069, Test Loss: 4.9851\n",
      "Epoch: 1665, Train Loss: 1.0152, Test Loss: 6.7433\n",
      "Epoch: 1666, Train Loss: 1.0127, Test Loss: 6.0878\n",
      "Epoch: 1667, Train Loss: 0.9506, Test Loss: 5.1541\n",
      "Epoch: 1668, Train Loss: 0.9495, Test Loss: 5.1438\n",
      "Epoch: 1669, Train Loss: 0.9459, Test Loss: 6.2461\n",
      "Epoch: 1670, Train Loss: 1.0765, Test Loss: 5.6435\n",
      "Epoch: 1671, Train Loss: 1.0241, Test Loss: 4.8767\n",
      "Epoch: 1672, Train Loss: 1.0189, Test Loss: 4.8657\n",
      "Epoch: 1673, Train Loss: 1.0387, Test Loss: 5.9781\n",
      "Epoch: 1674, Train Loss: 0.9689, Test Loss: 5.8813\n",
      "Epoch: 1675, Train Loss: 0.9179, Test Loss: 5.1138\n",
      "Epoch: 1676, Train Loss: 1.0317, Test Loss: 5.8327\n",
      "Epoch: 1677, Train Loss: 0.9490, Test Loss: 6.1100\n",
      "Epoch: 1678, Train Loss: 1.1034, Test Loss: 4.4863\n",
      "Epoch: 1679, Train Loss: 1.0914, Test Loss: 5.4184\n",
      "Epoch: 1680, Train Loss: 0.9114, Test Loss: 6.7200\n",
      "Epoch: 1681, Train Loss: 0.9893, Test Loss: 5.9065\n",
      "Epoch: 1682, Train Loss: 0.9026, Test Loss: 4.4879\n",
      "Epoch: 1683, Train Loss: 0.9974, Test Loss: 5.4944\n",
      "Epoch: 1684, Train Loss: 0.8234, Test Loss: 7.6438\n",
      "Epoch: 1685, Train Loss: 1.2152, Test Loss: 5.1990\n",
      "Epoch: 1686, Train Loss: 0.9114, Test Loss: 4.2870\n",
      "Epoch: 1687, Train Loss: 1.0491, Test Loss: 5.6689\n",
      "Epoch: 1688, Train Loss: 0.9495, Test Loss: 7.8230\n",
      "Epoch: 1689, Train Loss: 1.3291, Test Loss: 5.0695\n",
      "Epoch: 1690, Train Loss: 0.9556, Test Loss: 3.9799\n",
      "Epoch: 1691, Train Loss: 1.4414, Test Loss: 6.0601\n",
      "Epoch: 1692, Train Loss: 0.9058, Test Loss: 7.5499\n",
      "Epoch: 1693, Train Loss: 1.1308, Test Loss: 5.6673\n",
      "Epoch: 1694, Train Loss: 0.9097, Test Loss: 4.4947\n",
      "Epoch: 1695, Train Loss: 1.1047, Test Loss: 5.0615\n",
      "Epoch: 1696, Train Loss: 0.9743, Test Loss: 6.8479\n",
      "Epoch: 1697, Train Loss: 1.0663, Test Loss: 6.1941\n",
      "Epoch: 1698, Train Loss: 1.0710, Test Loss: 5.0031\n",
      "Epoch: 1699, Train Loss: 0.9012, Test Loss: 4.6561\n",
      "Epoch: 1700, Train Loss: 0.9546, Test Loss: 5.7745\n",
      "Epoch: 1701, Train Loss: 0.9587, Test Loss: 6.1026\n",
      "Epoch: 1702, Train Loss: 0.9224, Test Loss: 5.4720\n",
      "Epoch: 1703, Train Loss: 1.0071, Test Loss: 4.9245\n",
      "Epoch: 1704, Train Loss: 1.0382, Test Loss: 6.1879\n",
      "Epoch: 1705, Train Loss: 1.0593, Test Loss: 5.4661\n",
      "Epoch: 1706, Train Loss: 0.9953, Test Loss: 5.2277\n",
      "Epoch: 1707, Train Loss: 0.9949, Test Loss: 5.8666\n",
      "Epoch: 1708, Train Loss: 0.9144, Test Loss: 6.1385\n",
      "Epoch: 1709, Train Loss: 1.0026, Test Loss: 5.4673\n",
      "Epoch: 1710, Train Loss: 0.9868, Test Loss: 5.5888\n",
      "Epoch: 1711, Train Loss: 0.8264, Test Loss: 5.5893\n",
      "Epoch: 1712, Train Loss: 0.9798, Test Loss: 6.7971\n",
      "Epoch: 1713, Train Loss: 0.9401, Test Loss: 5.7714\n",
      "Epoch: 1714, Train Loss: 0.9164, Test Loss: 4.8708\n",
      "Epoch: 1715, Train Loss: 0.8806, Test Loss: 5.0579\n",
      "Epoch: 1716, Train Loss: 0.9835, Test Loss: 6.3207\n",
      "Epoch: 1717, Train Loss: 0.9255, Test Loss: 6.3051\n",
      "Epoch: 1718, Train Loss: 0.9935, Test Loss: 4.9842\n",
      "Epoch: 1719, Train Loss: 0.9002, Test Loss: 4.6011\n",
      "Epoch: 1720, Train Loss: 1.0455, Test Loss: 6.7547\n",
      "Epoch: 1721, Train Loss: 0.9284, Test Loss: 6.7357\n",
      "Epoch: 1722, Train Loss: 0.9529, Test Loss: 4.9872\n",
      "Epoch: 1723, Train Loss: 0.9393, Test Loss: 5.2791\n",
      "Epoch: 1724, Train Loss: 0.9308, Test Loss: 6.5597\n",
      "Epoch: 1725, Train Loss: 1.0339, Test Loss: 5.9150\n",
      "Epoch: 1726, Train Loss: 0.9422, Test Loss: 6.1973\n",
      "Epoch: 1727, Train Loss: 0.9541, Test Loss: 6.1961\n",
      "Epoch: 1728, Train Loss: 0.9409, Test Loss: 4.8907\n",
      "Epoch: 1729, Train Loss: 0.9069, Test Loss: 4.7911\n",
      "Epoch: 1730, Train Loss: 1.1172, Test Loss: 6.3718\n",
      "Epoch: 1731, Train Loss: 0.9332, Test Loss: 7.4249\n",
      "Epoch: 1732, Train Loss: 1.1632, Test Loss: 4.5228\n",
      "Epoch: 1733, Train Loss: 1.0042, Test Loss: 4.7562\n",
      "Epoch: 1734, Train Loss: 0.9740, Test Loss: 7.4712\n",
      "Epoch: 1735, Train Loss: 1.0583, Test Loss: 6.6677\n",
      "Epoch: 1736, Train Loss: 1.0807, Test Loss: 3.9264\n",
      "Epoch: 1737, Train Loss: 1.3726, Test Loss: 4.7386\n",
      "Epoch: 1738, Train Loss: 0.9250, Test Loss: 7.1658\n",
      "Epoch: 1739, Train Loss: 1.1071, Test Loss: 6.4341\n",
      "Epoch: 1740, Train Loss: 1.1156, Test Loss: 3.6510\n",
      "Epoch: 1741, Train Loss: 1.4460, Test Loss: 4.0365\n",
      "Epoch: 1742, Train Loss: 1.1819, Test Loss: 7.8638\n",
      "Epoch: 1743, Train Loss: 1.3272, Test Loss: 7.4016\n",
      "Epoch: 1744, Train Loss: 1.2132, Test Loss: 3.9150\n",
      "Epoch: 1745, Train Loss: 1.1942, Test Loss: 3.8085\n",
      "Epoch: 1746, Train Loss: 1.3071, Test Loss: 6.5237\n",
      "Epoch: 1747, Train Loss: 1.0641, Test Loss: 7.8117\n",
      "Epoch: 1748, Train Loss: 1.2356, Test Loss: 4.9485\n",
      "Epoch: 1749, Train Loss: 0.9712, Test Loss: 3.7782\n",
      "Epoch: 1750, Train Loss: 1.2536, Test Loss: 4.5764\n",
      "Epoch: 1751, Train Loss: 1.0202, Test Loss: 6.3216\n",
      "Epoch: 1752, Train Loss: 1.0295, Test Loss: 6.0802\n",
      "Epoch: 1753, Train Loss: 0.9434, Test Loss: 4.6982\n",
      "Epoch: 1754, Train Loss: 0.8429, Test Loss: 4.2697\n",
      "Epoch: 1755, Train Loss: 0.9805, Test Loss: 5.3429\n",
      "Epoch: 1756, Train Loss: 0.8833, Test Loss: 6.1174\n",
      "Epoch: 1757, Train Loss: 0.9563, Test Loss: 5.6303\n",
      "Epoch: 1758, Train Loss: 0.8748, Test Loss: 4.6636\n",
      "Epoch: 1759, Train Loss: 0.9411, Test Loss: 4.8119\n",
      "Epoch: 1760, Train Loss: 0.8884, Test Loss: 5.5602\n",
      "Epoch: 1761, Train Loss: 0.9641, Test Loss: 5.9238\n",
      "Epoch: 1762, Train Loss: 0.8865, Test Loss: 5.1503\n",
      "Epoch: 1763, Train Loss: 1.0286, Test Loss: 5.8710\n",
      "Epoch: 1764, Train Loss: 0.8728, Test Loss: 5.6621\n",
      "Epoch: 1765, Train Loss: 0.9050, Test Loss: 5.4767\n",
      "Epoch: 1766, Train Loss: 0.9691, Test Loss: 5.2479\n",
      "Epoch: 1767, Train Loss: 0.8077, Test Loss: 5.0998\n",
      "Epoch: 1768, Train Loss: 0.8558, Test Loss: 6.2041\n",
      "Epoch: 1769, Train Loss: 0.9129, Test Loss: 6.1389\n",
      "Epoch: 1770, Train Loss: 0.9374, Test Loss: 4.6466\n",
      "Epoch: 1771, Train Loss: 0.8939, Test Loss: 4.5992\n",
      "Epoch: 1772, Train Loss: 0.9703, Test Loss: 6.5569\n",
      "Epoch: 1773, Train Loss: 0.8851, Test Loss: 6.3865\n",
      "Epoch: 1774, Train Loss: 0.9435, Test Loss: 4.5948\n",
      "Epoch: 1775, Train Loss: 1.0313, Test Loss: 4.5428\n",
      "Epoch: 1776, Train Loss: 0.9977, Test Loss: 6.4016\n",
      "Epoch: 1777, Train Loss: 1.0951, Test Loss: 5.5630\n",
      "Epoch: 1778, Train Loss: 0.9512, Test Loss: 4.4745\n",
      "Epoch: 1779, Train Loss: 1.0145, Test Loss: 5.2078\n",
      "Epoch: 1780, Train Loss: 0.8775, Test Loss: 6.6999\n",
      "Epoch: 1781, Train Loss: 1.0632, Test Loss: 5.3921\n",
      "Epoch: 1782, Train Loss: 0.8788, Test Loss: 4.7924\n",
      "Epoch: 1783, Train Loss: 0.8735, Test Loss: 5.1657\n",
      "Epoch: 1784, Train Loss: 0.8999, Test Loss: 6.1105\n",
      "Epoch: 1785, Train Loss: 1.0040, Test Loss: 5.7991\n",
      "Epoch: 1786, Train Loss: 0.8861, Test Loss: 4.9063\n",
      "Epoch: 1787, Train Loss: 0.9413, Test Loss: 5.3649\n",
      "Epoch: 1788, Train Loss: 0.9137, Test Loss: 5.6535\n",
      "Epoch: 1789, Train Loss: 0.7737, Test Loss: 5.8030\n",
      "Epoch: 1790, Train Loss: 0.9546, Test Loss: 5.0435\n",
      "Epoch: 1791, Train Loss: 0.9441, Test Loss: 5.0988\n",
      "Epoch: 1792, Train Loss: 0.8531, Test Loss: 5.1115\n",
      "Epoch: 1793, Train Loss: 0.7680, Test Loss: 5.5026\n",
      "Epoch: 1794, Train Loss: 0.8638, Test Loss: 6.0114\n",
      "Epoch: 1795, Train Loss: 0.8834, Test Loss: 5.3324\n",
      "Epoch: 1796, Train Loss: 0.8404, Test Loss: 4.6383\n",
      "Epoch: 1797, Train Loss: 1.0430, Test Loss: 5.9347\n",
      "Epoch: 1798, Train Loss: 0.9560, Test Loss: 7.0063\n",
      "Epoch: 1799, Train Loss: 1.0479, Test Loss: 5.0589\n",
      "Epoch: 1800, Train Loss: 0.8939, Test Loss: 3.8651\n",
      "Epoch: 1801, Train Loss: 1.3086, Test Loss: 5.9280\n",
      "Epoch: 1802, Train Loss: 0.8040, Test Loss: 7.9947\n",
      "Epoch: 1803, Train Loss: 1.4357, Test Loss: 4.4967\n",
      "Epoch: 1804, Train Loss: 0.9475, Test Loss: 3.9412\n",
      "Epoch: 1805, Train Loss: 1.2263, Test Loss: 6.1463\n",
      "Epoch: 1806, Train Loss: 0.8824, Test Loss: 7.9621\n",
      "Epoch: 1807, Train Loss: 1.5239, Test Loss: 4.5429\n",
      "Epoch: 1808, Train Loss: 0.8834, Test Loss: 3.2755\n",
      "Epoch: 1809, Train Loss: 1.6449, Test Loss: 4.1370\n",
      "Epoch: 1810, Train Loss: 1.0914, Test Loss: 7.3042\n",
      "Epoch: 1811, Train Loss: 1.3970, Test Loss: 6.8799\n",
      "Epoch: 1812, Train Loss: 1.1185, Test Loss: 4.2549\n",
      "Epoch: 1813, Train Loss: 0.9689, Test Loss: 3.4246\n",
      "Epoch: 1814, Train Loss: 1.4885, Test Loss: 4.8044\n",
      "Epoch: 1815, Train Loss: 0.9161, Test Loss: 7.3295\n",
      "Epoch: 1816, Train Loss: 1.2237, Test Loss: 6.2370\n",
      "Epoch: 1817, Train Loss: 1.0531, Test Loss: 3.5963\n",
      "Epoch: 1818, Train Loss: 1.3101, Test Loss: 3.6502\n",
      "Epoch: 1819, Train Loss: 1.1649, Test Loss: 5.6694\n",
      "Epoch: 1820, Train Loss: 0.9838, Test Loss: 6.9205\n",
      "Epoch: 1821, Train Loss: 1.2716, Test Loss: 4.6595\n",
      "Epoch: 1822, Train Loss: 0.9701, Test Loss: 3.3517\n",
      "Epoch: 1823, Train Loss: 1.6407, Test Loss: 4.0919\n",
      "Epoch: 1824, Train Loss: 0.9503, Test Loss: 6.4731\n",
      "Epoch: 1825, Train Loss: 1.3448, Test Loss: 5.2397\n",
      "Epoch: 1826, Train Loss: 0.8599, Test Loss: 3.7572\n",
      "Epoch: 1827, Train Loss: 1.0435, Test Loss: 4.1763\n",
      "Epoch: 1828, Train Loss: 0.8999, Test Loss: 5.4286\n",
      "Epoch: 1829, Train Loss: 0.8796, Test Loss: 5.8632\n",
      "Epoch: 1830, Train Loss: 1.1003, Test Loss: 4.4019\n",
      "Epoch: 1831, Train Loss: 0.8684, Test Loss: 3.9108\n",
      "Epoch: 1832, Train Loss: 0.9887, Test Loss: 4.6563\n",
      "Epoch: 1833, Train Loss: 0.9398, Test Loss: 5.9377\n",
      "Epoch: 1834, Train Loss: 1.0068, Test Loss: 5.5080\n",
      "Epoch: 1835, Train Loss: 0.9354, Test Loss: 4.1838\n",
      "Epoch: 1836, Train Loss: 0.8843, Test Loss: 3.7822\n",
      "Epoch: 1837, Train Loss: 0.9953, Test Loss: 4.8441\n",
      "Epoch: 1838, Train Loss: 0.8642, Test Loss: 6.6476\n",
      "Epoch: 1839, Train Loss: 0.9711, Test Loss: 6.2134\n",
      "Epoch: 1840, Train Loss: 0.9072, Test Loss: 4.1584\n",
      "Epoch: 1841, Train Loss: 0.9377, Test Loss: 3.7630\n",
      "Epoch: 1842, Train Loss: 1.0874, Test Loss: 5.5027\n",
      "Epoch: 1843, Train Loss: 0.9666, Test Loss: 6.1637\n",
      "Epoch: 1844, Train Loss: 1.0018, Test Loss: 4.6649\n",
      "Epoch: 1845, Train Loss: 0.8489, Test Loss: 4.0991\n",
      "Epoch: 1846, Train Loss: 0.9641, Test Loss: 4.6803\n",
      "Epoch: 1847, Train Loss: 0.8959, Test Loss: 5.0683\n",
      "Epoch: 1848, Train Loss: 0.8047, Test Loss: 4.8752\n",
      "Epoch: 1849, Train Loss: 0.8707, Test Loss: 5.0937\n",
      "Epoch: 1850, Train Loss: 0.8577, Test Loss: 4.8998\n",
      "Epoch: 1851, Train Loss: 0.8376, Test Loss: 4.9616\n",
      "Epoch: 1852, Train Loss: 0.8488, Test Loss: 4.9516\n",
      "Epoch: 1853, Train Loss: 0.8814, Test Loss: 4.9627\n",
      "Epoch: 1854, Train Loss: 0.8138, Test Loss: 5.1870\n",
      "Epoch: 1855, Train Loss: 0.7950, Test Loss: 4.9009\n",
      "Epoch: 1856, Train Loss: 0.8665, Test Loss: 5.1404\n",
      "Epoch: 1857, Train Loss: 0.8854, Test Loss: 5.0036\n",
      "Epoch: 1858, Train Loss: 0.8221, Test Loss: 4.3768\n",
      "Epoch: 1859, Train Loss: 0.8830, Test Loss: 5.0180\n",
      "Epoch: 1860, Train Loss: 0.8563, Test Loss: 4.9315\n",
      "Epoch: 1861, Train Loss: 0.7961, Test Loss: 4.5468\n",
      "Epoch: 1862, Train Loss: 0.8197, Test Loss: 5.0698\n",
      "Epoch: 1863, Train Loss: 0.8416, Test Loss: 5.6910\n",
      "Epoch: 1864, Train Loss: 0.9408, Test Loss: 5.1652\n",
      "Epoch: 1865, Train Loss: 0.8507, Test Loss: 4.6176\n",
      "Epoch: 1866, Train Loss: 0.9032, Test Loss: 5.0706\n",
      "Epoch: 1867, Train Loss: 0.8192, Test Loss: 5.4737\n",
      "Epoch: 1868, Train Loss: 0.8359, Test Loss: 4.7139\n",
      "Epoch: 1869, Train Loss: 0.8827, Test Loss: 4.4887\n",
      "Epoch: 1870, Train Loss: 0.9151, Test Loss: 5.8877\n",
      "Epoch: 1871, Train Loss: 0.8855, Test Loss: 5.4502\n",
      "Epoch: 1872, Train Loss: 0.8376, Test Loss: 5.1932\n",
      "Epoch: 1873, Train Loss: 0.8056, Test Loss: 4.7980\n",
      "Epoch: 1874, Train Loss: 0.8627, Test Loss: 5.2513\n",
      "Epoch: 1875, Train Loss: 0.7936, Test Loss: 5.3383\n",
      "Epoch: 1876, Train Loss: 0.8522, Test Loss: 5.1370\n",
      "Epoch: 1877, Train Loss: 0.8225, Test Loss: 5.1321\n",
      "Epoch: 1878, Train Loss: 0.7556, Test Loss: 5.3346\n",
      "Epoch: 1879, Train Loss: 0.8573, Test Loss: 5.2719\n",
      "Epoch: 1880, Train Loss: 0.8126, Test Loss: 4.6953\n",
      "Epoch: 1881, Train Loss: 0.8432, Test Loss: 4.5649\n",
      "Epoch: 1882, Train Loss: 0.8294, Test Loss: 5.6203\n",
      "Epoch: 1883, Train Loss: 0.8690, Test Loss: 5.6986\n",
      "Epoch: 1884, Train Loss: 0.9253, Test Loss: 4.3991\n",
      "Epoch: 1885, Train Loss: 1.1657, Test Loss: 5.1733\n",
      "Epoch: 1886, Train Loss: 0.7739, Test Loss: 5.6381\n",
      "Epoch: 1887, Train Loss: 0.8445, Test Loss: 5.0965\n",
      "Epoch: 1888, Train Loss: 0.8366, Test Loss: 5.1515\n",
      "Epoch: 1889, Train Loss: 0.8496, Test Loss: 5.0212\n",
      "Epoch: 1890, Train Loss: 1.0090, Test Loss: 4.9230\n",
      "Epoch: 1891, Train Loss: 0.7539, Test Loss: 5.5037\n",
      "Epoch: 1892, Train Loss: 0.7791, Test Loss: 5.2934\n",
      "Epoch: 1893, Train Loss: 0.8782, Test Loss: 5.6583\n",
      "Epoch: 1894, Train Loss: 0.8504, Test Loss: 5.2019\n",
      "Epoch: 1895, Train Loss: 0.8509, Test Loss: 4.6717\n",
      "Epoch: 1896, Train Loss: 0.9356, Test Loss: 5.7624\n",
      "Epoch: 1897, Train Loss: 0.7907, Test Loss: 6.9593\n",
      "Epoch: 1898, Train Loss: 0.9779, Test Loss: 4.9620\n",
      "Epoch: 1899, Train Loss: 0.7796, Test Loss: 4.0081\n",
      "Epoch: 1900, Train Loss: 0.9677, Test Loss: 4.9246\n",
      "Epoch: 1901, Train Loss: 0.8434, Test Loss: 7.2350\n",
      "Epoch: 1902, Train Loss: 1.1609, Test Loss: 5.9341\n",
      "Epoch: 1903, Train Loss: 0.9102, Test Loss: 4.0849\n",
      "Epoch: 1904, Train Loss: 0.9705, Test Loss: 4.0481\n",
      "Epoch: 1905, Train Loss: 0.9931, Test Loss: 5.5811\n",
      "Epoch: 1906, Train Loss: 0.7985, Test Loss: 6.2898\n",
      "Epoch: 1907, Train Loss: 0.8662, Test Loss: 5.7255\n",
      "Epoch: 1908, Train Loss: 0.8431, Test Loss: 4.4506\n",
      "Epoch: 1909, Train Loss: 0.9530, Test Loss: 4.3396\n",
      "Epoch: 1910, Train Loss: 0.8529, Test Loss: 5.1833\n",
      "Epoch: 1911, Train Loss: 0.8483, Test Loss: 5.8342\n",
      "Epoch: 1912, Train Loss: 0.9226, Test Loss: 5.1637\n",
      "Epoch: 1913, Train Loss: 0.8050, Test Loss: 4.4484\n",
      "Epoch: 1914, Train Loss: 0.8830, Test Loss: 4.3610\n",
      "Epoch: 1915, Train Loss: 0.9986, Test Loss: 6.0203\n",
      "Epoch: 1916, Train Loss: 1.0411, Test Loss: 5.5939\n",
      "Epoch: 1917, Train Loss: 0.8567, Test Loss: 4.6938\n",
      "Epoch: 1918, Train Loss: 0.8961, Test Loss: 4.4261\n",
      "Epoch: 1919, Train Loss: 0.9719, Test Loss: 5.3723\n",
      "Epoch: 1920, Train Loss: 0.8605, Test Loss: 5.6689\n",
      "Epoch: 1921, Train Loss: 0.9330, Test Loss: 4.2676\n",
      "Epoch: 1922, Train Loss: 1.0071, Test Loss: 4.5506\n",
      "Epoch: 1923, Train Loss: 0.8578, Test Loss: 5.9146\n",
      "Epoch: 1924, Train Loss: 0.9280, Test Loss: 5.6494\n",
      "Epoch: 1925, Train Loss: 0.9543, Test Loss: 4.4757\n",
      "Epoch: 1926, Train Loss: 0.8770, Test Loss: 4.4869\n",
      "Epoch: 1927, Train Loss: 0.8572, Test Loss: 5.6118\n",
      "Epoch: 1928, Train Loss: 0.9009, Test Loss: 5.8498\n",
      "Epoch: 1929, Train Loss: 0.9721, Test Loss: 4.2380\n",
      "Epoch: 1930, Train Loss: 0.9274, Test Loss: 3.9455\n",
      "Epoch: 1931, Train Loss: 0.9529, Test Loss: 5.2052\n",
      "Epoch: 1932, Train Loss: 0.7993, Test Loss: 6.5335\n",
      "Epoch: 1933, Train Loss: 1.0010, Test Loss: 5.0832\n",
      "Epoch: 1934, Train Loss: 0.9267, Test Loss: 3.8650\n",
      "Epoch: 1935, Train Loss: 1.2113, Test Loss: 4.9681\n",
      "Epoch: 1936, Train Loss: 0.8147, Test Loss: 6.8655\n",
      "Epoch: 1937, Train Loss: 1.1236, Test Loss: 5.3169\n",
      "Epoch: 1938, Train Loss: 0.8348, Test Loss: 4.1335\n",
      "Epoch: 1939, Train Loss: 0.9045, Test Loss: 4.3752\n",
      "Epoch: 1940, Train Loss: 0.7994, Test Loss: 5.0267\n",
      "Epoch: 1941, Train Loss: 0.8878, Test Loss: 4.9545\n",
      "Epoch: 1942, Train Loss: 0.8285, Test Loss: 5.1330\n",
      "Epoch: 1943, Train Loss: 0.7935, Test Loss: 4.8097\n",
      "Epoch: 1944, Train Loss: 0.8133, Test Loss: 5.0363\n",
      "Epoch: 1945, Train Loss: 0.8377, Test Loss: 4.8249\n",
      "Epoch: 1946, Train Loss: 0.7519, Test Loss: 4.4357\n",
      "Epoch: 1947, Train Loss: 0.8894, Test Loss: 5.2218\n",
      "Epoch: 1948, Train Loss: 0.8253, Test Loss: 5.9433\n",
      "Epoch: 1949, Train Loss: 0.7924, Test Loss: 5.6687\n",
      "Epoch: 1950, Train Loss: 0.7868, Test Loss: 4.2555\n",
      "Epoch: 1951, Train Loss: 0.8351, Test Loss: 3.8454\n",
      "Epoch: 1952, Train Loss: 0.9635, Test Loss: 5.1502\n",
      "Epoch: 1953, Train Loss: 0.7992, Test Loss: 6.5843\n",
      "Epoch: 1954, Train Loss: 0.9373, Test Loss: 5.4995\n",
      "Epoch: 1955, Train Loss: 0.8100, Test Loss: 4.3252\n",
      "Epoch: 1956, Train Loss: 0.8694, Test Loss: 4.5700\n",
      "Epoch: 1957, Train Loss: 0.9157, Test Loss: 6.4685\n",
      "Epoch: 1958, Train Loss: 0.9218, Test Loss: 6.2720\n",
      "Epoch: 1959, Train Loss: 0.9381, Test Loss: 4.3661\n",
      "Epoch: 1960, Train Loss: 0.8975, Test Loss: 3.7204\n",
      "Epoch: 1961, Train Loss: 1.1139, Test Loss: 4.9526\n",
      "Epoch: 1962, Train Loss: 0.8013, Test Loss: 6.4364\n",
      "Epoch: 1963, Train Loss: 1.0587, Test Loss: 5.3295\n",
      "Epoch: 1964, Train Loss: 0.8618, Test Loss: 3.9429\n",
      "Epoch: 1965, Train Loss: 1.0698, Test Loss: 4.4928\n",
      "Epoch: 1966, Train Loss: 0.9223, Test Loss: 6.1465\n",
      "Epoch: 1967, Train Loss: 1.0357, Test Loss: 5.4120\n",
      "Epoch: 1968, Train Loss: 0.8683, Test Loss: 4.1514\n",
      "Epoch: 1969, Train Loss: 0.8925, Test Loss: 4.3585\n",
      "Epoch: 1970, Train Loss: 0.8494, Test Loss: 5.6415\n",
      "Epoch: 1971, Train Loss: 0.8392, Test Loss: 5.5458\n",
      "Epoch: 1972, Train Loss: 0.7867, Test Loss: 4.4386\n",
      "Epoch: 1973, Train Loss: 0.8121, Test Loss: 3.8408\n",
      "Epoch: 1974, Train Loss: 0.9368, Test Loss: 4.5050\n",
      "Epoch: 1975, Train Loss: 0.8637, Test Loss: 5.5784\n",
      "Epoch: 1976, Train Loss: 0.8609, Test Loss: 5.4370\n",
      "Epoch: 1977, Train Loss: 0.8257, Test Loss: 4.6750\n",
      "Epoch: 1978, Train Loss: 0.7906, Test Loss: 4.5843\n",
      "Epoch: 1979, Train Loss: 0.7973, Test Loss: 5.2506\n",
      "Epoch: 1980, Train Loss: 0.8245, Test Loss: 5.3062\n",
      "Epoch: 1981, Train Loss: 0.7948, Test Loss: 5.0648\n",
      "Epoch: 1982, Train Loss: 0.7829, Test Loss: 4.7705\n",
      "Epoch: 1983, Train Loss: 0.8237, Test Loss: 5.5612\n",
      "Epoch: 1984, Train Loss: 0.8181, Test Loss: 4.7985\n",
      "Epoch: 1985, Train Loss: 0.7547, Test Loss: 4.0892\n",
      "Epoch: 1986, Train Loss: 0.8094, Test Loss: 4.5415\n",
      "Epoch: 1987, Train Loss: 0.8207, Test Loss: 6.3160\n",
      "Epoch: 1988, Train Loss: 0.9411, Test Loss: 5.4178\n",
      "Epoch: 1989, Train Loss: 0.7961, Test Loss: 4.0978\n",
      "Epoch: 1990, Train Loss: 0.7846, Test Loss: 4.1464\n",
      "Epoch: 1991, Train Loss: 0.8443, Test Loss: 4.9349\n",
      "Epoch: 1992, Train Loss: 0.8664, Test Loss: 5.5537\n",
      "Epoch: 1993, Train Loss: 0.9096, Test Loss: 4.7936\n",
      "Epoch: 1994, Train Loss: 0.7968, Test Loss: 3.9831\n",
      "Epoch: 1995, Train Loss: 0.8451, Test Loss: 4.2196\n",
      "Epoch: 1996, Train Loss: 0.8169, Test Loss: 5.3559\n",
      "Epoch: 1997, Train Loss: 0.8206, Test Loss: 5.4560\n",
      "Epoch: 1998, Train Loss: 0.7974, Test Loss: 4.8523\n",
      "Epoch: 1999, Train Loss: 0.7358, Test Loss: 4.6010\n",
      "Epoch: 2000, Train Loss: 0.8064, Test Loss: 4.7551\n",
      "Epoch: 2001, Train Loss: 0.8649, Test Loss: 4.7284\n",
      "Epoch: 2002, Train Loss: 0.8006, Test Loss: 4.8582\n",
      "Epoch: 2003, Train Loss: 0.7498, Test Loss: 5.5907\n",
      "Epoch: 2004, Train Loss: 0.8646, Test Loss: 4.4611\n",
      "Epoch: 2005, Train Loss: 0.7941, Test Loss: 4.1234\n",
      "Epoch: 2006, Train Loss: 0.9293, Test Loss: 5.4145\n",
      "Epoch: 2007, Train Loss: 0.8666, Test Loss: 5.8507\n",
      "Epoch: 2008, Train Loss: 0.8499, Test Loss: 4.6219\n",
      "Epoch: 2009, Train Loss: 0.9055, Test Loss: 4.6357\n",
      "Epoch: 2010, Train Loss: 0.8057, Test Loss: 5.4890\n",
      "Epoch: 2011, Train Loss: 0.7922, Test Loss: 5.3856\n",
      "Epoch: 2012, Train Loss: 0.7725, Test Loss: 4.5717\n",
      "Epoch: 2013, Train Loss: 0.8355, Test Loss: 4.4318\n",
      "Epoch: 2014, Train Loss: 0.8508, Test Loss: 5.1346\n",
      "Epoch: 2015, Train Loss: 0.7898, Test Loss: 5.1398\n",
      "Epoch: 2016, Train Loss: 0.7722, Test Loss: 4.7224\n",
      "Epoch: 2017, Train Loss: 0.7217, Test Loss: 4.3051\n",
      "Epoch: 2018, Train Loss: 0.8960, Test Loss: 5.5545\n",
      "Epoch: 2019, Train Loss: 0.8026, Test Loss: 5.8201\n",
      "Epoch: 2020, Train Loss: 0.8535, Test Loss: 4.4742\n",
      "Epoch: 2021, Train Loss: 0.7965, Test Loss: 4.0909\n",
      "Epoch: 2022, Train Loss: 0.9503, Test Loss: 5.6368\n",
      "Epoch: 2023, Train Loss: 0.9147, Test Loss: 5.6448\n",
      "Epoch: 2024, Train Loss: 0.9316, Test Loss: 4.2613\n",
      "Epoch: 2025, Train Loss: 0.7886, Test Loss: 3.8764\n",
      "Epoch: 2026, Train Loss: 0.9493, Test Loss: 5.1944\n",
      "Epoch: 2027, Train Loss: 0.9738, Test Loss: 5.6951\n",
      "Epoch: 2028, Train Loss: 0.9168, Test Loss: 4.6062\n",
      "Epoch: 2029, Train Loss: 0.7409, Test Loss: 3.7487\n",
      "Epoch: 2030, Train Loss: 0.9617, Test Loss: 4.6208\n",
      "Epoch: 2031, Train Loss: 0.9027, Test Loss: 6.0056\n",
      "Epoch: 2032, Train Loss: 1.0283, Test Loss: 5.1412\n",
      "Epoch: 2033, Train Loss: 0.8494, Test Loss: 4.3919\n",
      "Epoch: 2034, Train Loss: 0.9326, Test Loss: 4.3159\n",
      "Epoch: 2035, Train Loss: 0.8216, Test Loss: 5.1383\n",
      "Epoch: 2036, Train Loss: 0.8136, Test Loss: 5.3065\n",
      "Epoch: 2037, Train Loss: 0.8823, Test Loss: 4.4063\n",
      "Epoch: 2038, Train Loss: 0.8576, Test Loss: 4.1003\n",
      "Epoch: 2039, Train Loss: 0.8692, Test Loss: 4.7702\n",
      "Epoch: 2040, Train Loss: 0.8393, Test Loss: 5.2182\n",
      "Epoch: 2041, Train Loss: 0.8193, Test Loss: 4.8187\n",
      "Epoch: 2042, Train Loss: 0.7719, Test Loss: 4.6276\n",
      "Epoch: 2043, Train Loss: 0.7743, Test Loss: 4.6753\n",
      "Epoch: 2044, Train Loss: 0.7724, Test Loss: 4.6365\n",
      "Epoch: 2045, Train Loss: 0.7948, Test Loss: 4.9836\n",
      "Epoch: 2046, Train Loss: 0.8154, Test Loss: 4.9459\n",
      "Epoch: 2047, Train Loss: 0.7653, Test Loss: 4.9163\n",
      "Epoch: 2048, Train Loss: 0.7397, Test Loss: 4.5875\n",
      "Epoch: 2049, Train Loss: 0.9524, Test Loss: 5.2255\n",
      "Epoch: 2050, Train Loss: 0.7515, Test Loss: 5.4720\n",
      "Epoch: 2051, Train Loss: 0.8503, Test Loss: 5.1245\n",
      "Epoch: 2052, Train Loss: 0.7616, Test Loss: 4.3439\n",
      "Epoch: 2053, Train Loss: 0.8515, Test Loss: 4.3121\n",
      "Epoch: 2054, Train Loss: 0.8774, Test Loss: 5.6985\n",
      "Epoch: 2055, Train Loss: 0.9932, Test Loss: 4.6960\n",
      "Epoch: 2056, Train Loss: 0.7753, Test Loss: 3.9069\n",
      "Epoch: 2057, Train Loss: 0.8310, Test Loss: 4.5622\n",
      "Epoch: 2058, Train Loss: 0.8094, Test Loss: 5.6032\n",
      "Epoch: 2059, Train Loss: 0.8559, Test Loss: 5.0738\n",
      "Epoch: 2060, Train Loss: 0.8122, Test Loss: 4.3424\n",
      "Epoch: 2061, Train Loss: 0.8125, Test Loss: 4.3179\n",
      "Epoch: 2062, Train Loss: 0.8407, Test Loss: 5.5944\n",
      "Epoch: 2063, Train Loss: 0.7596, Test Loss: 5.9198\n",
      "Epoch: 2064, Train Loss: 0.8338, Test Loss: 4.6680\n",
      "Epoch: 2065, Train Loss: 0.7432, Test Loss: 4.2873\n",
      "Epoch: 2066, Train Loss: 0.7948, Test Loss: 5.1371\n",
      "Epoch: 2067, Train Loss: 0.8084, Test Loss: 5.3883\n",
      "Epoch: 2068, Train Loss: 0.8861, Test Loss: 4.3586\n",
      "Epoch: 2069, Train Loss: 0.7635, Test Loss: 4.5661\n",
      "Epoch: 2070, Train Loss: 0.8593, Test Loss: 6.7025\n",
      "Epoch: 2071, Train Loss: 0.8662, Test Loss: 5.9897\n",
      "Epoch: 2072, Train Loss: 0.8772, Test Loss: 3.7413\n",
      "Epoch: 2073, Train Loss: 0.9977, Test Loss: 4.0681\n",
      "Epoch: 2074, Train Loss: 0.8555, Test Loss: 5.8280\n",
      "Epoch: 2075, Train Loss: 0.9218, Test Loss: 6.0228\n",
      "Epoch: 2076, Train Loss: 0.8693, Test Loss: 4.4578\n",
      "Epoch: 2077, Train Loss: 0.7597, Test Loss: 3.7761\n",
      "Epoch: 2078, Train Loss: 0.9611, Test Loss: 4.8955\n",
      "Epoch: 2079, Train Loss: 0.7855, Test Loss: 5.9911\n",
      "Epoch: 2080, Train Loss: 0.9611, Test Loss: 4.6355\n",
      "Epoch: 2081, Train Loss: 0.7466, Test Loss: 4.0407\n",
      "Epoch: 2082, Train Loss: 0.8571, Test Loss: 4.7009\n",
      "Epoch: 2083, Train Loss: 0.7831, Test Loss: 6.0917\n",
      "Epoch: 2084, Train Loss: 0.8630, Test Loss: 5.2328\n",
      "Epoch: 2085, Train Loss: 0.7673, Test Loss: 4.6792\n",
      "Epoch: 2086, Train Loss: 0.8265, Test Loss: 4.4166\n",
      "Epoch: 2087, Train Loss: 0.8009, Test Loss: 4.9523\n",
      "Epoch: 2088, Train Loss: 0.8090, Test Loss: 4.6453\n",
      "Epoch: 2089, Train Loss: 0.7668, Test Loss: 4.6614\n",
      "Epoch: 2090, Train Loss: 0.7446, Test Loss: 4.8009\n",
      "Epoch: 2091, Train Loss: 0.6767, Test Loss: 5.1299\n",
      "Epoch: 2092, Train Loss: 0.7722, Test Loss: 5.3489\n",
      "Epoch: 2093, Train Loss: 0.8841, Test Loss: 4.4772\n",
      "Epoch: 2094, Train Loss: 0.7345, Test Loss: 4.1203\n",
      "Epoch: 2095, Train Loss: 0.8280, Test Loss: 5.1225\n",
      "Epoch: 2096, Train Loss: 0.8072, Test Loss: 5.6340\n",
      "Epoch: 2097, Train Loss: 0.7684, Test Loss: 4.9232\n",
      "Epoch: 2098, Train Loss: 0.7811, Test Loss: 4.3392\n",
      "Epoch: 2099, Train Loss: 0.8696, Test Loss: 4.2544\n",
      "Epoch: 2100, Train Loss: 0.8090, Test Loss: 4.6513\n",
      "Epoch: 2101, Train Loss: 0.7436, Test Loss: 5.0843\n",
      "Epoch: 2102, Train Loss: 0.8624, Test Loss: 4.3508\n",
      "Epoch: 2103, Train Loss: 0.8017, Test Loss: 4.2064\n",
      "Epoch: 2104, Train Loss: 0.8173, Test Loss: 4.9183\n",
      "Epoch: 2105, Train Loss: 0.7518, Test Loss: 5.1798\n",
      "Epoch: 2106, Train Loss: 0.8761, Test Loss: 4.2235\n",
      "Epoch: 2107, Train Loss: 0.7520, Test Loss: 4.3370\n",
      "Epoch: 2108, Train Loss: 0.8184, Test Loss: 5.2027\n",
      "Epoch: 2109, Train Loss: 0.7433, Test Loss: 5.8161\n",
      "Epoch: 2110, Train Loss: 0.8197, Test Loss: 4.3429\n",
      "Epoch: 2111, Train Loss: 0.7421, Test Loss: 4.0833\n",
      "Epoch: 2112, Train Loss: 0.8389, Test Loss: 5.1510\n",
      "Epoch: 2113, Train Loss: 0.7190, Test Loss: 5.6640\n",
      "Epoch: 2114, Train Loss: 0.8112, Test Loss: 4.8498\n",
      "Epoch: 2115, Train Loss: 0.7228, Test Loss: 4.5042\n",
      "Epoch: 2116, Train Loss: 0.7587, Test Loss: 4.3760\n",
      "Epoch: 2117, Train Loss: 0.7492, Test Loss: 4.7139\n",
      "Epoch: 2118, Train Loss: 0.7331, Test Loss: 5.3647\n",
      "Epoch: 2119, Train Loss: 0.7490, Test Loss: 5.4487\n",
      "Epoch: 2120, Train Loss: 0.7989, Test Loss: 4.1923\n",
      "Epoch: 2121, Train Loss: 0.8635, Test Loss: 4.7211\n",
      "Epoch: 2122, Train Loss: 0.7120, Test Loss: 5.6725\n",
      "Epoch: 2123, Train Loss: 0.8207, Test Loss: 4.5345\n",
      "Epoch: 2124, Train Loss: 0.7332, Test Loss: 4.0973\n",
      "Epoch: 2125, Train Loss: 0.8154, Test Loss: 4.8247\n",
      "Epoch: 2126, Train Loss: 0.6920, Test Loss: 6.0848\n",
      "Epoch: 2127, Train Loss: 0.8108, Test Loss: 5.4253\n",
      "Epoch: 2128, Train Loss: 0.7872, Test Loss: 4.5469\n",
      "Epoch: 2129, Train Loss: 0.7412, Test Loss: 4.3560\n",
      "Epoch: 2130, Train Loss: 0.7662, Test Loss: 5.1047\n",
      "Epoch: 2131, Train Loss: 0.7551, Test Loss: 5.6504\n",
      "Epoch: 2132, Train Loss: 0.8570, Test Loss: 4.8188\n",
      "Epoch: 2133, Train Loss: 0.8910, Test Loss: 4.8245\n",
      "Epoch: 2134, Train Loss: 0.7624, Test Loss: 4.7750\n",
      "Epoch: 2135, Train Loss: 0.7594, Test Loss: 4.7625\n",
      "Epoch: 2136, Train Loss: 0.7402, Test Loss: 5.1553\n",
      "Epoch: 2137, Train Loss: 0.7651, Test Loss: 5.7280\n",
      "Epoch: 2138, Train Loss: 0.7570, Test Loss: 5.0991\n",
      "Epoch: 2139, Train Loss: 0.7921, Test Loss: 3.8623\n",
      "Epoch: 2140, Train Loss: 0.8740, Test Loss: 4.1913\n",
      "Epoch: 2141, Train Loss: 0.9441, Test Loss: 6.2222\n",
      "Epoch: 2142, Train Loss: 0.9657, Test Loss: 5.8757\n",
      "Epoch: 2143, Train Loss: 0.8060, Test Loss: 4.2378\n",
      "Epoch: 2144, Train Loss: 0.7740, Test Loss: 4.2809\n",
      "Epoch: 2145, Train Loss: 0.8456, Test Loss: 5.8382\n",
      "Epoch: 2146, Train Loss: 0.8031, Test Loss: 6.0260\n",
      "Epoch: 2147, Train Loss: 0.9270, Test Loss: 4.0862\n",
      "Epoch: 2148, Train Loss: 0.9146, Test Loss: 3.8288\n",
      "Epoch: 2149, Train Loss: 0.9201, Test Loss: 5.2428\n",
      "Epoch: 2150, Train Loss: 0.7314, Test Loss: 6.3451\n",
      "Epoch: 2151, Train Loss: 1.0258, Test Loss: 4.3644\n",
      "Epoch: 2152, Train Loss: 0.7616, Test Loss: 3.5626\n",
      "Epoch: 2153, Train Loss: 1.1286, Test Loss: 5.0745\n",
      "Epoch: 2154, Train Loss: 0.7443, Test Loss: 6.2198\n",
      "Epoch: 2155, Train Loss: 0.9586, Test Loss: 4.6820\n",
      "Epoch: 2156, Train Loss: 0.7222, Test Loss: 3.6341\n",
      "Epoch: 2157, Train Loss: 0.8856, Test Loss: 4.0089\n",
      "Epoch: 2158, Train Loss: 0.8930, Test Loss: 6.2185\n",
      "Epoch: 2159, Train Loss: 0.9000, Test Loss: 6.0019\n",
      "Epoch: 2160, Train Loss: 0.8618, Test Loss: 4.3289\n",
      "Epoch: 2161, Train Loss: 0.7801, Test Loss: 3.8412\n",
      "Epoch: 2162, Train Loss: 0.9399, Test Loss: 4.2114\n",
      "Epoch: 2163, Train Loss: 0.7835, Test Loss: 4.9314\n",
      "Epoch: 2164, Train Loss: 0.8357, Test Loss: 5.0996\n",
      "Epoch: 2165, Train Loss: 0.8091, Test Loss: 4.3530\n",
      "Epoch: 2166, Train Loss: 0.7358, Test Loss: 4.2410\n",
      "Epoch: 2167, Train Loss: 0.8138, Test Loss: 4.9985\n",
      "Epoch: 2168, Train Loss: 0.7614, Test Loss: 5.0090\n",
      "Epoch: 2169, Train Loss: 0.8167, Test Loss: 5.4354\n",
      "Epoch: 2170, Train Loss: 0.8214, Test Loss: 4.5552\n",
      "Epoch: 2171, Train Loss: 0.7612, Test Loss: 4.3574\n",
      "Epoch: 2172, Train Loss: 0.7939, Test Loss: 5.2987\n",
      "Epoch: 2173, Train Loss: 0.7511, Test Loss: 5.4279\n",
      "Epoch: 2174, Train Loss: 0.8243, Test Loss: 4.2659\n",
      "Epoch: 2175, Train Loss: 0.8127, Test Loss: 4.1154\n",
      "Epoch: 2176, Train Loss: 0.8029, Test Loss: 5.1568\n",
      "Epoch: 2177, Train Loss: 0.6929, Test Loss: 6.3778\n",
      "Epoch: 2178, Train Loss: 0.9387, Test Loss: 4.8153\n",
      "Epoch: 2179, Train Loss: 0.7960, Test Loss: 3.7749\n",
      "Epoch: 2180, Train Loss: 0.9482, Test Loss: 4.5589\n",
      "Epoch: 2181, Train Loss: 0.6858, Test Loss: 6.0377\n",
      "Epoch: 2182, Train Loss: 0.8680, Test Loss: 5.0877\n",
      "Epoch: 2183, Train Loss: 0.7953, Test Loss: 4.0836\n",
      "Epoch: 2184, Train Loss: 0.7931, Test Loss: 3.9995\n",
      "Epoch: 2185, Train Loss: 0.7733, Test Loss: 4.9159\n",
      "Epoch: 2186, Train Loss: 0.7389, Test Loss: 5.5557\n",
      "Epoch: 2187, Train Loss: 0.9517, Test Loss: 4.1072\n",
      "Epoch: 2188, Train Loss: 0.8506, Test Loss: 4.3056\n",
      "Epoch: 2189, Train Loss: 0.8365, Test Loss: 5.3378\n",
      "Epoch: 2190, Train Loss: 0.7843, Test Loss: 5.1616\n",
      "Epoch: 2191, Train Loss: 0.8049, Test Loss: 4.0643\n",
      "Epoch: 2192, Train Loss: 0.8267, Test Loss: 4.3430\n",
      "Epoch: 2193, Train Loss: 0.8056, Test Loss: 5.0515\n",
      "Epoch: 2194, Train Loss: 0.8030, Test Loss: 4.7570\n",
      "Epoch: 2195, Train Loss: 0.6811, Test Loss: 4.7526\n",
      "Epoch: 2196, Train Loss: 0.7308, Test Loss: 4.6655\n",
      "Epoch: 2197, Train Loss: 0.7413, Test Loss: 5.1814\n",
      "Epoch: 2198, Train Loss: 0.7227, Test Loss: 4.8329\n",
      "Epoch: 2199, Train Loss: 0.7211, Test Loss: 4.1421\n",
      "Epoch: 2200, Train Loss: 0.7133, Test Loss: 4.0795\n",
      "Epoch: 2201, Train Loss: 0.7457, Test Loss: 5.1416\n",
      "Epoch: 2202, Train Loss: 0.7377, Test Loss: 5.7685\n",
      "Epoch: 2203, Train Loss: 0.8730, Test Loss: 4.2007\n",
      "Epoch: 2204, Train Loss: 0.7315, Test Loss: 3.8173\n",
      "Epoch: 2205, Train Loss: 0.9138, Test Loss: 5.3333\n",
      "Epoch: 2206, Train Loss: 0.8229, Test Loss: 5.2864\n",
      "Epoch: 2207, Train Loss: 0.8226, Test Loss: 3.9821\n",
      "Epoch: 2208, Train Loss: 0.7767, Test Loss: 4.2112\n",
      "Epoch: 2209, Train Loss: 0.7786, Test Loss: 5.5152\n",
      "Epoch: 2210, Train Loss: 0.8243, Test Loss: 5.4948\n",
      "Epoch: 2211, Train Loss: 0.8471, Test Loss: 4.4120\n",
      "Epoch: 2212, Train Loss: 0.7409, Test Loss: 4.0082\n",
      "Epoch: 2213, Train Loss: 0.7824, Test Loss: 4.6985\n",
      "Epoch: 2214, Train Loss: 0.7833, Test Loss: 4.8367\n",
      "Epoch: 2215, Train Loss: 0.6802, Test Loss: 5.0283\n",
      "Epoch: 2216, Train Loss: 0.7585, Test Loss: 4.2290\n",
      "Epoch: 2217, Train Loss: 0.6915, Test Loss: 4.0951\n",
      "Epoch: 2218, Train Loss: 0.6731, Test Loss: 4.3261\n",
      "Epoch: 2219, Train Loss: 0.7841, Test Loss: 5.3384\n",
      "Epoch: 2220, Train Loss: 0.7440, Test Loss: 5.4954\n",
      "Epoch: 2221, Train Loss: 0.7963, Test Loss: 4.4815\n",
      "Epoch: 2222, Train Loss: 0.6937, Test Loss: 3.8229\n",
      "Epoch: 2223, Train Loss: 0.7948, Test Loss: 4.6471\n",
      "Epoch: 2224, Train Loss: 0.7115, Test Loss: 5.4054\n",
      "Epoch: 2225, Train Loss: 0.7182, Test Loss: 5.5087\n",
      "Epoch: 2226, Train Loss: 0.7419, Test Loss: 4.2309\n",
      "Epoch: 2227, Train Loss: 0.7321, Test Loss: 3.7583\n",
      "Epoch: 2228, Train Loss: 0.8787, Test Loss: 4.7640\n",
      "Epoch: 2229, Train Loss: 0.7239, Test Loss: 6.2978\n",
      "Epoch: 2230, Train Loss: 1.0302, Test Loss: 4.2900\n",
      "Epoch: 2231, Train Loss: 0.7284, Test Loss: 3.9063\n",
      "Epoch: 2232, Train Loss: 0.8392, Test Loss: 4.4659\n",
      "Epoch: 2233, Train Loss: 0.7966, Test Loss: 5.5453\n",
      "Epoch: 2234, Train Loss: 0.7816, Test Loss: 5.0987\n",
      "Epoch: 2235, Train Loss: 0.7469, Test Loss: 4.0710\n",
      "Epoch: 2236, Train Loss: 0.8072, Test Loss: 4.1073\n",
      "Epoch: 2237, Train Loss: 0.8016, Test Loss: 4.6120\n",
      "Epoch: 2238, Train Loss: 0.7224, Test Loss: 5.2701\n",
      "Epoch: 2239, Train Loss: 0.7241, Test Loss: 5.4747\n",
      "Epoch: 2240, Train Loss: 0.7901, Test Loss: 4.6507\n",
      "Epoch: 2241, Train Loss: 0.8088, Test Loss: 4.2985\n",
      "Epoch: 2242, Train Loss: 0.7162, Test Loss: 4.5299\n",
      "Epoch: 2243, Train Loss: 0.7183, Test Loss: 5.3145\n",
      "Epoch: 2244, Train Loss: 0.8300, Test Loss: 4.7545\n",
      "Epoch: 2245, Train Loss: 0.7445, Test Loss: 4.6493\n",
      "Epoch: 2246, Train Loss: 0.7151, Test Loss: 4.6932\n",
      "Epoch: 2247, Train Loss: 0.8697, Test Loss: 4.6290\n",
      "Epoch: 2248, Train Loss: 0.7262, Test Loss: 4.4816\n",
      "Epoch: 2249, Train Loss: 0.7423, Test Loss: 5.4127\n",
      "Epoch: 2250, Train Loss: 0.7664, Test Loss: 5.2694\n",
      "Epoch: 2251, Train Loss: 0.7379, Test Loss: 4.1172\n",
      "Epoch: 2252, Train Loss: 0.7405, Test Loss: 4.1207\n",
      "Epoch: 2253, Train Loss: 0.8260, Test Loss: 5.2417\n",
      "Epoch: 2254, Train Loss: 0.8784, Test Loss: 5.0645\n",
      "Epoch: 2255, Train Loss: 0.7048, Test Loss: 4.4904\n",
      "Epoch: 2256, Train Loss: 0.7157, Test Loss: 4.5170\n",
      "Epoch: 2257, Train Loss: 0.6971, Test Loss: 4.5163\n",
      "Epoch: 2258, Train Loss: 0.8559, Test Loss: 5.8042\n",
      "Epoch: 2259, Train Loss: 0.8367, Test Loss: 5.0722\n",
      "Epoch: 2260, Train Loss: 0.8246, Test Loss: 3.7135\n",
      "Epoch: 2261, Train Loss: 1.0322, Test Loss: 4.5543\n",
      "Epoch: 2262, Train Loss: 0.7036, Test Loss: 5.7443\n",
      "Epoch: 2263, Train Loss: 0.9072, Test Loss: 4.7409\n",
      "Epoch: 2264, Train Loss: 0.7173, Test Loss: 3.6252\n",
      "Epoch: 2265, Train Loss: 0.9729, Test Loss: 4.2576\n",
      "Epoch: 2266, Train Loss: 0.7509, Test Loss: 5.6646\n",
      "Epoch: 2267, Train Loss: 0.8435, Test Loss: 5.0057\n",
      "Epoch: 2268, Train Loss: 0.6697, Test Loss: 4.3799\n",
      "Epoch: 2269, Train Loss: 0.7463, Test Loss: 4.5158\n",
      "Epoch: 2270, Train Loss: 0.7293, Test Loss: 5.2120\n",
      "Epoch: 2271, Train Loss: 0.7870, Test Loss: 5.0149\n",
      "Epoch: 2272, Train Loss: 0.8762, Test Loss: 3.8409\n",
      "Epoch: 2273, Train Loss: 0.9155, Test Loss: 4.1913\n",
      "Epoch: 2274, Train Loss: 0.7123, Test Loss: 5.0525\n",
      "Epoch: 2275, Train Loss: 0.7230, Test Loss: 5.3660\n",
      "Epoch: 2276, Train Loss: 0.7266, Test Loss: 4.5568\n",
      "Epoch: 2277, Train Loss: 0.7194, Test Loss: 4.2661\n",
      "Epoch: 2278, Train Loss: 0.8212, Test Loss: 5.3477\n",
      "Epoch: 2279, Train Loss: 0.7092, Test Loss: 5.8585\n",
      "Epoch: 2280, Train Loss: 0.7955, Test Loss: 4.5351\n",
      "Epoch: 2281, Train Loss: 0.7214, Test Loss: 4.1269\n",
      "Epoch: 2282, Train Loss: 0.7990, Test Loss: 5.0274\n",
      "Epoch: 2283, Train Loss: 0.7523, Test Loss: 5.2920\n",
      "Epoch: 2284, Train Loss: 0.7229, Test Loss: 4.9201\n",
      "Epoch: 2285, Train Loss: 0.7354, Test Loss: 4.5049\n",
      "Epoch: 2286, Train Loss: 0.7239, Test Loss: 4.6221\n",
      "Epoch: 2287, Train Loss: 0.7078, Test Loss: 4.8209\n",
      "Epoch: 2288, Train Loss: 0.7174, Test Loss: 5.4910\n",
      "Epoch: 2289, Train Loss: 0.7419, Test Loss: 4.7751\n",
      "Epoch: 2290, Train Loss: 0.6960, Test Loss: 3.9487\n",
      "Epoch: 2291, Train Loss: 0.7303, Test Loss: 4.1460\n",
      "Epoch: 2292, Train Loss: 0.7248, Test Loss: 5.2503\n",
      "Epoch: 2293, Train Loss: 0.8132, Test Loss: 5.3745\n",
      "Epoch: 2294, Train Loss: 0.7517, Test Loss: 4.1541\n",
      "Epoch: 2295, Train Loss: 0.7942, Test Loss: 4.1595\n",
      "Epoch: 2296, Train Loss: 0.7929, Test Loss: 4.7110\n",
      "Epoch: 2297, Train Loss: 0.7457, Test Loss: 5.0878\n",
      "Epoch: 2298, Train Loss: 0.6799, Test Loss: 4.8961\n",
      "Epoch: 2299, Train Loss: 0.6917, Test Loss: 4.1571\n",
      "Epoch: 2300, Train Loss: 0.7487, Test Loss: 4.5475\n",
      "Epoch: 2301, Train Loss: 0.7588, Test Loss: 5.0300\n",
      "Epoch: 2302, Train Loss: 0.6984, Test Loss: 4.9633\n",
      "Epoch: 2303, Train Loss: 0.7140, Test Loss: 5.0874\n",
      "Epoch: 2304, Train Loss: 0.6421, Test Loss: 4.8522\n",
      "Epoch: 2305, Train Loss: 0.6996, Test Loss: 4.7129\n",
      "Epoch: 2306, Train Loss: 0.7096, Test Loss: 5.4011\n",
      "Epoch: 2307, Train Loss: 0.7263, Test Loss: 4.8038\n",
      "Epoch: 2308, Train Loss: 0.7402, Test Loss: 4.4548\n",
      "Epoch: 2309, Train Loss: 0.7402, Test Loss: 4.3469\n",
      "Epoch: 2310, Train Loss: 0.6822, Test Loss: 4.7008\n",
      "Epoch: 2311, Train Loss: 0.7006, Test Loss: 5.1088\n",
      "Epoch: 2312, Train Loss: 0.6712, Test Loss: 5.5057\n",
      "Epoch: 2313, Train Loss: 0.7617, Test Loss: 4.4480\n",
      "Epoch: 2314, Train Loss: 0.7270, Test Loss: 3.9662\n",
      "Epoch: 2315, Train Loss: 0.7621, Test Loss: 4.6920\n",
      "Epoch: 2316, Train Loss: 0.6939, Test Loss: 4.9912\n",
      "Epoch: 2317, Train Loss: 0.7352, Test Loss: 4.5166\n",
      "Epoch: 2318, Train Loss: 0.7002, Test Loss: 4.3236\n",
      "Epoch: 2319, Train Loss: 0.7260, Test Loss: 4.7041\n",
      "Epoch: 2320, Train Loss: 0.7017, Test Loss: 4.9616\n",
      "Epoch: 2321, Train Loss: 0.6762, Test Loss: 4.8510\n",
      "Epoch: 2322, Train Loss: 0.7267, Test Loss: 4.3682\n",
      "Epoch: 2323, Train Loss: 0.7598, Test Loss: 4.5940\n",
      "Epoch: 2324, Train Loss: 0.7198, Test Loss: 4.9570\n",
      "Epoch: 2325, Train Loss: 0.7114, Test Loss: 5.1262\n",
      "Epoch: 2326, Train Loss: 0.7525, Test Loss: 4.6329\n",
      "Epoch: 2327, Train Loss: 0.7280, Test Loss: 4.5838\n",
      "Epoch: 2328, Train Loss: 0.7433, Test Loss: 5.2211\n",
      "Epoch: 2329, Train Loss: 0.7072, Test Loss: 5.1995\n",
      "Epoch: 2330, Train Loss: 0.6690, Test Loss: 4.5868\n",
      "Epoch: 2331, Train Loss: 0.7621, Test Loss: 4.7420\n",
      "Epoch: 2332, Train Loss: 0.6761, Test Loss: 5.5103\n",
      "Epoch: 2333, Train Loss: 0.7525, Test Loss: 4.5078\n",
      "Epoch: 2334, Train Loss: 0.7059, Test Loss: 4.3078\n",
      "Epoch: 2335, Train Loss: 0.6844, Test Loss: 5.2105\n",
      "Epoch: 2336, Train Loss: 0.7536, Test Loss: 5.1290\n",
      "Epoch: 2337, Train Loss: 0.7280, Test Loss: 4.2051\n",
      "Epoch: 2338, Train Loss: 0.6876, Test Loss: 4.0806\n",
      "Epoch: 2339, Train Loss: 0.7649, Test Loss: 5.0926\n",
      "Epoch: 2340, Train Loss: 0.7243, Test Loss: 5.3870\n",
      "Epoch: 2341, Train Loss: 0.7078, Test Loss: 4.7296\n",
      "Epoch: 2342, Train Loss: 0.6661, Test Loss: 4.4294\n",
      "Epoch: 2343, Train Loss: 0.7819, Test Loss: 5.1202\n",
      "Epoch: 2344, Train Loss: 0.7501, Test Loss: 5.3080\n",
      "Epoch: 2345, Train Loss: 0.7567, Test Loss: 4.6000\n",
      "Epoch: 2346, Train Loss: 0.7514, Test Loss: 4.4743\n",
      "Epoch: 2347, Train Loss: 0.7674, Test Loss: 4.6808\n",
      "Epoch: 2348, Train Loss: 0.7895, Test Loss: 4.4942\n",
      "Epoch: 2349, Train Loss: 0.7243, Test Loss: 4.5726\n",
      "Epoch: 2350, Train Loss: 0.7567, Test Loss: 4.9807\n",
      "Epoch: 2351, Train Loss: 0.6867, Test Loss: 4.5637\n",
      "Epoch: 2352, Train Loss: 0.6775, Test Loss: 4.0537\n",
      "Epoch: 2353, Train Loss: 0.7799, Test Loss: 4.6101\n",
      "Epoch: 2354, Train Loss: 0.6702, Test Loss: 5.3280\n",
      "Epoch: 2355, Train Loss: 0.7257, Test Loss: 4.8617\n",
      "Epoch: 2356, Train Loss: 0.7419, Test Loss: 4.0566\n",
      "Epoch: 2357, Train Loss: 0.7475, Test Loss: 4.0087\n",
      "Epoch: 2358, Train Loss: 0.7686, Test Loss: 5.0690\n",
      "Epoch: 2359, Train Loss: 0.7468, Test Loss: 5.1436\n",
      "Epoch: 2360, Train Loss: 0.7282, Test Loss: 4.2581\n",
      "Epoch: 2361, Train Loss: 0.6541, Test Loss: 4.0797\n",
      "Epoch: 2362, Train Loss: 0.7392, Test Loss: 5.1742\n",
      "Epoch: 2363, Train Loss: 0.7404, Test Loss: 6.0402\n",
      "Epoch: 2364, Train Loss: 0.7945, Test Loss: 4.8758\n",
      "Epoch: 2365, Train Loss: 0.7491, Test Loss: 3.7564\n",
      "Epoch: 2366, Train Loss: 0.8206, Test Loss: 4.1057\n",
      "Epoch: 2367, Train Loss: 0.7419, Test Loss: 5.6028\n",
      "Epoch: 2368, Train Loss: 0.7092, Test Loss: 5.8867\n",
      "Epoch: 2369, Train Loss: 0.7427, Test Loss: 4.5156\n",
      "Epoch: 2370, Train Loss: 0.6540, Test Loss: 3.8009\n",
      "Epoch: 2371, Train Loss: 0.8984, Test Loss: 5.1980\n",
      "Epoch: 2372, Train Loss: 0.7677, Test Loss: 5.7800\n",
      "Epoch: 2373, Train Loss: 0.8158, Test Loss: 4.5895\n",
      "Epoch: 2374, Train Loss: 0.6603, Test Loss: 4.1395\n",
      "Epoch: 2375, Train Loss: 0.7496, Test Loss: 4.4727\n",
      "Epoch: 2376, Train Loss: 0.7482, Test Loss: 4.8188\n",
      "Epoch: 2377, Train Loss: 0.6955, Test Loss: 4.9823\n",
      "Epoch: 2378, Train Loss: 0.7037, Test Loss: 4.5544\n",
      "Epoch: 2379, Train Loss: 0.7120, Test Loss: 4.8738\n",
      "Epoch: 2380, Train Loss: 0.7443, Test Loss: 5.3840\n",
      "Epoch: 2381, Train Loss: 0.6804, Test Loss: 4.7402\n",
      "Epoch: 2382, Train Loss: 0.6869, Test Loss: 4.3860\n",
      "Epoch: 2383, Train Loss: 0.6930, Test Loss: 4.5244\n",
      "Epoch: 2384, Train Loss: 0.7374, Test Loss: 5.2680\n",
      "Epoch: 2385, Train Loss: 0.6860, Test Loss: 5.7310\n",
      "Epoch: 2386, Train Loss: 0.8642, Test Loss: 3.9490\n",
      "Epoch: 2387, Train Loss: 0.7396, Test Loss: 3.8657\n",
      "Epoch: 2388, Train Loss: 0.8257, Test Loss: 5.3346\n",
      "Epoch: 2389, Train Loss: 0.6776, Test Loss: 5.8959\n",
      "Epoch: 2390, Train Loss: 0.7921, Test Loss: 4.2881\n",
      "Epoch: 2391, Train Loss: 0.7911, Test Loss: 4.3572\n",
      "Epoch: 2392, Train Loss: 0.7518, Test Loss: 5.5438\n",
      "Epoch: 2393, Train Loss: 0.6357, Test Loss: 5.8604\n",
      "Epoch: 2394, Train Loss: 0.8184, Test Loss: 4.0401\n",
      "Epoch: 2395, Train Loss: 0.7488, Test Loss: 3.9136\n",
      "Epoch: 2396, Train Loss: 0.7539, Test Loss: 5.2268\n",
      "Epoch: 2397, Train Loss: 0.7807, Test Loss: 4.9143\n",
      "Epoch: 2398, Train Loss: 0.6596, Test Loss: 4.3407\n",
      "Epoch: 2399, Train Loss: 0.7049, Test Loss: 4.3324\n",
      "Epoch: 2400, Train Loss: 0.7296, Test Loss: 4.8352\n",
      "Epoch: 2401, Train Loss: 0.6892, Test Loss: 4.6567\n",
      "Epoch: 2402, Train Loss: 0.7157, Test Loss: 5.0381\n",
      "Epoch: 2403, Train Loss: 0.6627, Test Loss: 4.8413\n",
      "Epoch: 2404, Train Loss: 0.6939, Test Loss: 4.1115\n",
      "Epoch: 2405, Train Loss: 0.7477, Test Loss: 4.6749\n",
      "Epoch: 2406, Train Loss: 0.7144, Test Loss: 5.8058\n",
      "Epoch: 2407, Train Loss: 0.9112, Test Loss: 4.5233\n",
      "Epoch: 2408, Train Loss: 0.7353, Test Loss: 4.0351\n",
      "Epoch: 2409, Train Loss: 0.7269, Test Loss: 4.5551\n",
      "Epoch: 2410, Train Loss: 0.6903, Test Loss: 5.3924\n",
      "Epoch: 2411, Train Loss: 0.8203, Test Loss: 5.3165\n",
      "Epoch: 2412, Train Loss: 0.8589, Test Loss: 3.8441\n",
      "Epoch: 2413, Train Loss: 0.7831, Test Loss: 4.0443\n",
      "Epoch: 2414, Train Loss: 0.7358, Test Loss: 5.4021\n",
      "Epoch: 2415, Train Loss: 0.7038, Test Loss: 5.6421\n",
      "Epoch: 2416, Train Loss: 0.7626, Test Loss: 4.3594\n",
      "Epoch: 2417, Train Loss: 0.6874, Test Loss: 3.7774\n",
      "Epoch: 2418, Train Loss: 0.8737, Test Loss: 5.2262\n",
      "Epoch: 2419, Train Loss: 0.7290, Test Loss: 5.9834\n",
      "Epoch: 2420, Train Loss: 0.7969, Test Loss: 4.4892\n",
      "Epoch: 2421, Train Loss: 0.7432, Test Loss: 3.6439\n",
      "Epoch: 2422, Train Loss: 0.9423, Test Loss: 4.6216\n",
      "Epoch: 2423, Train Loss: 0.6830, Test Loss: 6.2144\n",
      "Epoch: 2424, Train Loss: 0.9348, Test Loss: 4.7788\n",
      "Epoch: 2425, Train Loss: 0.6654, Test Loss: 4.0056\n",
      "Epoch: 2426, Train Loss: 0.7832, Test Loss: 4.3463\n",
      "Epoch: 2427, Train Loss: 0.7132, Test Loss: 5.5882\n",
      "Epoch: 2428, Train Loss: 0.7056, Test Loss: 5.4739\n",
      "Epoch: 2429, Train Loss: 0.7536, Test Loss: 4.0374\n",
      "Epoch: 2430, Train Loss: 0.7355, Test Loss: 3.9114\n",
      "Epoch: 2431, Train Loss: 0.7829, Test Loss: 5.2788\n",
      "Epoch: 2432, Train Loss: 0.8581, Test Loss: 5.1157\n",
      "Epoch: 2433, Train Loss: 0.7251, Test Loss: 4.3329\n",
      "Epoch: 2434, Train Loss: 0.6918, Test Loss: 3.9456\n",
      "Epoch: 2435, Train Loss: 0.6944, Test Loss: 4.3462\n",
      "Epoch: 2436, Train Loss: 0.6865, Test Loss: 5.3392\n",
      "Epoch: 2437, Train Loss: 0.7408, Test Loss: 5.3304\n",
      "Epoch: 2438, Train Loss: 0.7256, Test Loss: 4.0914\n",
      "Epoch: 2439, Train Loss: 0.7103, Test Loss: 3.7829\n",
      "Epoch: 2440, Train Loss: 0.8495, Test Loss: 5.1055\n",
      "Epoch: 2441, Train Loss: 0.6867, Test Loss: 5.4386\n",
      "Epoch: 2442, Train Loss: 0.8254, Test Loss: 4.4009\n",
      "Epoch: 2443, Train Loss: 0.7099, Test Loss: 3.8418\n",
      "Epoch: 2444, Train Loss: 0.7498, Test Loss: 4.1663\n",
      "Epoch: 2445, Train Loss: 0.7624, Test Loss: 5.4052\n",
      "Epoch: 2446, Train Loss: 0.7124, Test Loss: 5.4702\n",
      "Epoch: 2447, Train Loss: 0.7125, Test Loss: 4.5955\n",
      "Epoch: 2448, Train Loss: 0.6533, Test Loss: 3.5359\n",
      "Epoch: 2449, Train Loss: 0.7545, Test Loss: 3.9072\n",
      "Epoch: 2450, Train Loss: 0.7065, Test Loss: 5.3393\n",
      "Epoch: 2451, Train Loss: 0.7573, Test Loss: 5.6922\n",
      "Epoch: 2452, Train Loss: 0.7830, Test Loss: 4.2661\n",
      "Epoch: 2453, Train Loss: 0.6859, Test Loss: 3.4665\n",
      "Epoch: 2454, Train Loss: 0.9321, Test Loss: 4.5877\n",
      "Epoch: 2455, Train Loss: 0.7167, Test Loss: 5.9702\n",
      "Epoch: 2456, Train Loss: 0.9669, Test Loss: 4.4757\n",
      "Epoch: 2457, Train Loss: 0.6961, Test Loss: 3.5259\n",
      "Epoch: 2458, Train Loss: 0.8349, Test Loss: 4.0540\n",
      "Epoch: 2459, Train Loss: 0.7038, Test Loss: 5.4099\n",
      "Epoch: 2460, Train Loss: 0.8706, Test Loss: 4.9797\n",
      "Epoch: 2461, Train Loss: 0.8024, Test Loss: 3.5731\n",
      "Epoch: 2462, Train Loss: 0.8213, Test Loss: 3.7430\n",
      "Epoch: 2463, Train Loss: 0.7680, Test Loss: 5.1289\n",
      "Epoch: 2464, Train Loss: 0.7230, Test Loss: 5.2798\n",
      "Epoch: 2465, Train Loss: 0.7665, Test Loss: 4.0598\n",
      "Epoch: 2466, Train Loss: 0.7330, Test Loss: 3.9451\n",
      "Epoch: 2467, Train Loss: 0.8129, Test Loss: 5.0620\n",
      "Epoch: 2468, Train Loss: 0.6826, Test Loss: 5.6512\n",
      "Epoch: 2469, Train Loss: 0.8694, Test Loss: 4.5029\n",
      "Epoch: 2470, Train Loss: 0.7037, Test Loss: 3.7033\n",
      "Epoch: 2471, Train Loss: 0.8214, Test Loss: 4.4224\n",
      "Epoch: 2472, Train Loss: 0.6933, Test Loss: 5.8344\n",
      "Epoch: 2473, Train Loss: 0.7886, Test Loss: 5.4365\n",
      "Epoch: 2474, Train Loss: 0.7117, Test Loss: 4.0417\n",
      "Epoch: 2475, Train Loss: 0.7537, Test Loss: 4.0867\n",
      "Epoch: 2476, Train Loss: 0.7051, Test Loss: 4.7305\n",
      "Epoch: 2477, Train Loss: 0.6646, Test Loss: 5.6127\n",
      "Epoch: 2478, Train Loss: 0.6751, Test Loss: 5.2845\n",
      "Epoch: 2479, Train Loss: 0.7463, Test Loss: 4.0388\n",
      "Epoch: 2480, Train Loss: 0.7075, Test Loss: 3.7157\n",
      "Epoch: 2481, Train Loss: 0.8031, Test Loss: 4.9422\n",
      "Epoch: 2482, Train Loss: 0.7192, Test Loss: 5.9140\n",
      "Epoch: 2483, Train Loss: 0.7715, Test Loss: 4.9532\n",
      "Epoch: 2484, Train Loss: 0.6850, Test Loss: 3.9296\n",
      "Epoch: 2485, Train Loss: 0.7473, Test Loss: 3.9629\n",
      "Epoch: 2486, Train Loss: 0.7336, Test Loss: 5.2686\n",
      "Epoch: 2487, Train Loss: 0.7058, Test Loss: 5.5156\n",
      "Epoch: 2488, Train Loss: 0.7590, Test Loss: 4.3201\n",
      "Epoch: 2489, Train Loss: 0.6511, Test Loss: 3.6980\n",
      "Epoch: 2490, Train Loss: 0.8598, Test Loss: 4.8243\n",
      "Epoch: 2491, Train Loss: 0.7316, Test Loss: 6.3458\n",
      "Epoch: 2492, Train Loss: 0.9688, Test Loss: 4.5825\n",
      "Epoch: 2493, Train Loss: 0.6247, Test Loss: 3.7066\n",
      "Epoch: 2494, Train Loss: 0.8546, Test Loss: 4.5649\n",
      "Epoch: 2495, Train Loss: 0.6795, Test Loss: 6.3302\n",
      "Epoch: 2496, Train Loss: 0.9127, Test Loss: 4.9916\n",
      "Epoch: 2497, Train Loss: 0.6883, Test Loss: 3.8459\n",
      "Epoch: 2498, Train Loss: 0.7638, Test Loss: 3.9444\n",
      "Epoch: 2499, Train Loss: 0.6724, Test Loss: 4.8389\n",
      "Epoch: 2500, Train Loss: 0.6960, Test Loss: 5.0807\n",
      "Epoch: 2501, Train Loss: 0.8350, Test Loss: 3.9989\n",
      "Epoch: 2502, Train Loss: 0.7087, Test Loss: 3.8706\n",
      "Epoch: 2503, Train Loss: 0.7644, Test Loss: 4.6883\n",
      "Epoch: 2504, Train Loss: 0.6303, Test Loss: 5.4597\n",
      "Epoch: 2505, Train Loss: 0.8795, Test Loss: 4.0933\n",
      "Epoch: 2506, Train Loss: 0.6813, Test Loss: 3.8115\n",
      "Epoch: 2507, Train Loss: 0.8065, Test Loss: 4.6667\n",
      "Epoch: 2508, Train Loss: 0.7175, Test Loss: 5.6639\n",
      "Epoch: 2509, Train Loss: 0.7511, Test Loss: 4.8288\n",
      "Epoch: 2510, Train Loss: 0.7073, Test Loss: 3.7200\n",
      "Epoch: 2511, Train Loss: 0.7886, Test Loss: 3.6463\n",
      "Epoch: 2512, Train Loss: 0.7038, Test Loss: 4.7171\n",
      "Epoch: 2513, Train Loss: 0.7085, Test Loss: 5.0587\n",
      "Epoch: 2514, Train Loss: 0.7254, Test Loss: 4.2400\n",
      "Epoch: 2515, Train Loss: 0.7711, Test Loss: 4.1189\n",
      "Epoch: 2516, Train Loss: 0.7102, Test Loss: 4.6868\n",
      "Epoch: 2517, Train Loss: 0.6805, Test Loss: 5.1844\n",
      "Epoch: 2518, Train Loss: 0.8527, Test Loss: 3.8995\n",
      "Epoch: 2519, Train Loss: 0.7191, Test Loss: 3.7057\n",
      "Epoch: 2520, Train Loss: 0.7282, Test Loss: 4.3764\n",
      "Epoch: 2521, Train Loss: 0.6837, Test Loss: 5.2485\n",
      "Epoch: 2522, Train Loss: 0.8182, Test Loss: 4.4142\n",
      "Epoch: 2523, Train Loss: 0.6771, Test Loss: 4.2229\n",
      "Epoch: 2524, Train Loss: 0.6761, Test Loss: 4.5032\n",
      "Epoch: 2525, Train Loss: 0.6856, Test Loss: 5.2309\n",
      "Epoch: 2526, Train Loss: 0.7154, Test Loss: 5.0881\n",
      "Epoch: 2527, Train Loss: 0.6682, Test Loss: 4.3361\n",
      "Epoch: 2528, Train Loss: 0.6932, Test Loss: 4.1443\n",
      "Epoch: 2529, Train Loss: 0.7216, Test Loss: 4.5530\n",
      "Epoch: 2530, Train Loss: 0.6605, Test Loss: 4.6803\n",
      "Epoch: 2531, Train Loss: 0.6839, Test Loss: 5.0479\n",
      "Epoch: 2532, Train Loss: 0.7553, Test Loss: 4.2364\n",
      "Epoch: 2533, Train Loss: 0.7307, Test Loss: 4.0575\n",
      "Epoch: 2534, Train Loss: 0.7091, Test Loss: 3.9777\n",
      "Epoch: 2535, Train Loss: 0.7125, Test Loss: 4.6267\n",
      "Epoch: 2536, Train Loss: 0.6885, Test Loss: 5.3888\n",
      "Epoch: 2537, Train Loss: 0.7537, Test Loss: 5.1059\n",
      "Epoch: 2538, Train Loss: 0.6249, Test Loss: 3.9779\n",
      "Epoch: 2539, Train Loss: 0.7673, Test Loss: 4.1604\n",
      "Epoch: 2540, Train Loss: 0.7251, Test Loss: 4.9923\n",
      "Epoch: 2541, Train Loss: 0.7756, Test Loss: 4.9088\n",
      "Epoch: 2542, Train Loss: 0.7378, Test Loss: 3.9975\n",
      "Epoch: 2543, Train Loss: 0.6350, Test Loss: 3.9013\n",
      "Epoch: 2544, Train Loss: 0.7091, Test Loss: 4.5640\n",
      "Epoch: 2545, Train Loss: 0.6933, Test Loss: 4.8862\n",
      "Epoch: 2546, Train Loss: 0.7082, Test Loss: 4.4662\n",
      "Epoch: 2547, Train Loss: 0.6765, Test Loss: 4.7467\n",
      "Epoch: 2548, Train Loss: 0.6448, Test Loss: 4.5962\n",
      "Epoch: 2549, Train Loss: 0.6478, Test Loss: 4.5558\n",
      "Epoch: 2550, Train Loss: 0.6859, Test Loss: 4.3122\n",
      "Epoch: 2551, Train Loss: 0.6293, Test Loss: 4.4796\n",
      "Epoch: 2552, Train Loss: 0.6351, Test Loss: 4.6484\n",
      "Epoch: 2553, Train Loss: 0.6900, Test Loss: 5.0244\n",
      "Epoch: 2554, Train Loss: 0.6581, Test Loss: 4.4867\n",
      "Epoch: 2555, Train Loss: 0.6452, Test Loss: 4.1132\n",
      "Epoch: 2556, Train Loss: 0.6459, Test Loss: 3.8072\n",
      "Epoch: 2557, Train Loss: 0.7383, Test Loss: 4.4930\n",
      "Epoch: 2558, Train Loss: 0.7146, Test Loss: 5.8530\n",
      "Epoch: 2559, Train Loss: 0.8149, Test Loss: 4.5394\n",
      "Epoch: 2560, Train Loss: 0.6850, Test Loss: 3.4421\n",
      "Epoch: 2561, Train Loss: 0.8694, Test Loss: 4.2143\n",
      "Epoch: 2562, Train Loss: 0.6561, Test Loss: 5.1837\n",
      "Epoch: 2563, Train Loss: 0.6671, Test Loss: 5.1767\n",
      "Epoch: 2564, Train Loss: 0.7382, Test Loss: 4.3735\n",
      "Epoch: 2565, Train Loss: 0.6400, Test Loss: 3.9280\n",
      "Epoch: 2566, Train Loss: 0.7094, Test Loss: 4.6211\n",
      "Epoch: 2567, Train Loss: 0.7008, Test Loss: 5.1324\n",
      "Epoch: 2568, Train Loss: 0.6737, Test Loss: 4.7464\n",
      "Epoch: 2569, Train Loss: 0.6001, Test Loss: 4.2633\n",
      "Epoch: 2570, Train Loss: 0.7038, Test Loss: 4.3491\n",
      "Epoch: 2571, Train Loss: 0.6563, Test Loss: 4.9869\n",
      "Epoch: 2572, Train Loss: 0.7033, Test Loss: 5.1127\n",
      "Epoch: 2573, Train Loss: 0.6922, Test Loss: 4.0148\n",
      "Epoch: 2574, Train Loss: 0.6641, Test Loss: 3.9214\n",
      "Epoch: 2575, Train Loss: 0.7454, Test Loss: 5.2435\n",
      "Epoch: 2576, Train Loss: 0.6993, Test Loss: 5.3625\n",
      "Epoch: 2577, Train Loss: 0.7461, Test Loss: 3.8271\n",
      "Epoch: 2578, Train Loss: 0.7480, Test Loss: 3.6771\n",
      "Epoch: 2579, Train Loss: 0.7725, Test Loss: 5.0193\n",
      "Epoch: 2580, Train Loss: 0.6689, Test Loss: 6.1114\n",
      "Epoch: 2581, Train Loss: 1.0093, Test Loss: 4.0100\n",
      "Epoch: 2582, Train Loss: 0.7168, Test Loss: 3.3126\n",
      "Epoch: 2583, Train Loss: 1.0239, Test Loss: 4.4268\n",
      "Epoch: 2584, Train Loss: 0.6787, Test Loss: 5.9558\n",
      "Epoch: 2585, Train Loss: 0.8912, Test Loss: 4.8944\n",
      "Epoch: 2586, Train Loss: 0.6912, Test Loss: 3.6324\n",
      "Epoch: 2587, Train Loss: 0.7334, Test Loss: 3.6040\n",
      "Epoch: 2588, Train Loss: 0.7897, Test Loss: 4.9461\n",
      "Epoch: 2589, Train Loss: 0.7320, Test Loss: 5.8572\n",
      "Epoch: 2590, Train Loss: 0.7778, Test Loss: 4.6190\n",
      "Epoch: 2591, Train Loss: 0.6523, Test Loss: 3.7770\n",
      "Epoch: 2592, Train Loss: 0.7801, Test Loss: 4.1487\n",
      "Epoch: 2593, Train Loss: 0.6530, Test Loss: 5.0542\n",
      "Epoch: 2594, Train Loss: 0.6917, Test Loss: 4.9759\n",
      "Epoch: 2595, Train Loss: 0.7096, Test Loss: 4.1646\n",
      "Epoch: 2596, Train Loss: 0.7674, Test Loss: 4.1252\n",
      "Epoch: 2597, Train Loss: 0.7021, Test Loss: 4.7414\n",
      "Epoch: 2598, Train Loss: 0.6526, Test Loss: 4.9679\n",
      "Epoch: 2599, Train Loss: 0.7968, Test Loss: 4.0449\n",
      "Epoch: 2600, Train Loss: 0.6828, Test Loss: 4.3009\n",
      "Epoch: 2601, Train Loss: 0.6639, Test Loss: 4.8551\n",
      "Epoch: 2602, Train Loss: 0.6407, Test Loss: 4.9742\n",
      "Epoch: 2603, Train Loss: 0.7205, Test Loss: 4.3607\n",
      "Epoch: 2604, Train Loss: 0.6168, Test Loss: 3.6985\n",
      "Epoch: 2605, Train Loss: 0.9223, Test Loss: 5.0166\n",
      "Epoch: 2606, Train Loss: 0.6661, Test Loss: 6.5346\n",
      "Epoch: 2607, Train Loss: 1.0561, Test Loss: 4.2349\n",
      "Epoch: 2608, Train Loss: 0.6990, Test Loss: 3.5685\n",
      "Epoch: 2609, Train Loss: 0.8469, Test Loss: 4.3606\n",
      "Epoch: 2610, Train Loss: 0.6255, Test Loss: 5.8284\n",
      "Epoch: 2611, Train Loss: 0.9065, Test Loss: 5.0196\n",
      "Epoch: 2612, Train Loss: 0.7232, Test Loss: 3.7292\n",
      "Epoch: 2613, Train Loss: 0.7144, Test Loss: 3.6936\n",
      "Epoch: 2614, Train Loss: 0.7427, Test Loss: 4.8833\n",
      "Epoch: 2615, Train Loss: 0.6944, Test Loss: 5.3710\n",
      "Epoch: 2616, Train Loss: 0.7312, Test Loss: 4.4531\n",
      "Epoch: 2617, Train Loss: 0.6552, Test Loss: 3.7959\n",
      "Epoch: 2618, Train Loss: 0.7117, Test Loss: 4.0461\n",
      "Epoch: 2619, Train Loss: 0.6928, Test Loss: 5.3895\n",
      "Epoch: 2620, Train Loss: 0.7835, Test Loss: 5.3058\n",
      "Epoch: 2621, Train Loss: 0.7278, Test Loss: 3.8814\n",
      "Epoch: 2622, Train Loss: 0.7439, Test Loss: 3.8833\n",
      "Epoch: 2623, Train Loss: 0.6421, Test Loss: 4.4766\n",
      "Epoch: 2624, Train Loss: 0.6617, Test Loss: 5.4662\n",
      "Epoch: 2625, Train Loss: 0.7354, Test Loss: 4.5545\n",
      "Epoch: 2626, Train Loss: 0.6344, Test Loss: 4.0182\n",
      "Epoch: 2627, Train Loss: 0.6755, Test Loss: 4.1855\n",
      "Epoch: 2628, Train Loss: 0.6626, Test Loss: 4.9879\n",
      "Epoch: 2629, Train Loss: 0.7225, Test Loss: 4.8461\n",
      "Epoch: 2630, Train Loss: 0.7490, Test Loss: 4.6851\n",
      "Epoch: 2631, Train Loss: 0.6289, Test Loss: 4.3523\n",
      "Epoch: 2632, Train Loss: 0.6753, Test Loss: 4.1181\n",
      "Epoch: 2633, Train Loss: 0.7068, Test Loss: 4.8237\n",
      "Epoch: 2634, Train Loss: 0.6162, Test Loss: 5.3331\n",
      "Epoch: 2635, Train Loss: 0.7129, Test Loss: 4.4446\n",
      "Epoch: 2636, Train Loss: 0.7350, Test Loss: 3.6939\n",
      "Epoch: 2637, Train Loss: 0.8591, Test Loss: 4.5688\n",
      "Epoch: 2638, Train Loss: 0.6221, Test Loss: 5.8875\n",
      "Epoch: 2639, Train Loss: 0.8385, Test Loss: 4.9165\n",
      "Epoch: 2640, Train Loss: 0.7291, Test Loss: 3.8558\n",
      "Epoch: 2641, Train Loss: 0.6953, Test Loss: 4.0819\n",
      "Epoch: 2642, Train Loss: 0.7924, Test Loss: 6.1064\n",
      "Epoch: 2643, Train Loss: 0.8153, Test Loss: 5.5472\n",
      "Epoch: 2644, Train Loss: 0.6237, Test Loss: 4.0068\n",
      "Epoch: 2645, Train Loss: 0.7034, Test Loss: 3.7964\n",
      "Epoch: 2646, Train Loss: 0.7812, Test Loss: 4.9361\n",
      "Epoch: 2647, Train Loss: 0.6581, Test Loss: 5.5901\n",
      "Epoch: 2648, Train Loss: 0.7761, Test Loss: 4.2895\n",
      "Epoch: 2649, Train Loss: 0.6846, Test Loss: 3.7785\n",
      "Epoch: 2650, Train Loss: 0.6784, Test Loss: 4.4637\n",
      "Epoch: 2651, Train Loss: 0.6236, Test Loss: 5.6233\n",
      "Epoch: 2652, Train Loss: 0.7054, Test Loss: 5.0755\n",
      "Epoch: 2653, Train Loss: 0.6775, Test Loss: 4.0195\n",
      "Epoch: 2654, Train Loss: 0.6931, Test Loss: 3.8319\n",
      "Epoch: 2655, Train Loss: 0.6937, Test Loss: 4.7262\n",
      "Epoch: 2656, Train Loss: 0.6309, Test Loss: 5.5924\n",
      "Epoch: 2657, Train Loss: 0.6922, Test Loss: 4.8842\n",
      "Epoch: 2658, Train Loss: 0.6721, Test Loss: 4.1897\n",
      "Epoch: 2659, Train Loss: 0.6133, Test Loss: 4.2556\n",
      "Epoch: 2660, Train Loss: 0.6641, Test Loss: 4.8810\n",
      "Epoch: 2661, Train Loss: 0.7128, Test Loss: 4.7332\n",
      "Epoch: 2662, Train Loss: 0.6322, Test Loss: 4.5273\n",
      "Epoch: 2663, Train Loss: 0.6903, Test Loss: 4.4346\n",
      "Epoch: 2664, Train Loss: 0.6802, Test Loss: 4.9195\n",
      "Epoch: 2665, Train Loss: 0.6513, Test Loss: 4.4762\n",
      "Epoch: 2666, Train Loss: 0.6563, Test Loss: 4.4510\n",
      "Epoch: 2667, Train Loss: 0.7306, Test Loss: 5.3648\n",
      "Epoch: 2668, Train Loss: 0.6471, Test Loss: 5.3009\n",
      "Epoch: 2669, Train Loss: 0.6530, Test Loss: 4.4606\n",
      "Epoch: 2670, Train Loss: 0.6094, Test Loss: 4.2352\n",
      "Epoch: 2671, Train Loss: 0.6774, Test Loss: 4.8327\n",
      "Epoch: 2672, Train Loss: 0.6616, Test Loss: 5.5145\n",
      "Epoch: 2673, Train Loss: 0.7798, Test Loss: 4.2115\n",
      "Epoch: 2674, Train Loss: 0.7661, Test Loss: 4.4138\n",
      "Epoch: 2675, Train Loss: 0.7004, Test Loss: 4.6165\n",
      "Epoch: 2676, Train Loss: 0.6887, Test Loss: 5.3157\n",
      "Epoch: 2677, Train Loss: 0.6854, Test Loss: 4.7530\n",
      "Epoch: 2678, Train Loss: 0.6521, Test Loss: 4.5038\n",
      "Epoch: 2679, Train Loss: 0.6614, Test Loss: 4.6012\n",
      "Epoch: 2680, Train Loss: 0.6434, Test Loss: 4.7108\n",
      "Epoch: 2681, Train Loss: 0.6435, Test Loss: 5.0946\n",
      "Epoch: 2682, Train Loss: 0.6461, Test Loss: 4.6724\n",
      "Epoch: 2683, Train Loss: 0.6432, Test Loss: 3.9321\n",
      "Epoch: 2684, Train Loss: 0.6796, Test Loss: 4.4856\n",
      "Epoch: 2685, Train Loss: 0.6254, Test Loss: 5.2450\n",
      "Epoch: 2686, Train Loss: 0.7024, Test Loss: 4.8499\n",
      "Epoch: 2687, Train Loss: 0.6484, Test Loss: 3.9908\n",
      "Epoch: 2688, Train Loss: 0.7238, Test Loss: 4.2391\n",
      "Epoch: 2689, Train Loss: 0.6435, Test Loss: 5.2602\n",
      "Epoch: 2690, Train Loss: 0.6905, Test Loss: 4.8896\n",
      "Epoch: 2691, Train Loss: 0.6347, Test Loss: 4.0993\n",
      "Epoch: 2692, Train Loss: 0.6673, Test Loss: 4.3565\n",
      "Epoch: 2693, Train Loss: 0.6282, Test Loss: 4.8547\n",
      "Epoch: 2694, Train Loss: 0.6575, Test Loss: 5.0549\n",
      "Epoch: 2695, Train Loss: 0.6457, Test Loss: 4.5645\n",
      "Epoch: 2696, Train Loss: 0.7171, Test Loss: 4.1464\n",
      "Epoch: 2697, Train Loss: 0.6294, Test Loss: 4.6060\n",
      "Epoch: 2698, Train Loss: 0.6862, Test Loss: 4.9596\n",
      "Epoch: 2699, Train Loss: 0.6514, Test Loss: 5.2385\n",
      "Epoch: 2700, Train Loss: 0.6599, Test Loss: 4.8585\n",
      "Epoch: 2701, Train Loss: 0.6488, Test Loss: 4.9639\n",
      "Epoch: 2702, Train Loss: 0.6189, Test Loss: 5.3364\n",
      "Epoch: 2703, Train Loss: 0.6822, Test Loss: 5.0049\n",
      "Epoch: 2704, Train Loss: 0.6677, Test Loss: 4.1270\n",
      "Epoch: 2705, Train Loss: 0.6942, Test Loss: 4.0986\n",
      "Epoch: 2706, Train Loss: 0.6770, Test Loss: 4.9385\n",
      "Epoch: 2707, Train Loss: 0.6285, Test Loss: 5.8245\n",
      "Epoch: 2708, Train Loss: 0.7327, Test Loss: 5.1497\n",
      "Epoch: 2709, Train Loss: 0.7195, Test Loss: 4.0678\n",
      "Epoch: 2710, Train Loss: 0.6832, Test Loss: 4.3781\n",
      "Epoch: 2711, Train Loss: 0.6154, Test Loss: 5.1490\n",
      "Epoch: 2712, Train Loss: 0.6713, Test Loss: 5.0535\n",
      "Epoch: 2713, Train Loss: 0.6404, Test Loss: 4.6441\n",
      "Epoch: 2714, Train Loss: 0.6187, Test Loss: 4.4130\n",
      "Epoch: 2715, Train Loss: 0.6704, Test Loss: 4.8037\n",
      "Epoch: 2716, Train Loss: 0.6303, Test Loss: 5.1728\n",
      "Epoch: 2717, Train Loss: 0.6014, Test Loss: 4.5501\n",
      "Epoch: 2718, Train Loss: 0.6333, Test Loss: 4.6710\n",
      "Epoch: 2719, Train Loss: 0.6362, Test Loss: 4.8439\n",
      "Epoch: 2720, Train Loss: 0.6961, Test Loss: 4.3922\n",
      "Epoch: 2721, Train Loss: 0.6277, Test Loss: 4.0638\n",
      "Epoch: 2722, Train Loss: 0.7136, Test Loss: 4.8691\n",
      "Epoch: 2723, Train Loss: 0.6525, Test Loss: 5.6934\n",
      "Epoch: 2724, Train Loss: 0.6780, Test Loss: 5.2940\n",
      "Epoch: 2725, Train Loss: 0.6283, Test Loss: 4.1908\n",
      "Epoch: 2726, Train Loss: 0.6923, Test Loss: 4.5347\n",
      "Epoch: 2727, Train Loss: 0.6732, Test Loss: 5.4458\n",
      "Epoch: 2728, Train Loss: 0.6855, Test Loss: 5.1645\n",
      "Epoch: 2729, Train Loss: 0.6698, Test Loss: 4.7120\n",
      "Epoch: 2730, Train Loss: 0.6111, Test Loss: 4.4149\n",
      "Epoch: 2731, Train Loss: 0.6131, Test Loss: 4.6133\n",
      "Epoch: 2732, Train Loss: 0.6324, Test Loss: 5.5318\n",
      "Epoch: 2733, Train Loss: 0.6610, Test Loss: 5.1071\n",
      "Epoch: 2734, Train Loss: 0.6430, Test Loss: 4.5741\n",
      "Epoch: 2735, Train Loss: 0.6873, Test Loss: 4.8837\n",
      "Epoch: 2736, Train Loss: 0.6170, Test Loss: 5.0758\n",
      "Epoch: 2737, Train Loss: 0.6711, Test Loss: 4.8627\n",
      "Epoch: 2738, Train Loss: 0.6322, Test Loss: 4.2892\n",
      "Epoch: 2739, Train Loss: 0.6666, Test Loss: 4.8513\n",
      "Epoch: 2740, Train Loss: 0.6750, Test Loss: 5.0156\n",
      "Epoch: 2741, Train Loss: 0.7254, Test Loss: 4.5241\n",
      "Epoch: 2742, Train Loss: 0.5913, Test Loss: 4.2169\n",
      "Epoch: 2743, Train Loss: 0.6305, Test Loss: 4.6614\n",
      "Epoch: 2744, Train Loss: 0.6134, Test Loss: 5.6588\n",
      "Epoch: 2745, Train Loss: 0.7141, Test Loss: 4.9397\n",
      "Epoch: 2746, Train Loss: 0.6036, Test Loss: 3.9713\n",
      "Epoch: 2747, Train Loss: 0.7539, Test Loss: 4.3652\n",
      "Epoch: 2748, Train Loss: 0.6778, Test Loss: 5.9437\n",
      "Epoch: 2749, Train Loss: 0.7492, Test Loss: 5.1131\n",
      "Epoch: 2750, Train Loss: 0.6473, Test Loss: 4.2385\n",
      "Epoch: 2751, Train Loss: 0.6302, Test Loss: 4.3300\n",
      "Epoch: 2752, Train Loss: 0.6380, Test Loss: 4.9922\n",
      "Epoch: 2753, Train Loss: 0.6919, Test Loss: 5.1380\n",
      "Epoch: 2754, Train Loss: 0.5933, Test Loss: 4.6190\n",
      "Epoch: 2755, Train Loss: 0.5647, Test Loss: 4.4932\n",
      "Epoch: 2756, Train Loss: 0.6670, Test Loss: 5.0588\n",
      "Epoch: 2757, Train Loss: 0.6202, Test Loss: 5.4910\n",
      "Epoch: 2758, Train Loss: 0.7914, Test Loss: 4.0789\n",
      "Epoch: 2759, Train Loss: 0.7111, Test Loss: 3.9672\n",
      "Epoch: 2760, Train Loss: 0.6473, Test Loss: 4.7296\n",
      "Epoch: 2761, Train Loss: 0.6799, Test Loss: 5.5091\n",
      "Epoch: 2762, Train Loss: 0.7184, Test Loss: 4.6622\n",
      "Epoch: 2763, Train Loss: 0.5853, Test Loss: 3.9737\n",
      "Epoch: 2764, Train Loss: 0.7304, Test Loss: 5.0084\n",
      "Epoch: 2765, Train Loss: 0.6036, Test Loss: 6.0468\n",
      "Epoch: 2766, Train Loss: 0.8294, Test Loss: 4.6472\n",
      "Epoch: 2767, Train Loss: 0.6176, Test Loss: 3.5131\n",
      "Epoch: 2768, Train Loss: 0.8539, Test Loss: 4.2707\n",
      "Epoch: 2769, Train Loss: 0.6356, Test Loss: 5.9670\n",
      "Epoch: 2770, Train Loss: 0.8007, Test Loss: 5.0193\n",
      "Epoch: 2771, Train Loss: 0.7091, Test Loss: 3.4817\n",
      "Epoch: 2772, Train Loss: 0.8377, Test Loss: 3.4613\n",
      "Epoch: 2773, Train Loss: 0.8868, Test Loss: 5.2768\n",
      "Epoch: 2774, Train Loss: 0.7146, Test Loss: 5.8425\n",
      "Epoch: 2775, Train Loss: 0.7795, Test Loss: 4.2828\n",
      "Epoch: 2776, Train Loss: 0.6734, Test Loss: 3.7094\n",
      "Epoch: 2777, Train Loss: 0.8074, Test Loss: 4.4134\n",
      "Epoch: 2778, Train Loss: 0.6273, Test Loss: 5.3121\n",
      "Epoch: 2779, Train Loss: 0.6435, Test Loss: 5.4172\n",
      "Epoch: 2780, Train Loss: 0.7071, Test Loss: 4.1522\n",
      "Epoch: 2781, Train Loss: 0.6492, Test Loss: 4.0687\n",
      "Epoch: 2782, Train Loss: 0.7396, Test Loss: 4.9685\n",
      "Epoch: 2783, Train Loss: 0.6741, Test Loss: 5.6460\n",
      "Epoch: 2784, Train Loss: 0.7933, Test Loss: 4.4285\n",
      "Epoch: 2785, Train Loss: 0.6060, Test Loss: 3.6953\n",
      "Epoch: 2786, Train Loss: 0.7565, Test Loss: 4.2134\n",
      "Epoch: 2787, Train Loss: 0.6039, Test Loss: 5.2228\n",
      "Epoch: 2788, Train Loss: 0.7262, Test Loss: 4.3985\n",
      "Epoch: 2789, Train Loss: 0.6561, Test Loss: 3.7167\n",
      "Epoch: 2790, Train Loss: 0.6930, Test Loss: 4.0651\n",
      "Epoch: 2791, Train Loss: 0.6237, Test Loss: 4.8101\n",
      "Epoch: 2792, Train Loss: 0.6349, Test Loss: 5.0032\n",
      "Epoch: 2793, Train Loss: 0.6552, Test Loss: 4.4078\n",
      "Epoch: 2794, Train Loss: 0.6650, Test Loss: 4.0846\n",
      "Epoch: 2795, Train Loss: 0.6085, Test Loss: 4.4678\n",
      "Epoch: 2796, Train Loss: 0.6203, Test Loss: 5.3727\n",
      "Epoch: 2797, Train Loss: 0.6885, Test Loss: 4.7499\n",
      "Epoch: 2798, Train Loss: 0.6346, Test Loss: 4.4807\n",
      "Epoch: 2799, Train Loss: 0.6428, Test Loss: 4.7578\n",
      "Epoch: 2800, Train Loss: 0.5712, Test Loss: 5.0841\n",
      "Epoch: 2801, Train Loss: 0.6631, Test Loss: 4.5959\n",
      "Epoch: 2802, Train Loss: 0.7028, Test Loss: 3.9378\n",
      "Epoch: 2803, Train Loss: 0.7078, Test Loss: 4.2597\n",
      "Epoch: 2804, Train Loss: 0.6792, Test Loss: 4.9986\n",
      "Epoch: 2805, Train Loss: 0.7302, Test Loss: 5.8561\n",
      "Epoch: 2806, Train Loss: 0.7805, Test Loss: 4.8230\n",
      "Epoch: 2807, Train Loss: 0.6397, Test Loss: 4.1253\n",
      "Epoch: 2808, Train Loss: 0.6243, Test Loss: 4.2052\n",
      "Epoch: 2809, Train Loss: 0.7028, Test Loss: 5.0754\n",
      "Epoch: 2810, Train Loss: 0.7661, Test Loss: 4.7292\n",
      "Epoch: 2811, Train Loss: 0.6385, Test Loss: 3.8931\n",
      "Epoch: 2812, Train Loss: 0.7162, Test Loss: 4.3362\n",
      "Epoch: 2813, Train Loss: 0.6551, Test Loss: 5.1849\n",
      "Epoch: 2814, Train Loss: 0.5945, Test Loss: 4.8180\n",
      "Epoch: 2815, Train Loss: 0.6784, Test Loss: 3.7054\n",
      "Epoch: 2816, Train Loss: 0.7080, Test Loss: 3.8529\n",
      "Epoch: 2817, Train Loss: 0.6652, Test Loss: 4.7369\n",
      "Epoch: 2818, Train Loss: 0.6511, Test Loss: 5.3543\n",
      "Epoch: 2819, Train Loss: 0.6294, Test Loss: 4.5099\n",
      "Epoch: 2820, Train Loss: 0.5723, Test Loss: 4.0234\n",
      "Epoch: 2821, Train Loss: 0.6436, Test Loss: 3.9586\n",
      "Epoch: 2822, Train Loss: 0.6441, Test Loss: 4.8302\n",
      "Epoch: 2823, Train Loss: 0.5959, Test Loss: 5.7084\n",
      "Epoch: 2824, Train Loss: 0.6832, Test Loss: 4.8093\n",
      "Epoch: 2825, Train Loss: 0.5924, Test Loss: 4.0307\n",
      "Epoch: 2826, Train Loss: 0.6551, Test Loss: 4.0286\n",
      "Epoch: 2827, Train Loss: 0.6165, Test Loss: 4.7633\n",
      "Epoch: 2828, Train Loss: 0.6235, Test Loss: 4.7594\n",
      "Epoch: 2829, Train Loss: 0.6382, Test Loss: 4.6091\n",
      "Epoch: 2830, Train Loss: 0.6230, Test Loss: 4.0896\n",
      "Epoch: 2831, Train Loss: 0.6480, Test Loss: 4.4790\n",
      "Epoch: 2832, Train Loss: 0.5798, Test Loss: 4.9674\n",
      "Epoch: 2833, Train Loss: 0.6455, Test Loss: 4.7261\n",
      "Epoch: 2834, Train Loss: 0.6446, Test Loss: 4.5057\n",
      "Epoch: 2835, Train Loss: 0.5778, Test Loss: 4.3419\n",
      "Epoch: 2836, Train Loss: 0.6324, Test Loss: 4.1807\n",
      "Epoch: 2837, Train Loss: 0.6054, Test Loss: 4.5057\n",
      "Epoch: 2838, Train Loss: 0.6708, Test Loss: 5.1093\n",
      "Epoch: 2839, Train Loss: 0.6096, Test Loss: 4.8172\n",
      "Epoch: 2840, Train Loss: 0.6362, Test Loss: 4.2496\n",
      "Epoch: 2841, Train Loss: 0.6176, Test Loss: 4.2062\n",
      "Epoch: 2842, Train Loss: 0.6552, Test Loss: 5.1529\n",
      "Epoch: 2843, Train Loss: 0.6512, Test Loss: 5.2580\n",
      "Epoch: 2844, Train Loss: 0.6472, Test Loss: 4.0623\n",
      "Epoch: 2845, Train Loss: 0.6359, Test Loss: 4.1222\n",
      "Epoch: 2846, Train Loss: 0.6878, Test Loss: 5.1801\n",
      "Epoch: 2847, Train Loss: 0.6465, Test Loss: 5.4409\n",
      "Epoch: 2848, Train Loss: 0.7312, Test Loss: 3.9415\n",
      "Epoch: 2849, Train Loss: 0.6180, Test Loss: 3.3872\n",
      "Epoch: 2850, Train Loss: 0.8011, Test Loss: 4.3830\n",
      "Epoch: 2851, Train Loss: 0.6647, Test Loss: 5.4765\n",
      "Epoch: 2852, Train Loss: 0.6841, Test Loss: 4.9936\n",
      "Epoch: 2853, Train Loss: 0.6482, Test Loss: 3.8829\n",
      "Epoch: 2854, Train Loss: 0.6355, Test Loss: 3.6407\n",
      "Epoch: 2855, Train Loss: 0.7908, Test Loss: 5.0336\n",
      "Epoch: 2856, Train Loss: 0.6580, Test Loss: 5.3615\n",
      "Epoch: 2857, Train Loss: 0.6974, Test Loss: 4.1295\n",
      "Epoch: 2858, Train Loss: 0.6343, Test Loss: 3.8443\n",
      "Epoch: 2859, Train Loss: 0.6558, Test Loss: 4.3826\n",
      "Epoch: 2860, Train Loss: 0.6282, Test Loss: 5.0094\n",
      "Epoch: 2861, Train Loss: 0.6555, Test Loss: 4.6902\n",
      "Epoch: 2862, Train Loss: 0.6574, Test Loss: 3.8784\n",
      "Epoch: 2863, Train Loss: 0.7187, Test Loss: 3.8314\n",
      "Epoch: 2864, Train Loss: 0.6774, Test Loss: 4.8841\n",
      "Epoch: 2865, Train Loss: 0.6245, Test Loss: 5.2720\n",
      "Epoch: 2866, Train Loss: 0.6334, Test Loss: 4.6377\n",
      "Epoch: 2867, Train Loss: 0.5948, Test Loss: 3.8913\n",
      "Epoch: 2868, Train Loss: 0.6882, Test Loss: 4.8053\n",
      "Epoch: 2869, Train Loss: 0.6431, Test Loss: 5.4414\n",
      "Epoch: 2870, Train Loss: 0.7587, Test Loss: 4.0249\n",
      "Epoch: 2871, Train Loss: 0.7053, Test Loss: 3.5717\n",
      "Epoch: 2872, Train Loss: 0.7464, Test Loss: 4.5278\n",
      "Epoch: 2873, Train Loss: 0.6114, Test Loss: 6.1600\n",
      "Epoch: 2874, Train Loss: 0.8325, Test Loss: 5.0441\n",
      "Epoch: 2875, Train Loss: 0.6435, Test Loss: 3.7022\n",
      "Epoch: 2876, Train Loss: 0.6863, Test Loss: 3.6995\n",
      "Epoch: 2877, Train Loss: 0.6867, Test Loss: 4.6726\n",
      "Epoch: 2878, Train Loss: 0.6331, Test Loss: 5.5362\n",
      "Epoch: 2879, Train Loss: 0.7522, Test Loss: 4.2712\n",
      "Epoch: 2880, Train Loss: 0.6793, Test Loss: 3.4287\n",
      "Epoch: 2881, Train Loss: 0.8907, Test Loss: 4.0940\n",
      "Epoch: 2882, Train Loss: 0.6382, Test Loss: 5.6188\n",
      "Epoch: 2883, Train Loss: 0.7423, Test Loss: 5.3125\n",
      "Epoch: 2884, Train Loss: 0.7094, Test Loss: 3.7684\n",
      "Epoch: 2885, Train Loss: 0.7115, Test Loss: 3.5972\n",
      "Epoch: 2886, Train Loss: 0.7485, Test Loss: 4.8197\n",
      "Epoch: 2887, Train Loss: 0.6537, Test Loss: 5.7942\n",
      "Epoch: 2888, Train Loss: 0.7046, Test Loss: 4.8226\n",
      "Epoch: 2889, Train Loss: 0.6618, Test Loss: 3.7219\n",
      "Epoch: 2890, Train Loss: 0.7453, Test Loss: 4.0484\n",
      "Epoch: 2891, Train Loss: 0.6497, Test Loss: 5.0467\n",
      "Epoch: 2892, Train Loss: 0.6559, Test Loss: 4.8629\n",
      "Epoch: 2893, Train Loss: 0.6408, Test Loss: 3.9680\n",
      "Epoch: 2894, Train Loss: 0.6705, Test Loss: 3.7257\n",
      "Epoch: 2895, Train Loss: 0.6758, Test Loss: 4.3743\n",
      "Epoch: 2896, Train Loss: 0.5793, Test Loss: 4.8126\n",
      "Epoch: 2897, Train Loss: 0.6715, Test Loss: 4.6989\n",
      "Epoch: 2898, Train Loss: 0.5990, Test Loss: 4.0773\n",
      "Epoch: 2899, Train Loss: 0.6287, Test Loss: 3.5467\n",
      "Epoch: 2900, Train Loss: 0.7274, Test Loss: 4.1985\n",
      "Epoch: 2901, Train Loss: 0.5692, Test Loss: 4.8906\n",
      "Epoch: 2902, Train Loss: 0.6992, Test Loss: 4.3104\n",
      "Epoch: 2903, Train Loss: 0.7304, Test Loss: 4.2683\n",
      "Epoch: 2904, Train Loss: 0.6341, Test Loss: 4.2015\n",
      "Epoch: 2905, Train Loss: 0.6037, Test Loss: 4.8623\n",
      "Epoch: 2906, Train Loss: 0.6613, Test Loss: 4.7954\n",
      "Epoch: 2907, Train Loss: 0.6908, Test Loss: 3.9065\n",
      "Epoch: 2908, Train Loss: 0.6306, Test Loss: 3.7095\n",
      "Epoch: 2909, Train Loss: 0.7667, Test Loss: 5.3232\n",
      "Epoch: 2910, Train Loss: 0.7592, Test Loss: 4.7541\n",
      "Epoch: 2911, Train Loss: 0.6778, Test Loss: 3.8793\n",
      "Epoch: 2912, Train Loss: 0.6302, Test Loss: 3.5538\n",
      "Epoch: 2913, Train Loss: 0.7216, Test Loss: 4.4229\n",
      "Epoch: 2914, Train Loss: 0.6533, Test Loss: 5.0014\n",
      "Epoch: 2915, Train Loss: 0.7909, Test Loss: 4.0996\n",
      "Epoch: 2916, Train Loss: 0.6022, Test Loss: 3.7216\n",
      "Epoch: 2917, Train Loss: 0.7086, Test Loss: 4.7466\n",
      "Epoch: 2918, Train Loss: 0.6097, Test Loss: 5.9440\n",
      "Epoch: 2919, Train Loss: 0.8099, Test Loss: 4.4554\n",
      "Epoch: 2920, Train Loss: 0.5993, Test Loss: 3.7789\n",
      "Epoch: 2921, Train Loss: 0.7635, Test Loss: 4.4303\n",
      "Epoch: 2922, Train Loss: 0.6530, Test Loss: 4.4903\n",
      "Epoch: 2923, Train Loss: 0.6158, Test Loss: 4.7326\n",
      "Epoch: 2924, Train Loss: 0.6816, Test Loss: 4.4036\n",
      "Epoch: 2925, Train Loss: 0.6197, Test Loss: 4.1986\n",
      "Epoch: 2926, Train Loss: 0.5897, Test Loss: 4.3841\n",
      "Epoch: 2927, Train Loss: 0.5723, Test Loss: 4.3948\n",
      "Epoch: 2928, Train Loss: 0.6503, Test Loss: 4.4286\n",
      "Epoch: 2929, Train Loss: 0.5966, Test Loss: 4.5204\n",
      "Epoch: 2930, Train Loss: 0.6088, Test Loss: 4.7225\n",
      "Epoch: 2931, Train Loss: 0.5995, Test Loss: 4.8183\n",
      "Epoch: 2932, Train Loss: 0.6174, Test Loss: 4.3838\n",
      "Epoch: 2933, Train Loss: 0.6627, Test Loss: 3.9187\n",
      "Epoch: 2934, Train Loss: 0.6354, Test Loss: 4.2553\n",
      "Epoch: 2935, Train Loss: 0.6448, Test Loss: 5.1360\n",
      "Epoch: 2936, Train Loss: 0.6175, Test Loss: 5.0418\n",
      "Epoch: 2937, Train Loss: 0.5980, Test Loss: 4.4759\n",
      "Epoch: 2938, Train Loss: 0.6055, Test Loss: 3.8295\n",
      "Epoch: 2939, Train Loss: 0.6603, Test Loss: 4.0155\n",
      "Epoch: 2940, Train Loss: 0.6076, Test Loss: 4.7941\n",
      "Epoch: 2941, Train Loss: 0.6437, Test Loss: 4.7658\n",
      "Epoch: 2942, Train Loss: 0.6524, Test Loss: 4.1299\n",
      "Epoch: 2943, Train Loss: 0.6736, Test Loss: 4.5530\n",
      "Epoch: 2944, Train Loss: 0.5996, Test Loss: 5.1160\n",
      "Epoch: 2945, Train Loss: 0.6593, Test Loss: 4.7534\n",
      "Epoch: 2946, Train Loss: 0.6015, Test Loss: 3.8420\n",
      "Epoch: 2947, Train Loss: 0.6336, Test Loss: 4.0349\n",
      "Epoch: 2948, Train Loss: 0.6296, Test Loss: 5.2154\n",
      "Epoch: 2949, Train Loss: 0.6800, Test Loss: 4.7194\n",
      "Epoch: 2950, Train Loss: 0.6144, Test Loss: 4.1273\n",
      "Epoch: 2951, Train Loss: 0.7628, Test Loss: 5.1152\n",
      "Epoch: 2952, Train Loss: 0.6297, Test Loss: 5.3201\n",
      "Epoch: 2953, Train Loss: 0.7253, Test Loss: 4.2664\n",
      "Epoch: 2954, Train Loss: 0.6035, Test Loss: 3.8648\n",
      "Epoch: 2955, Train Loss: 0.7004, Test Loss: 4.6882\n",
      "Epoch: 2956, Train Loss: 0.6288, Test Loss: 5.4066\n",
      "Epoch: 2957, Train Loss: 0.7521, Test Loss: 4.1886\n",
      "Epoch: 2958, Train Loss: 0.6223, Test Loss: 3.9413\n",
      "Epoch: 2959, Train Loss: 0.6323, Test Loss: 4.2434\n",
      "Epoch: 2960, Train Loss: 0.5919, Test Loss: 4.8367\n",
      "Epoch: 2961, Train Loss: 0.6305, Test Loss: 4.6247\n",
      "Epoch: 2962, Train Loss: 0.6588, Test Loss: 3.8125\n",
      "Epoch: 2963, Train Loss: 0.6567, Test Loss: 4.1280\n",
      "Epoch: 2964, Train Loss: 0.6391, Test Loss: 4.2623\n",
      "Epoch: 2965, Train Loss: 0.6170, Test Loss: 4.3799\n",
      "Epoch: 2966, Train Loss: 0.6115, Test Loss: 4.7183\n",
      "Epoch: 2967, Train Loss: 0.5830, Test Loss: 4.7851\n",
      "Epoch: 2968, Train Loss: 0.6075, Test Loss: 4.5772\n",
      "Epoch: 2969, Train Loss: 0.5916, Test Loss: 4.2325\n",
      "Epoch: 2970, Train Loss: 0.6151, Test Loss: 4.5908\n",
      "Epoch: 2971, Train Loss: 0.6724, Test Loss: 4.3463\n",
      "Epoch: 2972, Train Loss: 0.5879, Test Loss: 4.8439\n",
      "Epoch: 2973, Train Loss: 0.5586, Test Loss: 4.6601\n",
      "Epoch: 2974, Train Loss: 0.6088, Test Loss: 4.6492\n",
      "Epoch: 2975, Train Loss: 0.6110, Test Loss: 5.0390\n",
      "Epoch: 2976, Train Loss: 0.6738, Test Loss: 4.1672\n",
      "Epoch: 2977, Train Loss: 0.6779, Test Loss: 4.7230\n",
      "Epoch: 2978, Train Loss: 0.6325, Test Loss: 5.0056\n",
      "Epoch: 2979, Train Loss: 0.7073, Test Loss: 4.3774\n",
      "Epoch: 2980, Train Loss: 0.5917, Test Loss: 4.1200\n",
      "Epoch: 2981, Train Loss: 0.6318, Test Loss: 4.2747\n",
      "Epoch: 2982, Train Loss: 0.6511, Test Loss: 5.2604\n",
      "Epoch: 2983, Train Loss: 0.6792, Test Loss: 5.1269\n",
      "Epoch: 2984, Train Loss: 0.6183, Test Loss: 4.3719\n",
      "Epoch: 2985, Train Loss: 0.6124, Test Loss: 4.1589\n",
      "Epoch: 2986, Train Loss: 0.6519, Test Loss: 4.4927\n",
      "Epoch: 2987, Train Loss: 0.5894, Test Loss: 5.1122\n",
      "Epoch: 2988, Train Loss: 0.6328, Test Loss: 5.0685\n",
      "Epoch: 2989, Train Loss: 0.6087, Test Loss: 4.3158\n",
      "Epoch: 2990, Train Loss: 0.5959, Test Loss: 4.1927\n",
      "Epoch: 2991, Train Loss: 0.5685, Test Loss: 4.4067\n",
      "Epoch: 2992, Train Loss: 0.6363, Test Loss: 4.7019\n",
      "Epoch: 2993, Train Loss: 0.5751, Test Loss: 4.6696\n",
      "Epoch: 2994, Train Loss: 0.6149, Test Loss: 4.6786\n",
      "Epoch: 2995, Train Loss: 0.6717, Test Loss: 4.3133\n",
      "Epoch: 2996, Train Loss: 0.6053, Test Loss: 4.4962\n",
      "Epoch: 2997, Train Loss: 0.6131, Test Loss: 5.1210\n",
      "Epoch: 2998, Train Loss: 0.6085, Test Loss: 4.9611\n",
      "Epoch: 2999, Train Loss: 0.5880, Test Loss: 4.3651\n",
      "Epoch: 3000, Train Loss: 0.6121, Test Loss: 4.5515\n",
      "Epoch: 3001, Train Loss: 0.6269, Test Loss: 4.9237\n",
      "Epoch: 3002, Train Loss: 0.6560, Test Loss: 4.6310\n",
      "Epoch: 3003, Train Loss: 0.6003, Test Loss: 4.8526\n",
      "Epoch: 3004, Train Loss: 0.6226, Test Loss: 5.1305\n",
      "Epoch: 3005, Train Loss: 0.6525, Test Loss: 4.3674\n",
      "Epoch: 3006, Train Loss: 0.5947, Test Loss: 4.4064\n",
      "Epoch: 3007, Train Loss: 0.5936, Test Loss: 4.8693\n",
      "Epoch: 3008, Train Loss: 0.6026, Test Loss: 5.3096\n",
      "Epoch: 3009, Train Loss: 0.6814, Test Loss: 4.4250\n",
      "Epoch: 3010, Train Loss: 0.5762, Test Loss: 3.9310\n",
      "Epoch: 3011, Train Loss: 0.6433, Test Loss: 4.6028\n",
      "Epoch: 3012, Train Loss: 0.6167, Test Loss: 5.4187\n",
      "Epoch: 3013, Train Loss: 0.6699, Test Loss: 4.6533\n",
      "Epoch: 3014, Train Loss: 0.5919, Test Loss: 4.1185\n",
      "Epoch: 3015, Train Loss: 0.5673, Test Loss: 4.3679\n",
      "Epoch: 3016, Train Loss: 0.5740, Test Loss: 5.0368\n",
      "Epoch: 3017, Train Loss: 0.5969, Test Loss: 5.6065\n",
      "Epoch: 3018, Train Loss: 0.6876, Test Loss: 4.4836\n",
      "Epoch: 3019, Train Loss: 0.6734, Test Loss: 3.6185\n",
      "Epoch: 3020, Train Loss: 0.7680, Test Loss: 4.2593\n",
      "Epoch: 3021, Train Loss: 0.5981, Test Loss: 5.6249\n",
      "Epoch: 3022, Train Loss: 0.8123, Test Loss: 4.8830\n",
      "Epoch: 3023, Train Loss: 0.6201, Test Loss: 3.5434\n",
      "Epoch: 3024, Train Loss: 0.7301, Test Loss: 3.6944\n",
      "Epoch: 3025, Train Loss: 0.7131, Test Loss: 5.5313\n",
      "Epoch: 3026, Train Loss: 0.6523, Test Loss: 5.7337\n",
      "Epoch: 3027, Train Loss: 0.6832, Test Loss: 4.1289\n",
      "Epoch: 3028, Train Loss: 0.6182, Test Loss: 3.5272\n",
      "Epoch: 3029, Train Loss: 0.8019, Test Loss: 4.2933\n",
      "Epoch: 3030, Train Loss: 0.5818, Test Loss: 5.1458\n",
      "Epoch: 3031, Train Loss: 0.6350, Test Loss: 5.0119\n",
      "Epoch: 3032, Train Loss: 0.6115, Test Loss: 4.0067\n",
      "Epoch: 3033, Train Loss: 0.6881, Test Loss: 4.1479\n",
      "Epoch: 3034, Train Loss: 0.7005, Test Loss: 4.8756\n",
      "Epoch: 3035, Train Loss: 0.6538, Test Loss: 4.8962\n",
      "Epoch: 3036, Train Loss: 0.6234, Test Loss: 4.3099\n",
      "Epoch: 3037, Train Loss: 0.5615, Test Loss: 4.3799\n",
      "Epoch: 3038, Train Loss: 0.5721, Test Loss: 4.4769\n",
      "Epoch: 3039, Train Loss: 0.5811, Test Loss: 4.3945\n",
      "Epoch: 3040, Train Loss: 0.6306, Test Loss: 4.6966\n",
      "Epoch: 3041, Train Loss: 0.5910, Test Loss: 5.4918\n",
      "Epoch: 3042, Train Loss: 0.7312, Test Loss: 4.4213\n",
      "Epoch: 3043, Train Loss: 0.6056, Test Loss: 3.9343\n",
      "Epoch: 3044, Train Loss: 0.6546, Test Loss: 4.7142\n",
      "Epoch: 3045, Train Loss: 0.6063, Test Loss: 4.9463\n",
      "Epoch: 3046, Train Loss: 0.6416, Test Loss: 4.5128\n",
      "Epoch: 3047, Train Loss: 0.5736, Test Loss: 4.1171\n",
      "Epoch: 3048, Train Loss: 0.6487, Test Loss: 5.0526\n",
      "Epoch: 3049, Train Loss: 0.5949, Test Loss: 4.6480\n",
      "Epoch: 3050, Train Loss: 0.5608, Test Loss: 4.0642\n",
      "Epoch: 3051, Train Loss: 0.6688, Test Loss: 4.4198\n",
      "Epoch: 3052, Train Loss: 0.6267, Test Loss: 5.8015\n",
      "Epoch: 3053, Train Loss: 0.6988, Test Loss: 4.9722\n",
      "Epoch: 3054, Train Loss: 0.6758, Test Loss: 4.0365\n",
      "Epoch: 3055, Train Loss: 0.6539, Test Loss: 4.2465\n",
      "Epoch: 3056, Train Loss: 0.5858, Test Loss: 4.6186\n",
      "Epoch: 3057, Train Loss: 0.5947, Test Loss: 4.9530\n",
      "Epoch: 3058, Train Loss: 0.5993, Test Loss: 5.1135\n",
      "Epoch: 3059, Train Loss: 0.5800, Test Loss: 4.6752\n",
      "Epoch: 3060, Train Loss: 0.6162, Test Loss: 4.1218\n",
      "Epoch: 3061, Train Loss: 0.6595, Test Loss: 4.4013\n",
      "Epoch: 3062, Train Loss: 0.5876, Test Loss: 4.8825\n",
      "Epoch: 3063, Train Loss: 0.5847, Test Loss: 4.9110\n",
      "Epoch: 3064, Train Loss: 0.6040, Test Loss: 4.7447\n",
      "Epoch: 3065, Train Loss: 0.5959, Test Loss: 4.1911\n",
      "Epoch: 3066, Train Loss: 0.6467, Test Loss: 4.5486\n",
      "Epoch: 3067, Train Loss: 0.5766, Test Loss: 4.5646\n",
      "Epoch: 3068, Train Loss: 0.6026, Test Loss: 4.8820\n",
      "Epoch: 3069, Train Loss: 0.5928, Test Loss: 4.7779\n",
      "Epoch: 3070, Train Loss: 0.6383, Test Loss: 4.3701\n",
      "Epoch: 3071, Train Loss: 0.6313, Test Loss: 4.2048\n",
      "Epoch: 3072, Train Loss: 0.5913, Test Loss: 4.6250\n",
      "Epoch: 3073, Train Loss: 0.5611, Test Loss: 5.1652\n",
      "Epoch: 3074, Train Loss: 0.6327, Test Loss: 4.1911\n",
      "Epoch: 3075, Train Loss: 0.5680, Test Loss: 3.7894\n",
      "Epoch: 3076, Train Loss: 0.7617, Test Loss: 5.5906\n",
      "Epoch: 3077, Train Loss: 0.6910, Test Loss: 5.4486\n",
      "Epoch: 3078, Train Loss: 0.7352, Test Loss: 3.7249\n",
      "Epoch: 3079, Train Loss: 0.6981, Test Loss: 3.4056\n",
      "Epoch: 3080, Train Loss: 0.8060, Test Loss: 4.4176\n",
      "Epoch: 3081, Train Loss: 0.6083, Test Loss: 5.5193\n",
      "Epoch: 3082, Train Loss: 0.7111, Test Loss: 4.7731\n",
      "Epoch: 3083, Train Loss: 0.6670, Test Loss: 3.8167\n",
      "Epoch: 3084, Train Loss: 0.7845, Test Loss: 4.7182\n",
      "Epoch: 3085, Train Loss: 0.5670, Test Loss: 5.3367\n",
      "Epoch: 3086, Train Loss: 0.7256, Test Loss: 4.5633\n",
      "Epoch: 3087, Train Loss: 0.5835, Test Loss: 4.3481\n",
      "Epoch: 3088, Train Loss: 0.6038, Test Loss: 4.3563\n",
      "Epoch: 3089, Train Loss: 0.6240, Test Loss: 4.6387\n",
      "Epoch: 3090, Train Loss: 0.6639, Test Loss: 5.1847\n",
      "Epoch: 3091, Train Loss: 0.6581, Test Loss: 4.3720\n",
      "Epoch: 3092, Train Loss: 0.6124, Test Loss: 4.1621\n",
      "Epoch: 3093, Train Loss: 0.6186, Test Loss: 4.9747\n",
      "Epoch: 3094, Train Loss: 0.6080, Test Loss: 5.3682\n",
      "Epoch: 3095, Train Loss: 0.6363, Test Loss: 4.2670\n",
      "Epoch: 3096, Train Loss: 0.6140, Test Loss: 4.2889\n",
      "Epoch: 3097, Train Loss: 0.5644, Test Loss: 4.6913\n",
      "Epoch: 3098, Train Loss: 0.6195, Test Loss: 5.4525\n",
      "Epoch: 3099, Train Loss: 0.6242, Test Loss: 5.0848\n",
      "Epoch: 3100, Train Loss: 0.6003, Test Loss: 4.3251\n",
      "Epoch: 3101, Train Loss: 0.6473, Test Loss: 4.0922\n",
      "Epoch: 3102, Train Loss: 0.6639, Test Loss: 5.1853\n",
      "Epoch: 3103, Train Loss: 0.6932, Test Loss: 5.0417\n",
      "Epoch: 3104, Train Loss: 0.5936, Test Loss: 4.2811\n",
      "Epoch: 3105, Train Loss: 0.6193, Test Loss: 4.0820\n",
      "Epoch: 3106, Train Loss: 0.6487, Test Loss: 5.1191\n",
      "Epoch: 3107, Train Loss: 0.6228, Test Loss: 5.4532\n",
      "Epoch: 3108, Train Loss: 0.6952, Test Loss: 4.1729\n",
      "Epoch: 3109, Train Loss: 0.6261, Test Loss: 3.7338\n",
      "Epoch: 3110, Train Loss: 0.7096, Test Loss: 4.4424\n",
      "Epoch: 3111, Train Loss: 0.6108, Test Loss: 5.6708\n",
      "Epoch: 3112, Train Loss: 0.6409, Test Loss: 5.1210\n",
      "Epoch: 3113, Train Loss: 0.6521, Test Loss: 3.6382\n",
      "Epoch: 3114, Train Loss: 0.6986, Test Loss: 3.5527\n",
      "Epoch: 3115, Train Loss: 0.9687, Test Loss: 6.2078\n",
      "Epoch: 3116, Train Loss: 0.8878, Test Loss: 5.8642\n",
      "Epoch: 3117, Train Loss: 0.7725, Test Loss: 3.8464\n",
      "Epoch: 3118, Train Loss: 0.6854, Test Loss: 3.2929\n",
      "Epoch: 3119, Train Loss: 0.8523, Test Loss: 4.1067\n",
      "Epoch: 3120, Train Loss: 0.5582, Test Loss: 5.5005\n",
      "Epoch: 3121, Train Loss: 0.7340, Test Loss: 5.2023\n",
      "Epoch: 3122, Train Loss: 0.6732, Test Loss: 4.1464\n",
      "Epoch: 3123, Train Loss: 0.6251, Test Loss: 3.7753\n",
      "Epoch: 3124, Train Loss: 0.6915, Test Loss: 4.8302\n",
      "Epoch: 3125, Train Loss: 0.6336, Test Loss: 5.2391\n",
      "Epoch: 3126, Train Loss: 0.6653, Test Loss: 4.3576\n",
      "Epoch: 3127, Train Loss: 0.5878, Test Loss: 3.9495\n",
      "Epoch: 3128, Train Loss: 0.6768, Test Loss: 4.6021\n",
      "Epoch: 3129, Train Loss: 0.6183, Test Loss: 5.1550\n",
      "Epoch: 3130, Train Loss: 0.6848, Test Loss: 4.3681\n",
      "Epoch: 3131, Train Loss: 0.6183, Test Loss: 3.9570\n",
      "Epoch: 3132, Train Loss: 0.6282, Test Loss: 4.4357\n",
      "Epoch: 3133, Train Loss: 0.5597, Test Loss: 5.0002\n",
      "Epoch: 3134, Train Loss: 0.6454, Test Loss: 4.9452\n",
      "Epoch: 3135, Train Loss: 0.6693, Test Loss: 4.3384\n",
      "Epoch: 3136, Train Loss: 0.5559, Test Loss: 4.1527\n",
      "Epoch: 3137, Train Loss: 0.5970, Test Loss: 4.3463\n",
      "Epoch: 3138, Train Loss: 0.5910, Test Loss: 4.8096\n",
      "Epoch: 3139, Train Loss: 0.5923, Test Loss: 4.6151\n",
      "Epoch: 3140, Train Loss: 0.5913, Test Loss: 4.3832\n",
      "Epoch: 3141, Train Loss: 0.5593, Test Loss: 4.2144\n",
      "Epoch: 3142, Train Loss: 0.6116, Test Loss: 4.4814\n",
      "Epoch: 3143, Train Loss: 0.5591, Test Loss: 4.7867\n",
      "Epoch: 3144, Train Loss: 0.5893, Test Loss: 4.4722\n",
      "Epoch: 3145, Train Loss: 0.5303, Test Loss: 4.2636\n",
      "Epoch: 3146, Train Loss: 0.6233, Test Loss: 4.8436\n",
      "Epoch: 3147, Train Loss: 0.5989, Test Loss: 4.4433\n",
      "Epoch: 3148, Train Loss: 0.5914, Test Loss: 4.0930\n",
      "Epoch: 3149, Train Loss: 0.6130, Test Loss: 4.2957\n",
      "Epoch: 3150, Train Loss: 0.5905, Test Loss: 4.8191\n",
      "Epoch: 3151, Train Loss: 0.6320, Test Loss: 4.2169\n",
      "Epoch: 3152, Train Loss: 0.6373, Test Loss: 4.0213\n",
      "Epoch: 3153, Train Loss: 0.6352, Test Loss: 5.0062\n",
      "Epoch: 3154, Train Loss: 0.6729, Test Loss: 4.8727\n",
      "Epoch: 3155, Train Loss: 0.6858, Test Loss: 4.1851\n",
      "Epoch: 3156, Train Loss: 0.5604, Test Loss: 4.0307\n",
      "Epoch: 3157, Train Loss: 0.5800, Test Loss: 4.5001\n",
      "Epoch: 3158, Train Loss: 0.5621, Test Loss: 4.4547\n",
      "Epoch: 3159, Train Loss: 0.6209, Test Loss: 4.5604\n",
      "Epoch: 3160, Train Loss: 0.6233, Test Loss: 4.3663\n",
      "Epoch: 3161, Train Loss: 0.5914, Test Loss: 4.1325\n",
      "Epoch: 3162, Train Loss: 0.5561, Test Loss: 4.5095\n",
      "Epoch: 3163, Train Loss: 0.6052, Test Loss: 4.9637\n",
      "Epoch: 3164, Train Loss: 0.5916, Test Loss: 4.8079\n",
      "Epoch: 3165, Train Loss: 0.5792, Test Loss: 4.4033\n",
      "Epoch: 3166, Train Loss: 0.6450, Test Loss: 4.5314\n",
      "Epoch: 3167, Train Loss: 0.5570, Test Loss: 4.5566\n",
      "Epoch: 3168, Train Loss: 0.5464, Test Loss: 4.4000\n",
      "Epoch: 3169, Train Loss: 0.5438, Test Loss: 4.3135\n",
      "Epoch: 3170, Train Loss: 0.5848, Test Loss: 4.8648\n",
      "Epoch: 3171, Train Loss: 0.5841, Test Loss: 4.9824\n",
      "Epoch: 3172, Train Loss: 0.5999, Test Loss: 4.4939\n",
      "Epoch: 3173, Train Loss: 0.5991, Test Loss: 4.1720\n",
      "Epoch: 3174, Train Loss: 0.5709, Test Loss: 4.5853\n",
      "Epoch: 3175, Train Loss: 0.5444, Test Loss: 4.9377\n",
      "Epoch: 3176, Train Loss: 0.6267, Test Loss: 5.3484\n",
      "Epoch: 3177, Train Loss: 0.6472, Test Loss: 4.4553\n",
      "Epoch: 3178, Train Loss: 0.5623, Test Loss: 3.8126\n",
      "Epoch: 3179, Train Loss: 0.6730, Test Loss: 4.5842\n",
      "Epoch: 3180, Train Loss: 0.6335, Test Loss: 5.2372\n",
      "Epoch: 3181, Train Loss: 0.5846, Test Loss: 4.8315\n",
      "Epoch: 3182, Train Loss: 0.5659, Test Loss: 4.3935\n",
      "Epoch: 3183, Train Loss: 0.5831, Test Loss: 4.2376\n",
      "Epoch: 3184, Train Loss: 0.6055, Test Loss: 5.4160\n",
      "Epoch: 3185, Train Loss: 0.6187, Test Loss: 5.0714\n",
      "Epoch: 3186, Train Loss: 0.6352, Test Loss: 3.9887\n",
      "Epoch: 3187, Train Loss: 0.5997, Test Loss: 4.1461\n",
      "Epoch: 3188, Train Loss: 0.7056, Test Loss: 5.6973\n",
      "Epoch: 3189, Train Loss: 0.7520, Test Loss: 5.0590\n",
      "Epoch: 3190, Train Loss: 0.5832, Test Loss: 3.9245\n",
      "Epoch: 3191, Train Loss: 0.6536, Test Loss: 3.8218\n",
      "Epoch: 3192, Train Loss: 0.7235, Test Loss: 5.1005\n",
      "Epoch: 3193, Train Loss: 0.5629, Test Loss: 5.7229\n",
      "Epoch: 3194, Train Loss: 0.6339, Test Loss: 4.6249\n",
      "Epoch: 3195, Train Loss: 0.5356, Test Loss: 3.7308\n",
      "Epoch: 3196, Train Loss: 0.6331, Test Loss: 3.9213\n",
      "Epoch: 3197, Train Loss: 0.6259, Test Loss: 5.2349\n",
      "Epoch: 3198, Train Loss: 0.6138, Test Loss: 5.5200\n",
      "Epoch: 3199, Train Loss: 0.6792, Test Loss: 4.1492\n",
      "Epoch: 3200, Train Loss: 0.5902, Test Loss: 3.7908\n",
      "Epoch: 3201, Train Loss: 0.6689, Test Loss: 4.3719\n",
      "Epoch: 3202, Train Loss: 0.5861, Test Loss: 4.9018\n",
      "Epoch: 3203, Train Loss: 0.6260, Test Loss: 4.3657\n",
      "Epoch: 3204, Train Loss: 0.5958, Test Loss: 4.0995\n",
      "Epoch: 3205, Train Loss: 0.6815, Test Loss: 4.6545\n",
      "Epoch: 3206, Train Loss: 0.6056, Test Loss: 4.9752\n",
      "Epoch: 3207, Train Loss: 0.5903, Test Loss: 4.8998\n",
      "Epoch: 3208, Train Loss: 0.5626, Test Loss: 4.3716\n",
      "Epoch: 3209, Train Loss: 0.6175, Test Loss: 4.1226\n",
      "Epoch: 3210, Train Loss: 0.6072, Test Loss: 3.9777\n",
      "Epoch: 3211, Train Loss: 0.6428, Test Loss: 4.8674\n",
      "Epoch: 3212, Train Loss: 0.5865, Test Loss: 5.2930\n",
      "Epoch: 3213, Train Loss: 0.6153, Test Loss: 4.1512\n",
      "Epoch: 3214, Train Loss: 0.5785, Test Loss: 3.5210\n",
      "Epoch: 3215, Train Loss: 0.6637, Test Loss: 4.1037\n",
      "Epoch: 3216, Train Loss: 0.5849, Test Loss: 5.2982\n",
      "Epoch: 3217, Train Loss: 0.7454, Test Loss: 4.4125\n",
      "Epoch: 3218, Train Loss: 0.6220, Test Loss: 3.5471\n",
      "Epoch: 3219, Train Loss: 0.6838, Test Loss: 4.1226\n",
      "Epoch: 3220, Train Loss: 0.5768, Test Loss: 5.4068\n",
      "Epoch: 3221, Train Loss: 0.7531, Test Loss: 4.4773\n",
      "Epoch: 3222, Train Loss: 0.5509, Test Loss: 3.5135\n",
      "Epoch: 3223, Train Loss: 0.7318, Test Loss: 3.9889\n",
      "Epoch: 3224, Train Loss: 0.6127, Test Loss: 5.5028\n",
      "Epoch: 3225, Train Loss: 0.6907, Test Loss: 5.2079\n",
      "Epoch: 3226, Train Loss: 0.6474, Test Loss: 3.7241\n",
      "Epoch: 3227, Train Loss: 0.6935, Test Loss: 3.8274\n",
      "Epoch: 3228, Train Loss: 0.6127, Test Loss: 4.5992\n",
      "Epoch: 3229, Train Loss: 0.6438, Test Loss: 4.7241\n",
      "Epoch: 3230, Train Loss: 0.5792, Test Loss: 4.2032\n",
      "Epoch: 3231, Train Loss: 0.5634, Test Loss: 3.9829\n",
      "Epoch: 3232, Train Loss: 0.6199, Test Loss: 4.1133\n",
      "Epoch: 3233, Train Loss: 0.5500, Test Loss: 4.8275\n",
      "Epoch: 3234, Train Loss: 0.5699, Test Loss: 4.8066\n",
      "Epoch: 3235, Train Loss: 0.5657, Test Loss: 4.3906\n",
      "Epoch: 3236, Train Loss: 0.5833, Test Loss: 3.9864\n",
      "Epoch: 3237, Train Loss: 0.6337, Test Loss: 4.8800\n",
      "Epoch: 3238, Train Loss: 0.6042, Test Loss: 4.7757\n",
      "Epoch: 3239, Train Loss: 0.5898, Test Loss: 4.1375\n",
      "Epoch: 3240, Train Loss: 0.5977, Test Loss: 4.0305\n",
      "Epoch: 3241, Train Loss: 0.5767, Test Loss: 4.3843\n",
      "Epoch: 3242, Train Loss: 0.5522, Test Loss: 4.5611\n",
      "Epoch: 3243, Train Loss: 0.5886, Test Loss: 4.8445\n",
      "Epoch: 3244, Train Loss: 0.5974, Test Loss: 4.2676\n",
      "Epoch: 3245, Train Loss: 0.5863, Test Loss: 4.2539\n",
      "Epoch: 3246, Train Loss: 0.6018, Test Loss: 4.4460\n",
      "Epoch: 3247, Train Loss: 0.5785, Test Loss: 4.6036\n",
      "Epoch: 3248, Train Loss: 0.6078, Test Loss: 4.2260\n",
      "Epoch: 3249, Train Loss: 0.5895, Test Loss: 4.2041\n",
      "Epoch: 3250, Train Loss: 0.5779, Test Loss: 4.8773\n",
      "Epoch: 3251, Train Loss: 0.5281, Test Loss: 4.9813\n",
      "Epoch: 3252, Train Loss: 0.6269, Test Loss: 3.9497\n",
      "Epoch: 3253, Train Loss: 0.6131, Test Loss: 3.8136\n",
      "Epoch: 3254, Train Loss: 0.6013, Test Loss: 4.4237\n",
      "Epoch: 3255, Train Loss: 0.5714, Test Loss: 4.8346\n",
      "Epoch: 3256, Train Loss: 0.5993, Test Loss: 4.8066\n",
      "Epoch: 3257, Train Loss: 0.5856, Test Loss: 4.2653\n",
      "Epoch: 3258, Train Loss: 0.5383, Test Loss: 4.4209\n",
      "Epoch: 3259, Train Loss: 0.5752, Test Loss: 4.5732\n",
      "Epoch: 3260, Train Loss: 0.6078, Test Loss: 4.7795\n",
      "Epoch: 3261, Train Loss: 0.5894, Test Loss: 4.7769\n",
      "Epoch: 3262, Train Loss: 0.5571, Test Loss: 4.8544\n",
      "Epoch: 3263, Train Loss: 0.5399, Test Loss: 4.5284\n",
      "Epoch: 3264, Train Loss: 0.5903, Test Loss: 4.4292\n",
      "Epoch: 3265, Train Loss: 0.5635, Test Loss: 4.2432\n",
      "Epoch: 3266, Train Loss: 0.5628, Test Loss: 4.7186\n",
      "Epoch: 3267, Train Loss: 0.5711, Test Loss: 4.8645\n",
      "Epoch: 3268, Train Loss: 0.5471, Test Loss: 4.5154\n",
      "Epoch: 3269, Train Loss: 0.5486, Test Loss: 4.3894\n",
      "Epoch: 3270, Train Loss: 0.5416, Test Loss: 4.3098\n",
      "Epoch: 3271, Train Loss: 0.5590, Test Loss: 4.0577\n",
      "Epoch: 3272, Train Loss: 0.5573, Test Loss: 4.4599\n",
      "Epoch: 3273, Train Loss: 0.5490, Test Loss: 4.6430\n",
      "Epoch: 3274, Train Loss: 0.5614, Test Loss: 4.7785\n",
      "Epoch: 3275, Train Loss: 0.5246, Test Loss: 4.5604\n",
      "Epoch: 3276, Train Loss: 0.5512, Test Loss: 4.1682\n",
      "Epoch: 3277, Train Loss: 0.5801, Test Loss: 4.5072\n",
      "Epoch: 3278, Train Loss: 0.5748, Test Loss: 4.7438\n",
      "Epoch: 3279, Train Loss: 0.5670, Test Loss: 4.7873\n",
      "Epoch: 3280, Train Loss: 0.5807, Test Loss: 4.2390\n",
      "Epoch: 3281, Train Loss: 0.6100, Test Loss: 4.7887\n",
      "Epoch: 3282, Train Loss: 0.7000, Test Loss: 4.1550\n",
      "Epoch: 3283, Train Loss: 0.6135, Test Loss: 4.4556\n",
      "Epoch: 3284, Train Loss: 0.5767, Test Loss: 5.0392\n",
      "Epoch: 3285, Train Loss: 0.5733, Test Loss: 5.1731\n",
      "Epoch: 3286, Train Loss: 0.6625, Test Loss: 3.9087\n",
      "Epoch: 3287, Train Loss: 0.6712, Test Loss: 4.0315\n",
      "Epoch: 3288, Train Loss: 0.5719, Test Loss: 4.8792\n",
      "Epoch: 3289, Train Loss: 0.6273, Test Loss: 5.0818\n",
      "Epoch: 3290, Train Loss: 0.6273, Test Loss: 4.5408\n",
      "Epoch: 3291, Train Loss: 0.5298, Test Loss: 4.3205\n",
      "Epoch: 3292, Train Loss: 0.6059, Test Loss: 4.3155\n",
      "Epoch: 3293, Train Loss: 0.5272, Test Loss: 4.2166\n",
      "Epoch: 3294, Train Loss: 0.5732, Test Loss: 4.5845\n",
      "Epoch: 3295, Train Loss: 0.5442, Test Loss: 4.7412\n",
      "Epoch: 3296, Train Loss: 0.5451, Test Loss: 4.5239\n",
      "Epoch: 3297, Train Loss: 0.5983, Test Loss: 4.5537\n",
      "Epoch: 3298, Train Loss: 0.5721, Test Loss: 4.3876\n",
      "Epoch: 3299, Train Loss: 0.5802, Test Loss: 4.7577\n",
      "Epoch: 3300, Train Loss: 0.6963, Test Loss: 4.9967\n",
      "Epoch: 3301, Train Loss: 0.6056, Test Loss: 4.2599\n",
      "Epoch: 3302, Train Loss: 0.5710, Test Loss: 3.9566\n",
      "Epoch: 3303, Train Loss: 0.6394, Test Loss: 4.4609\n",
      "Epoch: 3304, Train Loss: 0.6238, Test Loss: 4.6588\n",
      "Epoch: 3305, Train Loss: 0.6410, Test Loss: 3.9954\n",
      "Epoch: 3306, Train Loss: 0.5860, Test Loss: 3.9912\n",
      "Epoch: 3307, Train Loss: 0.6318, Test Loss: 4.6301\n",
      "Epoch: 3308, Train Loss: 0.6294, Test Loss: 4.9785\n",
      "Epoch: 3309, Train Loss: 0.6648, Test Loss: 4.2105\n",
      "Epoch: 3310, Train Loss: 0.6308, Test Loss: 4.0479\n",
      "Epoch: 3311, Train Loss: 0.6029, Test Loss: 4.2037\n",
      "Epoch: 3312, Train Loss: 0.5844, Test Loss: 4.6346\n",
      "Epoch: 3313, Train Loss: 0.5889, Test Loss: 4.8603\n",
      "Epoch: 3314, Train Loss: 0.6310, Test Loss: 4.7760\n",
      "Epoch: 3315, Train Loss: 0.5209, Test Loss: 4.3717\n",
      "Epoch: 3316, Train Loss: 0.5648, Test Loss: 4.0585\n",
      "Epoch: 3317, Train Loss: 0.6164, Test Loss: 4.3995\n",
      "Epoch: 3318, Train Loss: 0.6350, Test Loss: 4.9769\n",
      "Epoch: 3319, Train Loss: 0.6061, Test Loss: 4.5558\n",
      "Epoch: 3320, Train Loss: 0.6003, Test Loss: 3.8911\n",
      "Epoch: 3321, Train Loss: 0.6007, Test Loss: 4.0828\n",
      "Epoch: 3322, Train Loss: 0.5816, Test Loss: 5.0738\n",
      "Epoch: 3323, Train Loss: 0.5561, Test Loss: 5.2523\n",
      "Epoch: 3324, Train Loss: 0.5817, Test Loss: 4.3620\n",
      "Epoch: 3325, Train Loss: 0.5580, Test Loss: 3.9979\n",
      "Epoch: 3326, Train Loss: 0.6182, Test Loss: 4.9213\n",
      "Epoch: 3327, Train Loss: 0.6699, Test Loss: 4.8517\n",
      "Epoch: 3328, Train Loss: 0.6289, Test Loss: 3.9180\n",
      "Epoch: 3329, Train Loss: 0.6332, Test Loss: 4.4605\n",
      "Epoch: 3330, Train Loss: 0.5651, Test Loss: 4.8815\n",
      "Epoch: 3331, Train Loss: 0.5941, Test Loss: 4.3038\n",
      "Epoch: 3332, Train Loss: 0.5642, Test Loss: 4.2969\n",
      "Epoch: 3333, Train Loss: 0.5832, Test Loss: 4.6055\n",
      "Epoch: 3334, Train Loss: 0.5788, Test Loss: 4.2799\n",
      "Epoch: 3335, Train Loss: 0.5969, Test Loss: 3.8882\n",
      "Epoch: 3336, Train Loss: 0.6441, Test Loss: 4.7604\n",
      "Epoch: 3337, Train Loss: 0.5724, Test Loss: 5.1890\n",
      "Epoch: 3338, Train Loss: 0.6275, Test Loss: 4.2310\n",
      "Epoch: 3339, Train Loss: 0.5890, Test Loss: 4.1616\n",
      "Epoch: 3340, Train Loss: 0.5717, Test Loss: 5.1538\n",
      "Epoch: 3341, Train Loss: 0.6506, Test Loss: 4.5429\n",
      "Epoch: 3342, Train Loss: 0.5770, Test Loss: 3.9333\n",
      "Epoch: 3343, Train Loss: 0.6144, Test Loss: 4.3126\n",
      "Epoch: 3344, Train Loss: 0.5301, Test Loss: 4.6043\n",
      "Epoch: 3345, Train Loss: 0.5458, Test Loss: 4.6280\n",
      "Epoch: 3346, Train Loss: 0.5830, Test Loss: 4.0540\n",
      "Epoch: 3347, Train Loss: 0.5796, Test Loss: 4.2737\n",
      "Epoch: 3348, Train Loss: 0.5726, Test Loss: 4.8201\n",
      "Epoch: 3349, Train Loss: 0.5907, Test Loss: 5.2342\n",
      "Epoch: 3350, Train Loss: 0.6353, Test Loss: 4.3433\n",
      "Epoch: 3351, Train Loss: 0.5934, Test Loss: 3.7957\n",
      "Epoch: 3352, Train Loss: 0.6466, Test Loss: 4.3353\n",
      "Epoch: 3353, Train Loss: 0.5411, Test Loss: 5.2273\n",
      "Epoch: 3354, Train Loss: 0.6522, Test Loss: 4.2744\n",
      "Epoch: 3355, Train Loss: 0.6118, Test Loss: 3.9730\n",
      "Epoch: 3356, Train Loss: 0.5187, Test Loss: 3.7777\n",
      "Epoch: 3357, Train Loss: 0.6462, Test Loss: 4.1683\n",
      "Epoch: 3358, Train Loss: 0.5437, Test Loss: 4.1299\n",
      "Epoch: 3359, Train Loss: 0.6153, Test Loss: 5.0470\n",
      "Epoch: 3360, Train Loss: 0.6514, Test Loss: 4.3779\n",
      "Epoch: 3361, Train Loss: 0.5701, Test Loss: 4.1338\n",
      "Epoch: 3362, Train Loss: 0.5565, Test Loss: 4.0651\n",
      "Epoch: 3363, Train Loss: 0.6192, Test Loss: 3.9887\n",
      "Epoch: 3364, Train Loss: 0.5538, Test Loss: 4.5729\n",
      "Epoch: 3365, Train Loss: 0.5797, Test Loss: 4.2705\n",
      "Epoch: 3366, Train Loss: 0.5944, Test Loss: 3.6379\n",
      "Epoch: 3367, Train Loss: 0.5968, Test Loss: 3.7698\n",
      "Epoch: 3368, Train Loss: 0.5846, Test Loss: 4.5222\n",
      "Epoch: 3369, Train Loss: 0.5548, Test Loss: 5.0902\n",
      "Epoch: 3370, Train Loss: 0.6268, Test Loss: 4.0134\n",
      "Epoch: 3371, Train Loss: 0.5934, Test Loss: 3.9868\n",
      "Epoch: 3372, Train Loss: 0.5718, Test Loss: 4.2238\n",
      "Epoch: 3373, Train Loss: 0.5401, Test Loss: 4.8792\n",
      "Epoch: 3374, Train Loss: 0.6354, Test Loss: 4.1204\n",
      "Epoch: 3375, Train Loss: 0.5820, Test Loss: 3.5527\n",
      "Epoch: 3376, Train Loss: 0.7063, Test Loss: 4.0874\n",
      "Epoch: 3377, Train Loss: 0.5546, Test Loss: 5.0793\n",
      "Epoch: 3378, Train Loss: 0.5950, Test Loss: 5.0234\n",
      "Epoch: 3379, Train Loss: 0.6691, Test Loss: 3.5641\n",
      "Epoch: 3380, Train Loss: 0.6351, Test Loss: 3.5108\n",
      "Epoch: 3381, Train Loss: 0.6718, Test Loss: 5.0905\n",
      "Epoch: 3382, Train Loss: 0.6014, Test Loss: 5.6023\n",
      "Epoch: 3383, Train Loss: 0.6948, Test Loss: 4.1917\n",
      "Epoch: 3384, Train Loss: 0.5406, Test Loss: 3.3232\n",
      "Epoch: 3385, Train Loss: 0.8666, Test Loss: 4.3667\n",
      "Epoch: 3386, Train Loss: 0.5666, Test Loss: 6.0578\n",
      "Epoch: 3387, Train Loss: 0.8860, Test Loss: 4.6053\n",
      "Epoch: 3388, Train Loss: 0.6003, Test Loss: 3.3244\n",
      "Epoch: 3389, Train Loss: 0.6961, Test Loss: 3.4248\n",
      "Epoch: 3390, Train Loss: 0.6833, Test Loss: 5.0231\n",
      "Epoch: 3391, Train Loss: 0.6121, Test Loss: 5.7594\n",
      "Epoch: 3392, Train Loss: 0.6948, Test Loss: 4.4264\n",
      "Epoch: 3393, Train Loss: 0.5696, Test Loss: 3.6047\n",
      "Epoch: 3394, Train Loss: 0.6728, Test Loss: 4.1498\n",
      "Epoch: 3395, Train Loss: 0.5868, Test Loss: 4.9580\n",
      "Epoch: 3396, Train Loss: 0.6244, Test Loss: 4.3957\n",
      "Epoch: 3397, Train Loss: 0.5882, Test Loss: 3.8402\n",
      "Epoch: 3398, Train Loss: 0.5820, Test Loss: 3.9213\n",
      "Epoch: 3399, Train Loss: 0.6029, Test Loss: 4.7768\n",
      "Epoch: 3400, Train Loss: 0.5698, Test Loss: 5.0483\n",
      "Epoch: 3401, Train Loss: 0.6544, Test Loss: 4.2114\n",
      "Epoch: 3402, Train Loss: 0.5658, Test Loss: 3.8953\n",
      "Epoch: 3403, Train Loss: 0.6001, Test Loss: 4.2731\n",
      "Epoch: 3404, Train Loss: 0.5635, Test Loss: 4.8480\n",
      "Epoch: 3405, Train Loss: 0.6271, Test Loss: 4.4824\n",
      "Epoch: 3406, Train Loss: 0.5555, Test Loss: 4.1843\n",
      "Epoch: 3407, Train Loss: 0.5592, Test Loss: 4.0178\n",
      "Epoch: 3408, Train Loss: 0.5961, Test Loss: 3.8096\n",
      "Epoch: 3409, Train Loss: 0.6348, Test Loss: 4.7065\n",
      "Epoch: 3410, Train Loss: 0.5937, Test Loss: 4.7717\n",
      "Epoch: 3411, Train Loss: 0.6206, Test Loss: 3.8787\n",
      "Epoch: 3412, Train Loss: 0.6117, Test Loss: 4.0389\n",
      "Epoch: 3413, Train Loss: 0.6106, Test Loss: 4.8499\n",
      "Epoch: 3414, Train Loss: 0.6591, Test Loss: 4.4290\n",
      "Epoch: 3415, Train Loss: 0.5765, Test Loss: 3.4314\n",
      "Epoch: 3416, Train Loss: 0.6746, Test Loss: 3.6186\n",
      "Epoch: 3417, Train Loss: 0.6340, Test Loss: 5.0231\n",
      "Epoch: 3418, Train Loss: 0.6515, Test Loss: 4.8616\n",
      "Epoch: 3419, Train Loss: 0.6210, Test Loss: 3.6343\n",
      "Epoch: 3420, Train Loss: 0.6483, Test Loss: 3.8146\n",
      "Epoch: 3421, Train Loss: 0.6645, Test Loss: 4.8758\n",
      "Epoch: 3422, Train Loss: 0.6582, Test Loss: 4.6527\n",
      "Epoch: 3423, Train Loss: 0.5942, Test Loss: 3.6475\n",
      "Epoch: 3424, Train Loss: 0.6777, Test Loss: 4.0651\n",
      "Epoch: 3425, Train Loss: 0.5525, Test Loss: 4.6865\n",
      "Epoch: 3426, Train Loss: 0.5430, Test Loss: 4.7253\n",
      "Epoch: 3427, Train Loss: 0.5631, Test Loss: 4.1042\n",
      "Epoch: 3428, Train Loss: 0.5308, Test Loss: 4.0223\n",
      "Epoch: 3429, Train Loss: 0.6039, Test Loss: 4.5588\n",
      "Epoch: 3430, Train Loss: 0.5750, Test Loss: 4.5192\n",
      "Epoch: 3431, Train Loss: 0.5703, Test Loss: 4.2600\n",
      "Epoch: 3432, Train Loss: 0.5423, Test Loss: 4.1511\n",
      "Epoch: 3433, Train Loss: 0.5625, Test Loss: 4.7328\n",
      "Epoch: 3434, Train Loss: 0.5890, Test Loss: 4.6614\n",
      "Epoch: 3435, Train Loss: 0.5947, Test Loss: 3.8059\n",
      "Epoch: 3436, Train Loss: 0.5738, Test Loss: 3.7346\n",
      "Epoch: 3437, Train Loss: 0.5860, Test Loss: 4.4461\n",
      "Epoch: 3438, Train Loss: 0.5595, Test Loss: 4.8979\n",
      "Epoch: 3439, Train Loss: 0.6852, Test Loss: 3.8333\n",
      "Epoch: 3440, Train Loss: 0.6001, Test Loss: 3.7648\n",
      "Epoch: 3441, Train Loss: 0.6007, Test Loss: 4.3271\n",
      "Epoch: 3442, Train Loss: 0.6127, Test Loss: 4.6048\n",
      "Epoch: 3443, Train Loss: 0.5213, Test Loss: 4.7111\n",
      "Epoch: 3444, Train Loss: 0.5241, Test Loss: 4.5289\n",
      "Epoch: 3445, Train Loss: 0.5388, Test Loss: 4.3216\n",
      "Epoch: 3446, Train Loss: 0.5821, Test Loss: 4.0518\n",
      "Epoch: 3447, Train Loss: 0.5767, Test Loss: 4.0807\n",
      "Epoch: 3448, Train Loss: 0.5456, Test Loss: 4.7927\n",
      "Epoch: 3449, Train Loss: 0.5646, Test Loss: 4.9494\n",
      "Epoch: 3450, Train Loss: 0.6579, Test Loss: 3.7101\n",
      "Epoch: 3451, Train Loss: 0.6407, Test Loss: 3.9694\n",
      "Epoch: 3452, Train Loss: 0.5515, Test Loss: 4.5014\n",
      "Epoch: 3453, Train Loss: 0.5835, Test Loss: 4.4431\n",
      "Epoch: 3454, Train Loss: 0.6239, Test Loss: 4.5015\n",
      "Epoch: 3455, Train Loss: 0.5716, Test Loss: 4.0516\n",
      "Epoch: 3456, Train Loss: 0.5453, Test Loss: 3.6563\n",
      "Epoch: 3457, Train Loss: 0.5654, Test Loss: 4.0834\n",
      "Epoch: 3458, Train Loss: 0.5291, Test Loss: 5.0718\n",
      "Epoch: 3459, Train Loss: 0.6316, Test Loss: 4.3643\n",
      "Epoch: 3460, Train Loss: 0.5450, Test Loss: 3.8636\n",
      "Epoch: 3461, Train Loss: 0.5638, Test Loss: 4.3425\n",
      "Epoch: 3462, Train Loss: 0.5739, Test Loss: 5.6430\n",
      "Epoch: 3463, Train Loss: 0.7026, Test Loss: 4.5257\n",
      "Epoch: 3464, Train Loss: 0.6084, Test Loss: 3.3445\n",
      "Epoch: 3465, Train Loss: 0.7683, Test Loss: 3.9981\n",
      "Epoch: 3466, Train Loss: 0.6268, Test Loss: 5.9987\n",
      "Epoch: 3467, Train Loss: 0.8166, Test Loss: 4.7463\n",
      "Epoch: 3468, Train Loss: 0.5982, Test Loss: 3.4695\n",
      "Epoch: 3469, Train Loss: 0.6460, Test Loss: 3.6988\n",
      "Epoch: 3470, Train Loss: 0.5676, Test Loss: 4.6602\n",
      "Epoch: 3471, Train Loss: 0.5573, Test Loss: 5.1544\n",
      "Epoch: 3472, Train Loss: 0.6149, Test Loss: 4.2557\n",
      "Epoch: 3473, Train Loss: 0.5385, Test Loss: 3.6494\n",
      "Epoch: 3474, Train Loss: 0.6619, Test Loss: 4.1106\n",
      "Epoch: 3475, Train Loss: 0.5495, Test Loss: 4.8674\n",
      "Epoch: 3476, Train Loss: 0.6338, Test Loss: 4.8608\n",
      "Epoch: 3477, Train Loss: 0.5816, Test Loss: 3.9206\n",
      "Epoch: 3478, Train Loss: 0.5559, Test Loss: 3.5568\n",
      "Epoch: 3479, Train Loss: 0.6936, Test Loss: 4.5537\n",
      "Epoch: 3480, Train Loss: 0.6399, Test Loss: 4.9757\n",
      "Epoch: 3481, Train Loss: 0.6178, Test Loss: 4.2249\n",
      "Epoch: 3482, Train Loss: 0.5598, Test Loss: 3.8016\n",
      "Epoch: 3483, Train Loss: 0.6039, Test Loss: 4.4046\n",
      "Epoch: 3484, Train Loss: 0.5790, Test Loss: 4.5667\n",
      "Epoch: 3485, Train Loss: 0.5439, Test Loss: 4.8532\n",
      "Epoch: 3486, Train Loss: 0.6033, Test Loss: 3.9181\n",
      "Epoch: 3487, Train Loss: 0.5994, Test Loss: 3.9808\n",
      "Epoch: 3488, Train Loss: 0.5541, Test Loss: 4.2615\n",
      "Epoch: 3489, Train Loss: 0.5460, Test Loss: 4.1655\n",
      "Epoch: 3490, Train Loss: 0.6318, Test Loss: 4.5446\n",
      "Epoch: 3491, Train Loss: 0.5381, Test Loss: 4.8527\n",
      "Epoch: 3492, Train Loss: 0.5928, Test Loss: 4.2246\n",
      "Epoch: 3493, Train Loss: 0.5930, Test Loss: 3.8471\n",
      "Epoch: 3494, Train Loss: 0.5736, Test Loss: 4.2843\n",
      "Epoch: 3495, Train Loss: 0.6299, Test Loss: 5.1000\n",
      "Epoch: 3496, Train Loss: 0.6101, Test Loss: 4.4247\n",
      "Epoch: 3497, Train Loss: 0.5904, Test Loss: 4.4726\n",
      "Epoch: 3498, Train Loss: 0.5300, Test Loss: 4.2374\n",
      "Epoch: 3499, Train Loss: 0.5372, Test Loss: 4.5802\n",
      "Epoch: 3500, Train Loss: 0.5483, Test Loss: 4.5376\n",
      "Epoch: 3501, Train Loss: 0.5842, Test Loss: 4.0899\n",
      "Epoch: 3502, Train Loss: 0.5635, Test Loss: 4.1617\n",
      "Epoch: 3503, Train Loss: 0.5446, Test Loss: 4.1803\n",
      "Epoch: 3504, Train Loss: 0.5622, Test Loss: 4.4694\n",
      "Epoch: 3505, Train Loss: 0.5586, Test Loss: 4.6819\n",
      "Epoch: 3506, Train Loss: 0.5523, Test Loss: 4.3238\n",
      "Epoch: 3507, Train Loss: 0.5233, Test Loss: 4.0385\n",
      "Epoch: 3508, Train Loss: 0.5838, Test Loss: 4.6330\n",
      "Epoch: 3509, Train Loss: 0.5664, Test Loss: 4.7934\n",
      "Epoch: 3510, Train Loss: 0.5398, Test Loss: 4.5904\n",
      "Epoch: 3511, Train Loss: 0.5867, Test Loss: 3.8866\n",
      "Epoch: 3512, Train Loss: 0.6289, Test Loss: 4.4477\n",
      "Epoch: 3513, Train Loss: 0.5711, Test Loss: 4.6621\n",
      "Epoch: 3514, Train Loss: 0.5129, Test Loss: 4.5831\n",
      "Epoch: 3515, Train Loss: 0.5364, Test Loss: 4.3024\n",
      "Epoch: 3516, Train Loss: 0.5183, Test Loss: 4.1429\n",
      "Epoch: 3517, Train Loss: 0.5744, Test Loss: 3.8447\n",
      "Epoch: 3518, Train Loss: 0.5515, Test Loss: 3.9261\n",
      "Epoch: 3519, Train Loss: 0.5502, Test Loss: 4.6022\n",
      "Epoch: 3520, Train Loss: 0.5421, Test Loss: 4.7015\n",
      "Epoch: 3521, Train Loss: 0.5743, Test Loss: 3.9621\n",
      "Epoch: 3522, Train Loss: 0.6366, Test Loss: 4.4288\n",
      "Epoch: 3523, Train Loss: 0.5096, Test Loss: 4.8749\n",
      "Epoch: 3524, Train Loss: 0.7030, Test Loss: 3.8685\n",
      "Epoch: 3525, Train Loss: 0.5967, Test Loss: 3.3892\n",
      "Epoch: 3526, Train Loss: 0.7134, Test Loss: 4.8504\n",
      "Epoch: 3527, Train Loss: 0.5635, Test Loss: 6.1099\n",
      "Epoch: 3528, Train Loss: 0.8300, Test Loss: 3.9138\n",
      "Epoch: 3529, Train Loss: 0.5122, Test Loss: 3.0842\n",
      "Epoch: 3530, Train Loss: 0.9907, Test Loss: 4.2012\n",
      "Epoch: 3531, Train Loss: 0.5301, Test Loss: 6.2467\n",
      "Epoch: 3532, Train Loss: 0.9408, Test Loss: 4.8256\n",
      "Epoch: 3533, Train Loss: 0.6120, Test Loss: 3.2895\n",
      "Epoch: 3534, Train Loss: 0.8703, Test Loss: 3.6468\n",
      "Epoch: 3535, Train Loss: 0.5873, Test Loss: 4.8660\n",
      "Epoch: 3536, Train Loss: 0.6462, Test Loss: 5.0218\n",
      "Epoch: 3537, Train Loss: 0.5963, Test Loss: 4.4782\n",
      "Epoch: 3538, Train Loss: 0.6275, Test Loss: 4.3132\n",
      "Epoch: 3539, Train Loss: 0.5429, Test Loss: 4.1631\n",
      "Epoch: 3540, Train Loss: 0.6103, Test Loss: 4.6615\n",
      "Epoch: 3541, Train Loss: 0.5476, Test Loss: 4.7289\n",
      "Epoch: 3542, Train Loss: 0.5796, Test Loss: 4.2784\n",
      "Epoch: 3543, Train Loss: 0.5679, Test Loss: 4.1626\n",
      "Epoch: 3544, Train Loss: 0.5900, Test Loss: 4.0050\n",
      "Epoch: 3545, Train Loss: 0.6129, Test Loss: 4.4025\n",
      "Epoch: 3546, Train Loss: 0.5644, Test Loss: 4.2189\n",
      "Epoch: 3547, Train Loss: 0.5599, Test Loss: 4.2505\n",
      "Epoch: 3548, Train Loss: 0.5396, Test Loss: 4.5806\n",
      "Epoch: 3549, Train Loss: 0.5750, Test Loss: 4.4493\n",
      "Epoch: 3550, Train Loss: 0.5637, Test Loss: 4.3438\n",
      "Epoch: 3551, Train Loss: 0.5377, Test Loss: 4.1823\n",
      "Epoch: 3552, Train Loss: 0.5430, Test Loss: 4.3317\n",
      "Epoch: 3553, Train Loss: 0.5737, Test Loss: 4.5592\n",
      "Epoch: 3554, Train Loss: 0.6323, Test Loss: 4.4501\n",
      "Epoch: 3555, Train Loss: 0.5739, Test Loss: 3.8905\n",
      "Epoch: 3556, Train Loss: 0.5280, Test Loss: 3.9714\n",
      "Epoch: 3557, Train Loss: 0.5965, Test Loss: 4.5476\n",
      "Epoch: 3558, Train Loss: 0.6397, Test Loss: 4.4620\n",
      "Epoch: 3559, Train Loss: 0.5482, Test Loss: 3.5959\n",
      "Epoch: 3560, Train Loss: 0.5689, Test Loss: 3.5743\n",
      "Epoch: 3561, Train Loss: 0.6804, Test Loss: 5.0877\n",
      "Epoch: 3562, Train Loss: 0.6784, Test Loss: 5.0745\n",
      "Epoch: 3563, Train Loss: 0.6139, Test Loss: 3.7799\n",
      "Epoch: 3564, Train Loss: 0.5610, Test Loss: 3.4314\n",
      "Epoch: 3565, Train Loss: 0.7686, Test Loss: 4.5249\n",
      "Epoch: 3566, Train Loss: 0.5863, Test Loss: 5.5494\n",
      "Epoch: 3567, Train Loss: 0.6807, Test Loss: 4.3363\n",
      "Epoch: 3568, Train Loss: 0.5374, Test Loss: 3.4011\n",
      "Epoch: 3569, Train Loss: 0.6482, Test Loss: 3.7499\n",
      "Epoch: 3570, Train Loss: 0.5709, Test Loss: 5.0511\n",
      "Epoch: 3571, Train Loss: 0.5887, Test Loss: 5.2116\n",
      "Epoch: 3572, Train Loss: 0.7183, Test Loss: 3.5424\n",
      "Epoch: 3573, Train Loss: 0.5654, Test Loss: 3.2204\n",
      "Epoch: 3574, Train Loss: 0.7811, Test Loss: 4.5537\n",
      "Epoch: 3575, Train Loss: 0.5731, Test Loss: 5.7631\n",
      "Epoch: 3576, Train Loss: 0.7876, Test Loss: 4.1830\n",
      "Epoch: 3577, Train Loss: 0.5812, Test Loss: 3.3786\n",
      "Epoch: 3578, Train Loss: 0.6587, Test Loss: 3.6502\n",
      "Epoch: 3579, Train Loss: 0.5852, Test Loss: 4.9362\n",
      "Epoch: 3580, Train Loss: 0.6149, Test Loss: 5.0811\n",
      "Epoch: 3581, Train Loss: 0.6265, Test Loss: 3.7230\n",
      "Epoch: 3582, Train Loss: 0.5727, Test Loss: 3.5669\n",
      "Epoch: 3583, Train Loss: 0.6374, Test Loss: 4.2382\n",
      "Epoch: 3584, Train Loss: 0.5764, Test Loss: 5.2967\n",
      "Epoch: 3585, Train Loss: 0.6623, Test Loss: 4.3530\n",
      "Epoch: 3586, Train Loss: 0.5903, Test Loss: 3.3725\n",
      "Epoch: 3587, Train Loss: 0.6543, Test Loss: 3.7118\n",
      "Epoch: 3588, Train Loss: 0.6974, Test Loss: 5.1384\n",
      "Epoch: 3589, Train Loss: 0.7635, Test Loss: 4.7320\n",
      "Epoch: 3590, Train Loss: 0.6198, Test Loss: 3.5174\n",
      "Epoch: 3591, Train Loss: 0.6308, Test Loss: 3.5451\n",
      "Epoch: 3592, Train Loss: 0.6095, Test Loss: 4.5241\n",
      "Epoch: 3593, Train Loss: 0.5861, Test Loss: 4.7807\n",
      "Epoch: 3594, Train Loss: 0.5917, Test Loss: 3.9775\n",
      "Epoch: 3595, Train Loss: 0.5393, Test Loss: 3.7178\n",
      "Epoch: 3596, Train Loss: 0.5422, Test Loss: 4.0203\n",
      "Epoch: 3597, Train Loss: 0.5470, Test Loss: 4.4895\n",
      "Epoch: 3598, Train Loss: 0.5612, Test Loss: 4.2087\n",
      "Epoch: 3599, Train Loss: 0.5849, Test Loss: 4.0144\n",
      "Epoch: 3600, Train Loss: 0.5693, Test Loss: 4.1671\n",
      "Epoch: 3601, Train Loss: 0.5709, Test Loss: 4.7587\n",
      "Epoch: 3602, Train Loss: 0.6587, Test Loss: 3.8070\n",
      "Epoch: 3603, Train Loss: 0.5910, Test Loss: 3.9119\n",
      "Epoch: 3604, Train Loss: 0.5652, Test Loss: 4.5164\n",
      "Epoch: 3605, Train Loss: 0.5209, Test Loss: 4.7128\n",
      "Epoch: 3606, Train Loss: 0.5913, Test Loss: 3.9222\n",
      "Epoch: 3607, Train Loss: 0.5702, Test Loss: 3.5051\n",
      "Epoch: 3608, Train Loss: 0.6194, Test Loss: 3.9383\n",
      "Epoch: 3609, Train Loss: 0.5391, Test Loss: 4.8252\n",
      "Epoch: 3610, Train Loss: 0.5596, Test Loss: 5.0330\n",
      "Epoch: 3611, Train Loss: 0.6577, Test Loss: 3.6940\n",
      "Epoch: 3612, Train Loss: 0.5838, Test Loss: 3.4083\n",
      "Epoch: 3613, Train Loss: 0.5793, Test Loss: 3.9933\n",
      "Epoch: 3614, Train Loss: 0.5848, Test Loss: 4.9211\n",
      "Epoch: 3615, Train Loss: 0.6799, Test Loss: 4.1473\n",
      "Epoch: 3616, Train Loss: 0.5703, Test Loss: 3.2989\n",
      "Epoch: 3617, Train Loss: 0.6469, Test Loss: 3.6412\n",
      "Epoch: 3618, Train Loss: 0.6076, Test Loss: 5.1804\n",
      "Epoch: 3619, Train Loss: 0.6393, Test Loss: 4.8558\n",
      "Epoch: 3620, Train Loss: 0.6103, Test Loss: 3.5376\n",
      "Epoch: 3621, Train Loss: 0.6381, Test Loss: 3.4277\n",
      "Epoch: 3622, Train Loss: 0.7019, Test Loss: 4.5704\n",
      "Epoch: 3623, Train Loss: 0.6136, Test Loss: 4.8973\n",
      "Epoch: 3624, Train Loss: 0.5534, Test Loss: 4.2378\n",
      "Epoch: 3625, Train Loss: 0.5140, Test Loss: 3.6677\n",
      "Epoch: 3626, Train Loss: 0.5575, Test Loss: 3.7630\n",
      "Epoch: 3627, Train Loss: 0.6028, Test Loss: 4.7745\n",
      "Epoch: 3628, Train Loss: 0.5521, Test Loss: 4.9531\n",
      "Epoch: 3629, Train Loss: 0.5481, Test Loss: 4.2077\n",
      "Epoch: 3630, Train Loss: 0.5372, Test Loss: 3.7271\n",
      "Epoch: 3631, Train Loss: 0.5558, Test Loss: 3.8325\n",
      "Epoch: 3632, Train Loss: 0.6036, Test Loss: 4.3420\n",
      "Epoch: 3633, Train Loss: 0.5812, Test Loss: 4.5256\n",
      "Epoch: 3634, Train Loss: 0.5789, Test Loss: 4.0616\n",
      "Epoch: 3635, Train Loss: 0.5448, Test Loss: 3.5717\n",
      "Epoch: 3636, Train Loss: 0.5931, Test Loss: 4.0084\n",
      "Epoch: 3637, Train Loss: 0.5251, Test Loss: 4.9371\n",
      "Epoch: 3638, Train Loss: 0.6112, Test Loss: 4.3270\n",
      "Epoch: 3639, Train Loss: 0.5219, Test Loss: 3.8618\n",
      "Epoch: 3640, Train Loss: 0.5596, Test Loss: 4.0333\n",
      "Epoch: 3641, Train Loss: 0.5664, Test Loss: 4.2400\n",
      "Epoch: 3642, Train Loss: 0.5034, Test Loss: 4.3126\n",
      "Epoch: 3643, Train Loss: 0.5242, Test Loss: 3.9550\n",
      "Epoch: 3644, Train Loss: 0.5672, Test Loss: 4.2029\n",
      "Epoch: 3645, Train Loss: 0.5645, Test Loss: 4.5858\n",
      "Epoch: 3646, Train Loss: 0.5486, Test Loss: 3.9070\n",
      "Epoch: 3647, Train Loss: 0.5244, Test Loss: 3.6081\n",
      "Epoch: 3648, Train Loss: 0.6413, Test Loss: 4.3996\n",
      "Epoch: 3649, Train Loss: 0.5426, Test Loss: 5.0990\n",
      "Epoch: 3650, Train Loss: 0.6157, Test Loss: 4.3979\n",
      "Epoch: 3651, Train Loss: 0.5763, Test Loss: 3.4331\n",
      "Epoch: 3652, Train Loss: 0.6061, Test Loss: 3.3787\n",
      "Epoch: 3653, Train Loss: 0.6672, Test Loss: 4.3011\n",
      "Epoch: 3654, Train Loss: 0.5700, Test Loss: 5.4614\n",
      "Epoch: 3655, Train Loss: 0.7910, Test Loss: 4.0494\n",
      "Epoch: 3656, Train Loss: 0.5381, Test Loss: 3.2246\n",
      "Epoch: 3657, Train Loss: 0.7820, Test Loss: 4.1122\n",
      "Epoch: 3658, Train Loss: 0.5305, Test Loss: 5.1782\n",
      "Epoch: 3659, Train Loss: 0.5890, Test Loss: 4.8299\n",
      "Epoch: 3660, Train Loss: 0.5908, Test Loss: 3.4663\n",
      "Epoch: 3661, Train Loss: 0.5758, Test Loss: 3.2549\n",
      "Epoch: 3662, Train Loss: 0.7761, Test Loss: 4.8708\n",
      "Epoch: 3663, Train Loss: 0.6057, Test Loss: 5.4093\n",
      "Epoch: 3664, Train Loss: 0.6907, Test Loss: 3.9330\n",
      "Epoch: 3665, Train Loss: 0.5521, Test Loss: 3.1379\n",
      "Epoch: 3666, Train Loss: 0.8366, Test Loss: 3.9753\n",
      "Epoch: 3667, Train Loss: 0.5789, Test Loss: 4.9180\n",
      "Epoch: 3668, Train Loss: 0.6322, Test Loss: 4.6902\n",
      "Epoch: 3669, Train Loss: 0.5880, Test Loss: 3.7977\n",
      "Epoch: 3670, Train Loss: 0.5077, Test Loss: 3.4362\n",
      "Epoch: 3671, Train Loss: 0.6814, Test Loss: 3.8521\n",
      "Epoch: 3672, Train Loss: 0.5654, Test Loss: 4.9743\n",
      "Epoch: 3673, Train Loss: 0.7131, Test Loss: 4.1863\n",
      "Epoch: 3674, Train Loss: 0.6272, Test Loss: 3.2110\n",
      "Epoch: 3675, Train Loss: 0.6935, Test Loss: 3.5161\n",
      "Epoch: 3676, Train Loss: 0.5359, Test Loss: 4.6936\n",
      "Epoch: 3677, Train Loss: 0.5857, Test Loss: 4.8107\n",
      "Epoch: 3678, Train Loss: 0.5961, Test Loss: 4.0699\n",
      "Epoch: 3679, Train Loss: 0.5156, Test Loss: 3.5554\n",
      "Epoch: 3680, Train Loss: 0.6161, Test Loss: 4.0719\n",
      "Epoch: 3681, Train Loss: 0.4948, Test Loss: 4.8861\n",
      "Epoch: 3682, Train Loss: 0.6128, Test Loss: 4.3361\n",
      "Epoch: 3683, Train Loss: 0.5175, Test Loss: 3.4657\n",
      "Epoch: 3684, Train Loss: 0.5978, Test Loss: 3.7423\n",
      "Epoch: 3685, Train Loss: 0.5995, Test Loss: 5.0696\n",
      "Epoch: 3686, Train Loss: 0.6638, Test Loss: 4.8979\n",
      "Epoch: 3687, Train Loss: 0.6897, Test Loss: 3.4256\n",
      "Epoch: 3688, Train Loss: 0.6260, Test Loss: 3.3639\n",
      "Epoch: 3689, Train Loss: 0.6316, Test Loss: 4.3596\n",
      "Epoch: 3690, Train Loss: 0.5969, Test Loss: 4.7809\n",
      "Epoch: 3691, Train Loss: 0.6240, Test Loss: 3.8560\n",
      "Epoch: 3692, Train Loss: 0.5598, Test Loss: 3.3516\n",
      "Epoch: 3693, Train Loss: 0.6338, Test Loss: 3.7230\n",
      "Epoch: 3694, Train Loss: 0.5446, Test Loss: 4.6482\n",
      "Epoch: 3695, Train Loss: 0.6258, Test Loss: 4.3050\n",
      "Epoch: 3696, Train Loss: 0.5469, Test Loss: 3.5507\n",
      "Epoch: 3697, Train Loss: 0.5405, Test Loss: 3.6585\n",
      "Epoch: 3698, Train Loss: 0.5372, Test Loss: 4.4978\n",
      "Epoch: 3699, Train Loss: 0.6179, Test Loss: 4.5028\n",
      "Epoch: 3700, Train Loss: 0.5863, Test Loss: 4.0107\n",
      "Epoch: 3701, Train Loss: 0.5261, Test Loss: 3.5354\n",
      "Epoch: 3702, Train Loss: 0.5656, Test Loss: 3.7898\n",
      "Epoch: 3703, Train Loss: 0.5233, Test Loss: 4.1967\n",
      "Epoch: 3704, Train Loss: 0.5322, Test Loss: 4.4149\n",
      "Epoch: 3705, Train Loss: 0.5500, Test Loss: 4.2731\n",
      "Epoch: 3706, Train Loss: 0.6078, Test Loss: 3.5021\n",
      "Epoch: 3707, Train Loss: 0.5633, Test Loss: 3.3186\n",
      "Epoch: 3708, Train Loss: 0.6454, Test Loss: 4.3590\n",
      "Epoch: 3709, Train Loss: 0.5821, Test Loss: 4.5336\n",
      "Epoch: 3710, Train Loss: 0.5697, Test Loss: 3.8184\n",
      "Epoch: 3711, Train Loss: 0.5832, Test Loss: 3.7844\n",
      "Epoch: 3712, Train Loss: 0.5673, Test Loss: 4.3483\n",
      "Epoch: 3713, Train Loss: 0.6482, Test Loss: 3.9736\n",
      "Epoch: 3714, Train Loss: 0.5585, Test Loss: 3.7498\n",
      "Epoch: 3715, Train Loss: 0.5776, Test Loss: 3.7226\n",
      "Epoch: 3716, Train Loss: 0.5602, Test Loss: 4.0757\n",
      "Epoch: 3717, Train Loss: 0.5164, Test Loss: 4.4439\n",
      "Epoch: 3718, Train Loss: 0.6092, Test Loss: 4.4319\n",
      "Epoch: 3719, Train Loss: 0.5996, Test Loss: 3.9095\n",
      "Epoch: 3720, Train Loss: 0.5158, Test Loss: 3.7391\n",
      "Epoch: 3721, Train Loss: 0.6008, Test Loss: 3.9372\n",
      "Epoch: 3722, Train Loss: 0.6238, Test Loss: 3.9899\n",
      "Epoch: 3723, Train Loss: 0.5031, Test Loss: 4.0918\n",
      "Epoch: 3724, Train Loss: 0.5198, Test Loss: 4.1842\n",
      "Epoch: 3725, Train Loss: 0.5686, Test Loss: 4.0037\n",
      "Epoch: 3726, Train Loss: 0.5378, Test Loss: 3.8997\n",
      "Epoch: 3727, Train Loss: 0.5633, Test Loss: 4.4505\n",
      "Epoch: 3728, Train Loss: 0.5479, Test Loss: 4.1808\n",
      "Epoch: 3729, Train Loss: 0.5510, Test Loss: 3.9099\n",
      "Epoch: 3730, Train Loss: 0.5538, Test Loss: 3.7533\n",
      "Epoch: 3731, Train Loss: 0.5557, Test Loss: 4.2248\n",
      "Epoch: 3732, Train Loss: 0.5365, Test Loss: 4.2749\n",
      "Epoch: 3733, Train Loss: 0.6680, Test Loss: 3.4862\n",
      "Epoch: 3734, Train Loss: 0.6128, Test Loss: 3.9626\n",
      "Epoch: 3735, Train Loss: 0.5573, Test Loss: 4.8336\n",
      "Epoch: 3736, Train Loss: 0.5646, Test Loss: 4.7692\n",
      "Epoch: 3737, Train Loss: 0.6253, Test Loss: 3.7186\n",
      "Epoch: 3738, Train Loss: 0.6022, Test Loss: 3.7131\n",
      "Epoch: 3739, Train Loss: 0.6074, Test Loss: 4.9757\n",
      "Epoch: 3740, Train Loss: 0.6132, Test Loss: 4.8779\n",
      "Epoch: 3741, Train Loss: 0.6530, Test Loss: 3.6593\n",
      "Epoch: 3742, Train Loss: 0.6420, Test Loss: 3.3858\n",
      "Epoch: 3743, Train Loss: 0.7333, Test Loss: 4.4552\n",
      "Epoch: 3744, Train Loss: 0.5207, Test Loss: 5.1792\n",
      "Epoch: 3745, Train Loss: 0.6568, Test Loss: 4.2346\n",
      "Epoch: 3746, Train Loss: 0.5585, Test Loss: 3.8616\n",
      "Epoch: 3747, Train Loss: 0.6017, Test Loss: 4.2738\n",
      "Epoch: 3748, Train Loss: 0.5895, Test Loss: 4.3035\n",
      "Epoch: 3749, Train Loss: 0.5652, Test Loss: 4.0265\n",
      "Epoch: 3750, Train Loss: 0.5648, Test Loss: 4.1332\n",
      "Epoch: 3751, Train Loss: 0.5463, Test Loss: 3.9742\n",
      "Epoch: 3752, Train Loss: 0.5661, Test Loss: 4.2195\n",
      "Epoch: 3753, Train Loss: 0.5492, Test Loss: 4.1557\n",
      "Epoch: 3754, Train Loss: 0.5721, Test Loss: 4.7032\n",
      "Epoch: 3755, Train Loss: 0.5567, Test Loss: 4.4130\n",
      "Epoch: 3756, Train Loss: 0.5747, Test Loss: 4.2114\n",
      "Epoch: 3757, Train Loss: 0.5320, Test Loss: 4.3411\n",
      "Epoch: 3758, Train Loss: 0.5316, Test Loss: 4.0996\n",
      "Epoch: 3759, Train Loss: 0.5457, Test Loss: 3.9959\n",
      "Epoch: 3760, Train Loss: 0.5265, Test Loss: 4.5647\n",
      "Epoch: 3761, Train Loss: 0.6049, Test Loss: 3.8008\n",
      "Epoch: 3762, Train Loss: 0.5197, Test Loss: 3.5938\n",
      "Epoch: 3763, Train Loss: 0.6489, Test Loss: 4.8059\n",
      "Epoch: 3764, Train Loss: 0.5765, Test Loss: 4.8534\n",
      "Epoch: 3765, Train Loss: 0.5956, Test Loss: 3.6630\n",
      "Epoch: 3766, Train Loss: 0.5899, Test Loss: 3.5964\n",
      "Epoch: 3767, Train Loss: 0.5915, Test Loss: 4.5490\n",
      "Epoch: 3768, Train Loss: 0.5344, Test Loss: 5.4161\n",
      "Epoch: 3769, Train Loss: 0.6989, Test Loss: 3.9316\n",
      "Epoch: 3770, Train Loss: 0.5443, Test Loss: 3.2186\n",
      "Epoch: 3771, Train Loss: 0.7112, Test Loss: 3.7788\n",
      "Epoch: 3772, Train Loss: 0.5000, Test Loss: 4.8042\n",
      "Epoch: 3773, Train Loss: 0.5717, Test Loss: 4.6823\n",
      "Epoch: 3774, Train Loss: 0.5717, Test Loss: 3.6315\n",
      "Epoch: 3775, Train Loss: 0.5931, Test Loss: 3.6331\n",
      "Epoch: 3776, Train Loss: 0.6539, Test Loss: 4.9559\n",
      "Epoch: 3777, Train Loss: 0.6262, Test Loss: 4.7515\n",
      "Epoch: 3778, Train Loss: 0.5758, Test Loss: 3.7878\n",
      "Epoch: 3779, Train Loss: 0.5291, Test Loss: 3.4693\n",
      "Epoch: 3780, Train Loss: 0.6748, Test Loss: 4.4169\n",
      "Epoch: 3781, Train Loss: 0.5583, Test Loss: 4.7078\n",
      "Epoch: 3782, Train Loss: 0.6868, Test Loss: 3.8399\n",
      "Epoch: 3783, Train Loss: 0.5739, Test Loss: 3.7085\n",
      "Epoch: 3784, Train Loss: 0.5620, Test Loss: 4.0739\n",
      "Epoch: 3785, Train Loss: 0.4838, Test Loss: 4.7074\n",
      "Epoch: 3786, Train Loss: 0.5562, Test Loss: 4.3311\n",
      "Epoch: 3787, Train Loss: 0.5645, Test Loss: 3.8822\n",
      "Epoch: 3788, Train Loss: 0.5613, Test Loss: 3.9618\n",
      "Epoch: 3789, Train Loss: 0.5091, Test Loss: 4.2497\n",
      "Epoch: 3790, Train Loss: 0.5310, Test Loss: 4.3490\n",
      "Epoch: 3791, Train Loss: 0.5629, Test Loss: 4.2081\n",
      "Epoch: 3792, Train Loss: 0.5105, Test Loss: 4.1394\n",
      "Epoch: 3793, Train Loss: 0.5419, Test Loss: 3.7360\n",
      "Epoch: 3794, Train Loss: 0.5975, Test Loss: 4.2351\n",
      "Epoch: 3795, Train Loss: 0.4973, Test Loss: 4.4803\n",
      "Epoch: 3796, Train Loss: 0.5225, Test Loss: 4.4381\n",
      "Epoch: 3797, Train Loss: 0.5206, Test Loss: 4.1154\n",
      "Epoch: 3798, Train Loss: 0.5463, Test Loss: 3.8320\n",
      "Epoch: 3799, Train Loss: 0.5257, Test Loss: 4.0262\n",
      "Epoch: 3800, Train Loss: 0.5176, Test Loss: 4.4897\n",
      "Epoch: 3801, Train Loss: 0.5616, Test Loss: 4.4617\n",
      "Epoch: 3802, Train Loss: 0.5609, Test Loss: 3.8873\n",
      "Epoch: 3803, Train Loss: 0.5563, Test Loss: 3.9061\n",
      "Epoch: 3804, Train Loss: 0.5596, Test Loss: 4.7115\n",
      "Epoch: 3805, Train Loss: 0.5844, Test Loss: 4.4037\n",
      "Epoch: 3806, Train Loss: 0.5192, Test Loss: 3.7844\n",
      "Epoch: 3807, Train Loss: 0.6002, Test Loss: 4.1553\n",
      "Epoch: 3808, Train Loss: 0.5346, Test Loss: 4.2623\n",
      "Epoch: 3809, Train Loss: 0.5198, Test Loss: 4.2019\n",
      "Epoch: 3810, Train Loss: 0.5158, Test Loss: 4.1508\n",
      "Epoch: 3811, Train Loss: 0.5055, Test Loss: 4.3797\n",
      "Epoch: 3812, Train Loss: 0.5149, Test Loss: 4.2346\n",
      "Epoch: 3813, Train Loss: 0.5091, Test Loss: 4.0214\n",
      "Epoch: 3814, Train Loss: 0.5762, Test Loss: 3.7628\n",
      "Epoch: 3815, Train Loss: 0.5333, Test Loss: 4.1470\n",
      "Epoch: 3816, Train Loss: 0.4885, Test Loss: 4.8664\n",
      "Epoch: 3817, Train Loss: 0.5353, Test Loss: 4.8932\n",
      "Epoch: 3818, Train Loss: 0.5608, Test Loss: 3.9859\n",
      "Epoch: 3819, Train Loss: 0.5113, Test Loss: 3.5752\n",
      "Epoch: 3820, Train Loss: 0.6011, Test Loss: 4.1885\n",
      "Epoch: 3821, Train Loss: 0.4996, Test Loss: 4.9438\n",
      "Epoch: 3822, Train Loss: 0.6579, Test Loss: 4.3090\n",
      "Epoch: 3823, Train Loss: 0.5044, Test Loss: 3.5724\n",
      "Epoch: 3824, Train Loss: 0.5941, Test Loss: 3.9988\n",
      "Epoch: 3825, Train Loss: 0.5784, Test Loss: 5.0513\n",
      "Epoch: 3826, Train Loss: 0.5937, Test Loss: 4.6458\n",
      "Epoch: 3827, Train Loss: 0.5666, Test Loss: 3.7791\n",
      "Epoch: 3828, Train Loss: 0.5992, Test Loss: 3.7409\n",
      "Epoch: 3829, Train Loss: 0.5835, Test Loss: 4.5579\n",
      "Epoch: 3830, Train Loss: 0.5402, Test Loss: 4.9763\n",
      "Epoch: 3831, Train Loss: 0.5887, Test Loss: 4.0937\n",
      "Epoch: 3832, Train Loss: 0.5024, Test Loss: 3.4209\n",
      "Epoch: 3833, Train Loss: 0.6733, Test Loss: 4.1023\n",
      "Epoch: 3834, Train Loss: 0.5500, Test Loss: 4.8356\n",
      "Epoch: 3835, Train Loss: 0.5303, Test Loss: 4.8159\n",
      "Epoch: 3836, Train Loss: 0.6209, Test Loss: 3.7867\n",
      "Epoch: 3837, Train Loss: 0.5571, Test Loss: 3.7107\n",
      "Epoch: 3838, Train Loss: 0.6035, Test Loss: 4.9544\n",
      "Epoch: 3839, Train Loss: 0.6071, Test Loss: 4.6495\n",
      "Epoch: 3840, Train Loss: 0.5439, Test Loss: 3.6972\n",
      "Epoch: 3841, Train Loss: 0.5419, Test Loss: 3.7767\n",
      "Epoch: 3842, Train Loss: 0.5613, Test Loss: 4.7086\n",
      "Epoch: 3843, Train Loss: 0.5770, Test Loss: 4.6092\n",
      "Epoch: 3844, Train Loss: 0.5044, Test Loss: 4.2326\n",
      "Epoch: 3845, Train Loss: 0.5163, Test Loss: 3.7544\n",
      "Epoch: 3846, Train Loss: 0.6034, Test Loss: 4.3756\n",
      "Epoch: 3847, Train Loss: 0.4926, Test Loss: 4.5848\n",
      "Epoch: 3848, Train Loss: 0.5917, Test Loss: 4.1770\n",
      "Epoch: 3849, Train Loss: 0.5180, Test Loss: 3.6241\n",
      "Epoch: 3850, Train Loss: 0.6101, Test Loss: 3.8126\n",
      "Epoch: 3851, Train Loss: 0.5608, Test Loss: 5.1073\n",
      "Epoch: 3852, Train Loss: 0.6197, Test Loss: 4.8058\n",
      "Epoch: 3853, Train Loss: 0.5271, Test Loss: 3.7621\n",
      "Epoch: 3854, Train Loss: 0.6021, Test Loss: 3.4640\n",
      "Epoch: 3855, Train Loss: 0.6258, Test Loss: 4.5239\n",
      "Epoch: 3856, Train Loss: 0.5581, Test Loss: 4.8573\n",
      "Epoch: 3857, Train Loss: 0.5953, Test Loss: 3.7681\n",
      "Epoch: 3858, Train Loss: 0.4880, Test Loss: 3.4492\n",
      "Epoch: 3859, Train Loss: 0.6512, Test Loss: 4.4708\n",
      "Epoch: 3860, Train Loss: 0.5334, Test Loss: 5.1581\n",
      "Epoch: 3861, Train Loss: 0.6941, Test Loss: 3.7772\n",
      "Epoch: 3862, Train Loss: 0.5378, Test Loss: 3.3276\n",
      "Epoch: 3863, Train Loss: 0.6229, Test Loss: 4.0885\n",
      "Epoch: 3864, Train Loss: 0.5694, Test Loss: 4.7235\n",
      "Epoch: 3865, Train Loss: 0.6354, Test Loss: 3.8656\n",
      "Epoch: 3866, Train Loss: 0.4902, Test Loss: 3.5647\n",
      "Epoch: 3867, Train Loss: 0.5524, Test Loss: 3.6758\n",
      "Epoch: 3868, Train Loss: 0.5358, Test Loss: 4.2358\n",
      "Epoch: 3869, Train Loss: 0.5531, Test Loss: 4.2482\n",
      "Epoch: 3870, Train Loss: 0.5130, Test Loss: 4.0343\n",
      "Epoch: 3871, Train Loss: 0.5383, Test Loss: 4.5012\n",
      "Epoch: 3872, Train Loss: 0.5530, Test Loss: 3.9863\n",
      "Epoch: 3873, Train Loss: 0.5376, Test Loss: 3.5861\n",
      "Epoch: 3874, Train Loss: 0.5969, Test Loss: 4.1140\n",
      "Epoch: 3875, Train Loss: 0.5555, Test Loss: 4.2381\n",
      "Epoch: 3876, Train Loss: 0.5331, Test Loss: 3.9720\n",
      "Epoch: 3877, Train Loss: 0.4945, Test Loss: 3.6298\n",
      "Epoch: 3878, Train Loss: 0.5248, Test Loss: 3.9633\n",
      "Epoch: 3879, Train Loss: 0.4885, Test Loss: 4.4790\n",
      "Epoch: 3880, Train Loss: 0.5379, Test Loss: 4.2610\n",
      "Epoch: 3881, Train Loss: 0.4904, Test Loss: 3.6543\n",
      "Epoch: 3882, Train Loss: 0.5941, Test Loss: 4.1389\n",
      "Epoch: 3883, Train Loss: 0.5355, Test Loss: 4.6201\n",
      "Epoch: 3884, Train Loss: 0.5415, Test Loss: 4.1223\n",
      "Epoch: 3885, Train Loss: 0.5031, Test Loss: 3.7674\n",
      "Epoch: 3886, Train Loss: 0.4889, Test Loss: 3.9432\n",
      "Epoch: 3887, Train Loss: 0.5802, Test Loss: 4.9688\n",
      "Epoch: 3888, Train Loss: 0.5522, Test Loss: 4.5668\n",
      "Epoch: 3889, Train Loss: 0.5337, Test Loss: 4.0897\n",
      "Epoch: 3890, Train Loss: 0.5399, Test Loss: 4.2223\n",
      "Epoch: 3891, Train Loss: 0.5265, Test Loss: 4.1018\n",
      "Epoch: 3892, Train Loss: 0.5221, Test Loss: 4.2515\n",
      "Epoch: 3893, Train Loss: 0.5276, Test Loss: 4.4418\n",
      "Epoch: 3894, Train Loss: 0.5093, Test Loss: 4.1394\n",
      "Epoch: 3895, Train Loss: 0.5092, Test Loss: 4.0056\n",
      "Epoch: 3896, Train Loss: 0.5094, Test Loss: 4.1086\n",
      "Epoch: 3897, Train Loss: 0.5088, Test Loss: 4.2908\n",
      "Epoch: 3898, Train Loss: 0.4797, Test Loss: 4.1472\n",
      "Epoch: 3899, Train Loss: 0.5218, Test Loss: 4.1677\n",
      "Epoch: 3900, Train Loss: 0.5014, Test Loss: 4.4961\n",
      "Epoch: 3901, Train Loss: 0.5561, Test Loss: 3.9973\n",
      "Epoch: 3902, Train Loss: 0.5182, Test Loss: 3.8973\n",
      "Epoch: 3903, Train Loss: 0.5161, Test Loss: 4.4088\n",
      "Epoch: 3904, Train Loss: 0.5336, Test Loss: 4.4422\n",
      "Epoch: 3905, Train Loss: 0.5007, Test Loss: 4.1033\n",
      "Epoch: 3906, Train Loss: 0.5538, Test Loss: 3.8380\n",
      "Epoch: 3907, Train Loss: 0.5442, Test Loss: 4.0314\n",
      "Epoch: 3908, Train Loss: 0.5050, Test Loss: 4.6753\n",
      "Epoch: 3909, Train Loss: 0.5609, Test Loss: 4.1832\n",
      "Epoch: 3910, Train Loss: 0.4960, Test Loss: 3.8089\n",
      "Epoch: 3911, Train Loss: 0.5685, Test Loss: 4.1957\n",
      "Epoch: 3912, Train Loss: 0.5234, Test Loss: 4.8747\n",
      "Epoch: 3913, Train Loss: 0.5750, Test Loss: 4.3306\n",
      "Epoch: 3914, Train Loss: 0.5321, Test Loss: 3.7817\n",
      "Epoch: 3915, Train Loss: 0.5207, Test Loss: 3.9999\n",
      "Epoch: 3916, Train Loss: 0.4812, Test Loss: 4.5643\n",
      "Epoch: 3917, Train Loss: 0.5677, Test Loss: 4.6146\n",
      "Epoch: 3918, Train Loss: 0.5269, Test Loss: 4.5732\n",
      "Epoch: 3919, Train Loss: 0.5653, Test Loss: 3.9700\n",
      "Epoch: 3920, Train Loss: 0.5386, Test Loss: 3.7715\n",
      "Epoch: 3921, Train Loss: 0.5449, Test Loss: 4.4622\n",
      "Epoch: 3922, Train Loss: 0.5133, Test Loss: 4.6186\n",
      "Epoch: 3923, Train Loss: 0.5129, Test Loss: 4.2866\n",
      "Epoch: 3924, Train Loss: 0.5159, Test Loss: 4.2824\n",
      "Epoch: 3925, Train Loss: 0.5128, Test Loss: 3.8493\n",
      "Epoch: 3926, Train Loss: 0.5834, Test Loss: 4.4320\n",
      "Epoch: 3927, Train Loss: 0.5219, Test Loss: 4.9476\n",
      "Epoch: 3928, Train Loss: 0.5577, Test Loss: 4.2501\n",
      "Epoch: 3929, Train Loss: 0.5061, Test Loss: 3.9447\n",
      "Epoch: 3930, Train Loss: 0.5096, Test Loss: 4.1003\n",
      "Epoch: 3931, Train Loss: 0.5328, Test Loss: 4.5798\n",
      "Epoch: 3932, Train Loss: 0.5128, Test Loss: 4.3314\n",
      "Epoch: 3933, Train Loss: 0.5444, Test Loss: 4.0398\n",
      "Epoch: 3934, Train Loss: 0.5026, Test Loss: 3.9256\n",
      "Epoch: 3935, Train Loss: 0.5257, Test Loss: 4.2901\n",
      "Epoch: 3936, Train Loss: 0.5556, Test Loss: 4.7486\n",
      "Epoch: 3937, Train Loss: 0.6061, Test Loss: 3.7898\n",
      "Epoch: 3938, Train Loss: 0.5248, Test Loss: 3.5574\n",
      "Epoch: 3939, Train Loss: 0.6217, Test Loss: 4.3990\n",
      "Epoch: 3940, Train Loss: 0.5275, Test Loss: 4.5815\n",
      "Epoch: 3941, Train Loss: 0.6187, Test Loss: 3.6372\n",
      "Epoch: 3942, Train Loss: 0.5464, Test Loss: 3.5023\n",
      "Epoch: 3943, Train Loss: 0.6157, Test Loss: 4.6401\n",
      "Epoch: 3944, Train Loss: 0.5499, Test Loss: 5.0110\n",
      "Epoch: 3945, Train Loss: 0.6194, Test Loss: 3.7900\n",
      "Epoch: 3946, Train Loss: 0.5901, Test Loss: 3.5571\n",
      "Epoch: 3947, Train Loss: 0.5423, Test Loss: 3.7914\n",
      "Epoch: 3948, Train Loss: 0.5158, Test Loss: 4.2424\n",
      "Epoch: 3949, Train Loss: 0.5184, Test Loss: 4.4795\n",
      "Epoch: 3950, Train Loss: 0.5725, Test Loss: 3.7919\n",
      "Epoch: 3951, Train Loss: 0.5904, Test Loss: 3.8524\n",
      "Epoch: 3952, Train Loss: 0.5321, Test Loss: 4.1332\n",
      "Epoch: 3953, Train Loss: 0.5534, Test Loss: 4.2395\n",
      "Epoch: 3954, Train Loss: 0.5552, Test Loss: 4.1477\n",
      "Epoch: 3955, Train Loss: 0.5967, Test Loss: 4.0938\n",
      "Epoch: 3956, Train Loss: 0.5498, Test Loss: 3.7044\n",
      "Epoch: 3957, Train Loss: 0.5679, Test Loss: 3.9101\n",
      "Epoch: 3958, Train Loss: 0.4827, Test Loss: 4.4785\n",
      "Epoch: 3959, Train Loss: 0.5292, Test Loss: 4.5997\n",
      "Epoch: 3960, Train Loss: 0.5505, Test Loss: 4.0565\n",
      "Epoch: 3961, Train Loss: 0.5118, Test Loss: 3.6118\n",
      "Epoch: 3962, Train Loss: 0.5390, Test Loss: 4.0195\n",
      "Epoch: 3963, Train Loss: 0.5474, Test Loss: 4.8341\n",
      "Epoch: 3964, Train Loss: 0.6023, Test Loss: 4.1990\n",
      "Epoch: 3965, Train Loss: 0.5127, Test Loss: 3.3165\n",
      "Epoch: 3966, Train Loss: 0.7269, Test Loss: 4.1259\n",
      "Epoch: 3967, Train Loss: 0.5467, Test Loss: 5.1214\n",
      "Epoch: 3968, Train Loss: 0.6536, Test Loss: 4.1926\n",
      "Epoch: 3969, Train Loss: 0.5167, Test Loss: 3.5240\n",
      "Epoch: 3970, Train Loss: 0.6424, Test Loss: 3.9291\n",
      "Epoch: 3971, Train Loss: 0.5199, Test Loss: 4.4283\n",
      "Epoch: 3972, Train Loss: 0.5875, Test Loss: 4.2801\n",
      "Epoch: 3973, Train Loss: 0.5476, Test Loss: 4.1739\n",
      "Epoch: 3974, Train Loss: 0.5112, Test Loss: 4.0771\n",
      "Epoch: 3975, Train Loss: 0.5184, Test Loss: 4.1639\n",
      "Epoch: 3976, Train Loss: 0.5369, Test Loss: 4.1719\n",
      "Epoch: 3977, Train Loss: 0.5136, Test Loss: 4.1801\n",
      "Epoch: 3978, Train Loss: 0.5519, Test Loss: 4.5637\n",
      "Epoch: 3979, Train Loss: 0.5384, Test Loss: 4.5258\n",
      "Epoch: 3980, Train Loss: 0.5501, Test Loss: 4.0025\n",
      "Epoch: 3981, Train Loss: 0.4999, Test Loss: 4.0457\n",
      "Epoch: 3982, Train Loss: 0.5032, Test Loss: 4.4408\n",
      "Epoch: 3983, Train Loss: 0.5080, Test Loss: 4.2953\n",
      "Epoch: 3984, Train Loss: 0.5462, Test Loss: 3.9904\n",
      "Epoch: 3985, Train Loss: 0.5391, Test Loss: 4.1388\n",
      "Epoch: 3986, Train Loss: 0.5339, Test Loss: 4.2735\n",
      "Epoch: 3987, Train Loss: 0.5291, Test Loss: 3.8478\n",
      "Epoch: 3988, Train Loss: 0.5508, Test Loss: 4.1824\n",
      "Epoch: 3989, Train Loss: 0.5006, Test Loss: 4.9607\n",
      "Epoch: 3990, Train Loss: 0.5553, Test Loss: 4.2651\n",
      "Epoch: 3991, Train Loss: 0.5791, Test Loss: 4.6185\n",
      "Epoch: 3992, Train Loss: 0.4848, Test Loss: 4.5647\n",
      "Epoch: 3993, Train Loss: 0.5693, Test Loss: 3.8825\n",
      "Epoch: 3994, Train Loss: 0.5150, Test Loss: 3.9170\n",
      "Epoch: 3995, Train Loss: 0.5218, Test Loss: 4.3597\n",
      "Epoch: 3996, Train Loss: 0.5186, Test Loss: 4.5734\n",
      "Epoch: 3997, Train Loss: 0.5122, Test Loss: 4.2241\n",
      "Epoch: 3998, Train Loss: 0.5035, Test Loss: 3.9780\n",
      "Epoch: 3999, Train Loss: 0.5194, Test Loss: 4.2931\n",
      "Epoch: 4000, Train Loss: 0.5125, Test Loss: 4.6127\n",
      "Epoch: 4001, Train Loss: 0.5389, Test Loss: 4.2886\n",
      "Epoch: 4002, Train Loss: 0.5302, Test Loss: 3.9212\n",
      "Epoch: 4003, Train Loss: 0.5391, Test Loss: 3.6554\n",
      "Epoch: 4004, Train Loss: 0.5440, Test Loss: 4.2003\n",
      "Epoch: 4005, Train Loss: 0.5645, Test Loss: 5.1303\n",
      "Epoch: 4006, Train Loss: 0.6708, Test Loss: 4.0460\n",
      "Epoch: 4007, Train Loss: 0.5706, Test Loss: 3.7555\n",
      "Epoch: 4008, Train Loss: 0.5626, Test Loss: 4.0586\n",
      "Epoch: 4009, Train Loss: 0.5070, Test Loss: 4.5829\n",
      "Epoch: 4010, Train Loss: 0.5092, Test Loss: 5.2621\n",
      "Epoch: 4011, Train Loss: 0.6490, Test Loss: 4.0911\n",
      "Epoch: 4012, Train Loss: 0.5443, Test Loss: 3.5026\n",
      "Epoch: 4013, Train Loss: 0.7239, Test Loss: 4.3707\n",
      "Epoch: 4014, Train Loss: 0.5065, Test Loss: 4.9389\n",
      "Epoch: 4015, Train Loss: 0.6334, Test Loss: 3.7361\n",
      "Epoch: 4016, Train Loss: 0.5822, Test Loss: 3.5329\n",
      "Epoch: 4017, Train Loss: 0.5973, Test Loss: 4.2914\n",
      "Epoch: 4018, Train Loss: 0.5710, Test Loss: 5.0646\n",
      "Epoch: 4019, Train Loss: 0.5616, Test Loss: 4.4669\n",
      "Epoch: 4020, Train Loss: 0.4979, Test Loss: 3.7971\n",
      "Epoch: 4021, Train Loss: 0.5287, Test Loss: 3.7418\n",
      "Epoch: 4022, Train Loss: 0.6272, Test Loss: 5.0870\n",
      "Epoch: 4023, Train Loss: 0.6125, Test Loss: 4.7961\n",
      "Epoch: 4024, Train Loss: 0.5354, Test Loss: 3.6822\n",
      "Epoch: 4025, Train Loss: 0.5618, Test Loss: 3.7472\n",
      "Epoch: 4026, Train Loss: 0.6061, Test Loss: 4.8855\n",
      "Epoch: 4027, Train Loss: 0.5550, Test Loss: 5.1092\n",
      "Epoch: 4028, Train Loss: 0.5529, Test Loss: 4.2822\n",
      "Epoch: 4029, Train Loss: 0.5129, Test Loss: 3.5413\n",
      "Epoch: 4030, Train Loss: 0.6625, Test Loss: 4.2455\n",
      "Epoch: 4031, Train Loss: 0.5325, Test Loss: 4.4448\n",
      "Epoch: 4032, Train Loss: 0.5035, Test Loss: 4.6029\n",
      "Epoch: 4033, Train Loss: 0.5056, Test Loss: 4.3433\n",
      "Epoch: 4034, Train Loss: 0.5202, Test Loss: 4.1751\n",
      "Epoch: 4035, Train Loss: 0.5106, Test Loss: 4.2941\n",
      "Epoch: 4036, Train Loss: 0.4826, Test Loss: 4.4818\n",
      "Epoch: 4037, Train Loss: 0.5283, Test Loss: 4.1984\n",
      "Epoch: 4038, Train Loss: 0.5155, Test Loss: 3.6982\n",
      "Epoch: 4039, Train Loss: 0.5515, Test Loss: 4.2627\n",
      "Epoch: 4040, Train Loss: 0.5504, Test Loss: 4.3887\n",
      "Epoch: 4041, Train Loss: 0.5305, Test Loss: 3.9116\n",
      "Epoch: 4042, Train Loss: 0.5334, Test Loss: 4.2518\n",
      "Epoch: 4043, Train Loss: 0.5430, Test Loss: 4.6841\n",
      "Epoch: 4044, Train Loss: 0.5532, Test Loss: 3.9971\n",
      "Epoch: 4045, Train Loss: 0.5112, Test Loss: 3.6265\n",
      "Epoch: 4046, Train Loss: 0.5427, Test Loss: 4.2723\n",
      "Epoch: 4047, Train Loss: 0.5388, Test Loss: 4.6178\n",
      "Epoch: 4048, Train Loss: 0.5397, Test Loss: 4.0638\n",
      "Epoch: 4049, Train Loss: 0.5403, Test Loss: 3.5423\n",
      "Epoch: 4050, Train Loss: 0.6529, Test Loss: 4.5343\n",
      "Epoch: 4051, Train Loss: 0.4740, Test Loss: 5.5989\n",
      "Epoch: 4052, Train Loss: 0.6827, Test Loss: 4.2735\n",
      "Epoch: 4053, Train Loss: 0.5241, Test Loss: 3.2778\n",
      "Epoch: 4054, Train Loss: 0.7024, Test Loss: 3.9396\n",
      "Epoch: 4055, Train Loss: 0.5427, Test Loss: 5.2889\n",
      "Epoch: 4056, Train Loss: 0.6995, Test Loss: 4.4139\n",
      "Epoch: 4057, Train Loss: 0.5366, Test Loss: 3.7398\n",
      "Epoch: 4058, Train Loss: 0.6141, Test Loss: 4.2847\n",
      "Epoch: 4059, Train Loss: 0.4937, Test Loss: 4.9629\n",
      "Epoch: 4060, Train Loss: 0.6015, Test Loss: 4.3171\n",
      "Epoch: 4061, Train Loss: 0.5661, Test Loss: 3.4808\n",
      "Epoch: 4062, Train Loss: 0.6961, Test Loss: 4.1048\n",
      "Epoch: 4063, Train Loss: 0.5776, Test Loss: 4.9818\n",
      "Epoch: 4064, Train Loss: 0.5889, Test Loss: 4.3356\n",
      "Epoch: 4065, Train Loss: 0.5593, Test Loss: 3.7373\n",
      "Epoch: 4066, Train Loss: 0.5024, Test Loss: 3.8965\n",
      "Epoch: 4067, Train Loss: 0.5263, Test Loss: 4.7984\n",
      "Epoch: 4068, Train Loss: 0.5914, Test Loss: 4.4797\n",
      "Epoch: 4069, Train Loss: 0.4840, Test Loss: 3.8775\n",
      "Epoch: 4070, Train Loss: 0.5407, Test Loss: 4.1482\n",
      "Epoch: 4071, Train Loss: 0.4912, Test Loss: 4.2439\n",
      "Epoch: 4072, Train Loss: 0.5102, Test Loss: 4.3597\n",
      "Epoch: 4073, Train Loss: 0.5646, Test Loss: 4.2878\n",
      "Epoch: 4074, Train Loss: 0.4973, Test Loss: 3.9519\n",
      "Epoch: 4075, Train Loss: 0.5403, Test Loss: 4.6168\n",
      "Epoch: 4076, Train Loss: 0.5798, Test Loss: 4.3124\n",
      "Epoch: 4077, Train Loss: 0.5280, Test Loss: 3.6473\n",
      "Epoch: 4078, Train Loss: 0.5699, Test Loss: 3.8954\n",
      "Epoch: 4079, Train Loss: 0.5043, Test Loss: 4.9400\n",
      "Epoch: 4080, Train Loss: 0.5551, Test Loss: 4.6765\n",
      "Epoch: 4081, Train Loss: 0.6053, Test Loss: 3.5795\n",
      "Epoch: 4082, Train Loss: 0.6296, Test Loss: 3.8373\n",
      "Epoch: 4083, Train Loss: 0.5067, Test Loss: 4.4379\n",
      "Epoch: 4084, Train Loss: 0.5476, Test Loss: 4.5018\n",
      "Epoch: 4085, Train Loss: 0.5149, Test Loss: 3.9083\n",
      "Epoch: 4086, Train Loss: 0.4872, Test Loss: 3.7347\n",
      "Epoch: 4087, Train Loss: 0.5283, Test Loss: 4.3582\n",
      "Epoch: 4088, Train Loss: 0.5333, Test Loss: 4.1614\n",
      "Epoch: 4089, Train Loss: 0.5258, Test Loss: 4.0426\n",
      "Epoch: 4090, Train Loss: 0.5355, Test Loss: 4.3409\n",
      "Epoch: 4091, Train Loss: 0.4841, Test Loss: 4.5199\n",
      "Epoch: 4092, Train Loss: 0.5394, Test Loss: 3.8901\n",
      "Epoch: 4093, Train Loss: 0.5501, Test Loss: 3.7986\n",
      "Epoch: 4094, Train Loss: 0.5316, Test Loss: 4.0916\n",
      "Epoch: 4095, Train Loss: 0.4918, Test Loss: 4.2899\n",
      "Epoch: 4096, Train Loss: 0.5792, Test Loss: 3.9891\n",
      "Epoch: 4097, Train Loss: 0.5211, Test Loss: 3.9909\n",
      "Epoch: 4098, Train Loss: 0.4931, Test Loss: 3.9217\n",
      "Epoch: 4099, Train Loss: 0.5156, Test Loss: 3.5749\n",
      "Epoch: 4100, Train Loss: 0.5276, Test Loss: 3.9492\n",
      "Epoch: 4101, Train Loss: 0.5489, Test Loss: 3.9159\n",
      "Epoch: 4102, Train Loss: 0.4987, Test Loss: 4.1416\n",
      "Epoch: 4103, Train Loss: 0.5096, Test Loss: 4.2115\n",
      "Epoch: 4104, Train Loss: 0.5125, Test Loss: 4.4009\n",
      "Epoch: 4105, Train Loss: 0.5083, Test Loss: 4.0980\n",
      "Epoch: 4106, Train Loss: 0.5142, Test Loss: 4.0292\n",
      "Epoch: 4107, Train Loss: 0.5313, Test Loss: 4.0436\n",
      "Epoch: 4108, Train Loss: 0.5305, Test Loss: 4.1388\n",
      "Epoch: 4109, Train Loss: 0.4933, Test Loss: 4.1178\n",
      "Epoch: 4110, Train Loss: 0.5060, Test Loss: 4.2099\n",
      "Epoch: 4111, Train Loss: 0.5058, Test Loss: 3.9960\n",
      "Epoch: 4112, Train Loss: 0.5321, Test Loss: 4.5720\n",
      "Epoch: 4113, Train Loss: 0.5244, Test Loss: 4.6398\n",
      "Epoch: 4114, Train Loss: 0.5299, Test Loss: 4.0578\n",
      "Epoch: 4115, Train Loss: 0.5111, Test Loss: 3.5054\n",
      "Epoch: 4116, Train Loss: 0.5941, Test Loss: 3.7953\n",
      "Epoch: 4117, Train Loss: 0.5038, Test Loss: 4.4932\n",
      "Epoch: 4118, Train Loss: 0.5654, Test Loss: 4.7584\n",
      "Epoch: 4119, Train Loss: 0.5338, Test Loss: 4.0424\n",
      "Epoch: 4120, Train Loss: 0.5158, Test Loss: 3.7151\n",
      "Epoch: 4121, Train Loss: 0.5623, Test Loss: 4.0637\n",
      "Epoch: 4122, Train Loss: 0.4935, Test Loss: 4.5328\n",
      "Epoch: 4123, Train Loss: 0.5605, Test Loss: 4.5042\n",
      "Epoch: 4124, Train Loss: 0.5355, Test Loss: 4.0551\n",
      "Epoch: 4125, Train Loss: 0.4922, Test Loss: 3.8580\n",
      "Epoch: 4126, Train Loss: 0.5591, Test Loss: 4.7086\n",
      "Epoch: 4127, Train Loss: 0.5080, Test Loss: 4.6974\n",
      "Epoch: 4128, Train Loss: 0.5145, Test Loss: 4.1233\n",
      "Epoch: 4129, Train Loss: 0.5072, Test Loss: 3.5226\n",
      "Epoch: 4130, Train Loss: 0.5809, Test Loss: 3.9895\n",
      "Epoch: 4131, Train Loss: 0.5372, Test Loss: 4.8866\n",
      "Epoch: 4132, Train Loss: 0.5598, Test Loss: 4.3908\n",
      "Epoch: 4133, Train Loss: 0.4828, Test Loss: 3.6747\n",
      "Epoch: 4134, Train Loss: 0.5769, Test Loss: 4.1517\n",
      "Epoch: 4135, Train Loss: 0.5269, Test Loss: 4.1530\n",
      "Epoch: 4136, Train Loss: 0.5076, Test Loss: 3.8065\n",
      "Epoch: 4137, Train Loss: 0.5074, Test Loss: 4.2518\n",
      "Epoch: 4138, Train Loss: 0.5152, Test Loss: 4.3218\n",
      "Epoch: 4139, Train Loss: 0.5153, Test Loss: 4.0600\n",
      "Epoch: 4140, Train Loss: 0.5390, Test Loss: 4.1805\n",
      "Epoch: 4141, Train Loss: 0.4999, Test Loss: 4.4212\n",
      "Epoch: 4142, Train Loss: 0.5387, Test Loss: 4.7349\n",
      "Epoch: 4143, Train Loss: 0.5518, Test Loss: 3.9664\n",
      "Epoch: 4144, Train Loss: 0.5289, Test Loss: 3.8516\n",
      "Epoch: 4145, Train Loss: 0.4896, Test Loss: 4.2317\n",
      "Epoch: 4146, Train Loss: 0.4995, Test Loss: 4.4712\n",
      "Epoch: 4147, Train Loss: 0.6107, Test Loss: 3.6809\n",
      "Epoch: 4148, Train Loss: 0.5202, Test Loss: 3.3294\n",
      "Epoch: 4149, Train Loss: 0.5547, Test Loss: 3.9056\n",
      "Epoch: 4150, Train Loss: 0.5965, Test Loss: 5.2028\n",
      "Epoch: 4151, Train Loss: 0.6609, Test Loss: 4.5011\n",
      "Epoch: 4152, Train Loss: 0.6139, Test Loss: 3.2927\n",
      "Epoch: 4153, Train Loss: 0.6942, Test Loss: 3.5973\n",
      "Epoch: 4154, Train Loss: 0.5191, Test Loss: 4.6325\n",
      "Epoch: 4155, Train Loss: 0.5244, Test Loss: 4.9412\n",
      "Epoch: 4156, Train Loss: 0.6643, Test Loss: 3.5648\n",
      "Epoch: 4157, Train Loss: 0.5863, Test Loss: 3.4290\n",
      "Epoch: 4158, Train Loss: 0.6029, Test Loss: 4.1262\n",
      "Epoch: 4159, Train Loss: 0.4985, Test Loss: 4.8033\n",
      "Epoch: 4160, Train Loss: 0.6520, Test Loss: 4.0493\n",
      "Epoch: 4161, Train Loss: 0.5181, Test Loss: 3.3546\n",
      "Epoch: 4162, Train Loss: 0.6044, Test Loss: 3.8305\n",
      "Epoch: 4163, Train Loss: 0.5152, Test Loss: 4.4328\n",
      "Epoch: 4164, Train Loss: 0.5519, Test Loss: 4.2116\n",
      "Epoch: 4165, Train Loss: 0.5424, Test Loss: 3.4862\n",
      "Epoch: 4166, Train Loss: 0.5642, Test Loss: 3.5251\n",
      "Epoch: 4167, Train Loss: 0.5387, Test Loss: 3.9956\n",
      "Epoch: 4168, Train Loss: 0.4893, Test Loss: 4.3186\n",
      "Epoch: 4169, Train Loss: 0.5489, Test Loss: 3.9751\n",
      "Epoch: 4170, Train Loss: 0.5054, Test Loss: 3.5671\n",
      "Epoch: 4171, Train Loss: 0.5394, Test Loss: 4.0259\n",
      "Epoch: 4172, Train Loss: 0.5025, Test Loss: 4.0492\n",
      "Epoch: 4173, Train Loss: 0.5330, Test Loss: 3.3960\n",
      "Epoch: 4174, Train Loss: 0.5061, Test Loss: 3.5112\n",
      "Epoch: 4175, Train Loss: 0.5298, Test Loss: 4.0672\n",
      "Epoch: 4176, Train Loss: 0.5515, Test Loss: 4.4540\n",
      "Epoch: 4177, Train Loss: 0.5131, Test Loss: 4.0961\n",
      "Epoch: 4178, Train Loss: 0.5163, Test Loss: 3.6359\n",
      "Epoch: 4179, Train Loss: 0.5030, Test Loss: 3.7535\n",
      "Epoch: 4180, Train Loss: 0.4955, Test Loss: 3.9371\n",
      "Epoch: 4181, Train Loss: 0.4977, Test Loss: 3.7367\n",
      "Epoch: 4182, Train Loss: 0.5747, Test Loss: 3.7129\n",
      "Epoch: 4183, Train Loss: 0.5150, Test Loss: 4.0951\n",
      "Epoch: 4184, Train Loss: 0.5150, Test Loss: 3.7656\n",
      "Epoch: 4185, Train Loss: 0.5063, Test Loss: 3.7315\n",
      "Epoch: 4186, Train Loss: 0.4971, Test Loss: 4.2141\n",
      "Epoch: 4187, Train Loss: 0.5794, Test Loss: 3.9583\n",
      "Epoch: 4188, Train Loss: 0.5235, Test Loss: 3.5251\n",
      "Epoch: 4189, Train Loss: 0.4950, Test Loss: 3.5954\n",
      "Epoch: 4190, Train Loss: 0.5463, Test Loss: 4.3520\n",
      "Epoch: 4191, Train Loss: 0.5320, Test Loss: 4.2817\n",
      "Epoch: 4192, Train Loss: 0.5607, Test Loss: 3.9444\n",
      "Epoch: 4193, Train Loss: 0.4702, Test Loss: 3.6528\n",
      "Epoch: 4194, Train Loss: 0.5083, Test Loss: 3.7379\n",
      "Epoch: 4195, Train Loss: 0.4920, Test Loss: 4.2345\n",
      "Epoch: 4196, Train Loss: 0.4995, Test Loss: 4.2065\n",
      "Epoch: 4197, Train Loss: 0.5205, Test Loss: 3.6944\n",
      "Epoch: 4198, Train Loss: 0.4962, Test Loss: 3.7099\n",
      "Epoch: 4199, Train Loss: 0.5370, Test Loss: 4.2858\n",
      "Epoch: 4200, Train Loss: 0.5630, Test Loss: 3.8004\n",
      "Epoch: 4201, Train Loss: 0.4929, Test Loss: 3.7493\n",
      "Epoch: 4202, Train Loss: 0.5284, Test Loss: 4.6315\n",
      "Epoch: 4203, Train Loss: 0.5580, Test Loss: 4.6684\n",
      "Epoch: 4204, Train Loss: 0.5448, Test Loss: 3.7273\n",
      "Epoch: 4205, Train Loss: 0.4731, Test Loss: 3.1722\n",
      "Epoch: 4206, Train Loss: 0.6603, Test Loss: 4.1151\n",
      "Epoch: 4207, Train Loss: 0.4982, Test Loss: 5.2336\n",
      "Epoch: 4208, Train Loss: 0.6745, Test Loss: 4.0721\n",
      "Epoch: 4209, Train Loss: 0.5624, Test Loss: 3.1525\n",
      "Epoch: 4210, Train Loss: 0.7588, Test Loss: 3.8880\n",
      "Epoch: 4211, Train Loss: 0.5023, Test Loss: 5.1094\n",
      "Epoch: 4212, Train Loss: 0.7467, Test Loss: 3.9676\n",
      "Epoch: 4213, Train Loss: 0.5041, Test Loss: 3.4161\n",
      "Epoch: 4214, Train Loss: 0.5308, Test Loss: 3.7444\n",
      "Epoch: 4215, Train Loss: 0.5527, Test Loss: 4.7000\n",
      "Epoch: 4216, Train Loss: 0.5585, Test Loss: 4.6218\n",
      "Epoch: 4217, Train Loss: 0.5464, Test Loss: 3.7773\n",
      "Epoch: 4218, Train Loss: 0.4970, Test Loss: 3.5091\n",
      "Epoch: 4219, Train Loss: 0.5667, Test Loss: 4.1957\n",
      "Epoch: 4220, Train Loss: 0.5287, Test Loss: 4.8278\n",
      "Epoch: 4221, Train Loss: 0.6007, Test Loss: 4.3905\n",
      "Epoch: 4222, Train Loss: 0.5159, Test Loss: 3.8913\n",
      "Epoch: 4223, Train Loss: 0.5649, Test Loss: 4.5092\n",
      "Epoch: 4224, Train Loss: 0.5273, Test Loss: 4.6307\n",
      "Epoch: 4225, Train Loss: 0.4964, Test Loss: 4.0650\n",
      "Epoch: 4226, Train Loss: 0.6147, Test Loss: 4.2509\n",
      "Epoch: 4227, Train Loss: 0.5569, Test Loss: 3.7099\n",
      "Epoch: 4228, Train Loss: 0.5186, Test Loss: 3.8121\n",
      "Epoch: 4229, Train Loss: 0.5047, Test Loss: 4.2101\n",
      "Epoch: 4230, Train Loss: 0.5325, Test Loss: 4.4460\n",
      "Epoch: 4231, Train Loss: 0.5159, Test Loss: 4.1703\n",
      "Epoch: 4232, Train Loss: 0.5073, Test Loss: 3.8704\n",
      "Epoch: 4233, Train Loss: 0.5640, Test Loss: 4.2301\n",
      "Epoch: 4234, Train Loss: 0.4952, Test Loss: 4.4377\n",
      "Epoch: 4235, Train Loss: 0.5316, Test Loss: 3.7784\n",
      "Epoch: 4236, Train Loss: 0.5813, Test Loss: 3.8915\n",
      "Epoch: 4237, Train Loss: 0.4808, Test Loss: 4.2598\n",
      "Epoch: 4238, Train Loss: 0.4870, Test Loss: 4.5605\n",
      "Epoch: 4239, Train Loss: 0.5983, Test Loss: 3.7994\n",
      "Epoch: 4240, Train Loss: 0.4913, Test Loss: 3.5432\n",
      "Epoch: 4241, Train Loss: 0.6296, Test Loss: 4.6109\n",
      "Epoch: 4242, Train Loss: 0.5173, Test Loss: 4.9861\n",
      "Epoch: 4243, Train Loss: 0.6671, Test Loss: 3.4747\n",
      "Epoch: 4244, Train Loss: 0.5674, Test Loss: 3.2553\n",
      "Epoch: 4245, Train Loss: 0.6128, Test Loss: 4.2556\n",
      "Epoch: 4246, Train Loss: 0.5637, Test Loss: 4.8144\n",
      "Epoch: 4247, Train Loss: 0.6076, Test Loss: 3.7673\n",
      "Epoch: 4248, Train Loss: 0.5033, Test Loss: 3.1561\n",
      "Epoch: 4249, Train Loss: 0.6631, Test Loss: 3.8118\n",
      "Epoch: 4250, Train Loss: 0.4759, Test Loss: 5.1361\n",
      "Epoch: 4251, Train Loss: 0.7474, Test Loss: 4.1595\n",
      "Epoch: 4252, Train Loss: 0.5121, Test Loss: 3.2146\n",
      "Epoch: 4253, Train Loss: 0.6198, Test Loss: 3.4092\n",
      "Epoch: 4254, Train Loss: 0.5374, Test Loss: 4.1791\n",
      "Epoch: 4255, Train Loss: 0.5230, Test Loss: 4.3077\n",
      "Epoch: 4256, Train Loss: 0.5208, Test Loss: 3.6094\n",
      "Epoch: 4257, Train Loss: 0.5596, Test Loss: 3.5067\n",
      "Epoch: 4258, Train Loss: 0.5247, Test Loss: 3.9714\n",
      "Epoch: 4259, Train Loss: 0.5588, Test Loss: 4.0217\n",
      "Epoch: 4260, Train Loss: 0.5100, Test Loss: 4.1795\n",
      "Epoch: 4261, Train Loss: 0.5310, Test Loss: 4.2504\n",
      "Epoch: 4262, Train Loss: 0.5343, Test Loss: 3.8583\n",
      "Epoch: 4263, Train Loss: 0.5125, Test Loss: 3.7588\n",
      "Epoch: 4264, Train Loss: 0.5230, Test Loss: 4.5114\n",
      "Epoch: 4265, Train Loss: 0.5827, Test Loss: 4.0731\n",
      "Epoch: 4266, Train Loss: 0.5553, Test Loss: 3.5596\n",
      "Epoch: 4267, Train Loss: 0.5438, Test Loss: 4.0321\n",
      "Epoch: 4268, Train Loss: 0.5015, Test Loss: 4.3598\n",
      "Epoch: 4269, Train Loss: 0.5451, Test Loss: 3.6492\n",
      "Epoch: 4270, Train Loss: 0.5090, Test Loss: 3.6659\n",
      "Epoch: 4271, Train Loss: 0.5342, Test Loss: 4.2226\n",
      "Epoch: 4272, Train Loss: 0.5428, Test Loss: 4.7907\n",
      "Epoch: 4273, Train Loss: 0.6543, Test Loss: 3.5571\n",
      "Epoch: 4274, Train Loss: 0.5108, Test Loss: 3.2670\n",
      "Epoch: 4275, Train Loss: 0.6568, Test Loss: 4.1306\n",
      "Epoch: 4276, Train Loss: 0.5464, Test Loss: 4.4754\n",
      "Epoch: 4277, Train Loss: 0.5918, Test Loss: 3.8347\n",
      "Epoch: 4278, Train Loss: 0.5124, Test Loss: 3.7086\n",
      "Epoch: 4279, Train Loss: 0.4933, Test Loss: 4.1159\n",
      "Epoch: 4280, Train Loss: 0.5371, Test Loss: 4.3650\n",
      "Epoch: 4281, Train Loss: 0.5361, Test Loss: 4.3159\n",
      "Epoch: 4282, Train Loss: 0.5527, Test Loss: 3.8028\n",
      "Epoch: 4283, Train Loss: 0.5417, Test Loss: 3.5760\n",
      "Epoch: 4284, Train Loss: 0.5534, Test Loss: 4.4210\n",
      "Epoch: 4285, Train Loss: 0.5076, Test Loss: 4.5479\n",
      "Epoch: 4286, Train Loss: 0.5277, Test Loss: 3.9907\n",
      "Epoch: 4287, Train Loss: 0.4749, Test Loss: 3.5830\n",
      "Epoch: 4288, Train Loss: 0.5363, Test Loss: 3.6900\n",
      "Epoch: 4289, Train Loss: 0.5239, Test Loss: 4.5516\n",
      "Epoch: 4290, Train Loss: 0.6078, Test Loss: 4.1021\n",
      "Epoch: 4291, Train Loss: 0.5068, Test Loss: 3.6858\n",
      "Epoch: 4292, Train Loss: 0.5083, Test Loss: 3.4783\n",
      "Epoch: 4293, Train Loss: 0.5040, Test Loss: 3.9472\n",
      "Epoch: 4294, Train Loss: 0.5253, Test Loss: 4.4677\n",
      "Epoch: 4295, Train Loss: 0.5478, Test Loss: 4.1092\n",
      "Epoch: 4296, Train Loss: 0.5131, Test Loss: 3.7153\n",
      "Epoch: 4297, Train Loss: 0.4611, Test Loss: 3.7848\n",
      "Epoch: 4298, Train Loss: 0.5586, Test Loss: 4.7414\n",
      "Epoch: 4299, Train Loss: 0.6035, Test Loss: 3.9336\n",
      "Epoch: 4300, Train Loss: 0.5035, Test Loss: 3.4179\n",
      "Epoch: 4301, Train Loss: 0.5513, Test Loss: 3.8381\n",
      "Epoch: 4302, Train Loss: 0.5091, Test Loss: 5.1245\n",
      "Epoch: 4303, Train Loss: 0.6510, Test Loss: 4.1261\n",
      "Epoch: 4304, Train Loss: 0.4877, Test Loss: 3.2803\n",
      "Epoch: 4305, Train Loss: 0.6259, Test Loss: 3.7295\n",
      "Epoch: 4306, Train Loss: 0.4763, Test Loss: 4.8554\n",
      "Epoch: 4307, Train Loss: 0.5695, Test Loss: 4.6344\n",
      "Epoch: 4308, Train Loss: 0.5484, Test Loss: 3.4674\n",
      "Epoch: 4309, Train Loss: 0.5566, Test Loss: 3.3025\n",
      "Epoch: 4310, Train Loss: 0.6614, Test Loss: 4.4780\n",
      "Epoch: 4311, Train Loss: 0.5230, Test Loss: 5.3032\n",
      "Epoch: 4312, Train Loss: 0.6918, Test Loss: 3.7427\n",
      "Epoch: 4313, Train Loss: 0.5400, Test Loss: 3.4524\n",
      "Epoch: 4314, Train Loss: 0.5344, Test Loss: 3.8783\n",
      "Epoch: 4315, Train Loss: 0.4719, Test Loss: 4.5450\n",
      "Epoch: 4316, Train Loss: 0.5461, Test Loss: 4.2195\n",
      "Epoch: 4317, Train Loss: 0.5004, Test Loss: 3.7623\n",
      "Epoch: 4318, Train Loss: 0.5179, Test Loss: 3.9212\n",
      "Epoch: 4319, Train Loss: 0.5232, Test Loss: 3.8913\n",
      "Epoch: 4320, Train Loss: 0.4772, Test Loss: 3.8190\n",
      "Epoch: 4321, Train Loss: 0.5048, Test Loss: 4.0662\n",
      "Epoch: 4322, Train Loss: 0.4928, Test Loss: 4.1765\n",
      "Epoch: 4323, Train Loss: 0.4859, Test Loss: 3.8757\n",
      "Epoch: 4324, Train Loss: 0.5303, Test Loss: 4.2541\n",
      "Epoch: 4325, Train Loss: 0.5412, Test Loss: 3.8954\n",
      "Epoch: 4326, Train Loss: 0.4758, Test Loss: 3.5341\n",
      "Epoch: 4327, Train Loss: 0.5690, Test Loss: 3.8479\n",
      "Epoch: 4328, Train Loss: 0.4687, Test Loss: 4.1971\n",
      "Epoch: 4329, Train Loss: 0.4613, Test Loss: 4.4573\n",
      "Epoch: 4330, Train Loss: 0.5032, Test Loss: 4.2150\n",
      "Epoch: 4331, Train Loss: 0.4711, Test Loss: 3.7935\n",
      "Epoch: 4332, Train Loss: 0.5056, Test Loss: 3.9238\n",
      "Epoch: 4333, Train Loss: 0.5641, Test Loss: 4.1572\n",
      "Epoch: 4334, Train Loss: 0.5270, Test Loss: 3.8292\n",
      "Epoch: 4335, Train Loss: 0.4930, Test Loss: 3.9491\n",
      "Epoch: 4336, Train Loss: 0.4806, Test Loss: 3.9809\n",
      "Epoch: 4337, Train Loss: 0.4624, Test Loss: 3.8740\n",
      "Epoch: 4338, Train Loss: 0.4851, Test Loss: 3.7363\n",
      "Epoch: 4339, Train Loss: 0.5254, Test Loss: 3.9253\n",
      "Epoch: 4340, Train Loss: 0.4735, Test Loss: 4.2188\n",
      "Epoch: 4341, Train Loss: 0.4965, Test Loss: 4.5606\n",
      "Epoch: 4342, Train Loss: 0.5310, Test Loss: 4.0929\n",
      "Epoch: 4343, Train Loss: 0.4770, Test Loss: 3.9027\n",
      "Epoch: 4344, Train Loss: 0.5138, Test Loss: 3.7887\n",
      "Epoch: 4345, Train Loss: 0.4917, Test Loss: 4.3170\n",
      "Epoch: 4346, Train Loss: 0.5367, Test Loss: 3.9049\n",
      "Epoch: 4347, Train Loss: 0.5455, Test Loss: 4.3983\n",
      "Epoch: 4348, Train Loss: 0.5061, Test Loss: 4.4163\n",
      "Epoch: 4349, Train Loss: 0.4991, Test Loss: 3.6708\n",
      "Epoch: 4350, Train Loss: 0.4557, Test Loss: 3.1659\n",
      "Epoch: 4351, Train Loss: 0.6967, Test Loss: 4.1781\n",
      "Epoch: 4352, Train Loss: 0.5014, Test Loss: 4.8217\n",
      "Epoch: 4353, Train Loss: 0.5585, Test Loss: 3.8963\n",
      "Epoch: 4354, Train Loss: 0.5125, Test Loss: 3.0271\n",
      "Epoch: 4355, Train Loss: 0.7268, Test Loss: 3.6229\n",
      "Epoch: 4356, Train Loss: 0.5379, Test Loss: 5.4205\n",
      "Epoch: 4357, Train Loss: 0.7721, Test Loss: 4.3904\n",
      "Epoch: 4358, Train Loss: 0.5148, Test Loss: 3.2526\n",
      "Epoch: 4359, Train Loss: 0.6153, Test Loss: 3.4316\n",
      "Epoch: 4360, Train Loss: 0.5175, Test Loss: 4.3607\n",
      "Epoch: 4361, Train Loss: 0.5099, Test Loss: 4.6031\n",
      "Epoch: 4362, Train Loss: 0.4867, Test Loss: 4.0637\n",
      "Epoch: 4363, Train Loss: 0.5009, Test Loss: 3.6855\n",
      "Epoch: 4364, Train Loss: 0.6118, Test Loss: 4.6170\n",
      "Epoch: 4365, Train Loss: 0.6421, Test Loss: 4.0664\n",
      "Epoch: 4366, Train Loss: 0.4783, Test Loss: 3.6821\n",
      "Epoch: 4367, Train Loss: 0.5177, Test Loss: 4.1226\n",
      "Epoch: 4368, Train Loss: 0.4950, Test Loss: 4.5103\n",
      "Epoch: 4369, Train Loss: 0.5323, Test Loss: 4.1320\n",
      "Epoch: 4370, Train Loss: 0.5064, Test Loss: 3.8961\n",
      "Epoch: 4371, Train Loss: 0.5004, Test Loss: 3.7292\n",
      "Epoch: 4372, Train Loss: 0.5058, Test Loss: 4.1281\n",
      "Epoch: 4373, Train Loss: 0.5037, Test Loss: 4.4679\n",
      "Epoch: 4374, Train Loss: 0.5390, Test Loss: 4.3977\n",
      "Epoch: 4375, Train Loss: 0.4864, Test Loss: 4.0591\n",
      "Epoch: 4376, Train Loss: 0.5064, Test Loss: 4.0288\n",
      "Epoch: 4377, Train Loss: 0.5400, Test Loss: 4.0092\n",
      "Epoch: 4378, Train Loss: 0.5105, Test Loss: 4.1027\n",
      "Epoch: 4379, Train Loss: 0.4804, Test Loss: 4.2014\n",
      "Epoch: 4380, Train Loss: 0.4744, Test Loss: 3.9416\n",
      "Epoch: 4381, Train Loss: 0.5214, Test Loss: 3.8251\n",
      "Epoch: 4382, Train Loss: 0.5148, Test Loss: 4.2376\n",
      "Epoch: 4383, Train Loss: 0.4788, Test Loss: 3.9572\n",
      "Epoch: 4384, Train Loss: 0.4928, Test Loss: 3.6333\n",
      "Epoch: 4385, Train Loss: 0.5490, Test Loss: 4.2200\n",
      "Epoch: 4386, Train Loss: 0.4886, Test Loss: 4.5826\n",
      "Epoch: 4387, Train Loss: 0.5153, Test Loss: 3.9417\n",
      "Epoch: 4388, Train Loss: 0.5392, Test Loss: 3.3741\n",
      "Epoch: 4389, Train Loss: 0.5688, Test Loss: 4.0651\n",
      "Epoch: 4390, Train Loss: 0.4916, Test Loss: 5.1579\n",
      "Epoch: 4391, Train Loss: 0.6139, Test Loss: 4.1361\n",
      "Epoch: 4392, Train Loss: 0.4896, Test Loss: 3.1454\n",
      "Epoch: 4393, Train Loss: 0.7009, Test Loss: 3.7894\n",
      "Epoch: 4394, Train Loss: 0.4842, Test Loss: 4.9790\n",
      "Epoch: 4395, Train Loss: 0.5951, Test Loss: 4.4522\n",
      "Epoch: 4396, Train Loss: 0.5039, Test Loss: 3.4181\n",
      "Epoch: 4397, Train Loss: 0.5653, Test Loss: 3.5538\n",
      "Epoch: 4398, Train Loss: 0.5798, Test Loss: 4.7372\n",
      "Epoch: 4399, Train Loss: 0.5635, Test Loss: 4.5499\n",
      "Epoch: 4400, Train Loss: 0.5226, Test Loss: 3.6914\n",
      "Epoch: 4401, Train Loss: 0.5186, Test Loss: 3.2751\n",
      "Epoch: 4402, Train Loss: 0.6803, Test Loss: 4.2910\n",
      "Epoch: 4403, Train Loss: 0.5194, Test Loss: 4.7493\n",
      "Epoch: 4404, Train Loss: 0.5507, Test Loss: 3.8278\n",
      "Epoch: 4405, Train Loss: 0.4913, Test Loss: 3.4890\n",
      "Epoch: 4406, Train Loss: 0.5622, Test Loss: 3.8094\n",
      "Epoch: 4407, Train Loss: 0.5412, Test Loss: 4.4561\n",
      "Epoch: 4408, Train Loss: 0.5329, Test Loss: 4.3540\n",
      "Epoch: 4409, Train Loss: 0.5092, Test Loss: 3.8785\n",
      "Epoch: 4410, Train Loss: 0.5178, Test Loss: 3.8261\n",
      "Epoch: 4411, Train Loss: 0.5228, Test Loss: 4.3143\n",
      "Epoch: 4412, Train Loss: 0.5040, Test Loss: 4.1443\n",
      "Epoch: 4413, Train Loss: 0.4860, Test Loss: 3.8527\n",
      "Epoch: 4414, Train Loss: 0.4880, Test Loss: 3.7444\n",
      "Epoch: 4415, Train Loss: 0.4995, Test Loss: 3.9531\n",
      "Epoch: 4416, Train Loss: 0.5007, Test Loss: 4.6538\n",
      "Epoch: 4417, Train Loss: 0.5306, Test Loss: 3.9543\n",
      "Epoch: 4418, Train Loss: 0.5146, Test Loss: 3.5395\n",
      "Epoch: 4419, Train Loss: 0.5320, Test Loss: 4.1689\n",
      "Epoch: 4420, Train Loss: 0.4615, Test Loss: 4.5433\n",
      "Epoch: 4421, Train Loss: 0.5042, Test Loss: 4.0892\n",
      "Epoch: 4422, Train Loss: 0.4777, Test Loss: 3.7446\n",
      "Epoch: 4423, Train Loss: 0.4823, Test Loss: 3.3736\n",
      "Epoch: 4424, Train Loss: 0.5290, Test Loss: 3.8127\n",
      "Epoch: 4425, Train Loss: 0.4662, Test Loss: 4.3950\n",
      "Epoch: 4426, Train Loss: 0.5062, Test Loss: 4.0466\n",
      "Epoch: 4427, Train Loss: 0.4747, Test Loss: 3.4619\n",
      "Epoch: 4428, Train Loss: 0.5846, Test Loss: 4.0311\n",
      "Epoch: 4429, Train Loss: 0.5002, Test Loss: 4.3364\n",
      "Epoch: 4430, Train Loss: 0.5187, Test Loss: 3.7605\n",
      "Epoch: 4431, Train Loss: 0.4861, Test Loss: 3.5688\n",
      "Epoch: 4432, Train Loss: 0.5172, Test Loss: 3.7364\n",
      "Epoch: 4433, Train Loss: 0.4907, Test Loss: 4.3985\n",
      "Epoch: 4434, Train Loss: 0.5265, Test Loss: 3.9515\n",
      "Epoch: 4435, Train Loss: 0.5455, Test Loss: 3.4491\n",
      "Epoch: 4436, Train Loss: 0.6176, Test Loss: 4.2336\n",
      "Epoch: 4437, Train Loss: 0.5226, Test Loss: 4.8746\n",
      "Epoch: 4438, Train Loss: 0.5598, Test Loss: 3.9646\n",
      "Epoch: 4439, Train Loss: 0.5033, Test Loss: 3.7307\n",
      "Epoch: 4440, Train Loss: 0.5150, Test Loss: 3.7971\n",
      "Epoch: 4441, Train Loss: 0.4889, Test Loss: 3.9914\n",
      "Epoch: 4442, Train Loss: 0.5021, Test Loss: 4.2502\n",
      "Epoch: 4443, Train Loss: 0.4853, Test Loss: 4.2628\n",
      "Epoch: 4444, Train Loss: 0.5167, Test Loss: 3.5338\n",
      "Epoch: 4445, Train Loss: 0.5328, Test Loss: 3.1937\n",
      "Epoch: 4446, Train Loss: 0.6484, Test Loss: 4.2293\n",
      "Epoch: 4447, Train Loss: 0.4734, Test Loss: 4.9344\n",
      "Epoch: 4448, Train Loss: 0.5887, Test Loss: 3.9904\n",
      "Epoch: 4449, Train Loss: 0.5029, Test Loss: 3.4369\n",
      "Epoch: 4450, Train Loss: 0.5090, Test Loss: 3.5414\n",
      "Epoch: 4451, Train Loss: 0.5004, Test Loss: 4.2474\n",
      "Epoch: 4452, Train Loss: 0.5508, Test Loss: 4.1369\n",
      "Epoch: 4453, Train Loss: 0.5017, Test Loss: 3.4598\n",
      "Epoch: 4454, Train Loss: 0.5325, Test Loss: 3.6552\n",
      "Epoch: 4455, Train Loss: 0.4887, Test Loss: 4.2928\n",
      "Epoch: 4456, Train Loss: 0.4946, Test Loss: 4.6651\n",
      "Epoch: 4457, Train Loss: 0.5140, Test Loss: 3.9699\n",
      "Epoch: 4458, Train Loss: 0.5012, Test Loss: 3.2623\n",
      "Epoch: 4459, Train Loss: 0.5966, Test Loss: 3.8790\n",
      "Epoch: 4460, Train Loss: 0.5014, Test Loss: 5.0890\n",
      "Epoch: 4461, Train Loss: 0.7487, Test Loss: 3.7421\n",
      "Epoch: 4462, Train Loss: 0.4943, Test Loss: 3.2888\n",
      "Epoch: 4463, Train Loss: 0.6393, Test Loss: 3.9246\n",
      "Epoch: 4464, Train Loss: 0.5368, Test Loss: 4.1639\n",
      "Epoch: 4465, Train Loss: 0.4916, Test Loss: 3.9910\n",
      "Epoch: 4466, Train Loss: 0.4901, Test Loss: 3.4598\n",
      "Epoch: 4467, Train Loss: 0.5303, Test Loss: 3.5917\n",
      "Epoch: 4468, Train Loss: 0.5234, Test Loss: 4.1186\n",
      "Epoch: 4469, Train Loss: 0.5650, Test Loss: 3.7428\n",
      "Epoch: 4470, Train Loss: 0.4940, Test Loss: 3.7077\n",
      "Epoch: 4471, Train Loss: 0.4829, Test Loss: 4.1295\n",
      "Epoch: 4472, Train Loss: 0.4698, Test Loss: 4.3177\n",
      "Epoch: 4473, Train Loss: 0.4928, Test Loss: 3.8300\n",
      "Epoch: 4474, Train Loss: 0.4668, Test Loss: 3.5144\n",
      "Epoch: 4475, Train Loss: 0.5056, Test Loss: 3.8583\n",
      "Epoch: 4476, Train Loss: 0.4891, Test Loss: 4.3487\n",
      "Epoch: 4477, Train Loss: 0.5299, Test Loss: 3.7016\n",
      "Epoch: 4478, Train Loss: 0.4874, Test Loss: 3.4475\n",
      "Epoch: 4479, Train Loss: 0.4928, Test Loss: 3.8735\n",
      "Epoch: 4480, Train Loss: 0.4781, Test Loss: 4.3689\n",
      "Epoch: 4481, Train Loss: 0.5122, Test Loss: 4.1018\n",
      "Epoch: 4482, Train Loss: 0.4989, Test Loss: 3.4842\n",
      "Epoch: 4483, Train Loss: 0.4783, Test Loss: 3.5981\n",
      "Epoch: 4484, Train Loss: 0.4999, Test Loss: 4.4561\n",
      "Epoch: 4485, Train Loss: 0.5699, Test Loss: 4.0912\n",
      "Epoch: 4486, Train Loss: 0.4910, Test Loss: 3.5124\n",
      "Epoch: 4487, Train Loss: 0.4998, Test Loss: 3.6609\n",
      "Epoch: 4488, Train Loss: 0.5259, Test Loss: 4.0166\n",
      "Epoch: 4489, Train Loss: 0.5034, Test Loss: 4.1965\n",
      "Epoch: 4490, Train Loss: 0.4780, Test Loss: 4.0005\n",
      "Epoch: 4491, Train Loss: 0.5140, Test Loss: 3.9726\n",
      "Epoch: 4492, Train Loss: 0.4879, Test Loss: 3.6318\n",
      "Epoch: 4493, Train Loss: 0.6470, Test Loss: 4.6215\n",
      "Epoch: 4494, Train Loss: 0.5615, Test Loss: 4.1159\n",
      "Epoch: 4495, Train Loss: 0.4851, Test Loss: 3.7056\n",
      "Epoch: 4496, Train Loss: 0.5095, Test Loss: 3.7753\n",
      "Epoch: 4497, Train Loss: 0.5031, Test Loss: 4.0597\n",
      "Epoch: 4498, Train Loss: 0.4818, Test Loss: 4.1227\n",
      "Epoch: 4499, Train Loss: 0.5219, Test Loss: 3.9110\n",
      "Epoch: 4500, Train Loss: 0.5558, Test Loss: 4.2391\n",
      "Epoch: 4501, Train Loss: 0.4912, Test Loss: 3.8154\n",
      "Epoch: 4502, Train Loss: 0.4750, Test Loss: 3.7393\n",
      "Epoch: 4503, Train Loss: 0.5314, Test Loss: 3.7454\n",
      "Epoch: 4504, Train Loss: 0.5012, Test Loss: 4.2012\n",
      "Epoch: 4505, Train Loss: 0.5275, Test Loss: 4.0029\n",
      "Epoch: 4506, Train Loss: 0.5246, Test Loss: 3.8001\n",
      "Epoch: 4507, Train Loss: 0.4707, Test Loss: 3.7349\n",
      "Epoch: 4508, Train Loss: 0.4759, Test Loss: 3.9486\n",
      "Epoch: 4509, Train Loss: 0.4620, Test Loss: 4.3905\n",
      "Epoch: 4510, Train Loss: 0.5373, Test Loss: 3.7628\n",
      "Epoch: 4511, Train Loss: 0.4911, Test Loss: 3.6836\n",
      "Epoch: 4512, Train Loss: 0.4992, Test Loss: 4.0585\n",
      "Epoch: 4513, Train Loss: 0.4841, Test Loss: 3.8969\n",
      "Epoch: 4514, Train Loss: 0.5026, Test Loss: 3.8200\n",
      "Epoch: 4515, Train Loss: 0.4709, Test Loss: 4.0149\n",
      "Epoch: 4516, Train Loss: 0.5065, Test Loss: 3.9882\n",
      "Epoch: 4517, Train Loss: 0.5163, Test Loss: 3.7453\n",
      "Epoch: 4518, Train Loss: 0.5299, Test Loss: 4.0746\n",
      "Epoch: 4519, Train Loss: 0.5240, Test Loss: 4.0790\n",
      "Epoch: 4520, Train Loss: 0.5307, Test Loss: 3.9065\n",
      "Epoch: 4521, Train Loss: 0.4941, Test Loss: 4.0395\n",
      "Epoch: 4522, Train Loss: 0.4681, Test Loss: 4.1571\n",
      "Epoch: 4523, Train Loss: 0.4854, Test Loss: 4.0288\n",
      "Epoch: 4524, Train Loss: 0.4803, Test Loss: 4.1572\n",
      "Epoch: 4525, Train Loss: 0.4857, Test Loss: 4.0879\n",
      "Epoch: 4526, Train Loss: 0.4806, Test Loss: 4.0298\n",
      "Epoch: 4527, Train Loss: 0.5058, Test Loss: 3.9511\n",
      "Epoch: 4528, Train Loss: 0.4865, Test Loss: 3.8907\n",
      "Epoch: 4529, Train Loss: 0.4992, Test Loss: 3.5793\n",
      "Epoch: 4530, Train Loss: 0.4732, Test Loss: 3.7012\n",
      "Epoch: 4531, Train Loss: 0.5054, Test Loss: 4.0919\n",
      "Epoch: 4532, Train Loss: 0.5364, Test Loss: 3.8346\n",
      "Epoch: 4533, Train Loss: 0.4641, Test Loss: 3.7351\n",
      "Epoch: 4534, Train Loss: 0.4913, Test Loss: 4.1001\n",
      "Epoch: 4535, Train Loss: 0.5117, Test Loss: 4.2424\n",
      "Epoch: 4536, Train Loss: 0.5336, Test Loss: 3.9208\n",
      "Epoch: 4537, Train Loss: 0.4640, Test Loss: 3.9789\n",
      "Epoch: 4538, Train Loss: 0.4912, Test Loss: 3.8352\n",
      "Epoch: 4539, Train Loss: 0.5104, Test Loss: 3.6963\n",
      "Epoch: 4540, Train Loss: 0.5495, Test Loss: 4.2031\n",
      "Epoch: 4541, Train Loss: 0.4951, Test Loss: 3.9134\n",
      "Epoch: 4542, Train Loss: 0.4616, Test Loss: 3.7980\n",
      "Epoch: 4543, Train Loss: 0.5405, Test Loss: 4.2141\n",
      "Epoch: 4544, Train Loss: 0.5352, Test Loss: 4.7110\n",
      "Epoch: 4545, Train Loss: 0.5708, Test Loss: 3.6697\n",
      "Epoch: 4546, Train Loss: 0.4804, Test Loss: 3.4043\n",
      "Epoch: 4547, Train Loss: 0.5110, Test Loss: 3.7997\n",
      "Epoch: 4548, Train Loss: 0.5037, Test Loss: 4.8621\n",
      "Epoch: 4549, Train Loss: 0.6844, Test Loss: 3.8107\n",
      "Epoch: 4550, Train Loss: 0.4699, Test Loss: 3.2024\n",
      "Epoch: 4551, Train Loss: 0.5845, Test Loss: 3.8790\n",
      "Epoch: 4552, Train Loss: 0.5027, Test Loss: 4.3722\n",
      "Epoch: 4553, Train Loss: 0.5086, Test Loss: 3.8808\n",
      "Epoch: 4554, Train Loss: 0.4824, Test Loss: 3.6096\n",
      "Epoch: 4555, Train Loss: 0.4676, Test Loss: 3.5434\n",
      "Epoch: 4556, Train Loss: 0.5832, Test Loss: 4.3935\n",
      "Epoch: 4557, Train Loss: 0.6045, Test Loss: 4.1222\n",
      "Epoch: 4558, Train Loss: 0.4694, Test Loss: 3.5550\n",
      "Epoch: 4559, Train Loss: 0.5475, Test Loss: 3.6214\n",
      "Epoch: 4560, Train Loss: 0.5073, Test Loss: 4.4685\n",
      "Epoch: 4561, Train Loss: 0.4991, Test Loss: 4.6665\n",
      "Epoch: 4562, Train Loss: 0.5761, Test Loss: 3.9063\n",
      "Epoch: 4563, Train Loss: 0.4789, Test Loss: 3.2555\n",
      "Epoch: 4564, Train Loss: 0.6222, Test Loss: 3.8724\n",
      "Epoch: 4565, Train Loss: 0.4929, Test Loss: 4.2010\n",
      "Epoch: 4566, Train Loss: 0.4632, Test Loss: 3.9206\n",
      "Epoch: 4567, Train Loss: 0.4994, Test Loss: 4.0031\n",
      "Epoch: 4568, Train Loss: 0.4800, Test Loss: 4.2534\n",
      "Epoch: 4569, Train Loss: 0.5388, Test Loss: 3.7267\n",
      "Epoch: 4570, Train Loss: 0.4870, Test Loss: 3.8865\n",
      "Epoch: 4571, Train Loss: 0.4598, Test Loss: 4.2837\n",
      "Epoch: 4572, Train Loss: 0.5440, Test Loss: 3.8355\n",
      "Epoch: 4573, Train Loss: 0.5067, Test Loss: 3.4284\n",
      "Epoch: 4574, Train Loss: 0.5127, Test Loss: 4.0113\n",
      "Epoch: 4575, Train Loss: 0.5004, Test Loss: 5.2864\n",
      "Epoch: 4576, Train Loss: 0.7024, Test Loss: 3.9552\n",
      "Epoch: 4577, Train Loss: 0.4759, Test Loss: 3.1262\n",
      "Epoch: 4578, Train Loss: 0.7726, Test Loss: 4.0148\n",
      "Epoch: 4579, Train Loss: 0.4688, Test Loss: 5.2750\n",
      "Epoch: 4580, Train Loss: 0.6856, Test Loss: 4.1004\n",
      "Epoch: 4581, Train Loss: 0.5311, Test Loss: 3.0434\n",
      "Epoch: 4582, Train Loss: 0.7452, Test Loss: 3.5047\n",
      "Epoch: 4583, Train Loss: 0.4837, Test Loss: 4.5828\n",
      "Epoch: 4584, Train Loss: 0.5354, Test Loss: 4.8604\n",
      "Epoch: 4585, Train Loss: 0.6641, Test Loss: 3.3965\n",
      "Epoch: 4586, Train Loss: 0.5194, Test Loss: 2.9890\n",
      "Epoch: 4587, Train Loss: 0.7183, Test Loss: 3.6460\n",
      "Epoch: 4588, Train Loss: 0.4745, Test Loss: 4.7384\n",
      "Epoch: 4589, Train Loss: 0.6476, Test Loss: 4.0411\n",
      "Epoch: 4590, Train Loss: 0.5749, Test Loss: 3.0407\n",
      "Epoch: 4591, Train Loss: 0.6541, Test Loss: 3.3898\n",
      "Epoch: 4592, Train Loss: 0.5362, Test Loss: 4.6229\n",
      "Epoch: 4593, Train Loss: 0.6163, Test Loss: 4.1575\n",
      "Epoch: 4594, Train Loss: 0.5299, Test Loss: 3.1376\n",
      "Epoch: 4595, Train Loss: 0.6422, Test Loss: 3.3858\n",
      "Epoch: 4596, Train Loss: 0.4611, Test Loss: 4.0806\n",
      "Epoch: 4597, Train Loss: 0.5106, Test Loss: 4.7099\n",
      "Epoch: 4598, Train Loss: 0.5815, Test Loss: 3.8720\n",
      "Epoch: 4599, Train Loss: 0.5053, Test Loss: 3.5209\n",
      "Epoch: 4600, Train Loss: 0.5219, Test Loss: 3.8303\n",
      "Epoch: 4601, Train Loss: 0.5369, Test Loss: 4.0771\n",
      "Epoch: 4602, Train Loss: 0.5525, Test Loss: 3.8748\n",
      "Epoch: 4603, Train Loss: 0.4978, Test Loss: 3.5148\n",
      "Epoch: 4604, Train Loss: 0.5241, Test Loss: 3.6676\n",
      "Epoch: 4605, Train Loss: 0.5113, Test Loss: 4.6745\n",
      "Epoch: 4606, Train Loss: 0.5879, Test Loss: 3.9845\n",
      "Epoch: 4607, Train Loss: 0.5063, Test Loss: 3.2870\n",
      "Epoch: 4608, Train Loss: 0.5363, Test Loss: 3.5030\n",
      "Epoch: 4609, Train Loss: 0.5659, Test Loss: 4.7202\n",
      "Epoch: 4610, Train Loss: 0.7042, Test Loss: 3.9462\n",
      "Epoch: 4611, Train Loss: 0.5335, Test Loss: 3.1883\n",
      "Epoch: 4612, Train Loss: 0.6184, Test Loss: 3.5703\n",
      "Epoch: 4613, Train Loss: 0.5851, Test Loss: 4.5795\n",
      "Epoch: 4614, Train Loss: 0.6607, Test Loss: 4.0052\n",
      "Epoch: 4615, Train Loss: 0.5008, Test Loss: 3.3831\n",
      "Epoch: 4616, Train Loss: 0.5594, Test Loss: 3.6526\n",
      "Epoch: 4617, Train Loss: 0.4729, Test Loss: 4.0924\n",
      "Epoch: 4618, Train Loss: 0.4786, Test Loss: 4.1535\n",
      "Epoch: 4619, Train Loss: 0.5072, Test Loss: 3.7485\n",
      "Epoch: 4620, Train Loss: 0.5085, Test Loss: 3.5418\n",
      "Epoch: 4621, Train Loss: 0.5272, Test Loss: 3.8238\n",
      "Epoch: 4622, Train Loss: 0.5304, Test Loss: 4.1899\n",
      "Epoch: 4623, Train Loss: 0.4907, Test Loss: 4.1448\n",
      "Epoch: 4624, Train Loss: 0.5341, Test Loss: 3.6200\n",
      "Epoch: 4625, Train Loss: 0.5202, Test Loss: 3.6186\n",
      "Epoch: 4626, Train Loss: 0.4914, Test Loss: 4.0690\n",
      "Epoch: 4627, Train Loss: 0.4744, Test Loss: 4.1675\n",
      "Epoch: 4628, Train Loss: 0.5380, Test Loss: 4.2372\n",
      "Epoch: 4629, Train Loss: 0.5301, Test Loss: 3.5565\n",
      "Epoch: 4630, Train Loss: 0.4732, Test Loss: 3.4310\n",
      "Epoch: 4631, Train Loss: 0.5097, Test Loss: 3.7694\n",
      "Epoch: 4632, Train Loss: 0.5180, Test Loss: 4.5215\n",
      "Epoch: 4633, Train Loss: 0.5701, Test Loss: 4.0911\n",
      "Epoch: 4634, Train Loss: 0.5235, Test Loss: 3.3822\n",
      "Epoch: 4635, Train Loss: 0.5939, Test Loss: 3.7605\n",
      "Epoch: 4636, Train Loss: 0.5212, Test Loss: 4.5524\n",
      "Epoch: 4637, Train Loss: 0.6081, Test Loss: 3.8075\n",
      "Epoch: 4638, Train Loss: 0.4936, Test Loss: 3.3215\n",
      "Epoch: 4639, Train Loss: 0.5539, Test Loss: 3.8057\n",
      "Epoch: 4640, Train Loss: 0.4643, Test Loss: 4.1904\n",
      "Epoch: 4641, Train Loss: 0.4910, Test Loss: 3.9070\n",
      "Epoch: 4642, Train Loss: 0.4620, Test Loss: 3.7554\n",
      "Epoch: 4643, Train Loss: 0.4764, Test Loss: 3.7200\n",
      "Epoch: 4644, Train Loss: 0.4678, Test Loss: 3.8040\n",
      "Epoch: 4645, Train Loss: 0.4562, Test Loss: 4.2119\n",
      "Epoch: 4646, Train Loss: 0.5140, Test Loss: 3.7042\n",
      "Epoch: 4647, Train Loss: 0.4669, Test Loss: 3.5242\n",
      "Epoch: 4648, Train Loss: 0.5526, Test Loss: 4.0134\n",
      "Epoch: 4649, Train Loss: 0.4639, Test Loss: 3.9967\n",
      "Epoch: 4650, Train Loss: 0.5005, Test Loss: 3.4942\n",
      "Epoch: 4651, Train Loss: 0.5481, Test Loss: 3.4874\n",
      "Epoch: 4652, Train Loss: 0.4827, Test Loss: 4.1297\n",
      "Epoch: 4653, Train Loss: 0.4761, Test Loss: 4.1732\n",
      "Epoch: 4654, Train Loss: 0.4699, Test Loss: 3.8072\n",
      "Epoch: 4655, Train Loss: 0.4789, Test Loss: 3.6003\n",
      "Epoch: 4656, Train Loss: 0.5022, Test Loss: 3.9322\n",
      "Epoch: 4657, Train Loss: 0.4566, Test Loss: 4.4039\n",
      "Epoch: 4658, Train Loss: 0.5349, Test Loss: 3.7493\n",
      "Epoch: 4659, Train Loss: 0.5205, Test Loss: 3.8684\n",
      "Epoch: 4660, Train Loss: 0.4474, Test Loss: 4.0749\n",
      "Epoch: 4661, Train Loss: 0.4742, Test Loss: 4.4003\n",
      "Epoch: 4662, Train Loss: 0.4991, Test Loss: 3.9624\n",
      "Epoch: 4663, Train Loss: 0.4785, Test Loss: 3.5118\n",
      "Epoch: 4664, Train Loss: 0.5396, Test Loss: 4.0377\n",
      "Epoch: 4665, Train Loss: 0.4580, Test Loss: 4.4787\n",
      "Epoch: 4666, Train Loss: 0.5569, Test Loss: 3.7627\n",
      "Epoch: 4667, Train Loss: 0.4927, Test Loss: 3.3487\n",
      "Epoch: 4668, Train Loss: 0.4888, Test Loss: 3.6397\n",
      "Epoch: 4669, Train Loss: 0.4891, Test Loss: 4.4601\n",
      "Epoch: 4670, Train Loss: 0.5550, Test Loss: 4.0979\n",
      "Epoch: 4671, Train Loss: 0.5013, Test Loss: 3.6758\n",
      "Epoch: 4672, Train Loss: 0.4956, Test Loss: 3.7173\n",
      "Epoch: 4673, Train Loss: 0.4729, Test Loss: 4.1189\n",
      "Epoch: 4674, Train Loss: 0.5017, Test Loss: 4.0152\n",
      "Epoch: 4675, Train Loss: 0.4594, Test Loss: 3.7495\n",
      "Epoch: 4676, Train Loss: 0.4911, Test Loss: 3.5491\n",
      "Epoch: 4677, Train Loss: 0.5239, Test Loss: 3.8565\n",
      "Epoch: 4678, Train Loss: 0.4985, Test Loss: 4.3774\n",
      "Epoch: 4679, Train Loss: 0.4944, Test Loss: 3.8158\n",
      "Epoch: 4680, Train Loss: 0.5033, Test Loss: 3.3213\n",
      "Epoch: 4681, Train Loss: 0.5354, Test Loss: 3.8427\n",
      "Epoch: 4682, Train Loss: 0.4738, Test Loss: 4.7207\n",
      "Epoch: 4683, Train Loss: 0.5746, Test Loss: 4.0614\n",
      "Epoch: 4684, Train Loss: 0.4696, Test Loss: 3.2981\n",
      "Epoch: 4685, Train Loss: 0.5606, Test Loss: 3.5159\n",
      "Epoch: 4686, Train Loss: 0.5925, Test Loss: 5.1182\n",
      "Epoch: 4687, Train Loss: 0.6240, Test Loss: 4.5934\n",
      "Epoch: 4688, Train Loss: 0.5829, Test Loss: 3.2834\n",
      "Epoch: 4689, Train Loss: 0.5700, Test Loss: 3.1782\n",
      "Epoch: 4690, Train Loss: 0.5354, Test Loss: 4.0449\n",
      "Epoch: 4691, Train Loss: 0.4989, Test Loss: 4.8454\n",
      "Epoch: 4692, Train Loss: 0.6740, Test Loss: 3.5714\n",
      "Epoch: 4693, Train Loss: 0.5012, Test Loss: 3.2849\n",
      "Epoch: 4694, Train Loss: 0.5546, Test Loss: 4.0262\n",
      "Epoch: 4695, Train Loss: 0.4766, Test Loss: 4.3348\n",
      "Epoch: 4696, Train Loss: 0.5420, Test Loss: 3.6548\n",
      "Epoch: 4697, Train Loss: 0.4792, Test Loss: 3.3413\n",
      "Epoch: 4698, Train Loss: 0.4994, Test Loss: 3.6691\n",
      "Epoch: 4699, Train Loss: 0.4925, Test Loss: 4.3976\n",
      "Epoch: 4700, Train Loss: 0.4913, Test Loss: 4.0747\n",
      "Epoch: 4701, Train Loss: 0.4437, Test Loss: 3.6615\n",
      "Epoch: 4702, Train Loss: 0.4554, Test Loss: 3.4285\n",
      "Epoch: 4703, Train Loss: 0.5732, Test Loss: 4.4161\n",
      "Epoch: 4704, Train Loss: 0.5592, Test Loss: 4.1971\n",
      "Epoch: 4705, Train Loss: 0.5252, Test Loss: 3.2302\n",
      "Epoch: 4706, Train Loss: 0.6178, Test Loss: 3.6851\n",
      "Epoch: 4707, Train Loss: 0.4727, Test Loss: 4.1698\n",
      "Epoch: 4708, Train Loss: 0.5206, Test Loss: 3.8396\n",
      "Epoch: 4709, Train Loss: 0.4710, Test Loss: 3.1531\n",
      "Epoch: 4710, Train Loss: 0.5788, Test Loss: 3.4375\n",
      "Epoch: 4711, Train Loss: 0.4918, Test Loss: 4.1535\n",
      "Epoch: 4712, Train Loss: 0.5060, Test Loss: 4.0631\n",
      "Epoch: 4713, Train Loss: 0.5205, Test Loss: 3.2149\n",
      "Epoch: 4714, Train Loss: 0.5232, Test Loss: 3.3204\n",
      "Epoch: 4715, Train Loss: 0.5093, Test Loss: 4.2096\n",
      "Epoch: 4716, Train Loss: 0.4554, Test Loss: 4.5041\n",
      "Epoch: 4717, Train Loss: 0.5546, Test Loss: 3.7464\n",
      "Epoch: 4718, Train Loss: 0.4265, Test Loss: 3.2294\n",
      "Epoch: 4719, Train Loss: 0.5511, Test Loss: 3.6606\n",
      "Epoch: 4720, Train Loss: 0.4990, Test Loss: 4.6847\n",
      "Epoch: 4721, Train Loss: 0.5820, Test Loss: 4.0918\n",
      "Epoch: 4722, Train Loss: 0.5211, Test Loss: 3.0649\n",
      "Epoch: 4723, Train Loss: 0.6396, Test Loss: 3.4556\n",
      "Epoch: 4724, Train Loss: 0.4520, Test Loss: 4.0364\n",
      "Epoch: 4725, Train Loss: 0.5079, Test Loss: 3.8445\n",
      "Epoch: 4726, Train Loss: 0.4995, Test Loss: 3.3965\n",
      "Epoch: 4727, Train Loss: 0.5008, Test Loss: 3.5911\n",
      "Epoch: 4728, Train Loss: 0.4795, Test Loss: 4.1285\n",
      "Epoch: 4729, Train Loss: 0.5000, Test Loss: 4.0348\n",
      "Epoch: 4730, Train Loss: 0.5847, Test Loss: 3.3514\n",
      "Epoch: 4731, Train Loss: 0.5132, Test Loss: 3.4046\n",
      "Epoch: 4732, Train Loss: 0.5451, Test Loss: 4.6413\n",
      "Epoch: 4733, Train Loss: 0.5837, Test Loss: 4.2240\n",
      "Epoch: 4734, Train Loss: 0.5240, Test Loss: 3.2441\n",
      "Epoch: 4735, Train Loss: 0.5958, Test Loss: 3.4342\n",
      "Epoch: 4736, Train Loss: 0.5035, Test Loss: 4.0052\n",
      "Epoch: 4737, Train Loss: 0.4809, Test Loss: 4.2472\n",
      "Epoch: 4738, Train Loss: 0.5061, Test Loss: 3.8301\n",
      "Epoch: 4739, Train Loss: 0.4715, Test Loss: 3.6909\n",
      "Epoch: 4740, Train Loss: 0.4642, Test Loss: 3.8205\n",
      "Epoch: 4741, Train Loss: 0.5181, Test Loss: 4.5640\n",
      "Epoch: 4742, Train Loss: 0.5587, Test Loss: 3.7454\n",
      "Epoch: 4743, Train Loss: 0.4682, Test Loss: 3.0889\n",
      "Epoch: 4744, Train Loss: 0.6170, Test Loss: 3.6885\n",
      "Epoch: 4745, Train Loss: 0.4839, Test Loss: 4.9165\n",
      "Epoch: 4746, Train Loss: 0.6218, Test Loss: 4.1126\n",
      "Epoch: 4747, Train Loss: 0.4628, Test Loss: 3.4548\n",
      "Epoch: 4748, Train Loss: 0.5028, Test Loss: 3.5234\n",
      "Epoch: 4749, Train Loss: 0.5002, Test Loss: 3.9050\n",
      "Epoch: 4750, Train Loss: 0.4766, Test Loss: 4.3520\n",
      "Epoch: 4751, Train Loss: 0.4972, Test Loss: 4.0371\n",
      "Epoch: 4752, Train Loss: 0.4725, Test Loss: 3.2539\n",
      "Epoch: 4753, Train Loss: 0.5887, Test Loss: 3.5139\n",
      "Epoch: 4754, Train Loss: 0.4486, Test Loss: 4.1652\n",
      "Epoch: 4755, Train Loss: 0.4942, Test Loss: 4.0344\n",
      "Epoch: 4756, Train Loss: 0.4514, Test Loss: 3.6426\n",
      "Epoch: 4757, Train Loss: 0.4923, Test Loss: 3.9900\n",
      "Epoch: 4758, Train Loss: 0.4526, Test Loss: 4.0957\n",
      "Epoch: 4759, Train Loss: 0.4990, Test Loss: 3.9896\n",
      "Epoch: 4760, Train Loss: 0.4863, Test Loss: 3.5338\n",
      "Epoch: 4761, Train Loss: 0.5297, Test Loss: 3.9802\n",
      "Epoch: 4762, Train Loss: 0.4363, Test Loss: 4.7117\n",
      "Epoch: 4763, Train Loss: 0.5102, Test Loss: 4.3730\n",
      "Epoch: 4764, Train Loss: 0.5388, Test Loss: 3.2169\n",
      "Epoch: 4765, Train Loss: 0.6258, Test Loss: 3.5773\n",
      "Epoch: 4766, Train Loss: 0.4801, Test Loss: 4.4273\n",
      "Epoch: 4767, Train Loss: 0.5274, Test Loss: 4.0476\n",
      "Epoch: 4768, Train Loss: 0.4902, Test Loss: 3.3086\n",
      "Epoch: 4769, Train Loss: 0.5361, Test Loss: 3.1781\n",
      "Epoch: 4770, Train Loss: 0.5659, Test Loss: 4.0594\n",
      "Epoch: 4771, Train Loss: 0.5044, Test Loss: 4.9987\n",
      "Epoch: 4772, Train Loss: 0.6198, Test Loss: 3.9138\n",
      "Epoch: 4773, Train Loss: 0.4756, Test Loss: 3.0529\n",
      "Epoch: 4774, Train Loss: 0.6519, Test Loss: 3.4945\n",
      "Epoch: 4775, Train Loss: 0.4864, Test Loss: 4.6421\n",
      "Epoch: 4776, Train Loss: 0.6149, Test Loss: 4.2436\n",
      "Epoch: 4777, Train Loss: 0.5296, Test Loss: 3.1766\n",
      "Epoch: 4778, Train Loss: 0.5955, Test Loss: 3.3090\n",
      "Epoch: 4779, Train Loss: 0.5870, Test Loss: 4.8259\n",
      "Epoch: 4780, Train Loss: 0.6041, Test Loss: 5.0079\n",
      "Epoch: 4781, Train Loss: 0.6178, Test Loss: 3.5234\n",
      "Epoch: 4782, Train Loss: 0.4788, Test Loss: 2.9404\n",
      "Epoch: 4783, Train Loss: 0.7626, Test Loss: 3.9270\n",
      "Epoch: 4784, Train Loss: 0.4846, Test Loss: 4.8554\n",
      "Epoch: 4785, Train Loss: 0.5486, Test Loss: 4.3154\n",
      "Epoch: 4786, Train Loss: 0.5137, Test Loss: 3.4253\n",
      "Epoch: 4787, Train Loss: 0.5460, Test Loss: 3.4605\n",
      "Epoch: 4788, Train Loss: 0.5438, Test Loss: 4.4675\n",
      "Epoch: 4789, Train Loss: 0.5655, Test Loss: 4.5656\n",
      "Epoch: 4790, Train Loss: 0.5875, Test Loss: 3.5326\n",
      "Epoch: 4791, Train Loss: 0.5259, Test Loss: 2.9309\n",
      "Epoch: 4792, Train Loss: 0.7962, Test Loss: 3.9092\n",
      "Epoch: 4793, Train Loss: 0.4836, Test Loss: 4.9217\n",
      "Epoch: 4794, Train Loss: 0.6277, Test Loss: 4.0787\n",
      "Epoch: 4795, Train Loss: 0.5102, Test Loss: 2.9688\n",
      "Epoch: 4796, Train Loss: 0.6697, Test Loss: 3.2500\n",
      "Epoch: 4797, Train Loss: 0.5144, Test Loss: 4.7268\n",
      "Epoch: 4798, Train Loss: 0.5592, Test Loss: 4.9444\n",
      "Epoch: 4799, Train Loss: 0.7860, Test Loss: 3.0903\n",
      "Epoch: 4800, Train Loss: 0.5422, Test Loss: 2.7737\n",
      "Epoch: 4801, Train Loss: 0.8122, Test Loss: 3.3462\n",
      "Epoch: 4802, Train Loss: 0.5575, Test Loss: 5.1272\n",
      "Epoch: 4803, Train Loss: 0.7995, Test Loss: 4.5997\n",
      "Epoch: 4804, Train Loss: 0.7215, Test Loss: 2.9595\n",
      "Epoch: 4805, Train Loss: 0.5909, Test Loss: 2.7757\n",
      "Epoch: 4806, Train Loss: 0.8998, Test Loss: 3.7834\n",
      "Epoch: 4807, Train Loss: 0.5840, Test Loss: 4.4950\n",
      "Epoch: 4808, Train Loss: 0.7368, Test Loss: 3.5919\n",
      "Epoch: 4809, Train Loss: 0.4577, Test Loss: 2.9654\n",
      "Epoch: 4810, Train Loss: 0.7229, Test Loss: 3.3097\n",
      "Epoch: 4811, Train Loss: 0.4946, Test Loss: 4.2644\n",
      "Epoch: 4812, Train Loss: 0.5728, Test Loss: 4.1075\n",
      "Epoch: 4813, Train Loss: 0.5728, Test Loss: 3.2385\n",
      "Epoch: 4814, Train Loss: 0.6186, Test Loss: 3.2314\n",
      "Epoch: 4815, Train Loss: 0.5277, Test Loss: 3.6187\n",
      "Epoch: 4816, Train Loss: 0.4818, Test Loss: 3.5972\n",
      "Epoch: 4817, Train Loss: 0.4924, Test Loss: 3.5474\n",
      "Epoch: 4818, Train Loss: 0.4806, Test Loss: 3.6628\n",
      "Epoch: 4819, Train Loss: 0.5232, Test Loss: 4.1697\n",
      "Epoch: 4820, Train Loss: 0.5365, Test Loss: 3.7873\n",
      "Epoch: 4821, Train Loss: 0.4689, Test Loss: 3.3020\n",
      "Epoch: 4822, Train Loss: 0.5316, Test Loss: 3.2143\n",
      "Epoch: 4823, Train Loss: 0.5697, Test Loss: 3.7619\n",
      "Epoch: 4824, Train Loss: 0.5198, Test Loss: 3.9111\n",
      "Epoch: 4825, Train Loss: 0.5142, Test Loss: 3.3155\n",
      "Epoch: 4826, Train Loss: 0.5087, Test Loss: 3.1750\n",
      "Epoch: 4827, Train Loss: 0.5748, Test Loss: 3.9073\n",
      "Epoch: 4828, Train Loss: 0.5197, Test Loss: 4.3895\n",
      "Epoch: 4829, Train Loss: 0.5920, Test Loss: 3.4964\n",
      "Epoch: 4830, Train Loss: 0.5090, Test Loss: 3.1438\n",
      "Epoch: 4831, Train Loss: 0.6296, Test Loss: 3.6712\n",
      "Epoch: 4832, Train Loss: 0.4850, Test Loss: 4.1216\n",
      "Epoch: 4833, Train Loss: 0.5895, Test Loss: 3.5034\n",
      "Epoch: 4834, Train Loss: 0.5024, Test Loss: 3.1119\n",
      "Epoch: 4835, Train Loss: 0.5654, Test Loss: 3.3407\n",
      "Epoch: 4836, Train Loss: 0.4742, Test Loss: 4.1067\n",
      "Epoch: 4837, Train Loss: 0.5443, Test Loss: 4.0269\n",
      "Epoch: 4838, Train Loss: 0.5177, Test Loss: 3.4985\n",
      "Epoch: 4839, Train Loss: 0.5048, Test Loss: 3.0639\n",
      "Epoch: 4840, Train Loss: 0.5706, Test Loss: 3.4013\n",
      "Epoch: 4841, Train Loss: 0.5060, Test Loss: 4.4478\n",
      "Epoch: 4842, Train Loss: 0.5795, Test Loss: 4.0987\n",
      "Epoch: 4843, Train Loss: 0.5075, Test Loss: 3.2842\n",
      "Epoch: 4844, Train Loss: 0.5427, Test Loss: 3.3838\n",
      "Epoch: 4845, Train Loss: 0.5445, Test Loss: 4.3392\n",
      "Epoch: 4846, Train Loss: 0.5977, Test Loss: 4.0749\n",
      "Epoch: 4847, Train Loss: 0.5686, Test Loss: 3.1536\n",
      "Epoch: 4848, Train Loss: 0.5291, Test Loss: 3.0358\n",
      "Epoch: 4849, Train Loss: 0.6413, Test Loss: 3.9390\n",
      "Epoch: 4850, Train Loss: 0.5360, Test Loss: 4.3004\n",
      "Epoch: 4851, Train Loss: 0.5794, Test Loss: 3.3669\n",
      "Epoch: 4852, Train Loss: 0.5017, Test Loss: 2.8126\n",
      "Epoch: 4853, Train Loss: 0.7267, Test Loss: 3.3655\n",
      "Epoch: 4854, Train Loss: 0.4748, Test Loss: 4.1062\n",
      "Epoch: 4855, Train Loss: 0.6287, Test Loss: 3.5591\n",
      "Epoch: 4856, Train Loss: 0.5207, Test Loss: 2.9393\n",
      "Epoch: 4857, Train Loss: 0.4903, Test Loss: 2.9772\n",
      "Epoch: 4858, Train Loss: 0.5990, Test Loss: 3.8216\n",
      "Epoch: 4859, Train Loss: 0.4705, Test Loss: 4.4781\n",
      "Epoch: 4860, Train Loss: 0.6525, Test Loss: 3.5638\n",
      "Epoch: 4861, Train Loss: 0.5047, Test Loss: 2.9348\n",
      "Epoch: 4862, Train Loss: 0.5727, Test Loss: 3.1787\n",
      "Epoch: 4863, Train Loss: 0.4918, Test Loss: 3.9972\n",
      "Epoch: 4864, Train Loss: 0.5902, Test Loss: 3.7999\n",
      "Epoch: 4865, Train Loss: 0.5834, Test Loss: 3.0622\n",
      "Epoch: 4866, Train Loss: 0.5806, Test Loss: 3.1315\n",
      "Epoch: 4867, Train Loss: 0.5703, Test Loss: 3.9581\n",
      "Epoch: 4868, Train Loss: 0.5582, Test Loss: 3.9667\n",
      "Epoch: 4869, Train Loss: 0.5636, Test Loss: 3.2360\n",
      "Epoch: 4870, Train Loss: 0.5598, Test Loss: 2.9486\n",
      "Epoch: 4871, Train Loss: 0.6138, Test Loss: 3.5887\n",
      "Epoch: 4872, Train Loss: 0.5142, Test Loss: 4.5161\n",
      "Epoch: 4873, Train Loss: 0.6633, Test Loss: 3.9136\n",
      "Epoch: 4874, Train Loss: 0.5885, Test Loss: 2.8299\n",
      "Epoch: 4875, Train Loss: 0.6966, Test Loss: 2.8401\n",
      "Epoch: 4876, Train Loss: 0.7360, Test Loss: 4.0301\n",
      "Epoch: 4877, Train Loss: 0.6545, Test Loss: 4.3239\n",
      "Epoch: 4878, Train Loss: 0.6498, Test Loss: 3.3413\n",
      "Epoch: 4879, Train Loss: 0.5074, Test Loss: 3.0337\n",
      "Epoch: 4880, Train Loss: 0.5420, Test Loss: 3.2965\n",
      "Epoch: 4881, Train Loss: 0.5099, Test Loss: 3.7395\n",
      "Epoch: 4882, Train Loss: 0.5005, Test Loss: 3.9684\n",
      "Epoch: 4883, Train Loss: 0.5379, Test Loss: 3.7696\n",
      "Epoch: 4884, Train Loss: 0.4566, Test Loss: 3.5584\n",
      "Epoch: 4885, Train Loss: 0.5219, Test Loss: 3.4247\n",
      "Epoch: 4886, Train Loss: 0.4893, Test Loss: 3.8502\n",
      "Epoch: 4887, Train Loss: 0.5088, Test Loss: 3.5386\n",
      "Epoch: 4888, Train Loss: 0.4560, Test Loss: 3.2822\n",
      "Epoch: 4889, Train Loss: 0.5198, Test Loss: 3.3585\n",
      "Epoch: 4890, Train Loss: 0.4587, Test Loss: 3.8591\n",
      "Epoch: 4891, Train Loss: 0.4921, Test Loss: 4.1636\n",
      "Epoch: 4892, Train Loss: 0.5396, Test Loss: 3.5899\n",
      "Epoch: 4893, Train Loss: 0.4743, Test Loss: 3.4186\n",
      "Epoch: 4894, Train Loss: 0.4959, Test Loss: 3.5070\n",
      "Epoch: 4895, Train Loss: 0.4665, Test Loss: 3.7984\n",
      "Epoch: 4896, Train Loss: 0.4905, Test Loss: 3.4953\n",
      "Epoch: 4897, Train Loss: 0.4704, Test Loss: 3.1866\n",
      "Epoch: 4898, Train Loss: 0.5107, Test Loss: 3.4412\n",
      "Epoch: 4899, Train Loss: 0.4742, Test Loss: 4.0994\n",
      "Epoch: 4900, Train Loss: 0.4866, Test Loss: 4.2909\n",
      "Epoch: 4901, Train Loss: 0.5318, Test Loss: 3.3466\n",
      "Epoch: 4902, Train Loss: 0.4879, Test Loss: 2.9477\n",
      "Epoch: 4903, Train Loss: 0.6223, Test Loss: 3.5656\n",
      "Epoch: 4904, Train Loss: 0.4519, Test Loss: 4.4062\n",
      "Epoch: 4905, Train Loss: 0.5468, Test Loss: 3.9374\n",
      "Epoch: 4906, Train Loss: 0.4737, Test Loss: 3.2164\n",
      "Epoch: 4907, Train Loss: 0.5362, Test Loss: 3.4248\n",
      "Epoch: 4908, Train Loss: 0.4925, Test Loss: 3.8129\n",
      "Epoch: 4909, Train Loss: 0.5313, Test Loss: 3.6204\n",
      "Epoch: 4910, Train Loss: 0.4415, Test Loss: 3.3121\n",
      "Epoch: 4911, Train Loss: 0.4992, Test Loss: 3.5624\n",
      "Epoch: 4912, Train Loss: 0.4810, Test Loss: 3.5232\n",
      "Epoch: 4913, Train Loss: 0.4862, Test Loss: 3.5448\n",
      "Epoch: 4914, Train Loss: 0.5063, Test Loss: 4.1359\n",
      "Epoch: 4915, Train Loss: 0.5026, Test Loss: 4.2040\n",
      "Epoch: 4916, Train Loss: 0.4826, Test Loss: 3.7287\n",
      "Epoch: 4917, Train Loss: 0.5010, Test Loss: 3.3369\n",
      "Epoch: 4918, Train Loss: 0.5356, Test Loss: 3.7653\n",
      "Epoch: 4919, Train Loss: 0.4765, Test Loss: 3.9424\n",
      "Epoch: 4920, Train Loss: 0.4863, Test Loss: 3.5574\n",
      "Epoch: 4921, Train Loss: 0.4836, Test Loss: 3.4409\n",
      "Epoch: 4922, Train Loss: 0.4908, Test Loss: 3.5248\n",
      "Epoch: 4923, Train Loss: 0.4641, Test Loss: 3.4077\n",
      "Epoch: 4924, Train Loss: 0.4560, Test Loss: 3.5376\n",
      "Epoch: 4925, Train Loss: 0.4546, Test Loss: 3.6488\n",
      "Epoch: 4926, Train Loss: 0.4417, Test Loss: 3.6145\n",
      "Epoch: 4927, Train Loss: 0.4588, Test Loss: 3.5538\n",
      "Epoch: 4928, Train Loss: 0.4619, Test Loss: 3.3178\n",
      "Epoch: 4929, Train Loss: 0.5409, Test Loss: 3.7320\n",
      "Epoch: 4930, Train Loss: 0.4578, Test Loss: 4.0696\n",
      "Epoch: 4931, Train Loss: 0.4744, Test Loss: 3.6779\n",
      "Epoch: 4932, Train Loss: 0.4564, Test Loss: 3.2473\n",
      "Epoch: 4933, Train Loss: 0.4745, Test Loss: 3.3457\n",
      "Epoch: 4934, Train Loss: 0.4690, Test Loss: 3.8878\n",
      "Epoch: 4935, Train Loss: 0.4573, Test Loss: 4.1850\n",
      "Epoch: 4936, Train Loss: 0.5131, Test Loss: 3.6274\n",
      "Epoch: 4937, Train Loss: 0.4666, Test Loss: 3.1490\n",
      "Epoch: 4938, Train Loss: 0.4903, Test Loss: 3.2708\n",
      "Epoch: 4939, Train Loss: 0.4854, Test Loss: 4.0391\n",
      "Epoch: 4940, Train Loss: 0.4852, Test Loss: 4.2762\n",
      "Epoch: 4941, Train Loss: 0.5991, Test Loss: 3.2243\n",
      "Epoch: 4942, Train Loss: 0.4888, Test Loss: 3.0304\n",
      "Epoch: 4943, Train Loss: 0.5502, Test Loss: 3.6019\n",
      "Epoch: 4944, Train Loss: 0.4671, Test Loss: 4.3841\n",
      "Epoch: 4945, Train Loss: 0.5730, Test Loss: 3.8388\n",
      "Epoch: 4946, Train Loss: 0.4759, Test Loss: 3.1849\n",
      "Epoch: 4947, Train Loss: 0.5126, Test Loss: 3.0984\n",
      "Epoch: 4948, Train Loss: 0.5460, Test Loss: 3.7550\n",
      "Epoch: 4949, Train Loss: 0.4685, Test Loss: 4.1019\n",
      "Epoch: 4950, Train Loss: 0.5825, Test Loss: 3.3694\n",
      "Epoch: 4951, Train Loss: 0.4904, Test Loss: 3.0282\n",
      "Epoch: 4952, Train Loss: 0.5770, Test Loss: 3.5899\n",
      "Epoch: 4953, Train Loss: 0.4866, Test Loss: 4.4965\n",
      "Epoch: 4954, Train Loss: 0.6330, Test Loss: 3.9866\n",
      "Epoch: 4955, Train Loss: 0.4757, Test Loss: 3.2733\n",
      "Epoch: 4956, Train Loss: 0.5865, Test Loss: 3.6285\n",
      "Epoch: 4957, Train Loss: 0.5774, Test Loss: 4.1735\n",
      "Epoch: 4958, Train Loss: 0.4567, Test Loss: 4.2957\n",
      "Epoch: 4959, Train Loss: 0.5092, Test Loss: 3.6702\n",
      "Epoch: 4960, Train Loss: 0.4333, Test Loss: 3.2046\n",
      "Epoch: 4961, Train Loss: 0.5365, Test Loss: 3.5797\n",
      "Epoch: 4962, Train Loss: 0.4589, Test Loss: 4.0931\n",
      "Epoch: 4963, Train Loss: 0.4957, Test Loss: 3.7532\n",
      "Epoch: 4964, Train Loss: 0.4673, Test Loss: 3.4048\n",
      "Epoch: 4965, Train Loss: 0.5040, Test Loss: 3.5205\n",
      "Epoch: 4966, Train Loss: 0.4620, Test Loss: 4.0654\n",
      "Epoch: 4967, Train Loss: 0.4640, Test Loss: 4.1970\n",
      "Epoch: 4968, Train Loss: 0.5282, Test Loss: 3.4336\n",
      "Epoch: 4969, Train Loss: 0.5170, Test Loss: 3.2714\n",
      "Epoch: 4970, Train Loss: 0.5634, Test Loss: 4.0169\n",
      "Epoch: 4971, Train Loss: 0.4638, Test Loss: 4.6164\n",
      "Epoch: 4972, Train Loss: 0.5815, Test Loss: 3.6782\n",
      "Epoch: 4973, Train Loss: 0.4752, Test Loss: 3.0083\n",
      "Epoch: 4974, Train Loss: 0.6079, Test Loss: 3.3658\n",
      "Epoch: 4975, Train Loss: 0.4901, Test Loss: 4.1034\n",
      "Epoch: 4976, Train Loss: 0.4669, Test Loss: 4.1770\n",
      "Epoch: 4977, Train Loss: 0.5148, Test Loss: 3.6825\n",
      "Epoch: 4978, Train Loss: 0.4633, Test Loss: 3.2056\n",
      "Epoch: 4979, Train Loss: 0.5037, Test Loss: 3.3715\n",
      "Epoch: 4980, Train Loss: 0.5008, Test Loss: 3.9829\n",
      "Epoch: 4981, Train Loss: 0.4546, Test Loss: 4.1637\n",
      "Epoch: 4982, Train Loss: 0.4890, Test Loss: 3.7340\n",
      "Epoch: 4983, Train Loss: 0.4413, Test Loss: 3.2241\n",
      "Epoch: 4984, Train Loss: 0.5676, Test Loss: 3.6813\n",
      "Epoch: 4985, Train Loss: 0.4363, Test Loss: 4.2057\n",
      "Epoch: 4986, Train Loss: 0.4872, Test Loss: 3.8489\n",
      "Epoch: 4987, Train Loss: 0.4819, Test Loss: 3.2656\n",
      "Epoch: 4988, Train Loss: 0.5048, Test Loss: 3.2291\n",
      "Epoch: 4989, Train Loss: 0.5509, Test Loss: 4.0407\n",
      "Epoch: 4990, Train Loss: 0.4616, Test Loss: 4.4450\n",
      "Epoch: 4991, Train Loss: 0.5621, Test Loss: 3.5785\n",
      "Epoch: 4992, Train Loss: 0.4690, Test Loss: 3.1846\n",
      "Epoch: 4993, Train Loss: 0.4898, Test Loss: 3.3568\n",
      "Epoch: 4994, Train Loss: 0.5249, Test Loss: 4.2574\n",
      "Epoch: 4995, Train Loss: 0.5031, Test Loss: 4.3602\n",
      "Epoch: 4996, Train Loss: 0.5367, Test Loss: 3.3449\n",
      "Epoch: 4997, Train Loss: 0.4703, Test Loss: 3.0282\n",
      "Epoch: 4998, Train Loss: 0.6282, Test Loss: 3.8015\n",
      "Epoch: 4999, Train Loss: 0.4745, Test Loss: 4.3733\n",
      "Epoch: 5000, Train Loss: 0.5427, Test Loss: 3.9544\n",
      "Epoch: 5001, Train Loss: 0.5210, Test Loss: 3.0703\n",
      "Epoch: 5002, Train Loss: 0.5765, Test Loss: 3.2525\n",
      "Epoch: 5003, Train Loss: 0.5116, Test Loss: 4.2391\n",
      "Epoch: 5004, Train Loss: 0.5183, Test Loss: 4.4479\n",
      "Epoch: 5005, Train Loss: 0.5777, Test Loss: 3.5050\n",
      "Epoch: 5006, Train Loss: 0.5306, Test Loss: 2.9962\n",
      "Epoch: 5007, Train Loss: 0.5905, Test Loss: 3.1548\n",
      "Epoch: 5008, Train Loss: 0.5694, Test Loss: 4.7274\n",
      "Epoch: 5009, Train Loss: 0.6047, Test Loss: 4.9629\n",
      "Epoch: 5010, Train Loss: 0.7446, Test Loss: 3.2511\n",
      "Epoch: 5011, Train Loss: 0.5435, Test Loss: 2.8694\n",
      "Epoch: 5012, Train Loss: 0.7699, Test Loss: 3.8909\n",
      "Epoch: 5013, Train Loss: 0.4546, Test Loss: 5.1983\n",
      "Epoch: 5014, Train Loss: 0.7560, Test Loss: 4.0951\n",
      "Epoch: 5015, Train Loss: 0.5186, Test Loss: 2.9553\n",
      "Epoch: 5016, Train Loss: 0.7181, Test Loss: 3.0154\n",
      "Epoch: 5017, Train Loss: 0.6756, Test Loss: 4.0470\n",
      "Epoch: 5018, Train Loss: 0.5466, Test Loss: 4.6993\n",
      "Epoch: 5019, Train Loss: 0.6445, Test Loss: 3.6769\n",
      "Epoch: 5020, Train Loss: 0.4977, Test Loss: 3.1367\n",
      "Epoch: 5021, Train Loss: 0.5773, Test Loss: 3.4464\n",
      "Epoch: 5022, Train Loss: 0.4594, Test Loss: 3.9726\n",
      "Epoch: 5023, Train Loss: 0.5696, Test Loss: 3.6506\n",
      "Epoch: 5024, Train Loss: 0.4856, Test Loss: 3.4756\n",
      "Epoch: 5025, Train Loss: 0.4602, Test Loss: 3.4112\n",
      "Epoch: 5026, Train Loss: 0.4889, Test Loss: 3.7141\n",
      "Epoch: 5027, Train Loss: 0.4696, Test Loss: 3.9564\n",
      "Epoch: 5028, Train Loss: 0.4668, Test Loss: 3.8624\n",
      "Epoch: 5029, Train Loss: 0.4461, Test Loss: 3.7487\n",
      "Epoch: 5030, Train Loss: 0.5146, Test Loss: 3.5601\n",
      "Epoch: 5031, Train Loss: 0.4881, Test Loss: 3.5585\n",
      "Epoch: 5032, Train Loss: 0.4447, Test Loss: 3.6202\n",
      "Epoch: 5033, Train Loss: 0.4539, Test Loss: 3.6747\n",
      "Epoch: 5034, Train Loss: 0.4549, Test Loss: 3.4976\n",
      "Epoch: 5035, Train Loss: 0.4416, Test Loss: 3.3296\n",
      "Epoch: 5036, Train Loss: 0.4969, Test Loss: 3.7339\n",
      "Epoch: 5037, Train Loss: 0.4897, Test Loss: 4.2135\n",
      "Epoch: 5038, Train Loss: 0.5577, Test Loss: 3.5951\n",
      "Epoch: 5039, Train Loss: 0.4824, Test Loss: 3.3139\n",
      "Epoch: 5040, Train Loss: 0.4739, Test Loss: 3.3669\n",
      "Epoch: 5041, Train Loss: 0.4500, Test Loss: 3.7080\n",
      "Epoch: 5042, Train Loss: 0.4370, Test Loss: 3.9761\n",
      "Epoch: 5043, Train Loss: 0.5523, Test Loss: 3.5670\n",
      "Epoch: 5044, Train Loss: 0.4470, Test Loss: 3.3108\n",
      "Epoch: 5045, Train Loss: 0.4941, Test Loss: 3.5861\n",
      "Epoch: 5046, Train Loss: 0.4609, Test Loss: 4.1406\n",
      "Epoch: 5047, Train Loss: 0.5655, Test Loss: 3.5010\n",
      "Epoch: 5048, Train Loss: 0.4527, Test Loss: 3.1113\n",
      "Epoch: 5049, Train Loss: 0.5845, Test Loss: 3.4655\n",
      "Epoch: 5050, Train Loss: 0.4498, Test Loss: 4.0295\n",
      "Epoch: 5051, Train Loss: 0.4860, Test Loss: 3.9702\n",
      "Epoch: 5052, Train Loss: 0.4801, Test Loss: 3.3651\n",
      "Epoch: 5053, Train Loss: 0.4494, Test Loss: 3.2519\n",
      "Epoch: 5054, Train Loss: 0.5260, Test Loss: 3.5179\n",
      "Epoch: 5055, Train Loss: 0.4646, Test Loss: 3.9798\n",
      "Epoch: 5056, Train Loss: 0.5046, Test Loss: 3.6449\n",
      "Epoch: 5057, Train Loss: 0.4759, Test Loss: 3.2680\n",
      "Epoch: 5058, Train Loss: 0.5201, Test Loss: 3.7112\n",
      "Epoch: 5059, Train Loss: 0.4485, Test Loss: 4.5331\n",
      "Epoch: 5060, Train Loss: 0.5485, Test Loss: 4.0466\n",
      "Epoch: 5061, Train Loss: 0.4809, Test Loss: 3.2850\n",
      "Epoch: 5062, Train Loss: 0.5069, Test Loss: 3.1466\n",
      "Epoch: 5063, Train Loss: 0.5540, Test Loss: 3.7849\n",
      "Epoch: 5064, Train Loss: 0.5332, Test Loss: 4.6935\n",
      "Epoch: 5065, Train Loss: 0.6574, Test Loss: 3.8124\n",
      "Epoch: 5066, Train Loss: 0.4531, Test Loss: 2.9633\n",
      "Epoch: 5067, Train Loss: 0.6362, Test Loss: 3.3106\n",
      "Epoch: 5068, Train Loss: 0.5379, Test Loss: 4.5200\n",
      "Epoch: 5069, Train Loss: 0.6521, Test Loss: 4.3318\n",
      "Epoch: 5070, Train Loss: 0.5240, Test Loss: 3.3249\n",
      "Epoch: 5071, Train Loss: 0.4891, Test Loss: 3.1321\n",
      "Epoch: 5072, Train Loss: 0.6528, Test Loss: 4.0771\n",
      "Epoch: 5073, Train Loss: 0.5146, Test Loss: 4.5698\n",
      "Epoch: 5074, Train Loss: 0.6224, Test Loss: 3.8495\n",
      "Epoch: 5075, Train Loss: 0.5493, Test Loss: 3.1938\n",
      "Epoch: 5076, Train Loss: 0.5410, Test Loss: 3.2001\n",
      "Epoch: 5077, Train Loss: 0.4754, Test Loss: 3.8573\n",
      "Epoch: 5078, Train Loss: 0.4747, Test Loss: 4.2996\n",
      "Epoch: 5079, Train Loss: 0.5518, Test Loss: 3.7557\n",
      "Epoch: 5080, Train Loss: 0.4913, Test Loss: 3.2404\n",
      "Epoch: 5081, Train Loss: 0.5465, Test Loss: 3.4119\n",
      "Epoch: 5082, Train Loss: 0.4998, Test Loss: 4.1180\n",
      "Epoch: 5083, Train Loss: 0.4807, Test Loss: 4.2738\n",
      "Epoch: 5084, Train Loss: 0.5701, Test Loss: 3.3650\n",
      "Epoch: 5085, Train Loss: 0.4709, Test Loss: 3.0148\n",
      "Epoch: 5086, Train Loss: 0.6018, Test Loss: 3.6367\n",
      "Epoch: 5087, Train Loss: 0.4376, Test Loss: 4.5173\n",
      "Epoch: 5088, Train Loss: 0.5315, Test Loss: 4.1378\n",
      "Epoch: 5089, Train Loss: 0.5146, Test Loss: 3.1454\n",
      "Epoch: 5090, Train Loss: 0.5626, Test Loss: 3.2616\n",
      "Epoch: 5091, Train Loss: 0.5417, Test Loss: 4.0802\n",
      "Epoch: 5092, Train Loss: 0.5228, Test Loss: 3.9275\n",
      "Epoch: 5093, Train Loss: 0.5265, Test Loss: 3.1085\n",
      "Epoch: 5094, Train Loss: 0.5775, Test Loss: 3.3142\n",
      "Epoch: 5095, Train Loss: 0.5363, Test Loss: 3.8136\n",
      "Epoch: 5096, Train Loss: 0.4987, Test Loss: 4.0866\n",
      "Epoch: 5097, Train Loss: 0.5595, Test Loss: 3.2973\n",
      "Epoch: 5098, Train Loss: 0.5010, Test Loss: 3.1668\n",
      "Epoch: 5099, Train Loss: 0.5347, Test Loss: 3.9617\n",
      "Epoch: 5100, Train Loss: 0.4527, Test Loss: 4.3019\n",
      "Epoch: 5101, Train Loss: 0.5783, Test Loss: 3.3482\n",
      "Epoch: 5102, Train Loss: 0.4502, Test Loss: 3.0211\n",
      "Epoch: 5103, Train Loss: 0.5470, Test Loss: 3.4562\n",
      "Epoch: 5104, Train Loss: 0.4507, Test Loss: 4.2866\n",
      "Epoch: 5105, Train Loss: 0.5238, Test Loss: 4.1283\n",
      "Epoch: 5106, Train Loss: 0.5096, Test Loss: 3.1869\n",
      "Epoch: 5107, Train Loss: 0.4944, Test Loss: 2.9584\n",
      "Epoch: 5108, Train Loss: 0.6450, Test Loss: 3.5263\n",
      "Epoch: 5109, Train Loss: 0.4717, Test Loss: 4.5995\n",
      "Epoch: 5110, Train Loss: 0.5993, Test Loss: 3.9019\n",
      "Epoch: 5111, Train Loss: 0.4666, Test Loss: 3.1743\n",
      "Epoch: 5112, Train Loss: 0.5303, Test Loss: 3.2596\n",
      "Epoch: 5113, Train Loss: 0.4798, Test Loss: 3.8949\n",
      "Epoch: 5114, Train Loss: 0.4657, Test Loss: 4.3340\n",
      "Epoch: 5115, Train Loss: 0.5713, Test Loss: 3.6728\n",
      "Epoch: 5116, Train Loss: 0.4597, Test Loss: 3.2109\n",
      "Epoch: 5117, Train Loss: 0.5301, Test Loss: 3.6379\n",
      "Epoch: 5118, Train Loss: 0.4647, Test Loss: 3.9270\n",
      "Epoch: 5119, Train Loss: 0.4569, Test Loss: 3.8184\n",
      "Epoch: 5120, Train Loss: 0.5000, Test Loss: 3.3288\n",
      "Epoch: 5121, Train Loss: 0.4424, Test Loss: 3.1969\n",
      "Epoch: 5122, Train Loss: 0.4937, Test Loss: 3.5989\n",
      "Epoch: 5123, Train Loss: 0.4446, Test Loss: 3.9980\n",
      "Epoch: 5124, Train Loss: 0.4574, Test Loss: 3.7925\n",
      "Epoch: 5125, Train Loss: 0.4825, Test Loss: 3.4818\n",
      "Epoch: 5126, Train Loss: 0.4990, Test Loss: 3.3576\n",
      "Epoch: 5127, Train Loss: 0.4478, Test Loss: 3.6248\n",
      "Epoch: 5128, Train Loss: 0.4564, Test Loss: 3.8490\n",
      "Epoch: 5129, Train Loss: 0.4880, Test Loss: 3.5510\n",
      "Epoch: 5130, Train Loss: 0.4482, Test Loss: 3.1975\n",
      "Epoch: 5131, Train Loss: 0.5159, Test Loss: 3.4725\n",
      "Epoch: 5132, Train Loss: 0.4495, Test Loss: 4.2093\n",
      "Epoch: 5133, Train Loss: 0.5443, Test Loss: 3.7237\n",
      "Epoch: 5134, Train Loss: 0.4545, Test Loss: 3.2907\n",
      "Epoch: 5135, Train Loss: 0.4976, Test Loss: 3.4413\n",
      "Epoch: 5136, Train Loss: 0.4872, Test Loss: 3.9153\n",
      "Epoch: 5137, Train Loss: 0.4611, Test Loss: 4.0142\n",
      "Epoch: 5138, Train Loss: 0.4348, Test Loss: 3.6637\n",
      "Epoch: 5139, Train Loss: 0.4698, Test Loss: 3.3942\n",
      "Epoch: 5140, Train Loss: 0.4776, Test Loss: 3.5629\n",
      "Epoch: 5141, Train Loss: 0.4504, Test Loss: 3.9005\n",
      "Epoch: 5142, Train Loss: 0.4840, Test Loss: 3.7399\n",
      "Epoch: 5143, Train Loss: 0.4580, Test Loss: 3.2563\n",
      "Epoch: 5144, Train Loss: 0.5129, Test Loss: 3.5196\n",
      "Epoch: 5145, Train Loss: 0.4489, Test Loss: 4.1633\n",
      "Epoch: 5146, Train Loss: 0.5174, Test Loss: 3.7656\n",
      "Epoch: 5147, Train Loss: 0.4546, Test Loss: 3.2256\n",
      "Epoch: 5148, Train Loss: 0.5005, Test Loss: 3.4648\n",
      "Epoch: 5149, Train Loss: 0.4857, Test Loss: 4.1729\n",
      "Epoch: 5150, Train Loss: 0.5340, Test Loss: 3.7814\n",
      "Epoch: 5151, Train Loss: 0.4595, Test Loss: 3.1887\n",
      "Epoch: 5152, Train Loss: 0.5817, Test Loss: 3.6298\n",
      "Epoch: 5153, Train Loss: 0.4433, Test Loss: 4.3371\n",
      "Epoch: 5154, Train Loss: 0.5648, Test Loss: 3.6299\n",
      "Epoch: 5155, Train Loss: 0.4599, Test Loss: 3.1249\n",
      "Epoch: 5156, Train Loss: 0.5331, Test Loss: 3.4215\n",
      "Epoch: 5157, Train Loss: 0.4456, Test Loss: 3.8151\n",
      "Epoch: 5158, Train Loss: 0.4580, Test Loss: 3.8638\n",
      "Epoch: 5159, Train Loss: 0.4905, Test Loss: 3.4430\n",
      "Epoch: 5160, Train Loss: 0.4322, Test Loss: 3.3630\n",
      "Epoch: 5161, Train Loss: 0.5178, Test Loss: 3.9444\n",
      "Epoch: 5162, Train Loss: 0.4568, Test Loss: 4.0605\n",
      "Epoch: 5163, Train Loss: 0.4865, Test Loss: 3.3881\n",
      "Epoch: 5164, Train Loss: 0.4569, Test Loss: 3.2023\n",
      "Epoch: 5165, Train Loss: 0.5261, Test Loss: 3.8107\n",
      "Epoch: 5166, Train Loss: 0.4595, Test Loss: 4.0788\n",
      "Epoch: 5167, Train Loss: 0.4955, Test Loss: 3.6780\n",
      "Epoch: 5168, Train Loss: 0.4710, Test Loss: 3.0467\n",
      "Epoch: 5169, Train Loss: 0.6333, Test Loss: 3.5541\n",
      "Epoch: 5170, Train Loss: 0.4629, Test Loss: 4.5811\n",
      "Epoch: 5171, Train Loss: 0.5679, Test Loss: 4.0461\n",
      "Epoch: 5172, Train Loss: 0.4591, Test Loss: 3.0978\n",
      "Epoch: 5173, Train Loss: 0.6044, Test Loss: 3.4137\n",
      "Epoch: 5174, Train Loss: 0.5019, Test Loss: 4.6101\n",
      "Epoch: 5175, Train Loss: 0.5644, Test Loss: 4.6093\n",
      "Epoch: 5176, Train Loss: 0.6220, Test Loss: 3.1386\n",
      "Epoch: 5177, Train Loss: 0.5167, Test Loss: 2.8705\n",
      "Epoch: 5178, Train Loss: 0.5976, Test Loss: 3.5227\n",
      "Epoch: 5179, Train Loss: 0.4362, Test Loss: 4.5104\n",
      "Epoch: 5180, Train Loss: 0.6191, Test Loss: 3.8259\n",
      "Epoch: 5181, Train Loss: 0.4652, Test Loss: 3.0684\n",
      "Epoch: 5182, Train Loss: 0.5154, Test Loss: 2.9595\n",
      "Epoch: 5183, Train Loss: 0.6003, Test Loss: 3.8329\n",
      "Epoch: 5184, Train Loss: 0.5007, Test Loss: 4.1814\n",
      "Epoch: 5185, Train Loss: 0.5222, Test Loss: 3.4577\n",
      "Epoch: 5186, Train Loss: 0.4519, Test Loss: 3.2052\n",
      "Epoch: 5187, Train Loss: 0.4817, Test Loss: 3.3959\n",
      "Epoch: 5188, Train Loss: 0.4692, Test Loss: 3.9817\n",
      "Epoch: 5189, Train Loss: 0.4816, Test Loss: 3.7681\n",
      "Epoch: 5190, Train Loss: 0.4592, Test Loss: 3.3713\n",
      "Epoch: 5191, Train Loss: 0.5029, Test Loss: 3.6661\n",
      "Epoch: 5192, Train Loss: 0.4790, Test Loss: 3.9433\n",
      "Epoch: 5193, Train Loss: 0.4450, Test Loss: 3.9776\n",
      "Epoch: 5194, Train Loss: 0.4805, Test Loss: 3.8415\n",
      "Epoch: 5195, Train Loss: 0.4516, Test Loss: 3.4735\n",
      "Epoch: 5196, Train Loss: 0.4597, Test Loss: 3.4712\n",
      "Epoch: 5197, Train Loss: 0.4482, Test Loss: 3.8529\n",
      "Epoch: 5198, Train Loss: 0.4763, Test Loss: 3.7973\n",
      "Epoch: 5199, Train Loss: 0.4629, Test Loss: 3.4828\n",
      "Epoch: 5200, Train Loss: 0.4809, Test Loss: 3.5760\n",
      "Epoch: 5201, Train Loss: 0.4288, Test Loss: 3.7726\n",
      "Epoch: 5202, Train Loss: 0.4510, Test Loss: 4.0475\n",
      "Epoch: 5203, Train Loss: 0.4460, Test Loss: 4.0094\n",
      "Epoch: 5204, Train Loss: 0.4942, Test Loss: 3.3957\n",
      "Epoch: 5205, Train Loss: 0.5077, Test Loss: 3.5610\n",
      "Epoch: 5206, Train Loss: 0.4258, Test Loss: 3.9749\n",
      "Epoch: 5207, Train Loss: 0.5246, Test Loss: 3.7501\n",
      "Epoch: 5208, Train Loss: 0.4150, Test Loss: 3.3858\n",
      "Epoch: 5209, Train Loss: 0.4351, Test Loss: 3.3098\n",
      "Epoch: 5210, Train Loss: 0.4969, Test Loss: 3.8518\n",
      "Epoch: 5211, Train Loss: 0.4411, Test Loss: 4.0521\n",
      "Epoch: 5212, Train Loss: 0.4705, Test Loss: 3.6221\n",
      "Epoch: 5213, Train Loss: 0.4510, Test Loss: 3.5377\n",
      "Epoch: 5214, Train Loss: 0.4239, Test Loss: 3.4724\n",
      "Epoch: 5215, Train Loss: 0.4384, Test Loss: 3.4493\n",
      "Epoch: 5216, Train Loss: 0.4525, Test Loss: 3.6413\n",
      "Epoch: 5217, Train Loss: 0.4470, Test Loss: 3.7161\n",
      "Epoch: 5218, Train Loss: 0.4220, Test Loss: 3.7677\n",
      "Epoch: 5219, Train Loss: 0.4923, Test Loss: 3.4511\n",
      "Epoch: 5220, Train Loss: 0.4353, Test Loss: 3.5066\n",
      "Epoch: 5221, Train Loss: 0.4517, Test Loss: 3.5859\n",
      "Epoch: 5222, Train Loss: 0.4740, Test Loss: 3.7466\n",
      "Epoch: 5223, Train Loss: 0.4792, Test Loss: 4.0783\n",
      "Epoch: 5224, Train Loss: 0.4677, Test Loss: 3.8859\n",
      "Epoch: 5225, Train Loss: 0.5001, Test Loss: 3.6966\n",
      "Epoch: 5226, Train Loss: 0.4477, Test Loss: 3.4678\n",
      "Epoch: 5227, Train Loss: 0.4999, Test Loss: 3.5402\n",
      "Epoch: 5228, Train Loss: 0.4706, Test Loss: 3.7599\n",
      "Epoch: 5229, Train Loss: 0.4643, Test Loss: 3.8397\n",
      "Epoch: 5230, Train Loss: 0.4541, Test Loss: 3.7619\n",
      "Epoch: 5231, Train Loss: 0.4428, Test Loss: 3.5188\n",
      "Epoch: 5232, Train Loss: 0.4304, Test Loss: 3.4216\n",
      "Epoch: 5233, Train Loss: 0.4826, Test Loss: 3.9558\n",
      "Epoch: 5234, Train Loss: 0.4930, Test Loss: 3.8345\n",
      "Epoch: 5235, Train Loss: 0.4647, Test Loss: 3.4452\n",
      "Epoch: 5236, Train Loss: 0.4632, Test Loss: 3.4765\n",
      "Epoch: 5237, Train Loss: 0.4515, Test Loss: 3.7960\n",
      "Epoch: 5238, Train Loss: 0.4484, Test Loss: 4.1789\n",
      "Epoch: 5239, Train Loss: 0.4742, Test Loss: 3.9995\n",
      "Epoch: 5240, Train Loss: 0.4627, Test Loss: 3.5825\n",
      "Epoch: 5241, Train Loss: 0.4869, Test Loss: 3.8009\n",
      "Epoch: 5242, Train Loss: 0.5129, Test Loss: 4.3917\n",
      "Epoch: 5243, Train Loss: 0.5621, Test Loss: 3.7632\n",
      "Epoch: 5244, Train Loss: 0.4761, Test Loss: 3.3648\n",
      "Epoch: 5245, Train Loss: 0.4811, Test Loss: 3.6388\n",
      "Epoch: 5246, Train Loss: 0.4554, Test Loss: 3.8976\n",
      "Epoch: 5247, Train Loss: 0.4816, Test Loss: 3.9493\n",
      "Epoch: 5248, Train Loss: 0.4598, Test Loss: 3.7173\n",
      "Epoch: 5249, Train Loss: 0.4418, Test Loss: 3.5036\n",
      "Epoch: 5250, Train Loss: 0.4622, Test Loss: 3.6048\n",
      "Epoch: 5251, Train Loss: 0.4799, Test Loss: 3.7682\n",
      "Epoch: 5252, Train Loss: 0.4482, Test Loss: 4.0083\n",
      "Epoch: 5253, Train Loss: 0.4682, Test Loss: 3.9004\n",
      "Epoch: 5254, Train Loss: 0.4686, Test Loss: 3.6234\n",
      "Epoch: 5255, Train Loss: 0.5111, Test Loss: 4.0243\n",
      "Epoch: 5256, Train Loss: 0.5031, Test Loss: 3.7292\n",
      "Epoch: 5257, Train Loss: 0.4582, Test Loss: 3.5939\n",
      "Epoch: 5258, Train Loss: 0.5332, Test Loss: 3.6744\n",
      "Epoch: 5259, Train Loss: 0.4426, Test Loss: 4.1259\n",
      "Epoch: 5260, Train Loss: 0.4577, Test Loss: 4.0895\n",
      "Epoch: 5261, Train Loss: 0.5033, Test Loss: 3.6920\n",
      "Epoch: 5262, Train Loss: 0.4575, Test Loss: 3.4485\n",
      "Epoch: 5263, Train Loss: 0.4602, Test Loss: 3.9132\n",
      "Epoch: 5264, Train Loss: 0.4635, Test Loss: 4.2569\n",
      "Epoch: 5265, Train Loss: 0.4864, Test Loss: 3.8449\n",
      "Epoch: 5266, Train Loss: 0.5136, Test Loss: 3.3658\n",
      "Epoch: 5267, Train Loss: 0.5316, Test Loss: 3.3481\n",
      "Epoch: 5268, Train Loss: 0.4817, Test Loss: 3.9209\n",
      "Epoch: 5269, Train Loss: 0.4913, Test Loss: 4.1567\n",
      "Epoch: 5270, Train Loss: 0.5010, Test Loss: 3.4563\n",
      "Epoch: 5271, Train Loss: 0.4729, Test Loss: 3.3252\n",
      "Epoch: 5272, Train Loss: 0.5198, Test Loss: 3.8671\n",
      "Epoch: 5273, Train Loss: 0.4490, Test Loss: 4.3225\n",
      "Epoch: 5274, Train Loss: 0.6222, Test Loss: 3.4982\n",
      "Epoch: 5275, Train Loss: 0.4802, Test Loss: 2.9547\n",
      "Epoch: 5276, Train Loss: 0.6032, Test Loss: 3.2111\n",
      "Epoch: 5277, Train Loss: 0.4768, Test Loss: 4.1294\n",
      "Epoch: 5278, Train Loss: 0.5574, Test Loss: 3.8761\n",
      "Epoch: 5279, Train Loss: 0.5320, Test Loss: 3.0996\n",
      "Epoch: 5280, Train Loss: 0.5278, Test Loss: 3.1571\n",
      "Epoch: 5281, Train Loss: 0.5454, Test Loss: 4.2251\n",
      "Epoch: 5282, Train Loss: 0.5866, Test Loss: 4.1735\n",
      "Epoch: 5283, Train Loss: 0.6612, Test Loss: 3.0223\n",
      "Epoch: 5284, Train Loss: 0.5598, Test Loss: 3.0448\n",
      "Epoch: 5285, Train Loss: 0.5693, Test Loss: 3.9299\n",
      "Epoch: 5286, Train Loss: 0.4893, Test Loss: 4.4522\n",
      "Epoch: 5287, Train Loss: 0.6299, Test Loss: 3.4568\n",
      "Epoch: 5288, Train Loss: 0.4863, Test Loss: 2.7455\n",
      "Epoch: 5289, Train Loss: 0.7201, Test Loss: 3.0386\n",
      "Epoch: 5290, Train Loss: 0.5236, Test Loss: 4.2959\n",
      "Epoch: 5291, Train Loss: 0.5501, Test Loss: 4.4437\n",
      "Epoch: 5292, Train Loss: 0.6914, Test Loss: 3.0694\n",
      "Epoch: 5293, Train Loss: 0.4888, Test Loss: 2.6933\n",
      "Epoch: 5294, Train Loss: 0.7111, Test Loss: 3.2341\n",
      "Epoch: 5295, Train Loss: 0.4329, Test Loss: 4.2893\n",
      "Epoch: 5296, Train Loss: 0.6030, Test Loss: 3.9085\n",
      "Epoch: 5297, Train Loss: 0.4824, Test Loss: 3.0666\n",
      "Epoch: 5298, Train Loss: 0.5573, Test Loss: 3.1031\n",
      "Epoch: 5299, Train Loss: 0.5190, Test Loss: 3.9017\n",
      "Epoch: 5300, Train Loss: 0.4750, Test Loss: 4.2635\n",
      "Epoch: 5301, Train Loss: 0.5752, Test Loss: 3.4148\n",
      "Epoch: 5302, Train Loss: 0.4639, Test Loss: 2.8360\n",
      "Epoch: 5303, Train Loss: 0.5969, Test Loss: 3.0910\n",
      "Epoch: 5304, Train Loss: 0.4865, Test Loss: 4.1437\n",
      "Epoch: 5305, Train Loss: 0.5368, Test Loss: 4.3935\n",
      "Epoch: 5306, Train Loss: 0.6209, Test Loss: 3.1384\n",
      "Epoch: 5307, Train Loss: 0.4659, Test Loss: 2.7415\n",
      "Epoch: 5308, Train Loss: 0.7149, Test Loss: 3.3852\n",
      "Epoch: 5309, Train Loss: 0.4477, Test Loss: 4.4359\n",
      "Epoch: 5310, Train Loss: 0.6712, Test Loss: 3.6314\n",
      "Epoch: 5311, Train Loss: 0.4939, Test Loss: 2.7604\n",
      "Epoch: 5312, Train Loss: 0.6079, Test Loss: 2.8548\n",
      "Epoch: 5313, Train Loss: 0.5411, Test Loss: 3.6830\n",
      "Epoch: 5314, Train Loss: 0.4684, Test Loss: 4.4709\n",
      "Epoch: 5315, Train Loss: 0.6624, Test Loss: 3.5862\n",
      "Epoch: 5316, Train Loss: 0.4629, Test Loss: 2.8268\n",
      "Epoch: 5317, Train Loss: 0.6466, Test Loss: 3.0714\n",
      "Epoch: 5318, Train Loss: 0.5013, Test Loss: 4.0861\n",
      "Epoch: 5319, Train Loss: 0.5498, Test Loss: 4.1341\n",
      "Epoch: 5320, Train Loss: 0.5554, Test Loss: 3.2895\n",
      "Epoch: 5321, Train Loss: 0.4524, Test Loss: 2.9798\n",
      "Epoch: 5322, Train Loss: 0.5074, Test Loss: 3.2835\n",
      "Epoch: 5323, Train Loss: 0.4461, Test Loss: 3.9086\n",
      "Epoch: 5324, Train Loss: 0.5251, Test Loss: 3.6048\n",
      "Epoch: 5325, Train Loss: 0.4461, Test Loss: 3.2587\n",
      "Epoch: 5326, Train Loss: 0.4646, Test Loss: 3.1829\n",
      "Epoch: 5327, Train Loss: 0.5213, Test Loss: 3.8982\n",
      "Epoch: 5328, Train Loss: 0.4694, Test Loss: 4.0096\n",
      "Epoch: 5329, Train Loss: 0.4616, Test Loss: 3.5150\n",
      "Epoch: 5330, Train Loss: 0.5154, Test Loss: 3.0908\n",
      "Epoch: 5331, Train Loss: 0.4834, Test Loss: 3.1446\n",
      "Epoch: 5332, Train Loss: 0.4784, Test Loss: 3.7880\n",
      "Epoch: 5333, Train Loss: 0.4709, Test Loss: 3.8636\n",
      "Epoch: 5334, Train Loss: 0.5019, Test Loss: 3.4631\n",
      "Epoch: 5335, Train Loss: 0.4850, Test Loss: 3.4549\n",
      "Epoch: 5336, Train Loss: 0.4418, Test Loss: 3.4928\n",
      "Epoch: 5337, Train Loss: 0.4720, Test Loss: 3.3973\n",
      "Epoch: 5338, Train Loss: 0.4548, Test Loss: 3.4697\n",
      "Epoch: 5339, Train Loss: 0.4494, Test Loss: 3.6589\n",
      "Epoch: 5340, Train Loss: 0.4807, Test Loss: 3.4692\n",
      "Epoch: 5341, Train Loss: 0.5048, Test Loss: 3.6603\n",
      "Epoch: 5342, Train Loss: 0.4953, Test Loss: 3.5375\n",
      "Epoch: 5343, Train Loss: 0.4903, Test Loss: 3.2261\n",
      "Epoch: 5344, Train Loss: 0.5027, Test Loss: 3.4330\n",
      "Epoch: 5345, Train Loss: 0.4394, Test Loss: 3.9515\n",
      "Epoch: 5346, Train Loss: 0.4646, Test Loss: 3.9587\n",
      "Epoch: 5347, Train Loss: 0.4874, Test Loss: 3.2831\n",
      "Epoch: 5348, Train Loss: 0.5271, Test Loss: 3.3085\n",
      "Epoch: 5349, Train Loss: 0.4693, Test Loss: 3.8289\n",
      "Epoch: 5350, Train Loss: 0.4654, Test Loss: 3.8652\n",
      "Epoch: 5351, Train Loss: 0.4996, Test Loss: 3.2397\n",
      "Epoch: 5352, Train Loss: 0.4705, Test Loss: 3.0292\n",
      "Epoch: 5353, Train Loss: 0.5012, Test Loss: 3.3918\n",
      "Epoch: 5354, Train Loss: 0.4239, Test Loss: 3.8108\n",
      "Epoch: 5355, Train Loss: 0.4863, Test Loss: 3.7488\n",
      "Epoch: 5356, Train Loss: 0.4236, Test Loss: 3.4325\n",
      "Epoch: 5357, Train Loss: 0.4284, Test Loss: 3.3546\n",
      "Epoch: 5358, Train Loss: 0.4643, Test Loss: 3.4579\n",
      "Epoch: 5359, Train Loss: 0.4732, Test Loss: 3.8349\n",
      "Epoch: 5360, Train Loss: 0.4521, Test Loss: 3.8290\n",
      "Epoch: 5361, Train Loss: 0.5046, Test Loss: 3.2002\n",
      "Epoch: 5362, Train Loss: 0.5327, Test Loss: 3.3762\n",
      "Epoch: 5363, Train Loss: 0.4937, Test Loss: 3.9190\n",
      "Epoch: 5364, Train Loss: 0.4499, Test Loss: 4.1553\n",
      "Epoch: 5365, Train Loss: 0.4674, Test Loss: 3.5377\n",
      "Epoch: 5366, Train Loss: 0.4640, Test Loss: 3.0786\n",
      "Epoch: 5367, Train Loss: 0.5172, Test Loss: 3.2420\n",
      "Epoch: 5368, Train Loss: 0.4756, Test Loss: 3.9112\n",
      "Epoch: 5369, Train Loss: 0.4696, Test Loss: 4.0677\n",
      "Epoch: 5370, Train Loss: 0.4996, Test Loss: 3.3388\n",
      "Epoch: 5371, Train Loss: 0.4646, Test Loss: 3.0437\n",
      "Epoch: 5372, Train Loss: 0.5816, Test Loss: 3.8206\n",
      "Epoch: 5373, Train Loss: 0.4543, Test Loss: 4.3597\n",
      "Epoch: 5374, Train Loss: 0.5308, Test Loss: 3.6690\n",
      "Epoch: 5375, Train Loss: 0.4405, Test Loss: 3.2180\n",
      "Epoch: 5376, Train Loss: 0.4737, Test Loss: 3.3528\n",
      "Epoch: 5377, Train Loss: 0.4416, Test Loss: 3.9056\n",
      "Epoch: 5378, Train Loss: 0.5725, Test Loss: 3.3980\n",
      "Epoch: 5379, Train Loss: 0.4821, Test Loss: 3.4497\n",
      "Epoch: 5380, Train Loss: 0.4605, Test Loss: 3.5735\n",
      "Epoch: 5381, Train Loss: 0.4380, Test Loss: 3.5673\n",
      "Epoch: 5382, Train Loss: 0.4622, Test Loss: 3.4704\n",
      "Epoch: 5383, Train Loss: 0.4592, Test Loss: 3.6270\n",
      "Epoch: 5384, Train Loss: 0.4222, Test Loss: 3.7862\n",
      "Epoch: 5385, Train Loss: 0.4486, Test Loss: 3.5184\n",
      "Epoch: 5386, Train Loss: 0.4334, Test Loss: 3.1191\n",
      "Epoch: 5387, Train Loss: 0.5788, Test Loss: 3.6962\n",
      "Epoch: 5388, Train Loss: 0.4304, Test Loss: 4.1394\n",
      "Epoch: 5389, Train Loss: 0.5176, Test Loss: 3.5611\n",
      "Epoch: 5390, Train Loss: 0.4583, Test Loss: 3.0891\n",
      "Epoch: 5391, Train Loss: 0.5390, Test Loss: 3.5064\n",
      "Epoch: 5392, Train Loss: 0.4601, Test Loss: 4.5042\n",
      "Epoch: 5393, Train Loss: 0.6169, Test Loss: 3.7829\n",
      "Epoch: 5394, Train Loss: 0.4568, Test Loss: 3.0768\n",
      "Epoch: 5395, Train Loss: 0.4821, Test Loss: 3.0504\n",
      "Epoch: 5396, Train Loss: 0.5366, Test Loss: 3.9166\n",
      "Epoch: 5397, Train Loss: 0.4507, Test Loss: 4.7593\n",
      "Epoch: 5398, Train Loss: 0.6563, Test Loss: 3.6299\n",
      "Epoch: 5399, Train Loss: 0.4277, Test Loss: 2.9824\n",
      "Epoch: 5400, Train Loss: 0.6128, Test Loss: 3.3215\n",
      "Epoch: 5401, Train Loss: 0.4535, Test Loss: 4.2054\n",
      "Epoch: 5402, Train Loss: 0.5409, Test Loss: 4.1421\n",
      "Epoch: 5403, Train Loss: 0.5495, Test Loss: 3.1682\n",
      "Epoch: 5404, Train Loss: 0.5478, Test Loss: 3.1151\n",
      "Epoch: 5405, Train Loss: 0.5537, Test Loss: 3.8047\n",
      "Epoch: 5406, Train Loss: 0.4664, Test Loss: 4.1344\n",
      "Epoch: 5407, Train Loss: 0.5738, Test Loss: 3.5277\n",
      "Epoch: 5408, Train Loss: 0.4711, Test Loss: 3.1188\n",
      "Epoch: 5409, Train Loss: 0.4975, Test Loss: 3.3489\n",
      "Epoch: 5410, Train Loss: 0.4695, Test Loss: 4.0463\n",
      "Epoch: 5411, Train Loss: 0.4882, Test Loss: 3.8870\n",
      "Epoch: 5412, Train Loss: 0.4686, Test Loss: 3.3354\n",
      "Epoch: 5413, Train Loss: 0.4700, Test Loss: 3.0694\n",
      "Epoch: 5414, Train Loss: 0.4799, Test Loss: 3.3469\n",
      "Epoch: 5415, Train Loss: 0.4596, Test Loss: 4.0628\n",
      "Epoch: 5416, Train Loss: 0.4865, Test Loss: 3.8721\n",
      "Epoch: 5417, Train Loss: 0.4819, Test Loss: 3.0872\n",
      "Epoch: 5418, Train Loss: 0.5240, Test Loss: 3.1575\n",
      "Epoch: 5419, Train Loss: 0.4584, Test Loss: 3.5554\n",
      "Epoch: 5420, Train Loss: 0.4324, Test Loss: 3.8560\n",
      "Epoch: 5421, Train Loss: 0.4569, Test Loss: 3.9965\n",
      "Epoch: 5422, Train Loss: 0.5135, Test Loss: 3.3015\n",
      "Epoch: 5423, Train Loss: 0.4873, Test Loss: 3.0381\n",
      "Epoch: 5424, Train Loss: 0.4733, Test Loss: 3.3501\n",
      "Epoch: 5425, Train Loss: 0.4458, Test Loss: 3.5897\n",
      "Epoch: 5426, Train Loss: 0.5035, Test Loss: 3.3083\n",
      "Epoch: 5427, Train Loss: 0.4452, Test Loss: 3.1051\n",
      "Epoch: 5428, Train Loss: 0.4932, Test Loss: 3.4924\n",
      "Epoch: 5429, Train Loss: 0.4559, Test Loss: 4.0504\n",
      "Epoch: 5430, Train Loss: 0.5097, Test Loss: 3.7103\n",
      "Epoch: 5431, Train Loss: 0.4639, Test Loss: 3.0992\n",
      "Epoch: 5432, Train Loss: 0.4977, Test Loss: 3.0561\n",
      "Epoch: 5433, Train Loss: 0.5648, Test Loss: 3.9450\n",
      "Epoch: 5434, Train Loss: 0.4880, Test Loss: 4.0897\n",
      "Epoch: 5435, Train Loss: 0.5275, Test Loss: 3.3622\n",
      "Epoch: 5436, Train Loss: 0.4797, Test Loss: 2.9652\n",
      "Epoch: 5437, Train Loss: 0.4892, Test Loss: 3.2245\n",
      "Epoch: 5438, Train Loss: 0.5195, Test Loss: 4.1086\n",
      "Epoch: 5439, Train Loss: 0.5773, Test Loss: 3.8687\n",
      "Epoch: 5440, Train Loss: 0.4844, Test Loss: 3.0566\n",
      "Epoch: 5441, Train Loss: 0.5103, Test Loss: 3.1555\n",
      "Epoch: 5442, Train Loss: 0.5423, Test Loss: 4.0634\n",
      "Epoch: 5443, Train Loss: 0.5495, Test Loss: 4.1741\n",
      "Epoch: 5444, Train Loss: 0.5234, Test Loss: 3.4445\n",
      "Epoch: 5445, Train Loss: 0.4410, Test Loss: 3.0136\n",
      "Epoch: 5446, Train Loss: 0.5784, Test Loss: 3.3716\n",
      "Epoch: 5447, Train Loss: 0.4521, Test Loss: 4.0912\n",
      "Epoch: 5448, Train Loss: 0.4904, Test Loss: 3.9919\n",
      "Epoch: 5449, Train Loss: 0.4523, Test Loss: 3.3581\n",
      "Epoch: 5450, Train Loss: 0.5044, Test Loss: 3.3787\n",
      "Epoch: 5451, Train Loss: 0.4984, Test Loss: 4.1581\n",
      "Epoch: 5452, Train Loss: 0.5193, Test Loss: 3.8278\n",
      "Epoch: 5453, Train Loss: 0.4732, Test Loss: 3.0529\n",
      "Epoch: 5454, Train Loss: 0.5040, Test Loss: 3.0165\n",
      "Epoch: 5455, Train Loss: 0.5580, Test Loss: 3.8922\n",
      "Epoch: 5456, Train Loss: 0.4743, Test Loss: 4.3480\n",
      "Epoch: 5457, Train Loss: 0.5476, Test Loss: 3.6934\n",
      "Epoch: 5458, Train Loss: 0.5028, Test Loss: 2.8993\n",
      "Epoch: 5459, Train Loss: 0.5705, Test Loss: 3.0194\n",
      "Epoch: 5460, Train Loss: 0.5303, Test Loss: 3.9853\n",
      "Epoch: 5461, Train Loss: 0.5074, Test Loss: 4.2369\n",
      "Epoch: 5462, Train Loss: 0.5178, Test Loss: 3.4492\n",
      "Epoch: 5463, Train Loss: 0.4886, Test Loss: 2.7301\n",
      "Epoch: 5464, Train Loss: 0.7250, Test Loss: 3.1850\n",
      "Epoch: 5465, Train Loss: 0.4765, Test Loss: 4.3107\n",
      "Epoch: 5466, Train Loss: 0.5359, Test Loss: 4.3937\n",
      "Epoch: 5467, Train Loss: 0.5782, Test Loss: 3.2967\n",
      "Epoch: 5468, Train Loss: 0.4594, Test Loss: 2.9124\n",
      "Epoch: 5469, Train Loss: 0.6366, Test Loss: 3.4017\n",
      "Epoch: 5470, Train Loss: 0.4609, Test Loss: 4.4158\n",
      "Epoch: 5471, Train Loss: 0.6024, Test Loss: 3.9161\n",
      "Epoch: 5472, Train Loss: 0.5108, Test Loss: 2.9821\n",
      "Epoch: 5473, Train Loss: 0.5283, Test Loss: 2.9477\n",
      "Epoch: 5474, Train Loss: 0.5511, Test Loss: 3.7007\n",
      "Epoch: 5475, Train Loss: 0.4529, Test Loss: 4.4278\n",
      "Epoch: 5476, Train Loss: 0.5363, Test Loss: 3.9007\n",
      "Epoch: 5477, Train Loss: 0.5045, Test Loss: 2.9672\n",
      "Epoch: 5478, Train Loss: 0.5714, Test Loss: 2.8992\n",
      "Epoch: 5479, Train Loss: 0.6036, Test Loss: 3.8312\n",
      "Epoch: 5480, Train Loss: 0.4823, Test Loss: 4.3143\n",
      "Epoch: 5481, Train Loss: 0.5260, Test Loss: 3.6786\n",
      "Epoch: 5482, Train Loss: 0.4613, Test Loss: 3.1263\n",
      "Epoch: 5483, Train Loss: 0.4647, Test Loss: 3.0531\n",
      "Epoch: 5484, Train Loss: 0.5172, Test Loss: 3.3722\n",
      "Epoch: 5485, Train Loss: 0.4497, Test Loss: 3.7601\n",
      "Epoch: 5486, Train Loss: 0.4944, Test Loss: 3.6817\n",
      "Epoch: 5487, Train Loss: 0.4577, Test Loss: 3.5768\n",
      "Epoch: 5488, Train Loss: 0.4353, Test Loss: 3.4648\n",
      "Epoch: 5489, Train Loss: 0.4524, Test Loss: 3.2936\n",
      "Epoch: 5490, Train Loss: 0.4534, Test Loss: 3.3445\n",
      "Epoch: 5491, Train Loss: 0.4902, Test Loss: 3.8250\n",
      "Epoch: 5492, Train Loss: 0.5033, Test Loss: 3.4215\n",
      "Epoch: 5493, Train Loss: 0.4342, Test Loss: 2.9906\n",
      "Epoch: 5494, Train Loss: 0.5054, Test Loss: 3.1782\n",
      "Epoch: 5495, Train Loss: 0.4296, Test Loss: 3.3783\n",
      "Epoch: 5496, Train Loss: 0.4669, Test Loss: 3.7447\n",
      "Epoch: 5497, Train Loss: 0.4462, Test Loss: 3.8342\n",
      "Epoch: 5498, Train Loss: 0.4708, Test Loss: 3.5478\n",
      "Epoch: 5499, Train Loss: 0.4632, Test Loss: 3.0682\n",
      "Epoch: 5500, Train Loss: 0.5114, Test Loss: 3.2807\n",
      "Epoch: 5501, Train Loss: 0.4820, Test Loss: 4.1756\n",
      "Epoch: 5502, Train Loss: 0.5643, Test Loss: 3.6470\n",
      "Epoch: 5503, Train Loss: 0.4679, Test Loss: 2.8801\n",
      "Epoch: 5504, Train Loss: 0.5556, Test Loss: 2.9794\n",
      "Epoch: 5505, Train Loss: 0.4961, Test Loss: 3.8127\n",
      "Epoch: 5506, Train Loss: 0.4555, Test Loss: 4.1862\n",
      "Epoch: 5507, Train Loss: 0.6065, Test Loss: 3.1601\n",
      "Epoch: 5508, Train Loss: 0.4317, Test Loss: 2.8283\n",
      "Epoch: 5509, Train Loss: 0.5787, Test Loss: 3.2095\n",
      "Epoch: 5510, Train Loss: 0.4375, Test Loss: 3.6050\n",
      "Epoch: 5511, Train Loss: 0.4781, Test Loss: 3.4437\n",
      "Epoch: 5512, Train Loss: 0.4755, Test Loss: 3.3837\n",
      "Epoch: 5513, Train Loss: 0.4192, Test Loss: 3.3603\n",
      "Epoch: 5514, Train Loss: 0.4563, Test Loss: 3.2841\n",
      "Epoch: 5515, Train Loss: 0.4524, Test Loss: 3.5085\n",
      "Epoch: 5516, Train Loss: 0.4469, Test Loss: 3.5385\n",
      "Epoch: 5517, Train Loss: 0.4018, Test Loss: 3.5287\n",
      "Epoch: 5518, Train Loss: 0.4269, Test Loss: 3.3390\n",
      "Epoch: 5519, Train Loss: 0.4536, Test Loss: 3.2227\n",
      "Epoch: 5520, Train Loss: 0.4893, Test Loss: 3.7623\n",
      "Epoch: 5521, Train Loss: 0.4742, Test Loss: 3.8610\n",
      "Epoch: 5522, Train Loss: 0.4793, Test Loss: 3.5216\n",
      "Epoch: 5523, Train Loss: 0.4399, Test Loss: 3.1908\n",
      "Epoch: 5524, Train Loss: 0.4595, Test Loss: 3.3496\n",
      "Epoch: 5525, Train Loss: 0.4476, Test Loss: 4.0759\n",
      "Epoch: 5526, Train Loss: 0.5557, Test Loss: 3.5479\n",
      "Epoch: 5527, Train Loss: 0.4671, Test Loss: 3.0175\n",
      "Epoch: 5528, Train Loss: 0.5310, Test Loss: 3.2434\n",
      "Epoch: 5529, Train Loss: 0.4891, Test Loss: 3.9068\n",
      "Epoch: 5530, Train Loss: 0.5483, Test Loss: 3.4663\n",
      "Epoch: 5531, Train Loss: 0.4400, Test Loss: 3.1040\n",
      "Epoch: 5532, Train Loss: 0.4484, Test Loss: 3.1563\n",
      "Epoch: 5533, Train Loss: 0.4318, Test Loss: 3.4772\n",
      "Epoch: 5534, Train Loss: 0.4585, Test Loss: 3.2791\n",
      "Epoch: 5535, Train Loss: 0.4364, Test Loss: 3.2850\n",
      "Epoch: 5536, Train Loss: 0.4895, Test Loss: 3.6237\n",
      "Epoch: 5537, Train Loss: 0.4561, Test Loss: 3.6710\n",
      "Epoch: 5538, Train Loss: 0.4689, Test Loss: 3.3389\n",
      "Epoch: 5539, Train Loss: 0.4144, Test Loss: 3.2527\n",
      "Epoch: 5540, Train Loss: 0.4460, Test Loss: 3.5035\n",
      "Epoch: 5541, Train Loss: 0.4105, Test Loss: 3.7127\n",
      "Epoch: 5542, Train Loss: 0.4351, Test Loss: 3.4749\n",
      "Epoch: 5543, Train Loss: 0.4819, Test Loss: 3.5095\n",
      "Epoch: 5544, Train Loss: 0.4178, Test Loss: 3.4394\n",
      "Epoch: 5545, Train Loss: 0.4362, Test Loss: 3.4667\n",
      "Epoch: 5546, Train Loss: 0.4692, Test Loss: 3.6573\n",
      "Epoch: 5547, Train Loss: 0.4479, Test Loss: 3.6684\n",
      "Epoch: 5548, Train Loss: 0.4411, Test Loss: 3.6774\n",
      "Epoch: 5549, Train Loss: 0.4328, Test Loss: 3.5165\n",
      "Epoch: 5550, Train Loss: 0.4556, Test Loss: 3.8837\n",
      "Epoch: 5551, Train Loss: 0.4800, Test Loss: 3.5302\n",
      "Epoch: 5552, Train Loss: 0.4404, Test Loss: 3.2016\n",
      "Epoch: 5553, Train Loss: 0.5113, Test Loss: 3.6062\n",
      "Epoch: 5554, Train Loss: 0.4240, Test Loss: 4.0822\n",
      "Epoch: 5555, Train Loss: 0.5108, Test Loss: 3.5658\n",
      "Epoch: 5556, Train Loss: 0.4212, Test Loss: 3.2265\n",
      "Epoch: 5557, Train Loss: 0.4380, Test Loss: 3.2418\n",
      "Epoch: 5558, Train Loss: 0.4138, Test Loss: 3.6631\n",
      "Epoch: 5559, Train Loss: 0.4482, Test Loss: 3.6685\n",
      "Epoch: 5560, Train Loss: 0.4161, Test Loss: 3.3945\n",
      "Epoch: 5561, Train Loss: 0.4295, Test Loss: 3.3333\n",
      "Epoch: 5562, Train Loss: 0.4400, Test Loss: 3.5436\n",
      "Epoch: 5563, Train Loss: 0.4510, Test Loss: 3.6783\n",
      "Epoch: 5564, Train Loss: 0.4568, Test Loss: 3.6792\n",
      "Epoch: 5565, Train Loss: 0.4299, Test Loss: 3.6602\n",
      "Epoch: 5566, Train Loss: 0.4220, Test Loss: 3.3997\n",
      "Epoch: 5567, Train Loss: 0.4305, Test Loss: 3.6213\n",
      "Epoch: 5568, Train Loss: 0.5039, Test Loss: 4.2835\n",
      "Epoch: 5569, Train Loss: 0.5088, Test Loss: 3.9069\n",
      "Epoch: 5570, Train Loss: 0.4352, Test Loss: 3.2157\n",
      "Epoch: 5571, Train Loss: 0.4689, Test Loss: 3.1623\n",
      "Epoch: 5572, Train Loss: 0.4682, Test Loss: 3.7260\n",
      "Epoch: 5573, Train Loss: 0.4519, Test Loss: 3.9359\n",
      "Epoch: 5574, Train Loss: 0.5049, Test Loss: 3.5975\n",
      "Epoch: 5575, Train Loss: 0.4645, Test Loss: 3.0910\n",
      "Epoch: 5576, Train Loss: 0.5096, Test Loss: 3.4163\n",
      "Epoch: 5577, Train Loss: 0.4650, Test Loss: 3.9732\n",
      "Epoch: 5578, Train Loss: 0.5291, Test Loss: 3.5636\n",
      "Epoch: 5579, Train Loss: 0.4738, Test Loss: 3.3473\n",
      "Epoch: 5580, Train Loss: 0.4310, Test Loss: 3.5442\n",
      "Epoch: 5581, Train Loss: 0.4292, Test Loss: 3.6791\n",
      "Epoch: 5582, Train Loss: 0.4818, Test Loss: 3.9429\n",
      "Epoch: 5583, Train Loss: 0.4770, Test Loss: 3.4572\n",
      "Epoch: 5584, Train Loss: 0.4796, Test Loss: 3.6409\n",
      "Epoch: 5585, Train Loss: 0.4249, Test Loss: 3.6065\n",
      "Epoch: 5586, Train Loss: 0.4394, Test Loss: 3.2535\n",
      "Epoch: 5587, Train Loss: 0.4790, Test Loss: 3.6230\n",
      "Epoch: 5588, Train Loss: 0.4537, Test Loss: 3.7687\n",
      "Epoch: 5589, Train Loss: 0.4343, Test Loss: 3.6834\n",
      "Epoch: 5590, Train Loss: 0.4415, Test Loss: 3.3137\n",
      "Epoch: 5591, Train Loss: 0.4782, Test Loss: 3.5547\n",
      "Epoch: 5592, Train Loss: 0.4136, Test Loss: 4.0238\n",
      "Epoch: 5593, Train Loss: 0.4485, Test Loss: 4.0181\n",
      "Epoch: 5594, Train Loss: 0.4609, Test Loss: 3.4989\n",
      "Epoch: 5595, Train Loss: 0.4446, Test Loss: 3.2898\n",
      "Epoch: 5596, Train Loss: 0.4873, Test Loss: 3.7686\n",
      "Epoch: 5597, Train Loss: 0.4541, Test Loss: 4.3731\n",
      "Epoch: 5598, Train Loss: 0.6200, Test Loss: 3.3532\n",
      "Epoch: 5599, Train Loss: 0.4526, Test Loss: 3.0760\n",
      "Epoch: 5600, Train Loss: 0.4719, Test Loss: 3.3736\n",
      "Epoch: 5601, Train Loss: 0.4547, Test Loss: 3.9111\n",
      "Epoch: 5602, Train Loss: 0.4927, Test Loss: 4.0610\n",
      "Epoch: 5603, Train Loss: 0.4283, Test Loss: 3.6333\n",
      "Epoch: 5604, Train Loss: 0.4248, Test Loss: 3.1274\n",
      "Epoch: 5605, Train Loss: 0.4647, Test Loss: 3.1657\n",
      "Epoch: 5606, Train Loss: 0.4567, Test Loss: 3.7131\n",
      "Epoch: 5607, Train Loss: 0.4539, Test Loss: 3.9875\n",
      "Epoch: 5608, Train Loss: 0.4969, Test Loss: 3.5588\n",
      "Epoch: 5609, Train Loss: 0.4260, Test Loss: 3.4436\n",
      "Epoch: 5610, Train Loss: 0.4703, Test Loss: 3.4241\n",
      "Epoch: 5611, Train Loss: 0.4626, Test Loss: 3.5804\n",
      "Epoch: 5612, Train Loss: 0.4489, Test Loss: 3.6810\n",
      "Epoch: 5613, Train Loss: 0.4216, Test Loss: 3.8201\n",
      "Epoch: 5614, Train Loss: 0.4567, Test Loss: 3.6852\n",
      "Epoch: 5615, Train Loss: 0.4437, Test Loss: 3.1261\n",
      "Epoch: 5616, Train Loss: 0.4589, Test Loss: 3.1054\n",
      "Epoch: 5617, Train Loss: 0.5380, Test Loss: 4.0390\n",
      "Epoch: 5618, Train Loss: 0.4576, Test Loss: 4.5870\n",
      "Epoch: 5619, Train Loss: 0.6377, Test Loss: 3.4131\n",
      "Epoch: 5620, Train Loss: 0.4376, Test Loss: 2.8788\n",
      "Epoch: 5621, Train Loss: 0.5762, Test Loss: 3.2589\n",
      "Epoch: 5622, Train Loss: 0.4517, Test Loss: 4.2288\n",
      "Epoch: 5623, Train Loss: 0.5178, Test Loss: 4.1996\n",
      "Epoch: 5624, Train Loss: 0.5663, Test Loss: 3.0639\n",
      "Epoch: 5625, Train Loss: 0.5619, Test Loss: 3.0261\n",
      "Epoch: 5626, Train Loss: 0.5267, Test Loss: 3.8545\n",
      "Epoch: 5627, Train Loss: 0.5199, Test Loss: 3.9743\n",
      "Epoch: 5628, Train Loss: 0.4992, Test Loss: 3.2588\n",
      "Epoch: 5629, Train Loss: 0.4887, Test Loss: 3.4041\n",
      "Epoch: 5630, Train Loss: 0.4361, Test Loss: 3.8868\n",
      "Epoch: 5631, Train Loss: 0.4239, Test Loss: 4.0997\n",
      "Epoch: 5632, Train Loss: 0.4920, Test Loss: 3.4882\n",
      "Epoch: 5633, Train Loss: 0.4434, Test Loss: 3.1385\n",
      "Epoch: 5634, Train Loss: 0.5026, Test Loss: 3.3659\n",
      "Epoch: 5635, Train Loss: 0.4384, Test Loss: 4.2423\n",
      "Epoch: 5636, Train Loss: 0.5242, Test Loss: 3.9339\n",
      "Epoch: 5637, Train Loss: 0.4772, Test Loss: 3.2285\n",
      "Epoch: 5638, Train Loss: 0.5085, Test Loss: 3.3563\n",
      "Epoch: 5639, Train Loss: 0.4628, Test Loss: 3.8451\n",
      "Epoch: 5640, Train Loss: 0.4542, Test Loss: 3.9412\n",
      "Epoch: 5641, Train Loss: 0.4938, Test Loss: 3.3620\n",
      "Epoch: 5642, Train Loss: 0.4663, Test Loss: 3.2105\n",
      "Epoch: 5643, Train Loss: 0.5071, Test Loss: 3.8055\n",
      "Epoch: 5644, Train Loss: 0.4507, Test Loss: 4.0530\n",
      "Epoch: 5645, Train Loss: 0.4733, Test Loss: 3.6116\n",
      "Epoch: 5646, Train Loss: 0.4647, Test Loss: 3.3442\n",
      "Epoch: 5647, Train Loss: 0.4536, Test Loss: 3.5005\n",
      "Epoch: 5648, Train Loss: 0.4646, Test Loss: 3.5629\n",
      "Epoch: 5649, Train Loss: 0.4439, Test Loss: 3.4465\n",
      "Epoch: 5650, Train Loss: 0.4394, Test Loss: 3.5871\n",
      "Epoch: 5651, Train Loss: 0.4574, Test Loss: 3.7781\n",
      "Epoch: 5652, Train Loss: 0.4226, Test Loss: 3.7454\n",
      "Epoch: 5653, Train Loss: 0.4332, Test Loss: 3.5934\n",
      "Epoch: 5654, Train Loss: 0.4154, Test Loss: 3.4462\n",
      "Epoch: 5655, Train Loss: 0.4529, Test Loss: 3.5037\n",
      "Epoch: 5656, Train Loss: 0.4608, Test Loss: 3.4631\n",
      "Epoch: 5657, Train Loss: 0.4714, Test Loss: 3.6343\n",
      "Epoch: 5658, Train Loss: 0.4482, Test Loss: 3.7779\n",
      "Epoch: 5659, Train Loss: 0.4266, Test Loss: 3.4806\n",
      "Epoch: 5660, Train Loss: 0.4461, Test Loss: 3.1064\n",
      "Epoch: 5661, Train Loss: 0.4836, Test Loss: 3.4799\n",
      "Epoch: 5662, Train Loss: 0.4467, Test Loss: 4.3701\n",
      "Epoch: 5663, Train Loss: 0.5706, Test Loss: 3.7547\n",
      "Epoch: 5664, Train Loss: 0.4651, Test Loss: 2.9171\n",
      "Epoch: 5665, Train Loss: 0.5220, Test Loss: 2.9657\n",
      "Epoch: 5666, Train Loss: 0.5306, Test Loss: 3.8179\n",
      "Epoch: 5667, Train Loss: 0.4547, Test Loss: 4.0721\n",
      "Epoch: 5668, Train Loss: 0.5316, Test Loss: 3.2783\n",
      "Epoch: 5669, Train Loss: 0.4381, Test Loss: 3.0114\n",
      "Epoch: 5670, Train Loss: 0.5422, Test Loss: 3.6253\n",
      "Epoch: 5671, Train Loss: 0.4535, Test Loss: 4.2547\n",
      "Epoch: 5672, Train Loss: 0.6322, Test Loss: 3.2778\n",
      "Epoch: 5673, Train Loss: 0.4750, Test Loss: 2.7780\n",
      "Epoch: 5674, Train Loss: 0.6378, Test Loss: 3.2195\n",
      "Epoch: 5675, Train Loss: 0.4527, Test Loss: 4.3593\n",
      "Epoch: 5676, Train Loss: 0.5644, Test Loss: 4.0776\n",
      "Epoch: 5677, Train Loss: 0.5215, Test Loss: 3.0091\n",
      "Epoch: 5678, Train Loss: 0.4568, Test Loss: 2.7851\n",
      "Epoch: 5679, Train Loss: 0.7099, Test Loss: 3.8528\n",
      "Epoch: 5680, Train Loss: 0.4928, Test Loss: 4.7058\n",
      "Epoch: 5681, Train Loss: 0.6558, Test Loss: 3.6489\n",
      "Epoch: 5682, Train Loss: 0.4295, Test Loss: 2.8078\n",
      "Epoch: 5683, Train Loss: 0.6206, Test Loss: 2.9765\n",
      "Epoch: 5684, Train Loss: 0.5766, Test Loss: 4.1367\n",
      "Epoch: 5685, Train Loss: 0.5048, Test Loss: 4.6702\n",
      "Epoch: 5686, Train Loss: 0.7106, Test Loss: 3.4148\n",
      "Epoch: 5687, Train Loss: 0.4419, Test Loss: 2.8074\n",
      "Epoch: 5688, Train Loss: 0.6527, Test Loss: 2.9634\n",
      "Epoch: 5689, Train Loss: 0.5251, Test Loss: 3.8122\n",
      "Epoch: 5690, Train Loss: 0.4949, Test Loss: 4.3678\n",
      "Epoch: 5691, Train Loss: 0.5926, Test Loss: 3.6035\n",
      "Epoch: 5692, Train Loss: 0.4342, Test Loss: 3.1485\n",
      "Epoch: 5693, Train Loss: 0.4899, Test Loss: 3.2613\n",
      "Epoch: 5694, Train Loss: 0.4867, Test Loss: 3.4766\n",
      "Epoch: 5695, Train Loss: 0.4820, Test Loss: 4.1184\n",
      "Epoch: 5696, Train Loss: 0.5504, Test Loss: 3.5639\n",
      "Epoch: 5697, Train Loss: 0.4418, Test Loss: 2.9009\n",
      "Epoch: 5698, Train Loss: 0.5592, Test Loss: 3.0384\n",
      "Epoch: 5699, Train Loss: 0.4521, Test Loss: 3.7600\n",
      "Epoch: 5700, Train Loss: 0.4445, Test Loss: 4.0362\n",
      "Epoch: 5701, Train Loss: 0.5469, Test Loss: 3.2377\n",
      "Epoch: 5702, Train Loss: 0.4195, Test Loss: 2.9207\n",
      "Epoch: 5703, Train Loss: 0.4892, Test Loss: 3.1739\n",
      "Epoch: 5704, Train Loss: 0.4338, Test Loss: 3.7997\n",
      "Epoch: 5705, Train Loss: 0.4602, Test Loss: 3.8337\n",
      "Epoch: 5706, Train Loss: 0.5693, Test Loss: 3.0409\n",
      "Epoch: 5707, Train Loss: 0.4656, Test Loss: 2.9212\n",
      "Epoch: 5708, Train Loss: 0.5079, Test Loss: 3.1767\n",
      "Epoch: 5709, Train Loss: 0.4316, Test Loss: 3.7916\n",
      "Epoch: 5710, Train Loss: 0.4630, Test Loss: 3.8570\n",
      "Epoch: 5711, Train Loss: 0.5368, Test Loss: 2.9324\n",
      "Epoch: 5712, Train Loss: 0.5005, Test Loss: 2.8097\n",
      "Epoch: 5713, Train Loss: 0.5860, Test Loss: 3.6545\n",
      "Epoch: 5714, Train Loss: 0.4291, Test Loss: 4.3038\n",
      "Epoch: 5715, Train Loss: 0.6057, Test Loss: 3.3761\n",
      "Epoch: 5716, Train Loss: 0.4020, Test Loss: 2.8226\n",
      "Epoch: 5717, Train Loss: 0.5564, Test Loss: 3.1176\n",
      "Epoch: 5718, Train Loss: 0.4863, Test Loss: 4.1483\n",
      "Epoch: 5719, Train Loss: 0.5126, Test Loss: 4.2341\n",
      "Epoch: 5720, Train Loss: 0.5287, Test Loss: 3.2598\n",
      "Epoch: 5721, Train Loss: 0.4563, Test Loss: 2.7727\n",
      "Epoch: 5722, Train Loss: 0.6606, Test Loss: 3.1772\n",
      "Epoch: 5723, Train Loss: 0.4123, Test Loss: 4.0425\n",
      "Epoch: 5724, Train Loss: 0.5394, Test Loss: 3.8848\n",
      "Epoch: 5725, Train Loss: 0.4885, Test Loss: 3.1500\n",
      "Epoch: 5726, Train Loss: 0.4513, Test Loss: 2.8848\n",
      "Epoch: 5727, Train Loss: 0.5197, Test Loss: 3.3137\n",
      "Epoch: 5728, Train Loss: 0.4373, Test Loss: 3.9473\n",
      "Epoch: 5729, Train Loss: 0.5033, Test Loss: 3.6813\n",
      "Epoch: 5730, Train Loss: 0.4803, Test Loss: 2.9870\n",
      "Epoch: 5731, Train Loss: 0.4628, Test Loss: 2.8771\n",
      "Epoch: 5732, Train Loss: 0.5171, Test Loss: 3.5481\n",
      "Epoch: 5733, Train Loss: 0.4381, Test Loss: 3.9300\n",
      "Epoch: 5734, Train Loss: 0.5179, Test Loss: 3.3563\n",
      "Epoch: 5735, Train Loss: 0.4414, Test Loss: 3.1094\n",
      "Epoch: 5736, Train Loss: 0.4771, Test Loss: 3.3179\n",
      "Epoch: 5737, Train Loss: 0.4439, Test Loss: 3.7734\n",
      "Epoch: 5738, Train Loss: 0.5247, Test Loss: 3.6679\n",
      "Epoch: 5739, Train Loss: 0.4953, Test Loss: 2.9995\n",
      "Epoch: 5740, Train Loss: 0.4486, Test Loss: 2.8539\n",
      "Epoch: 5741, Train Loss: 0.4912, Test Loss: 3.3760\n",
      "Epoch: 5742, Train Loss: 0.4759, Test Loss: 4.0749\n",
      "Epoch: 5743, Train Loss: 0.5869, Test Loss: 3.5321\n",
      "Epoch: 5744, Train Loss: 0.4809, Test Loss: 2.8385\n",
      "Epoch: 5745, Train Loss: 0.6161, Test Loss: 3.0965\n",
      "Epoch: 5746, Train Loss: 0.5167, Test Loss: 3.9534\n",
      "Epoch: 5747, Train Loss: 0.5443, Test Loss: 3.8811\n",
      "Epoch: 5748, Train Loss: 0.5256, Test Loss: 3.1561\n",
      "Epoch: 5749, Train Loss: 0.4334, Test Loss: 2.8664\n",
      "Epoch: 5750, Train Loss: 0.5001, Test Loss: 3.1425\n",
      "Epoch: 5751, Train Loss: 0.4807, Test Loss: 3.8452\n",
      "Epoch: 5752, Train Loss: 0.4896, Test Loss: 3.9479\n",
      "Epoch: 5753, Train Loss: 0.5368, Test Loss: 3.0734\n",
      "Epoch: 5754, Train Loss: 0.4867, Test Loss: 2.9889\n",
      "Epoch: 5755, Train Loss: 0.4535, Test Loss: 3.4335\n",
      "Epoch: 5756, Train Loss: 0.4375, Test Loss: 3.6421\n",
      "Epoch: 5757, Train Loss: 0.4769, Test Loss: 3.3669\n",
      "Epoch: 5758, Train Loss: 0.4286, Test Loss: 3.0578\n",
      "Epoch: 5759, Train Loss: 0.5124, Test Loss: 3.2312\n",
      "Epoch: 5760, Train Loss: 0.4213, Test Loss: 3.6336\n",
      "Epoch: 5761, Train Loss: 0.4206, Test Loss: 3.8665\n",
      "Epoch: 5762, Train Loss: 0.4769, Test Loss: 3.3706\n",
      "Epoch: 5763, Train Loss: 0.4449, Test Loss: 2.9308\n",
      "Epoch: 5764, Train Loss: 0.4896, Test Loss: 3.0084\n",
      "Epoch: 5765, Train Loss: 0.4933, Test Loss: 3.6591\n",
      "Epoch: 5766, Train Loss: 0.4971, Test Loss: 3.8128\n",
      "Epoch: 5767, Train Loss: 0.4680, Test Loss: 3.4256\n",
      "Epoch: 5768, Train Loss: 0.4289, Test Loss: 3.1027\n",
      "Epoch: 5769, Train Loss: 0.4269, Test Loss: 3.0747\n",
      "Epoch: 5770, Train Loss: 0.4681, Test Loss: 3.3917\n",
      "Epoch: 5771, Train Loss: 0.4335, Test Loss: 3.9426\n",
      "Epoch: 5772, Train Loss: 0.5023, Test Loss: 3.6778\n",
      "Epoch: 5773, Train Loss: 0.4635, Test Loss: 3.0009\n",
      "Epoch: 5774, Train Loss: 0.4497, Test Loss: 2.9116\n",
      "Epoch: 5775, Train Loss: 0.5146, Test Loss: 3.4371\n",
      "Epoch: 5776, Train Loss: 0.4469, Test Loss: 3.9664\n",
      "Epoch: 5777, Train Loss: 0.5493, Test Loss: 3.7199\n",
      "Epoch: 5778, Train Loss: 0.4522, Test Loss: 3.0776\n",
      "Epoch: 5779, Train Loss: 0.4567, Test Loss: 2.8841\n",
      "Epoch: 5780, Train Loss: 0.6950, Test Loss: 3.8379\n",
      "Epoch: 5781, Train Loss: 0.4878, Test Loss: 4.2165\n",
      "Epoch: 5782, Train Loss: 0.5644, Test Loss: 3.3803\n",
      "Epoch: 5783, Train Loss: 0.4736, Test Loss: 3.0703\n",
      "Epoch: 5784, Train Loss: 0.5138, Test Loss: 3.5859\n",
      "Epoch: 5785, Train Loss: 0.4160, Test Loss: 3.8547\n",
      "Epoch: 5786, Train Loss: 0.4840, Test Loss: 3.4053\n",
      "Epoch: 5787, Train Loss: 0.4220, Test Loss: 3.0551\n",
      "Epoch: 5788, Train Loss: 0.5208, Test Loss: 3.4864\n",
      "Epoch: 5789, Train Loss: 0.4142, Test Loss: 3.8960\n",
      "Epoch: 5790, Train Loss: 0.4713, Test Loss: 3.5280\n",
      "Epoch: 5791, Train Loss: 0.4698, Test Loss: 3.2513\n",
      "Epoch: 5792, Train Loss: 0.4528, Test Loss: 3.1165\n",
      "Epoch: 5793, Train Loss: 0.4961, Test Loss: 3.6001\n",
      "Epoch: 5794, Train Loss: 0.4625, Test Loss: 4.0244\n",
      "Epoch: 5795, Train Loss: 0.4928, Test Loss: 3.7463\n",
      "Epoch: 5796, Train Loss: 0.4457, Test Loss: 3.1101\n",
      "Epoch: 5797, Train Loss: 0.4989, Test Loss: 3.2054\n",
      "Epoch: 5798, Train Loss: 0.4439, Test Loss: 3.5571\n",
      "Epoch: 5799, Train Loss: 0.4542, Test Loss: 3.6520\n",
      "Epoch: 5800, Train Loss: 0.4454, Test Loss: 3.3891\n",
      "Epoch: 5801, Train Loss: 0.4342, Test Loss: 3.4333\n",
      "Epoch: 5802, Train Loss: 0.5355, Test Loss: 4.0581\n",
      "Epoch: 5803, Train Loss: 0.4682, Test Loss: 3.7973\n",
      "Epoch: 5804, Train Loss: 0.4346, Test Loss: 3.2475\n",
      "Epoch: 5805, Train Loss: 0.4784, Test Loss: 3.0930\n",
      "Epoch: 5806, Train Loss: 0.4649, Test Loss: 3.2332\n",
      "Epoch: 5807, Train Loss: 0.4744, Test Loss: 3.8072\n",
      "Epoch: 5808, Train Loss: 0.4409, Test Loss: 3.8571\n",
      "Epoch: 5809, Train Loss: 0.4809, Test Loss: 3.3513\n",
      "Epoch: 5810, Train Loss: 0.4174, Test Loss: 3.0466\n",
      "Epoch: 5811, Train Loss: 0.5331, Test Loss: 3.2641\n",
      "Epoch: 5812, Train Loss: 0.4468, Test Loss: 3.6281\n",
      "Epoch: 5813, Train Loss: 0.4632, Test Loss: 3.5276\n",
      "Epoch: 5814, Train Loss: 0.4483, Test Loss: 3.3230\n",
      "Epoch: 5815, Train Loss: 0.4224, Test Loss: 3.2792\n",
      "Epoch: 5816, Train Loss: 0.4471, Test Loss: 3.4685\n",
      "Epoch: 5817, Train Loss: 0.4477, Test Loss: 3.9955\n",
      "Epoch: 5818, Train Loss: 0.4711, Test Loss: 3.8600\n",
      "Epoch: 5819, Train Loss: 0.4641, Test Loss: 3.3526\n",
      "Epoch: 5820, Train Loss: 0.4215, Test Loss: 3.2652\n",
      "Epoch: 5821, Train Loss: 0.4402, Test Loss: 3.4883\n",
      "Epoch: 5822, Train Loss: 0.4359, Test Loss: 4.0133\n",
      "Epoch: 5823, Train Loss: 0.4816, Test Loss: 3.7057\n",
      "Epoch: 5824, Train Loss: 0.4279, Test Loss: 3.0239\n",
      "Epoch: 5825, Train Loss: 0.4765, Test Loss: 2.9634\n",
      "Epoch: 5826, Train Loss: 0.4608, Test Loss: 3.5331\n",
      "Epoch: 5827, Train Loss: 0.4436, Test Loss: 4.1216\n",
      "Epoch: 5828, Train Loss: 0.4764, Test Loss: 3.8496\n",
      "Epoch: 5829, Train Loss: 0.4255, Test Loss: 3.4383\n",
      "Epoch: 5830, Train Loss: 0.4352, Test Loss: 3.5131\n",
      "Epoch: 5831, Train Loss: 0.4493, Test Loss: 3.7294\n",
      "Epoch: 5832, Train Loss: 0.4256, Test Loss: 3.7256\n",
      "Epoch: 5833, Train Loss: 0.4601, Test Loss: 3.5570\n",
      "Epoch: 5834, Train Loss: 0.4194, Test Loss: 3.2013\n",
      "Epoch: 5835, Train Loss: 0.4856, Test Loss: 3.4721\n",
      "Epoch: 5836, Train Loss: 0.4625, Test Loss: 3.8323\n",
      "Epoch: 5837, Train Loss: 0.4877, Test Loss: 3.8941\n",
      "Epoch: 5838, Train Loss: 0.5187, Test Loss: 3.2170\n",
      "Epoch: 5839, Train Loss: 0.4359, Test Loss: 3.1309\n",
      "Epoch: 5840, Train Loss: 0.4850, Test Loss: 3.4648\n",
      "Epoch: 5841, Train Loss: 0.5174, Test Loss: 4.4530\n",
      "Epoch: 5842, Train Loss: 0.5581, Test Loss: 3.9851\n",
      "Epoch: 5843, Train Loss: 0.5384, Test Loss: 2.9343\n",
      "Epoch: 5844, Train Loss: 0.5326, Test Loss: 2.8578\n",
      "Epoch: 5845, Train Loss: 0.5214, Test Loss: 3.4475\n",
      "Epoch: 5846, Train Loss: 0.4472, Test Loss: 4.2187\n",
      "Epoch: 5847, Train Loss: 0.5979, Test Loss: 3.6952\n",
      "Epoch: 5848, Train Loss: 0.4566, Test Loss: 3.0676\n",
      "Epoch: 5849, Train Loss: 0.5103, Test Loss: 3.1523\n",
      "Epoch: 5850, Train Loss: 0.4785, Test Loss: 3.7192\n",
      "Epoch: 5851, Train Loss: 0.4392, Test Loss: 4.1125\n",
      "Epoch: 5852, Train Loss: 0.5639, Test Loss: 3.3166\n",
      "Epoch: 5853, Train Loss: 0.4997, Test Loss: 3.3072\n",
      "Epoch: 5854, Train Loss: 0.4728, Test Loss: 3.9342\n",
      "Epoch: 5855, Train Loss: 0.5645, Test Loss: 3.5847\n",
      "Epoch: 5856, Train Loss: 0.4329, Test Loss: 3.1809\n",
      "Epoch: 5857, Train Loss: 0.4220, Test Loss: 3.0142\n",
      "Epoch: 5858, Train Loss: 0.4647, Test Loss: 3.4172\n",
      "Epoch: 5859, Train Loss: 0.4170, Test Loss: 3.8009\n",
      "Epoch: 5860, Train Loss: 0.4947, Test Loss: 3.4045\n",
      "Epoch: 5861, Train Loss: 0.4511, Test Loss: 3.1938\n",
      "Epoch: 5862, Train Loss: 0.4352, Test Loss: 3.0959\n",
      "Epoch: 5863, Train Loss: 0.4766, Test Loss: 3.3971\n",
      "Epoch: 5864, Train Loss: 0.4043, Test Loss: 3.6690\n",
      "Epoch: 5865, Train Loss: 0.4578, Test Loss: 3.3113\n",
      "Epoch: 5866, Train Loss: 0.4398, Test Loss: 3.2351\n",
      "Epoch: 5867, Train Loss: 0.4702, Test Loss: 3.6883\n",
      "Epoch: 5868, Train Loss: 0.4590, Test Loss: 3.6629\n",
      "Epoch: 5869, Train Loss: 0.4675, Test Loss: 3.2308\n",
      "Epoch: 5870, Train Loss: 0.4220, Test Loss: 3.0055\n",
      "Epoch: 5871, Train Loss: 0.4834, Test Loss: 3.3514\n",
      "Epoch: 5872, Train Loss: 0.4632, Test Loss: 3.9524\n",
      "Epoch: 5873, Train Loss: 0.4910, Test Loss: 3.6907\n",
      "Epoch: 5874, Train Loss: 0.3989, Test Loss: 3.2293\n",
      "Epoch: 5875, Train Loss: 0.4773, Test Loss: 3.4306\n",
      "Epoch: 5876, Train Loss: 0.4320, Test Loss: 3.6094\n",
      "Epoch: 5877, Train Loss: 0.4471, Test Loss: 3.7403\n",
      "Epoch: 5878, Train Loss: 0.4087, Test Loss: 3.6425\n",
      "Epoch: 5879, Train Loss: 0.4546, Test Loss: 3.1643\n",
      "Epoch: 5880, Train Loss: 0.5004, Test Loss: 3.4902\n",
      "Epoch: 5881, Train Loss: 0.4230, Test Loss: 4.2444\n",
      "Epoch: 5882, Train Loss: 0.4936, Test Loss: 3.9275\n",
      "Epoch: 5883, Train Loss: 0.4595, Test Loss: 3.1187\n",
      "Epoch: 5884, Train Loss: 0.4728, Test Loss: 2.9738\n",
      "Epoch: 5885, Train Loss: 0.4702, Test Loss: 3.4141\n",
      "Epoch: 5886, Train Loss: 0.4183, Test Loss: 3.8434\n",
      "Epoch: 5887, Train Loss: 0.4591, Test Loss: 4.0471\n",
      "Epoch: 5888, Train Loss: 0.4820, Test Loss: 3.3395\n",
      "Epoch: 5889, Train Loss: 0.4507, Test Loss: 3.1131\n",
      "Epoch: 5890, Train Loss: 0.5158, Test Loss: 3.7308\n",
      "Epoch: 5891, Train Loss: 0.4349, Test Loss: 4.1807\n",
      "Epoch: 5892, Train Loss: 0.4895, Test Loss: 3.6803\n",
      "Epoch: 5893, Train Loss: 0.4715, Test Loss: 3.2079\n",
      "Epoch: 5894, Train Loss: 0.5257, Test Loss: 3.6690\n",
      "Epoch: 5895, Train Loss: 0.4076, Test Loss: 4.0899\n",
      "Epoch: 5896, Train Loss: 0.4783, Test Loss: 3.9433\n",
      "Epoch: 5897, Train Loss: 0.4512, Test Loss: 3.2874\n",
      "Epoch: 5898, Train Loss: 0.4522, Test Loss: 3.0807\n",
      "Epoch: 5899, Train Loss: 0.4553, Test Loss: 3.2585\n",
      "Epoch: 5900, Train Loss: 0.4924, Test Loss: 3.9264\n",
      "Epoch: 5901, Train Loss: 0.5300, Test Loss: 3.6589\n",
      "Epoch: 5902, Train Loss: 0.4428, Test Loss: 3.0941\n",
      "Epoch: 5903, Train Loss: 0.4913, Test Loss: 3.0592\n",
      "Epoch: 5904, Train Loss: 0.4890, Test Loss: 3.6384\n",
      "Epoch: 5905, Train Loss: 0.4375, Test Loss: 4.0468\n",
      "Epoch: 5906, Train Loss: 0.5279, Test Loss: 3.4444\n",
      "Epoch: 5907, Train Loss: 0.4143, Test Loss: 3.0136\n",
      "Epoch: 5908, Train Loss: 0.5498, Test Loss: 3.2389\n",
      "Epoch: 5909, Train Loss: 0.4241, Test Loss: 3.9675\n",
      "Epoch: 5910, Train Loss: 0.4721, Test Loss: 3.9386\n",
      "Epoch: 5911, Train Loss: 0.5067, Test Loss: 3.2123\n",
      "Epoch: 5912, Train Loss: 0.4557, Test Loss: 3.0798\n",
      "Epoch: 5913, Train Loss: 0.4097, Test Loss: 3.2961\n",
      "Epoch: 5914, Train Loss: 0.4554, Test Loss: 3.9079\n",
      "Epoch: 5915, Train Loss: 0.4710, Test Loss: 3.6422\n",
      "Epoch: 5916, Train Loss: 0.4435, Test Loss: 2.9895\n",
      "Epoch: 5917, Train Loss: 0.4909, Test Loss: 3.0489\n",
      "Epoch: 5918, Train Loss: 0.4921, Test Loss: 3.4461\n",
      "Epoch: 5919, Train Loss: 0.4251, Test Loss: 3.6165\n",
      "Epoch: 5920, Train Loss: 0.4574, Test Loss: 3.2385\n",
      "Epoch: 5921, Train Loss: 0.4029, Test Loss: 3.0024\n",
      "Epoch: 5922, Train Loss: 0.4290, Test Loss: 2.9801\n",
      "Epoch: 5923, Train Loss: 0.4658, Test Loss: 3.6003\n",
      "Epoch: 5924, Train Loss: 0.4706, Test Loss: 3.6787\n",
      "Epoch: 5925, Train Loss: 0.4370, Test Loss: 3.2506\n",
      "Epoch: 5926, Train Loss: 0.4036, Test Loss: 2.9404\n",
      "Epoch: 5927, Train Loss: 0.4748, Test Loss: 3.2420\n",
      "Epoch: 5928, Train Loss: 0.4698, Test Loss: 4.0576\n",
      "Epoch: 5929, Train Loss: 0.5321, Test Loss: 3.6292\n",
      "Epoch: 5930, Train Loss: 0.4385, Test Loss: 3.0584\n",
      "Epoch: 5931, Train Loss: 0.4637, Test Loss: 3.2006\n",
      "Epoch: 5932, Train Loss: 0.4165, Test Loss: 3.6727\n",
      "Epoch: 5933, Train Loss: 0.4260, Test Loss: 3.7171\n",
      "Epoch: 5934, Train Loss: 0.4509, Test Loss: 3.4687\n",
      "Epoch: 5935, Train Loss: 0.4637, Test Loss: 3.0711\n",
      "Epoch: 5936, Train Loss: 0.4932, Test Loss: 3.2095\n",
      "Epoch: 5937, Train Loss: 0.4484, Test Loss: 3.6522\n",
      "Epoch: 5938, Train Loss: 0.4173, Test Loss: 3.8196\n",
      "Epoch: 5939, Train Loss: 0.4520, Test Loss: 3.3260\n",
      "Epoch: 5940, Train Loss: 0.4432, Test Loss: 2.9204\n",
      "Epoch: 5941, Train Loss: 0.4984, Test Loss: 3.1648\n",
      "Epoch: 5942, Train Loss: 0.4106, Test Loss: 3.8329\n",
      "Epoch: 5943, Train Loss: 0.4918, Test Loss: 3.5662\n",
      "Epoch: 5944, Train Loss: 0.4349, Test Loss: 3.2818\n",
      "Epoch: 5945, Train Loss: 0.4418, Test Loss: 3.3053\n",
      "Epoch: 5946, Train Loss: 0.4322, Test Loss: 3.5722\n",
      "Epoch: 5947, Train Loss: 0.4490, Test Loss: 3.5319\n",
      "Epoch: 5948, Train Loss: 0.4203, Test Loss: 3.2695\n",
      "Epoch: 5949, Train Loss: 0.4592, Test Loss: 3.1826\n",
      "Epoch: 5950, Train Loss: 0.4163, Test Loss: 3.3680\n",
      "Epoch: 5951, Train Loss: 0.4376, Test Loss: 3.8411\n",
      "Epoch: 5952, Train Loss: 0.4634, Test Loss: 3.6727\n",
      "Epoch: 5953, Train Loss: 0.4556, Test Loss: 3.4965\n",
      "Epoch: 5954, Train Loss: 0.4133, Test Loss: 3.2521\n",
      "Epoch: 5955, Train Loss: 0.4319, Test Loss: 3.2568\n",
      "Epoch: 5956, Train Loss: 0.4349, Test Loss: 3.5736\n",
      "Epoch: 5957, Train Loss: 0.4645, Test Loss: 3.4692\n",
      "Epoch: 5958, Train Loss: 0.4397, Test Loss: 3.3755\n",
      "Epoch: 5959, Train Loss: 0.4702, Test Loss: 3.4417\n",
      "Epoch: 5960, Train Loss: 0.4355, Test Loss: 3.6970\n",
      "Epoch: 5961, Train Loss: 0.4695, Test Loss: 3.3782\n",
      "Epoch: 5962, Train Loss: 0.4231, Test Loss: 3.1857\n",
      "Epoch: 5963, Train Loss: 0.4365, Test Loss: 3.3367\n",
      "Epoch: 5964, Train Loss: 0.4289, Test Loss: 3.6060\n",
      "Epoch: 5965, Train Loss: 0.4045, Test Loss: 3.6870\n",
      "Epoch: 5966, Train Loss: 0.4553, Test Loss: 3.6678\n",
      "Epoch: 5967, Train Loss: 0.4472, Test Loss: 3.3141\n",
      "Epoch: 5968, Train Loss: 0.4532, Test Loss: 3.3328\n",
      "Epoch: 5969, Train Loss: 0.4259, Test Loss: 3.6631\n",
      "Epoch: 5970, Train Loss: 0.4445, Test Loss: 3.4648\n",
      "Epoch: 5971, Train Loss: 0.4478, Test Loss: 3.5411\n",
      "Epoch: 5972, Train Loss: 0.4375, Test Loss: 3.6169\n",
      "Epoch: 5973, Train Loss: 0.4096, Test Loss: 3.4269\n",
      "Epoch: 5974, Train Loss: 0.3932, Test Loss: 3.2468\n",
      "Epoch: 5975, Train Loss: 0.4595, Test Loss: 3.5160\n",
      "Epoch: 5976, Train Loss: 0.4432, Test Loss: 3.8168\n",
      "Epoch: 5977, Train Loss: 0.4671, Test Loss: 3.4750\n",
      "Epoch: 5978, Train Loss: 0.4301, Test Loss: 3.1242\n",
      "Epoch: 5979, Train Loss: 0.4497, Test Loss: 3.2152\n",
      "Epoch: 5980, Train Loss: 0.4454, Test Loss: 3.7043\n",
      "Epoch: 5981, Train Loss: 0.4374, Test Loss: 3.7077\n",
      "Epoch: 5982, Train Loss: 0.4508, Test Loss: 3.3207\n",
      "Epoch: 5983, Train Loss: 0.4487, Test Loss: 3.1800\n",
      "Epoch: 5984, Train Loss: 0.5302, Test Loss: 4.0059\n",
      "Epoch: 5985, Train Loss: 0.5052, Test Loss: 3.9461\n",
      "Epoch: 5986, Train Loss: 0.4724, Test Loss: 3.1643\n",
      "Epoch: 5987, Train Loss: 0.4850, Test Loss: 3.1636\n",
      "Epoch: 5988, Train Loss: 0.5440, Test Loss: 3.9900\n",
      "Epoch: 5989, Train Loss: 0.4604, Test Loss: 4.0244\n",
      "Epoch: 5990, Train Loss: 0.4498, Test Loss: 3.3864\n",
      "Epoch: 5991, Train Loss: 0.4248, Test Loss: 3.0806\n",
      "Epoch: 5992, Train Loss: 0.5047, Test Loss: 3.6199\n",
      "Epoch: 5993, Train Loss: 0.4149, Test Loss: 3.6831\n",
      "Epoch: 5994, Train Loss: 0.4326, Test Loss: 3.3828\n",
      "Epoch: 5995, Train Loss: 0.4500, Test Loss: 3.2287\n",
      "Epoch: 5996, Train Loss: 0.5086, Test Loss: 3.7942\n",
      "Epoch: 5997, Train Loss: 0.4427, Test Loss: 3.7875\n",
      "Epoch: 5998, Train Loss: 0.4247, Test Loss: 3.4833\n",
      "Epoch: 5999, Train Loss: 0.4515, Test Loss: 3.5690\n",
      "Epoch: 6000, Train Loss: 0.4113, Test Loss: 3.6563\n",
      "Epoch: 6001, Train Loss: 0.4368, Test Loss: 3.4698\n",
      "Epoch: 6002, Train Loss: 0.4776, Test Loss: 2.9870\n",
      "Epoch: 6003, Train Loss: 0.5599, Test Loss: 3.4056\n",
      "Epoch: 6004, Train Loss: 0.4199, Test Loss: 4.0840\n",
      "Epoch: 6005, Train Loss: 0.4738, Test Loss: 3.9047\n",
      "Epoch: 6006, Train Loss: 0.4912, Test Loss: 3.0303\n",
      "Epoch: 6007, Train Loss: 0.4643, Test Loss: 2.8354\n",
      "Epoch: 6008, Train Loss: 0.5704, Test Loss: 3.5330\n",
      "Epoch: 6009, Train Loss: 0.4069, Test Loss: 4.2915\n",
      "Epoch: 6010, Train Loss: 0.5820, Test Loss: 3.6674\n",
      "Epoch: 6011, Train Loss: 0.4263, Test Loss: 3.0260\n",
      "Epoch: 6012, Train Loss: 0.4922, Test Loss: 3.1296\n",
      "Epoch: 6013, Train Loss: 0.4359, Test Loss: 3.6585\n",
      "Epoch: 6014, Train Loss: 0.4526, Test Loss: 3.8690\n",
      "Epoch: 6015, Train Loss: 0.4636, Test Loss: 3.4425\n",
      "Epoch: 6016, Train Loss: 0.3918, Test Loss: 3.1486\n",
      "Epoch: 6017, Train Loss: 0.4533, Test Loss: 3.4557\n",
      "Epoch: 6018, Train Loss: 0.4192, Test Loss: 3.8131\n",
      "Epoch: 6019, Train Loss: 0.4491, Test Loss: 3.5134\n",
      "Epoch: 6020, Train Loss: 0.4094, Test Loss: 3.2330\n",
      "Epoch: 6021, Train Loss: 0.4817, Test Loss: 3.6488\n",
      "Epoch: 6022, Train Loss: 0.4484, Test Loss: 3.5004\n",
      "Epoch: 6023, Train Loss: 0.3986, Test Loss: 3.1402\n",
      "Epoch: 6024, Train Loss: 0.4532, Test Loss: 3.4229\n",
      "Epoch: 6025, Train Loss: 0.4080, Test Loss: 3.8029\n",
      "Epoch: 6026, Train Loss: 0.4133, Test Loss: 3.8569\n",
      "Epoch: 6027, Train Loss: 0.4522, Test Loss: 3.3330\n",
      "Epoch: 6028, Train Loss: 0.4227, Test Loss: 3.1876\n",
      "Epoch: 6029, Train Loss: 0.4306, Test Loss: 3.6037\n",
      "Epoch: 6030, Train Loss: 0.4219, Test Loss: 3.8999\n",
      "Epoch: 6031, Train Loss: 0.4565, Test Loss: 3.4072\n",
      "Epoch: 6032, Train Loss: 0.4245, Test Loss: 3.2194\n",
      "Epoch: 6033, Train Loss: 0.4220, Test Loss: 3.4843\n",
      "Epoch: 6034, Train Loss: 0.4109, Test Loss: 3.4726\n",
      "Epoch: 6035, Train Loss: 0.4286, Test Loss: 3.6047\n",
      "Epoch: 6036, Train Loss: 0.4050, Test Loss: 3.4469\n",
      "Epoch: 6037, Train Loss: 0.4109, Test Loss: 3.3624\n",
      "Epoch: 6038, Train Loss: 0.4214, Test Loss: 3.5195\n",
      "Epoch: 6039, Train Loss: 0.4777, Test Loss: 3.3281\n",
      "Epoch: 6040, Train Loss: 0.4084, Test Loss: 3.4606\n",
      "Epoch: 6041, Train Loss: 0.4055, Test Loss: 3.5355\n",
      "Epoch: 6042, Train Loss: 0.4064, Test Loss: 3.6012\n",
      "Epoch: 6043, Train Loss: 0.4225, Test Loss: 3.3425\n",
      "Epoch: 6044, Train Loss: 0.4434, Test Loss: 3.3092\n",
      "Epoch: 6045, Train Loss: 0.4261, Test Loss: 3.7921\n",
      "Epoch: 6046, Train Loss: 0.4433, Test Loss: 3.8208\n",
      "Epoch: 6047, Train Loss: 0.4345, Test Loss: 3.3346\n",
      "Epoch: 6048, Train Loss: 0.4134, Test Loss: 3.1910\n",
      "Epoch: 6049, Train Loss: 0.4574, Test Loss: 3.5106\n",
      "Epoch: 6050, Train Loss: 0.4338, Test Loss: 3.8428\n",
      "Epoch: 6051, Train Loss: 0.4438, Test Loss: 3.5333\n",
      "Epoch: 6052, Train Loss: 0.4192, Test Loss: 3.2173\n",
      "Epoch: 6053, Train Loss: 0.4969, Test Loss: 3.6257\n",
      "Epoch: 6054, Train Loss: 0.4347, Test Loss: 4.0097\n",
      "Epoch: 6055, Train Loss: 0.4996, Test Loss: 3.4563\n",
      "Epoch: 6056, Train Loss: 0.4453, Test Loss: 3.3308\n",
      "Epoch: 6057, Train Loss: 0.4543, Test Loss: 3.4575\n",
      "Epoch: 6058, Train Loss: 0.4477, Test Loss: 4.2040\n",
      "Epoch: 6059, Train Loss: 0.4747, Test Loss: 4.1367\n",
      "Epoch: 6060, Train Loss: 0.4552, Test Loss: 3.3156\n",
      "Epoch: 6061, Train Loss: 0.4333, Test Loss: 2.8869\n",
      "Epoch: 6062, Train Loss: 0.5929, Test Loss: 3.4784\n",
      "Epoch: 6063, Train Loss: 0.4541, Test Loss: 4.3241\n",
      "Epoch: 6064, Train Loss: 0.5106, Test Loss: 3.8701\n",
      "Epoch: 6065, Train Loss: 0.4647, Test Loss: 2.9874\n",
      "Epoch: 6066, Train Loss: 0.5601, Test Loss: 3.1488\n",
      "Epoch: 6067, Train Loss: 0.4817, Test Loss: 3.8222\n",
      "Epoch: 6068, Train Loss: 0.4366, Test Loss: 4.3249\n",
      "Epoch: 6069, Train Loss: 0.5096, Test Loss: 3.6345\n",
      "Epoch: 6070, Train Loss: 0.4284, Test Loss: 3.0260\n",
      "Epoch: 6071, Train Loss: 0.5540, Test Loss: 3.5260\n",
      "Epoch: 6072, Train Loss: 0.3974, Test Loss: 4.2076\n",
      "Epoch: 6073, Train Loss: 0.4934, Test Loss: 3.6871\n",
      "Epoch: 6074, Train Loss: 0.4053, Test Loss: 2.9726\n",
      "Epoch: 6075, Train Loss: 0.6029, Test Loss: 3.3932\n",
      "Epoch: 6076, Train Loss: 0.4274, Test Loss: 4.0151\n",
      "Epoch: 6077, Train Loss: 0.4468, Test Loss: 4.0327\n",
      "Epoch: 6078, Train Loss: 0.4945, Test Loss: 3.3511\n",
      "Epoch: 6079, Train Loss: 0.4117, Test Loss: 3.0672\n",
      "Epoch: 6080, Train Loss: 0.4978, Test Loss: 3.5213\n",
      "Epoch: 6081, Train Loss: 0.4902, Test Loss: 4.3951\n",
      "Epoch: 6082, Train Loss: 0.5629, Test Loss: 3.7568\n",
      "Epoch: 6083, Train Loss: 0.4206, Test Loss: 3.0219\n",
      "Epoch: 6084, Train Loss: 0.4865, Test Loss: 3.0615\n",
      "Epoch: 6085, Train Loss: 0.6218, Test Loss: 4.2567\n",
      "Epoch: 6086, Train Loss: 0.5780, Test Loss: 4.2371\n",
      "Epoch: 6087, Train Loss: 0.5408, Test Loss: 3.1294\n",
      "Epoch: 6088, Train Loss: 0.4778, Test Loss: 2.8990\n",
      "Epoch: 6089, Train Loss: 0.5328, Test Loss: 3.2527\n",
      "Epoch: 6090, Train Loss: 0.4512, Test Loss: 3.7938\n",
      "Epoch: 6091, Train Loss: 0.4779, Test Loss: 3.5927\n",
      "Epoch: 6092, Train Loss: 0.4206, Test Loss: 3.2283\n",
      "Epoch: 6093, Train Loss: 0.4741, Test Loss: 3.4563\n",
      "Epoch: 6094, Train Loss: 0.4171, Test Loss: 3.7986\n",
      "Epoch: 6095, Train Loss: 0.4705, Test Loss: 3.4419\n",
      "Epoch: 6096, Train Loss: 0.4462, Test Loss: 3.2265\n",
      "Epoch: 6097, Train Loss: 0.4300, Test Loss: 3.3069\n",
      "Epoch: 6098, Train Loss: 0.4370, Test Loss: 3.7813\n",
      "Epoch: 6099, Train Loss: 0.4329, Test Loss: 3.8894\n",
      "Epoch: 6100, Train Loss: 0.5164, Test Loss: 3.2597\n",
      "Epoch: 6101, Train Loss: 0.4673, Test Loss: 3.1622\n",
      "Epoch: 6102, Train Loss: 0.4649, Test Loss: 3.4957\n",
      "Epoch: 6103, Train Loss: 0.4242, Test Loss: 3.7897\n",
      "Epoch: 6104, Train Loss: 0.4827, Test Loss: 3.3834\n",
      "Epoch: 6105, Train Loss: 0.4127, Test Loss: 3.0931\n",
      "Epoch: 6106, Train Loss: 0.4746, Test Loss: 3.4171\n",
      "Epoch: 6107, Train Loss: 0.4276, Test Loss: 3.7160\n",
      "Epoch: 6108, Train Loss: 0.4637, Test Loss: 3.7827\n",
      "Epoch: 6109, Train Loss: 0.4581, Test Loss: 3.3393\n",
      "Epoch: 6110, Train Loss: 0.4085, Test Loss: 3.1807\n",
      "Epoch: 6111, Train Loss: 0.4388, Test Loss: 3.4526\n",
      "Epoch: 6112, Train Loss: 0.4534, Test Loss: 3.7011\n",
      "Epoch: 6113, Train Loss: 0.4371, Test Loss: 3.4386\n",
      "Epoch: 6114, Train Loss: 0.4002, Test Loss: 3.3406\n",
      "Epoch: 6115, Train Loss: 0.3903, Test Loss: 3.3379\n",
      "Epoch: 6116, Train Loss: 0.4327, Test Loss: 3.4755\n",
      "Epoch: 6117, Train Loss: 0.4275, Test Loss: 3.4517\n",
      "Epoch: 6118, Train Loss: 0.4474, Test Loss: 3.0855\n",
      "Epoch: 6119, Train Loss: 0.4076, Test Loss: 2.9197\n",
      "Epoch: 6120, Train Loss: 0.4361, Test Loss: 3.3166\n",
      "Epoch: 6121, Train Loss: 0.4030, Test Loss: 3.9292\n",
      "Epoch: 6122, Train Loss: 0.4539, Test Loss: 3.7461\n",
      "Epoch: 6123, Train Loss: 0.4253, Test Loss: 3.1342\n",
      "Epoch: 6124, Train Loss: 0.4292, Test Loss: 3.0181\n",
      "Epoch: 6125, Train Loss: 0.5066, Test Loss: 3.4462\n",
      "Epoch: 6126, Train Loss: 0.4384, Test Loss: 4.1794\n",
      "Epoch: 6127, Train Loss: 0.5092, Test Loss: 3.6848\n",
      "Epoch: 6128, Train Loss: 0.4363, Test Loss: 2.8318\n",
      "Epoch: 6129, Train Loss: 0.5656, Test Loss: 2.9775\n",
      "Epoch: 6130, Train Loss: 0.4657, Test Loss: 3.8536\n",
      "Epoch: 6131, Train Loss: 0.4752, Test Loss: 4.0279\n",
      "Epoch: 6132, Train Loss: 0.5301, Test Loss: 3.1721\n",
      "Epoch: 6133, Train Loss: 0.4287, Test Loss: 2.9429\n",
      "Epoch: 6134, Train Loss: 0.5068, Test Loss: 3.3267\n",
      "Epoch: 6135, Train Loss: 0.4114, Test Loss: 3.9007\n",
      "Epoch: 6136, Train Loss: 0.4633, Test Loss: 3.6602\n",
      "Epoch: 6137, Train Loss: 0.4725, Test Loss: 3.0092\n",
      "Epoch: 6138, Train Loss: 0.4992, Test Loss: 3.1437\n",
      "Epoch: 6139, Train Loss: 0.4663, Test Loss: 3.8023\n",
      "Epoch: 6140, Train Loss: 0.5123, Test Loss: 3.8281\n",
      "Epoch: 6141, Train Loss: 0.4536, Test Loss: 3.7518\n",
      "Epoch: 6142, Train Loss: 0.4405, Test Loss: 3.4722\n",
      "Epoch: 6143, Train Loss: 0.4897, Test Loss: 3.3462\n",
      "Epoch: 6144, Train Loss: 0.4247, Test Loss: 3.4154\n",
      "Epoch: 6145, Train Loss: 0.4226, Test Loss: 3.6545\n",
      "Epoch: 6146, Train Loss: 0.4440, Test Loss: 3.6914\n",
      "Epoch: 6147, Train Loss: 0.4386, Test Loss: 3.2541\n",
      "Epoch: 6148, Train Loss: 0.4318, Test Loss: 2.8976\n",
      "Epoch: 6149, Train Loss: 0.5000, Test Loss: 3.2185\n",
      "Epoch: 6150, Train Loss: 0.4384, Test Loss: 3.6760\n",
      "Epoch: 6151, Train Loss: 0.4313, Test Loss: 3.7579\n",
      "Epoch: 6152, Train Loss: 0.4577, Test Loss: 3.2939\n",
      "Epoch: 6153, Train Loss: 0.4236, Test Loss: 3.1760\n",
      "Epoch: 6154, Train Loss: 0.4727, Test Loss: 3.7740\n",
      "Epoch: 6155, Train Loss: 0.4503, Test Loss: 3.7043\n",
      "Epoch: 6156, Train Loss: 0.5044, Test Loss: 2.9904\n",
      "Epoch: 6157, Train Loss: 0.4717, Test Loss: 3.0141\n",
      "Epoch: 6158, Train Loss: 0.4509, Test Loss: 3.7028\n",
      "Epoch: 6159, Train Loss: 0.4074, Test Loss: 4.1403\n",
      "Epoch: 6160, Train Loss: 0.5556, Test Loss: 3.2076\n",
      "Epoch: 6161, Train Loss: 0.4576, Test Loss: 3.0059\n",
      "Epoch: 6162, Train Loss: 0.5469, Test Loss: 3.5006\n",
      "Epoch: 6163, Train Loss: 0.4384, Test Loss: 3.6394\n",
      "Epoch: 6164, Train Loss: 0.4169, Test Loss: 3.4006\n",
      "Epoch: 6165, Train Loss: 0.4362, Test Loss: 3.0497\n",
      "Epoch: 6166, Train Loss: 0.5171, Test Loss: 3.4682\n",
      "Epoch: 6167, Train Loss: 0.4148, Test Loss: 3.9424\n",
      "Epoch: 6168, Train Loss: 0.4807, Test Loss: 3.5281\n",
      "Epoch: 6169, Train Loss: 0.4182, Test Loss: 2.9724\n",
      "Epoch: 6170, Train Loss: 0.4428, Test Loss: 2.9589\n",
      "Epoch: 6171, Train Loss: 0.4541, Test Loss: 3.5740\n",
      "Epoch: 6172, Train Loss: 0.4292, Test Loss: 4.1789\n",
      "Epoch: 6173, Train Loss: 0.5069, Test Loss: 3.6438\n",
      "Epoch: 6174, Train Loss: 0.4632, Test Loss: 2.7925\n",
      "Epoch: 6175, Train Loss: 0.6204, Test Loss: 2.9566\n",
      "Epoch: 6176, Train Loss: 0.4741, Test Loss: 3.9445\n",
      "Epoch: 6177, Train Loss: 0.4524, Test Loss: 4.3418\n",
      "Epoch: 6178, Train Loss: 0.7379, Test Loss: 3.0433\n",
      "Epoch: 6179, Train Loss: 0.4946, Test Loss: 2.7731\n",
      "Epoch: 6180, Train Loss: 0.6164, Test Loss: 3.3260\n",
      "Epoch: 6181, Train Loss: 0.4373, Test Loss: 4.1483\n",
      "Epoch: 6182, Train Loss: 0.5740, Test Loss: 3.6115\n",
      "Epoch: 6183, Train Loss: 0.4118, Test Loss: 2.9770\n",
      "Epoch: 6184, Train Loss: 0.4575, Test Loss: 2.9648\n",
      "Epoch: 6185, Train Loss: 0.4871, Test Loss: 3.5766\n",
      "Epoch: 6186, Train Loss: 0.4554, Test Loss: 3.9286\n",
      "Epoch: 6187, Train Loss: 0.4536, Test Loss: 3.4926\n",
      "Epoch: 6188, Train Loss: 0.3968, Test Loss: 3.0168\n",
      "Epoch: 6189, Train Loss: 0.5271, Test Loss: 3.3324\n",
      "Epoch: 6190, Train Loss: 0.4325, Test Loss: 3.7788\n",
      "Epoch: 6191, Train Loss: 0.4635, Test Loss: 3.6145\n",
      "Epoch: 6192, Train Loss: 0.4455, Test Loss: 3.3647\n",
      "Epoch: 6193, Train Loss: 0.4209, Test Loss: 3.1752\n",
      "Epoch: 6194, Train Loss: 0.4284, Test Loss: 3.4778\n",
      "Epoch: 6195, Train Loss: 0.4035, Test Loss: 4.1229\n",
      "Epoch: 6196, Train Loss: 0.4640, Test Loss: 3.8171\n",
      "Epoch: 6197, Train Loss: 0.4603, Test Loss: 2.9303\n",
      "Epoch: 6198, Train Loss: 0.4443, Test Loss: 2.7607\n",
      "Epoch: 6199, Train Loss: 0.5464, Test Loss: 3.4450\n",
      "Epoch: 6200, Train Loss: 0.4616, Test Loss: 4.1432\n",
      "Epoch: 6201, Train Loss: 0.5121, Test Loss: 3.7001\n",
      "Epoch: 6202, Train Loss: 0.5164, Test Loss: 2.8347\n",
      "Epoch: 6203, Train Loss: 0.5420, Test Loss: 2.8715\n",
      "Epoch: 6204, Train Loss: 0.5337, Test Loss: 3.9408\n",
      "Epoch: 6205, Train Loss: 0.5428, Test Loss: 4.0386\n",
      "Epoch: 6206, Train Loss: 0.4946, Test Loss: 3.1091\n",
      "Epoch: 6207, Train Loss: 0.5117, Test Loss: 2.9702\n",
      "Epoch: 6208, Train Loss: 0.5030, Test Loss: 3.6093\n",
      "Epoch: 6209, Train Loss: 0.4239, Test Loss: 3.9678\n",
      "Epoch: 6210, Train Loss: 0.4718, Test Loss: 3.5916\n",
      "Epoch: 6211, Train Loss: 0.4326, Test Loss: 3.0437\n",
      "Epoch: 6212, Train Loss: 0.5269, Test Loss: 3.3769\n",
      "Epoch: 6213, Train Loss: 0.4850, Test Loss: 3.4313\n",
      "Epoch: 6214, Train Loss: 0.4468, Test Loss: 3.2249\n",
      "Epoch: 6215, Train Loss: 0.4441, Test Loss: 3.3437\n",
      "Epoch: 6216, Train Loss: 0.4705, Test Loss: 3.9460\n",
      "Epoch: 6217, Train Loss: 0.4820, Test Loss: 3.5664\n",
      "Epoch: 6218, Train Loss: 0.4079, Test Loss: 3.1691\n",
      "Epoch: 6219, Train Loss: 0.4246, Test Loss: 3.1117\n",
      "Epoch: 6220, Train Loss: 0.4637, Test Loss: 3.5575\n",
      "Epoch: 6221, Train Loss: 0.4256, Test Loss: 4.0555\n",
      "Epoch: 6222, Train Loss: 0.4618, Test Loss: 3.6610\n",
      "Epoch: 6223, Train Loss: 0.4271, Test Loss: 3.1404\n",
      "Epoch: 6224, Train Loss: 0.4394, Test Loss: 2.8616\n",
      "Epoch: 6225, Train Loss: 0.5661, Test Loss: 3.5074\n",
      "Epoch: 6226, Train Loss: 0.4217, Test Loss: 4.2690\n",
      "Epoch: 6227, Train Loss: 0.5731, Test Loss: 3.6395\n",
      "Epoch: 6228, Train Loss: 0.4674, Test Loss: 2.7924\n",
      "Epoch: 6229, Train Loss: 0.4945, Test Loss: 2.7390\n",
      "Epoch: 6230, Train Loss: 0.5546, Test Loss: 3.3583\n",
      "Epoch: 6231, Train Loss: 0.4325, Test Loss: 3.9675\n",
      "Epoch: 6232, Train Loss: 0.5537, Test Loss: 3.4838\n",
      "Epoch: 6233, Train Loss: 0.4493, Test Loss: 2.9556\n",
      "Epoch: 6234, Train Loss: 0.4947, Test Loss: 3.0416\n",
      "Epoch: 6235, Train Loss: 0.4505, Test Loss: 3.6017\n",
      "Epoch: 6236, Train Loss: 0.4396, Test Loss: 4.0960\n",
      "Epoch: 6237, Train Loss: 0.5950, Test Loss: 3.2956\n",
      "Epoch: 6238, Train Loss: 0.4130, Test Loss: 2.8763\n",
      "Epoch: 6239, Train Loss: 0.5176, Test Loss: 3.2755\n",
      "Epoch: 6240, Train Loss: 0.4335, Test Loss: 4.0128\n",
      "Epoch: 6241, Train Loss: 0.4797, Test Loss: 4.0616\n",
      "Epoch: 6242, Train Loss: 0.4862, Test Loss: 3.3016\n",
      "Epoch: 6243, Train Loss: 0.4299, Test Loss: 3.0748\n",
      "Epoch: 6244, Train Loss: 0.5016, Test Loss: 3.3492\n",
      "Epoch: 6245, Train Loss: 0.4542, Test Loss: 3.7606\n",
      "Epoch: 6246, Train Loss: 0.4469, Test Loss: 3.6842\n",
      "Epoch: 6247, Train Loss: 0.4466, Test Loss: 3.2097\n",
      "Epoch: 6248, Train Loss: 0.4249, Test Loss: 3.1776\n",
      "Epoch: 6249, Train Loss: 0.4332, Test Loss: 3.6090\n",
      "Epoch: 6250, Train Loss: 0.4310, Test Loss: 3.8139\n",
      "Epoch: 6251, Train Loss: 0.4834, Test Loss: 3.3805\n",
      "Epoch: 6252, Train Loss: 0.4268, Test Loss: 3.0525\n",
      "Epoch: 6253, Train Loss: 0.4548, Test Loss: 3.3496\n",
      "Epoch: 6254, Train Loss: 0.4194, Test Loss: 4.0006\n",
      "Epoch: 6255, Train Loss: 0.5033, Test Loss: 3.4791\n",
      "Epoch: 6256, Train Loss: 0.4258, Test Loss: 3.0657\n",
      "Epoch: 6257, Train Loss: 0.4350, Test Loss: 3.1004\n",
      "Epoch: 6258, Train Loss: 0.4230, Test Loss: 3.5154\n",
      "Epoch: 6259, Train Loss: 0.4141, Test Loss: 4.0177\n",
      "Epoch: 6260, Train Loss: 0.4717, Test Loss: 3.6389\n",
      "Epoch: 6261, Train Loss: 0.4480, Test Loss: 3.2472\n",
      "Epoch: 6262, Train Loss: 0.4087, Test Loss: 3.1065\n",
      "Epoch: 6263, Train Loss: 0.4248, Test Loss: 3.2486\n",
      "Epoch: 6264, Train Loss: 0.4182, Test Loss: 3.4261\n",
      "Epoch: 6265, Train Loss: 0.4084, Test Loss: 3.5669\n",
      "Epoch: 6266, Train Loss: 0.4316, Test Loss: 3.4877\n",
      "Epoch: 6267, Train Loss: 0.4170, Test Loss: 3.5217\n",
      "Epoch: 6268, Train Loss: 0.4282, Test Loss: 3.3998\n",
      "Epoch: 6269, Train Loss: 0.4372, Test Loss: 3.2639\n",
      "Epoch: 6270, Train Loss: 0.4218, Test Loss: 3.3048\n",
      "Epoch: 6271, Train Loss: 0.4311, Test Loss: 3.5649\n",
      "Epoch: 6272, Train Loss: 0.4107, Test Loss: 3.7332\n",
      "Epoch: 6273, Train Loss: 0.4292, Test Loss: 3.6408\n",
      "Epoch: 6274, Train Loss: 0.4656, Test Loss: 3.1458\n",
      "Epoch: 6275, Train Loss: 0.4239, Test Loss: 3.2222\n",
      "Epoch: 6276, Train Loss: 0.4511, Test Loss: 3.7964\n",
      "Epoch: 6277, Train Loss: 0.4914, Test Loss: 3.5235\n",
      "Epoch: 6278, Train Loss: 0.4423, Test Loss: 3.0712\n",
      "Epoch: 6279, Train Loss: 0.4649, Test Loss: 3.3134\n",
      "Epoch: 6280, Train Loss: 0.4528, Test Loss: 3.7304\n",
      "Epoch: 6281, Train Loss: 0.4781, Test Loss: 3.6938\n",
      "Epoch: 6282, Train Loss: 0.4255, Test Loss: 3.4077\n",
      "Epoch: 6283, Train Loss: 0.3861, Test Loss: 3.2271\n",
      "Epoch: 6284, Train Loss: 0.4379, Test Loss: 3.4382\n",
      "Epoch: 6285, Train Loss: 0.4087, Test Loss: 3.7899\n",
      "Epoch: 6286, Train Loss: 0.4643, Test Loss: 3.4310\n",
      "Epoch: 6287, Train Loss: 0.4080, Test Loss: 3.1233\n",
      "Epoch: 6288, Train Loss: 0.4577, Test Loss: 3.4277\n",
      "Epoch: 6289, Train Loss: 0.4321, Test Loss: 3.8545\n",
      "Epoch: 6290, Train Loss: 0.4663, Test Loss: 3.5576\n",
      "Epoch: 6291, Train Loss: 0.3900, Test Loss: 3.1468\n",
      "Epoch: 6292, Train Loss: 0.4241, Test Loss: 3.1943\n",
      "Epoch: 6293, Train Loss: 0.4419, Test Loss: 3.8111\n",
      "Epoch: 6294, Train Loss: 0.4435, Test Loss: 3.9567\n",
      "Epoch: 6295, Train Loss: 0.4481, Test Loss: 3.4615\n",
      "Epoch: 6296, Train Loss: 0.4094, Test Loss: 3.2250\n",
      "Epoch: 6297, Train Loss: 0.4710, Test Loss: 3.6595\n",
      "Epoch: 6298, Train Loss: 0.4675, Test Loss: 3.6652\n",
      "Epoch: 6299, Train Loss: 0.4316, Test Loss: 3.2071\n",
      "Epoch: 6300, Train Loss: 0.4499, Test Loss: 3.1389\n",
      "Epoch: 6301, Train Loss: 0.4369, Test Loss: 3.6568\n",
      "Epoch: 6302, Train Loss: 0.4395, Test Loss: 3.9105\n",
      "Epoch: 6303, Train Loss: 0.4786, Test Loss: 3.3302\n",
      "Epoch: 6304, Train Loss: 0.5033, Test Loss: 3.4507\n",
      "Epoch: 6305, Train Loss: 0.4455, Test Loss: 3.2497\n",
      "Epoch: 6306, Train Loss: 0.4468, Test Loss: 3.6114\n",
      "Epoch: 6307, Train Loss: 0.4423, Test Loss: 3.7471\n",
      "Epoch: 6308, Train Loss: 0.4181, Test Loss: 3.4611\n",
      "Epoch: 6309, Train Loss: 0.4295, Test Loss: 3.2648\n",
      "Epoch: 6310, Train Loss: 0.4554, Test Loss: 3.4635\n",
      "Epoch: 6311, Train Loss: 0.4264, Test Loss: 3.7953\n",
      "Epoch: 6312, Train Loss: 0.4287, Test Loss: 3.5933\n",
      "Epoch: 6313, Train Loss: 0.4110, Test Loss: 3.0704\n",
      "Epoch: 6314, Train Loss: 0.4426, Test Loss: 3.1964\n",
      "Epoch: 6315, Train Loss: 0.4396, Test Loss: 3.7926\n",
      "Epoch: 6316, Train Loss: 0.4158, Test Loss: 3.9121\n",
      "Epoch: 6317, Train Loss: 0.4499, Test Loss: 3.4592\n",
      "Epoch: 6318, Train Loss: 0.4304, Test Loss: 3.2282\n",
      "Epoch: 6319, Train Loss: 0.4012, Test Loss: 3.3329\n",
      "Epoch: 6320, Train Loss: 0.4284, Test Loss: 3.9873\n",
      "Epoch: 6321, Train Loss: 0.5207, Test Loss: 3.8226\n",
      "Epoch: 6322, Train Loss: 0.4335, Test Loss: 3.3572\n",
      "Epoch: 6323, Train Loss: 0.3848, Test Loss: 3.1273\n",
      "Epoch: 6324, Train Loss: 0.4493, Test Loss: 3.6428\n",
      "Epoch: 6325, Train Loss: 0.4311, Test Loss: 3.8307\n",
      "Epoch: 6326, Train Loss: 0.4403, Test Loss: 3.5033\n",
      "Epoch: 6327, Train Loss: 0.3989, Test Loss: 3.2549\n",
      "Epoch: 6328, Train Loss: 0.4182, Test Loss: 3.1757\n",
      "Epoch: 6329, Train Loss: 0.4950, Test Loss: 3.6918\n",
      "Epoch: 6330, Train Loss: 0.4292, Test Loss: 3.8532\n",
      "Epoch: 6331, Train Loss: 0.4413, Test Loss: 3.4063\n",
      "Epoch: 6332, Train Loss: 0.4082, Test Loss: 3.1335\n",
      "Epoch: 6333, Train Loss: 0.4302, Test Loss: 3.2275\n",
      "Epoch: 6334, Train Loss: 0.3786, Test Loss: 3.6074\n",
      "Epoch: 6335, Train Loss: 0.4202, Test Loss: 3.9175\n",
      "Epoch: 6336, Train Loss: 0.4432, Test Loss: 3.6405\n",
      "Epoch: 6337, Train Loss: 0.4110, Test Loss: 3.3139\n",
      "Epoch: 6338, Train Loss: 0.4168, Test Loss: 3.2077\n",
      "Epoch: 6339, Train Loss: 0.4681, Test Loss: 3.7387\n",
      "Epoch: 6340, Train Loss: 0.4364, Test Loss: 3.7887\n",
      "Epoch: 6341, Train Loss: 0.4260, Test Loss: 3.3949\n",
      "Epoch: 6342, Train Loss: 0.3883, Test Loss: 3.1861\n",
      "Epoch: 6343, Train Loss: 0.4385, Test Loss: 3.4366\n",
      "Epoch: 6344, Train Loss: 0.4078, Test Loss: 3.7606\n",
      "Epoch: 6345, Train Loss: 0.4703, Test Loss: 3.4573\n",
      "Epoch: 6346, Train Loss: 0.4111, Test Loss: 3.1544\n",
      "Epoch: 6347, Train Loss: 0.4301, Test Loss: 3.3399\n",
      "Epoch: 6348, Train Loss: 0.4171, Test Loss: 3.8592\n",
      "Epoch: 6349, Train Loss: 0.4187, Test Loss: 4.1039\n",
      "Epoch: 6350, Train Loss: 0.4788, Test Loss: 3.2952\n",
      "Epoch: 6351, Train Loss: 0.4163, Test Loss: 3.0746\n",
      "Epoch: 6352, Train Loss: 0.4672, Test Loss: 3.5366\n",
      "Epoch: 6353, Train Loss: 0.4325, Test Loss: 4.0619\n",
      "Epoch: 6354, Train Loss: 0.4809, Test Loss: 3.5490\n",
      "Epoch: 6355, Train Loss: 0.4258, Test Loss: 2.9483\n",
      "Epoch: 6356, Train Loss: 0.5681, Test Loss: 3.2282\n",
      "Epoch: 6357, Train Loss: 0.4174, Test Loss: 4.0810\n",
      "Epoch: 6358, Train Loss: 0.4395, Test Loss: 4.1591\n",
      "Epoch: 6359, Train Loss: 0.5655, Test Loss: 3.1069\n",
      "Epoch: 6360, Train Loss: 0.4547, Test Loss: 2.9143\n",
      "Epoch: 6361, Train Loss: 0.4723, Test Loss: 3.2400\n",
      "Epoch: 6362, Train Loss: 0.4199, Test Loss: 3.9597\n",
      "Epoch: 6363, Train Loss: 0.5005, Test Loss: 3.6356\n",
      "Epoch: 6364, Train Loss: 0.4231, Test Loss: 3.0741\n",
      "Epoch: 6365, Train Loss: 0.4307, Test Loss: 3.1136\n",
      "Epoch: 6366, Train Loss: 0.4461, Test Loss: 3.6837\n",
      "Epoch: 6367, Train Loss: 0.4573, Test Loss: 3.8216\n",
      "Epoch: 6368, Train Loss: 0.4303, Test Loss: 3.4460\n",
      "Epoch: 6369, Train Loss: 0.4356, Test Loss: 3.1846\n",
      "Epoch: 6370, Train Loss: 0.4323, Test Loss: 3.2662\n",
      "Epoch: 6371, Train Loss: 0.4252, Test Loss: 3.6338\n",
      "Epoch: 6372, Train Loss: 0.4162, Test Loss: 4.0375\n",
      "Epoch: 6373, Train Loss: 0.5458, Test Loss: 3.4336\n",
      "Epoch: 6374, Train Loss: 0.4226, Test Loss: 3.1353\n",
      "Epoch: 6375, Train Loss: 0.4131, Test Loss: 3.3092\n",
      "Epoch: 6376, Train Loss: 0.4099, Test Loss: 3.6569\n",
      "Epoch: 6377, Train Loss: 0.4241, Test Loss: 3.5453\n",
      "Epoch: 6378, Train Loss: 0.4340, Test Loss: 3.0914\n",
      "Epoch: 6379, Train Loss: 0.4534, Test Loss: 3.2470\n",
      "Epoch: 6380, Train Loss: 0.4352, Test Loss: 3.8981\n",
      "Epoch: 6381, Train Loss: 0.4353, Test Loss: 4.0086\n",
      "Epoch: 6382, Train Loss: 0.4500, Test Loss: 3.5179\n",
      "Epoch: 6383, Train Loss: 0.3925, Test Loss: 3.0779\n",
      "Epoch: 6384, Train Loss: 0.4516, Test Loss: 3.2519\n",
      "Epoch: 6385, Train Loss: 0.4224, Test Loss: 3.9149\n",
      "Epoch: 6386, Train Loss: 0.4726, Test Loss: 3.7477\n",
      "Epoch: 6387, Train Loss: 0.4201, Test Loss: 3.1827\n",
      "Epoch: 6388, Train Loss: 0.4093, Test Loss: 2.9696\n",
      "Epoch: 6389, Train Loss: 0.5348, Test Loss: 3.5523\n",
      "Epoch: 6390, Train Loss: 0.4022, Test Loss: 4.2707\n",
      "Epoch: 6391, Train Loss: 0.5366, Test Loss: 3.5508\n",
      "Epoch: 6392, Train Loss: 0.4142, Test Loss: 3.0326\n",
      "Epoch: 6393, Train Loss: 0.4556, Test Loss: 3.1409\n",
      "Epoch: 6394, Train Loss: 0.4413, Test Loss: 3.5121\n",
      "Epoch: 6395, Train Loss: 0.4389, Test Loss: 3.7739\n",
      "Epoch: 6396, Train Loss: 0.4647, Test Loss: 3.3579\n",
      "Epoch: 6397, Train Loss: 0.4537, Test Loss: 3.1042\n",
      "Epoch: 6398, Train Loss: 0.4414, Test Loss: 3.5073\n",
      "Epoch: 6399, Train Loss: 0.4191, Test Loss: 3.5381\n",
      "Epoch: 6400, Train Loss: 0.4100, Test Loss: 3.4599\n",
      "Epoch: 6401, Train Loss: 0.4000, Test Loss: 3.3711\n",
      "Epoch: 6402, Train Loss: 0.4475, Test Loss: 3.4788\n",
      "Epoch: 6403, Train Loss: 0.4246, Test Loss: 3.5929\n",
      "Epoch: 6404, Train Loss: 0.4162, Test Loss: 3.5286\n",
      "Epoch: 6405, Train Loss: 0.4104, Test Loss: 3.5803\n",
      "Epoch: 6406, Train Loss: 0.4063, Test Loss: 3.5102\n",
      "Epoch: 6407, Train Loss: 0.3936, Test Loss: 3.4074\n",
      "Epoch: 6408, Train Loss: 0.4539, Test Loss: 3.6960\n",
      "Epoch: 6409, Train Loss: 0.4528, Test Loss: 3.2047\n",
      "Epoch: 6410, Train Loss: 0.4176, Test Loss: 3.0304\n",
      "Epoch: 6411, Train Loss: 0.4438, Test Loss: 3.3563\n",
      "Epoch: 6412, Train Loss: 0.4204, Test Loss: 4.0988\n",
      "Epoch: 6413, Train Loss: 0.5041, Test Loss: 3.6271\n",
      "Epoch: 6414, Train Loss: 0.4223, Test Loss: 3.0402\n",
      "Epoch: 6415, Train Loss: 0.5004, Test Loss: 3.3305\n",
      "Epoch: 6416, Train Loss: 0.3963, Test Loss: 3.5970\n",
      "Epoch: 6417, Train Loss: 0.4535, Test Loss: 3.5484\n",
      "Epoch: 6418, Train Loss: 0.4579, Test Loss: 3.2189\n",
      "Epoch: 6419, Train Loss: 0.4412, Test Loss: 3.4256\n",
      "Epoch: 6420, Train Loss: 0.4109, Test Loss: 3.5097\n",
      "Epoch: 6421, Train Loss: 0.4197, Test Loss: 3.6965\n",
      "Epoch: 6422, Train Loss: 0.3910, Test Loss: 3.4614\n",
      "Epoch: 6423, Train Loss: 0.4067, Test Loss: 3.3584\n",
      "Epoch: 6424, Train Loss: 0.4063, Test Loss: 3.3914\n",
      "Epoch: 6425, Train Loss: 0.4206, Test Loss: 3.6732\n",
      "Epoch: 6426, Train Loss: 0.4619, Test Loss: 3.6920\n",
      "Epoch: 6427, Train Loss: 0.4176, Test Loss: 3.3010\n",
      "Epoch: 6428, Train Loss: 0.4463, Test Loss: 3.2000\n",
      "Epoch: 6429, Train Loss: 0.4053, Test Loss: 3.5050\n",
      "Epoch: 6430, Train Loss: 0.4250, Test Loss: 3.7708\n",
      "Epoch: 6431, Train Loss: 0.4330, Test Loss: 3.4916\n",
      "Epoch: 6432, Train Loss: 0.4096, Test Loss: 3.0816\n",
      "Epoch: 6433, Train Loss: 0.5942, Test Loss: 3.9350\n",
      "Epoch: 6434, Train Loss: 0.4359, Test Loss: 3.9832\n",
      "Epoch: 6435, Train Loss: 0.4399, Test Loss: 3.3922\n",
      "Epoch: 6436, Train Loss: 0.4138, Test Loss: 3.0216\n",
      "Epoch: 6437, Train Loss: 0.4993, Test Loss: 3.4952\n",
      "Epoch: 6438, Train Loss: 0.4012, Test Loss: 3.6873\n",
      "Epoch: 6439, Train Loss: 0.4222, Test Loss: 3.4769\n",
      "Epoch: 6440, Train Loss: 0.3859, Test Loss: 3.1306\n",
      "Epoch: 6441, Train Loss: 0.4514, Test Loss: 3.4142\n",
      "Epoch: 6442, Train Loss: 0.3967, Test Loss: 3.6249\n",
      "Epoch: 6443, Train Loss: 0.4447, Test Loss: 3.3316\n",
      "Epoch: 6444, Train Loss: 0.3953, Test Loss: 3.1633\n",
      "Epoch: 6445, Train Loss: 0.4653, Test Loss: 3.3024\n",
      "Epoch: 6446, Train Loss: 0.4119, Test Loss: 3.7751\n",
      "Epoch: 6447, Train Loss: 0.4087, Test Loss: 3.8631\n",
      "Epoch: 6448, Train Loss: 0.4634, Test Loss: 3.2189\n",
      "Epoch: 6449, Train Loss: 0.4188, Test Loss: 2.9542\n",
      "Epoch: 6450, Train Loss: 0.5195, Test Loss: 3.5078\n",
      "Epoch: 6451, Train Loss: 0.4138, Test Loss: 3.7834\n",
      "Epoch: 6452, Train Loss: 0.4337, Test Loss: 3.2685\n",
      "Epoch: 6453, Train Loss: 0.4055, Test Loss: 3.2004\n",
      "Epoch: 6454, Train Loss: 0.4266, Test Loss: 3.5302\n",
      "Epoch: 6455, Train Loss: 0.4197, Test Loss: 3.5232\n",
      "Epoch: 6456, Train Loss: 0.4438, Test Loss: 3.3130\n",
      "Epoch: 6457, Train Loss: 0.3969, Test Loss: 3.1071\n",
      "Epoch: 6458, Train Loss: 0.4308, Test Loss: 3.1012\n",
      "Epoch: 6459, Train Loss: 0.4284, Test Loss: 3.6707\n",
      "Epoch: 6460, Train Loss: 0.4182, Test Loss: 3.8775\n",
      "Epoch: 6461, Train Loss: 0.4390, Test Loss: 3.4841\n",
      "Epoch: 6462, Train Loss: 0.4246, Test Loss: 2.9626\n",
      "Epoch: 6463, Train Loss: 0.4406, Test Loss: 3.1254\n",
      "Epoch: 6464, Train Loss: 0.4776, Test Loss: 4.2367\n",
      "Epoch: 6465, Train Loss: 0.5481, Test Loss: 3.9028\n",
      "Epoch: 6466, Train Loss: 0.4441, Test Loss: 2.9950\n",
      "Epoch: 6467, Train Loss: 0.4699, Test Loss: 2.9008\n",
      "Epoch: 6468, Train Loss: 0.4736, Test Loss: 3.5117\n",
      "Epoch: 6469, Train Loss: 0.4172, Test Loss: 4.0591\n",
      "Epoch: 6470, Train Loss: 0.5018, Test Loss: 3.4686\n",
      "Epoch: 6471, Train Loss: 0.4007, Test Loss: 2.9084\n",
      "Epoch: 6472, Train Loss: 0.4675, Test Loss: 2.9550\n",
      "Epoch: 6473, Train Loss: 0.4582, Test Loss: 3.6323\n",
      "Epoch: 6474, Train Loss: 0.4646, Test Loss: 4.0901\n",
      "Epoch: 6475, Train Loss: 0.5209, Test Loss: 3.4971\n",
      "Epoch: 6476, Train Loss: 0.4209, Test Loss: 3.0235\n",
      "Epoch: 6477, Train Loss: 0.4950, Test Loss: 3.0336\n",
      "Epoch: 6478, Train Loss: 0.4740, Test Loss: 3.7809\n",
      "Epoch: 6479, Train Loss: 0.5345, Test Loss: 3.7216\n",
      "Epoch: 6480, Train Loss: 0.4309, Test Loss: 3.1731\n",
      "Epoch: 6481, Train Loss: 0.4218, Test Loss: 2.9599\n",
      "Epoch: 6482, Train Loss: 0.4713, Test Loss: 3.4164\n",
      "Epoch: 6483, Train Loss: 0.4087, Test Loss: 3.7587\n",
      "Epoch: 6484, Train Loss: 0.4813, Test Loss: 3.4011\n",
      "Epoch: 6485, Train Loss: 0.4245, Test Loss: 2.9573\n",
      "Epoch: 6486, Train Loss: 0.4498, Test Loss: 3.1596\n",
      "Epoch: 6487, Train Loss: 0.4819, Test Loss: 4.2290\n",
      "Epoch: 6488, Train Loss: 0.5814, Test Loss: 3.8086\n",
      "Epoch: 6489, Train Loss: 0.4607, Test Loss: 3.0033\n",
      "Epoch: 6490, Train Loss: 0.4447, Test Loss: 2.8038\n",
      "Epoch: 6491, Train Loss: 0.4956, Test Loss: 3.2298\n",
      "Epoch: 6492, Train Loss: 0.4527, Test Loss: 3.9585\n",
      "Epoch: 6493, Train Loss: 0.5307, Test Loss: 3.5523\n",
      "Epoch: 6494, Train Loss: 0.4901, Test Loss: 3.2032\n",
      "Epoch: 6495, Train Loss: 0.4088, Test Loss: 3.1615\n",
      "Epoch: 6496, Train Loss: 0.4235, Test Loss: 3.3593\n",
      "Epoch: 6497, Train Loss: 0.4091, Test Loss: 3.6475\n",
      "Epoch: 6498, Train Loss: 0.4590, Test Loss: 3.3509\n",
      "Epoch: 6499, Train Loss: 0.4279, Test Loss: 3.3397\n",
      "Epoch: 6500, Train Loss: 0.4249, Test Loss: 3.2042\n",
      "Epoch: 6501, Train Loss: 0.4115, Test Loss: 3.4402\n",
      "Epoch: 6502, Train Loss: 0.3993, Test Loss: 3.6898\n",
      "Epoch: 6503, Train Loss: 0.4012, Test Loss: 3.5565\n",
      "Epoch: 6504, Train Loss: 0.3942, Test Loss: 3.1643\n",
      "Epoch: 6505, Train Loss: 0.4187, Test Loss: 3.1669\n",
      "Epoch: 6506, Train Loss: 0.4379, Test Loss: 3.7476\n",
      "Epoch: 6507, Train Loss: 0.4225, Test Loss: 3.7151\n",
      "Epoch: 6508, Train Loss: 0.4014, Test Loss: 3.3524\n",
      "Epoch: 6509, Train Loss: 0.4065, Test Loss: 2.9934\n",
      "Epoch: 6510, Train Loss: 0.4628, Test Loss: 3.3055\n",
      "Epoch: 6511, Train Loss: 0.4366, Test Loss: 3.6873\n",
      "Epoch: 6512, Train Loss: 0.4454, Test Loss: 3.5628\n",
      "Epoch: 6513, Train Loss: 0.4034, Test Loss: 3.2089\n",
      "Epoch: 6514, Train Loss: 0.4079, Test Loss: 3.1478\n",
      "Epoch: 6515, Train Loss: 0.4225, Test Loss: 3.2490\n",
      "Epoch: 6516, Train Loss: 0.4361, Test Loss: 3.6468\n",
      "Epoch: 6517, Train Loss: 0.4371, Test Loss: 3.3978\n",
      "Epoch: 6518, Train Loss: 0.4083, Test Loss: 3.2398\n",
      "Epoch: 6519, Train Loss: 0.4145, Test Loss: 3.2815\n",
      "Epoch: 6520, Train Loss: 0.4351, Test Loss: 3.5142\n",
      "Epoch: 6521, Train Loss: 0.4156, Test Loss: 3.3661\n",
      "Epoch: 6522, Train Loss: 0.3949, Test Loss: 3.2576\n",
      "Epoch: 6523, Train Loss: 0.3976, Test Loss: 3.3976\n",
      "Epoch: 6524, Train Loss: 0.3949, Test Loss: 3.5373\n",
      "Epoch: 6525, Train Loss: 0.4128, Test Loss: 3.6119\n",
      "Epoch: 6526, Train Loss: 0.3844, Test Loss: 3.5223\n",
      "Epoch: 6527, Train Loss: 0.4095, Test Loss: 3.2266\n",
      "Epoch: 6528, Train Loss: 0.4297, Test Loss: 3.1954\n",
      "Epoch: 6529, Train Loss: 0.4294, Test Loss: 3.7475\n",
      "Epoch: 6530, Train Loss: 0.4329, Test Loss: 4.0541\n",
      "Epoch: 6531, Train Loss: 0.5009, Test Loss: 3.3697\n",
      "Epoch: 6532, Train Loss: 0.4237, Test Loss: 3.0963\n",
      "Epoch: 6533, Train Loss: 0.4900, Test Loss: 3.7065\n",
      "Epoch: 6534, Train Loss: 0.4183, Test Loss: 3.9202\n",
      "Epoch: 6535, Train Loss: 0.4234, Test Loss: 3.5928\n",
      "Epoch: 6536, Train Loss: 0.3916, Test Loss: 3.2699\n",
      "Epoch: 6537, Train Loss: 0.5783, Test Loss: 3.8443\n",
      "Epoch: 6538, Train Loss: 0.4273, Test Loss: 3.8127\n",
      "Epoch: 6539, Train Loss: 0.4087, Test Loss: 3.4005\n",
      "Epoch: 6540, Train Loss: 0.4857, Test Loss: 2.9257\n",
      "Epoch: 6541, Train Loss: 0.5177, Test Loss: 3.1976\n",
      "Epoch: 6542, Train Loss: 0.4438, Test Loss: 3.6608\n",
      "Epoch: 6543, Train Loss: 0.4666, Test Loss: 3.6598\n",
      "Epoch: 6544, Train Loss: 0.4490, Test Loss: 2.9830\n",
      "Epoch: 6545, Train Loss: 0.4857, Test Loss: 3.0276\n",
      "Epoch: 6546, Train Loss: 0.5058, Test Loss: 3.7617\n",
      "Epoch: 6547, Train Loss: 0.4527, Test Loss: 4.0381\n",
      "Epoch: 6548, Train Loss: 0.5157, Test Loss: 3.2913\n",
      "Epoch: 6549, Train Loss: 0.4413, Test Loss: 2.8948\n",
      "Epoch: 6550, Train Loss: 0.5394, Test Loss: 3.2953\n",
      "Epoch: 6551, Train Loss: 0.4172, Test Loss: 3.9636\n",
      "Epoch: 6552, Train Loss: 0.4997, Test Loss: 3.5649\n",
      "Epoch: 6553, Train Loss: 0.4507, Test Loss: 2.8682\n",
      "Epoch: 6554, Train Loss: 0.5149, Test Loss: 3.0675\n",
      "Epoch: 6555, Train Loss: 0.4618, Test Loss: 3.8144\n",
      "Epoch: 6556, Train Loss: 0.4604, Test Loss: 3.7402\n",
      "Epoch: 6557, Train Loss: 0.4816, Test Loss: 2.8380\n",
      "Epoch: 6558, Train Loss: 0.5246, Test Loss: 2.8973\n",
      "Epoch: 6559, Train Loss: 0.4622, Test Loss: 3.4332\n",
      "Epoch: 6560, Train Loss: 0.4092, Test Loss: 4.0714\n",
      "Epoch: 6561, Train Loss: 0.5090, Test Loss: 3.4995\n",
      "Epoch: 6562, Train Loss: 0.5179, Test Loss: 2.7065\n",
      "Epoch: 6563, Train Loss: 0.6072, Test Loss: 2.7910\n",
      "Epoch: 6564, Train Loss: 0.5095, Test Loss: 3.7521\n",
      "Epoch: 6565, Train Loss: 0.4556, Test Loss: 4.1471\n",
      "Epoch: 6566, Train Loss: 0.6096, Test Loss: 3.1241\n",
      "Epoch: 6567, Train Loss: 0.4158, Test Loss: 2.7795\n",
      "Epoch: 6568, Train Loss: 0.4927, Test Loss: 3.1633\n",
      "Epoch: 6569, Train Loss: 0.4168, Test Loss: 3.8542\n",
      "Epoch: 6570, Train Loss: 0.5098, Test Loss: 3.4149\n",
      "Epoch: 6571, Train Loss: 0.4061, Test Loss: 2.9275\n",
      "Epoch: 6572, Train Loss: 0.4835, Test Loss: 3.0250\n",
      "Epoch: 6573, Train Loss: 0.4527, Test Loss: 3.4808\n",
      "Epoch: 6574, Train Loss: 0.4169, Test Loss: 3.7006\n",
      "Epoch: 6575, Train Loss: 0.4252, Test Loss: 3.2835\n",
      "Epoch: 6576, Train Loss: 0.4421, Test Loss: 3.0335\n",
      "Epoch: 6577, Train Loss: 0.4405, Test Loss: 3.1264\n",
      "Epoch: 6578, Train Loss: 0.5004, Test Loss: 3.8101\n",
      "Epoch: 6579, Train Loss: 0.4460, Test Loss: 3.6179\n",
      "Epoch: 6580, Train Loss: 0.4336, Test Loss: 3.0997\n",
      "Epoch: 6581, Train Loss: 0.4245, Test Loss: 2.9613\n",
      "Epoch: 6582, Train Loss: 0.4626, Test Loss: 3.3403\n",
      "Epoch: 6583, Train Loss: 0.4057, Test Loss: 3.7356\n",
      "Epoch: 6584, Train Loss: 0.4800, Test Loss: 3.4248\n",
      "Epoch: 6585, Train Loss: 0.4165, Test Loss: 2.8657\n",
      "Epoch: 6586, Train Loss: 0.5208, Test Loss: 3.1850\n",
      "Epoch: 6587, Train Loss: 0.3911, Test Loss: 3.7116\n",
      "Epoch: 6588, Train Loss: 0.4517, Test Loss: 3.6053\n",
      "Epoch: 6589, Train Loss: 0.4492, Test Loss: 2.9355\n",
      "Epoch: 6590, Train Loss: 0.4816, Test Loss: 2.9921\n",
      "Epoch: 6591, Train Loss: 0.4720, Test Loss: 3.6446\n",
      "Epoch: 6592, Train Loss: 0.4572, Test Loss: 3.5845\n",
      "Epoch: 6593, Train Loss: 0.4425, Test Loss: 3.0403\n",
      "Epoch: 6594, Train Loss: 0.4220, Test Loss: 2.8612\n",
      "Epoch: 6595, Train Loss: 0.4519, Test Loss: 3.1294\n",
      "Epoch: 6596, Train Loss: 0.4305, Test Loss: 3.9858\n",
      "Epoch: 6597, Train Loss: 0.4860, Test Loss: 3.7321\n",
      "Epoch: 6598, Train Loss: 0.4275, Test Loss: 2.9734\n",
      "Epoch: 6599, Train Loss: 0.4490, Test Loss: 2.9249\n",
      "Epoch: 6600, Train Loss: 0.4195, Test Loss: 3.1294\n",
      "Epoch: 6601, Train Loss: 0.4068, Test Loss: 3.4785\n",
      "Epoch: 6602, Train Loss: 0.3864, Test Loss: 3.4650\n",
      "Epoch: 6603, Train Loss: 0.3915, Test Loss: 3.2031\n",
      "Epoch: 6604, Train Loss: 0.3892, Test Loss: 3.0809\n",
      "Epoch: 6605, Train Loss: 0.4601, Test Loss: 3.4715\n",
      "Epoch: 6606, Train Loss: 0.4316, Test Loss: 3.6017\n",
      "Epoch: 6607, Train Loss: 0.4553, Test Loss: 3.0322\n",
      "Epoch: 6608, Train Loss: 0.4424, Test Loss: 2.8653\n",
      "Epoch: 6609, Train Loss: 0.4873, Test Loss: 3.1967\n",
      "Epoch: 6610, Train Loss: 0.3992, Test Loss: 3.9410\n",
      "Epoch: 6611, Train Loss: 0.6074, Test Loss: 3.1823\n",
      "Epoch: 6612, Train Loss: 0.3926, Test Loss: 2.6673\n",
      "Epoch: 6613, Train Loss: 0.6869, Test Loss: 3.3622\n",
      "Epoch: 6614, Train Loss: 0.4187, Test Loss: 3.9488\n",
      "Epoch: 6615, Train Loss: 0.5113, Test Loss: 3.3760\n",
      "Epoch: 6616, Train Loss: 0.4248, Test Loss: 2.8390\n",
      "Epoch: 6617, Train Loss: 0.4567, Test Loss: 2.8125\n",
      "Epoch: 6618, Train Loss: 0.4652, Test Loss: 3.4724\n",
      "Epoch: 6619, Train Loss: 0.4123, Test Loss: 4.1361\n",
      "Epoch: 6620, Train Loss: 0.5989, Test Loss: 3.2514\n",
      "Epoch: 6621, Train Loss: 0.4389, Test Loss: 2.8122\n",
      "Epoch: 6622, Train Loss: 0.5452, Test Loss: 3.1945\n",
      "Epoch: 6623, Train Loss: 0.4140, Test Loss: 3.7764\n",
      "Epoch: 6624, Train Loss: 0.4937, Test Loss: 3.4875\n",
      "Epoch: 6625, Train Loss: 0.4261, Test Loss: 2.9643\n",
      "Epoch: 6626, Train Loss: 0.4486, Test Loss: 2.9513\n",
      "Epoch: 6627, Train Loss: 0.4509, Test Loss: 3.3903\n",
      "Epoch: 6628, Train Loss: 0.4502, Test Loss: 3.8738\n",
      "Epoch: 6629, Train Loss: 0.4574, Test Loss: 3.3714\n",
      "Epoch: 6630, Train Loss: 0.4155, Test Loss: 2.8414\n",
      "Epoch: 6631, Train Loss: 0.4477, Test Loss: 2.8804\n",
      "Epoch: 6632, Train Loss: 0.4819, Test Loss: 3.7250\n",
      "Epoch: 6633, Train Loss: 0.4919, Test Loss: 3.8681\n",
      "Epoch: 6634, Train Loss: 0.5011, Test Loss: 3.0386\n",
      "Epoch: 6635, Train Loss: 0.4445, Test Loss: 2.8093\n",
      "Epoch: 6636, Train Loss: 0.4725, Test Loss: 3.1244\n",
      "Epoch: 6637, Train Loss: 0.4228, Test Loss: 3.8855\n",
      "Epoch: 6638, Train Loss: 0.4844, Test Loss: 3.6247\n",
      "Epoch: 6639, Train Loss: 0.4397, Test Loss: 2.9425\n",
      "Epoch: 6640, Train Loss: 0.4463, Test Loss: 2.8617\n",
      "Epoch: 6641, Train Loss: 0.4695, Test Loss: 3.2955\n",
      "Epoch: 6642, Train Loss: 0.4355, Test Loss: 3.5159\n",
      "Epoch: 6643, Train Loss: 0.4567, Test Loss: 3.1331\n",
      "Epoch: 6644, Train Loss: 0.4130, Test Loss: 3.0005\n",
      "Epoch: 6645, Train Loss: 0.4540, Test Loss: 3.4032\n",
      "Epoch: 6646, Train Loss: 0.4345, Test Loss: 3.6502\n",
      "Epoch: 6647, Train Loss: 0.4653, Test Loss: 3.2456\n",
      "Epoch: 6648, Train Loss: 0.4612, Test Loss: 2.9281\n",
      "Epoch: 6649, Train Loss: 0.4566, Test Loss: 3.1867\n",
      "Epoch: 6650, Train Loss: 0.3879, Test Loss: 3.6739\n",
      "Epoch: 6651, Train Loss: 0.4659, Test Loss: 3.4665\n",
      "Epoch: 6652, Train Loss: 0.4193, Test Loss: 3.0874\n",
      "Epoch: 6653, Train Loss: 0.4470, Test Loss: 3.2099\n",
      "Epoch: 6654, Train Loss: 0.4425, Test Loss: 3.7544\n",
      "Epoch: 6655, Train Loss: 0.4247, Test Loss: 3.5504\n",
      "Epoch: 6656, Train Loss: 0.4439, Test Loss: 3.0201\n",
      "Epoch: 6657, Train Loss: 0.4494, Test Loss: 2.8214\n",
      "Epoch: 6658, Train Loss: 0.4718, Test Loss: 3.3024\n",
      "Epoch: 6659, Train Loss: 0.4134, Test Loss: 3.8548\n",
      "Epoch: 6660, Train Loss: 0.5993, Test Loss: 3.1971\n",
      "Epoch: 6661, Train Loss: 0.4419, Test Loss: 2.7473\n",
      "Epoch: 6662, Train Loss: 0.4992, Test Loss: 2.9249\n",
      "Epoch: 6663, Train Loss: 0.4552, Test Loss: 3.5503\n",
      "Epoch: 6664, Train Loss: 0.4400, Test Loss: 3.7019\n",
      "Epoch: 6665, Train Loss: 0.4390, Test Loss: 3.0665\n",
      "Epoch: 6666, Train Loss: 0.4720, Test Loss: 2.9784\n",
      "Epoch: 6667, Train Loss: 0.4474, Test Loss: 3.5087\n",
      "Epoch: 6668, Train Loss: 0.4012, Test Loss: 3.6240\n",
      "Epoch: 6669, Train Loss: 0.4457, Test Loss: 3.2204\n",
      "Epoch: 6670, Train Loss: 0.3840, Test Loss: 2.8712\n",
      "Epoch: 6671, Train Loss: 0.4786, Test Loss: 3.0930\n",
      "Epoch: 6672, Train Loss: 0.4019, Test Loss: 3.6618\n",
      "Epoch: 6673, Train Loss: 0.4460, Test Loss: 3.5293\n",
      "Epoch: 6674, Train Loss: 0.4760, Test Loss: 2.8151\n",
      "Epoch: 6675, Train Loss: 0.4703, Test Loss: 2.7737\n",
      "Epoch: 6676, Train Loss: 0.4451, Test Loss: 3.1908\n",
      "Epoch: 6677, Train Loss: 0.3957, Test Loss: 3.6853\n",
      "Epoch: 6678, Train Loss: 0.5470, Test Loss: 3.0743\n",
      "Epoch: 6679, Train Loss: 0.4103, Test Loss: 2.7591\n",
      "Epoch: 6680, Train Loss: 0.5011, Test Loss: 3.2086\n",
      "Epoch: 6681, Train Loss: 0.4076, Test Loss: 3.8251\n",
      "Epoch: 6682, Train Loss: 0.4771, Test Loss: 3.5280\n",
      "Epoch: 6683, Train Loss: 0.3822, Test Loss: 3.0007\n",
      "Epoch: 6684, Train Loss: 0.4792, Test Loss: 3.1999\n",
      "Epoch: 6685, Train Loss: 0.4864, Test Loss: 3.6138\n",
      "Epoch: 6686, Train Loss: 0.4070, Test Loss: 3.7748\n",
      "Epoch: 6687, Train Loss: 0.5012, Test Loss: 3.1143\n",
      "Epoch: 6688, Train Loss: 0.4388, Test Loss: 2.9345\n",
      "Epoch: 6689, Train Loss: 0.4996, Test Loss: 3.3930\n",
      "Epoch: 6690, Train Loss: 0.4030, Test Loss: 3.8464\n",
      "Epoch: 6691, Train Loss: 0.4840, Test Loss: 3.2701\n",
      "Epoch: 6692, Train Loss: 0.4641, Test Loss: 3.0135\n",
      "Epoch: 6693, Train Loss: 0.4259, Test Loss: 3.1815\n",
      "Epoch: 6694, Train Loss: 0.3867, Test Loss: 3.4566\n",
      "Epoch: 6695, Train Loss: 0.3859, Test Loss: 3.7431\n",
      "Epoch: 6696, Train Loss: 0.4604, Test Loss: 3.3435\n",
      "Epoch: 6697, Train Loss: 0.3900, Test Loss: 3.0595\n",
      "Epoch: 6698, Train Loss: 0.4480, Test Loss: 3.3344\n",
      "Epoch: 6699, Train Loss: 0.4056, Test Loss: 3.8096\n",
      "Epoch: 6700, Train Loss: 0.4677, Test Loss: 3.4191\n",
      "Epoch: 6701, Train Loss: 0.4023, Test Loss: 3.0250\n",
      "Epoch: 6702, Train Loss: 0.4487, Test Loss: 3.1132\n",
      "Epoch: 6703, Train Loss: 0.4446, Test Loss: 3.5632\n",
      "Epoch: 6704, Train Loss: 0.4199, Test Loss: 3.6179\n",
      "Epoch: 6705, Train Loss: 0.4395, Test Loss: 3.5154\n",
      "Epoch: 6706, Train Loss: 0.4449, Test Loss: 3.1606\n",
      "Epoch: 6707, Train Loss: 0.4058, Test Loss: 3.0847\n",
      "Epoch: 6708, Train Loss: 0.4728, Test Loss: 3.2950\n",
      "Epoch: 6709, Train Loss: 0.4148, Test Loss: 3.8632\n",
      "Epoch: 6710, Train Loss: 0.4453, Test Loss: 3.5816\n",
      "Epoch: 6711, Train Loss: 0.4805, Test Loss: 3.2180\n",
      "Epoch: 6712, Train Loss: 0.4541, Test Loss: 3.3093\n",
      "Epoch: 6713, Train Loss: 0.4483, Test Loss: 3.6298\n",
      "Epoch: 6714, Train Loss: 0.4170, Test Loss: 3.5704\n",
      "Epoch: 6715, Train Loss: 0.4600, Test Loss: 3.0131\n",
      "Epoch: 6716, Train Loss: 0.4680, Test Loss: 2.9861\n",
      "Epoch: 6717, Train Loss: 0.4527, Test Loss: 3.4709\n",
      "Epoch: 6718, Train Loss: 0.3961, Test Loss: 3.8683\n",
      "Epoch: 6719, Train Loss: 0.5010, Test Loss: 3.2930\n",
      "Epoch: 6720, Train Loss: 0.3994, Test Loss: 2.9888\n",
      "Epoch: 6721, Train Loss: 0.4787, Test Loss: 3.2392\n",
      "Epoch: 6722, Train Loss: 0.3922, Test Loss: 3.6265\n",
      "Epoch: 6723, Train Loss: 0.4335, Test Loss: 3.4513\n",
      "Epoch: 6724, Train Loss: 0.4528, Test Loss: 3.2081\n",
      "Epoch: 6725, Train Loss: 0.4095, Test Loss: 3.2326\n",
      "Epoch: 6726, Train Loss: 0.4182, Test Loss: 3.4211\n",
      "Epoch: 6727, Train Loss: 0.4065, Test Loss: 3.3873\n",
      "Epoch: 6728, Train Loss: 0.4127, Test Loss: 3.5755\n",
      "Epoch: 6729, Train Loss: 0.4503, Test Loss: 3.0209\n",
      "Epoch: 6730, Train Loss: 0.3968, Test Loss: 2.7432\n",
      "Epoch: 6731, Train Loss: 0.5018, Test Loss: 3.1696\n",
      "Epoch: 6732, Train Loss: 0.4390, Test Loss: 3.7284\n",
      "Epoch: 6733, Train Loss: 0.4545, Test Loss: 3.5506\n",
      "Epoch: 6734, Train Loss: 0.4155, Test Loss: 3.3285\n",
      "Epoch: 6735, Train Loss: 0.4366, Test Loss: 2.8582\n",
      "Epoch: 6736, Train Loss: 0.4957, Test Loss: 3.0771\n",
      "Epoch: 6737, Train Loss: 0.3972, Test Loss: 3.6489\n",
      "Epoch: 6738, Train Loss: 0.4446, Test Loss: 3.5739\n",
      "Epoch: 6739, Train Loss: 0.4087, Test Loss: 3.2988\n",
      "Epoch: 6740, Train Loss: 0.4283, Test Loss: 3.3269\n",
      "Epoch: 6741, Train Loss: 0.4098, Test Loss: 3.3736\n",
      "Epoch: 6742, Train Loss: 0.4340, Test Loss: 3.4461\n",
      "Epoch: 6743, Train Loss: 0.4023, Test Loss: 3.4187\n",
      "Epoch: 6744, Train Loss: 0.4152, Test Loss: 3.3679\n",
      "Epoch: 6745, Train Loss: 0.4112, Test Loss: 3.2566\n",
      "Epoch: 6746, Train Loss: 0.4140, Test Loss: 3.3071\n",
      "Epoch: 6747, Train Loss: 0.4396, Test Loss: 3.7386\n",
      "Epoch: 6748, Train Loss: 0.4425, Test Loss: 3.4751\n",
      "Epoch: 6749, Train Loss: 0.3955, Test Loss: 3.3825\n",
      "Epoch: 6750, Train Loss: 0.4123, Test Loss: 3.4747\n",
      "Epoch: 6751, Train Loss: 0.4409, Test Loss: 3.4286\n",
      "Epoch: 6752, Train Loss: 0.3889, Test Loss: 3.3911\n",
      "Epoch: 6753, Train Loss: 0.4058, Test Loss: 3.2395\n",
      "Epoch: 6754, Train Loss: 0.4102, Test Loss: 3.1741\n",
      "Epoch: 6755, Train Loss: 0.4371, Test Loss: 3.4377\n",
      "Epoch: 6756, Train Loss: 0.4008, Test Loss: 3.7413\n",
      "Epoch: 6757, Train Loss: 0.4504, Test Loss: 3.2614\n",
      "Epoch: 6758, Train Loss: 0.4101, Test Loss: 3.0227\n",
      "Epoch: 6759, Train Loss: 0.4206, Test Loss: 3.1517\n",
      "Epoch: 6760, Train Loss: 0.4107, Test Loss: 3.6293\n",
      "Epoch: 6761, Train Loss: 0.4116, Test Loss: 3.6083\n",
      "Epoch: 6762, Train Loss: 0.4161, Test Loss: 3.3460\n",
      "Epoch: 6763, Train Loss: 0.4390, Test Loss: 3.2908\n",
      "Epoch: 6764, Train Loss: 0.3964, Test Loss: 3.2956\n",
      "Epoch: 6765, Train Loss: 0.4142, Test Loss: 3.5346\n",
      "Epoch: 6766, Train Loss: 0.4329, Test Loss: 3.5092\n",
      "Epoch: 6767, Train Loss: 0.4105, Test Loss: 3.2575\n",
      "Epoch: 6768, Train Loss: 0.4655, Test Loss: 3.5586\n",
      "Epoch: 6769, Train Loss: 0.4084, Test Loss: 3.7681\n",
      "Epoch: 6770, Train Loss: 0.4424, Test Loss: 3.4346\n",
      "Epoch: 6771, Train Loss: 0.3892, Test Loss: 3.2719\n",
      "Epoch: 6772, Train Loss: 0.4102, Test Loss: 3.5295\n",
      "Epoch: 6773, Train Loss: 0.3991, Test Loss: 3.6742\n",
      "Epoch: 6774, Train Loss: 0.3810, Test Loss: 3.5217\n",
      "Epoch: 6775, Train Loss: 0.4024, Test Loss: 3.3348\n",
      "Epoch: 6776, Train Loss: 0.3827, Test Loss: 3.0743\n",
      "Epoch: 6777, Train Loss: 0.4071, Test Loss: 3.2100\n",
      "Epoch: 6778, Train Loss: 0.4760, Test Loss: 4.1541\n",
      "Epoch: 6779, Train Loss: 0.5343, Test Loss: 3.5246\n",
      "Epoch: 6780, Train Loss: 0.3820, Test Loss: 3.0229\n",
      "Epoch: 6781, Train Loss: 0.4478, Test Loss: 3.2967\n",
      "Epoch: 6782, Train Loss: 0.4225, Test Loss: 3.6992\n",
      "Epoch: 6783, Train Loss: 0.4564, Test Loss: 3.3675\n",
      "Epoch: 6784, Train Loss: 0.4665, Test Loss: 2.9608\n",
      "Epoch: 6785, Train Loss: 0.4911, Test Loss: 3.3633\n",
      "Epoch: 6786, Train Loss: 0.3895, Test Loss: 3.7882\n",
      "Epoch: 6787, Train Loss: 0.4643, Test Loss: 3.6169\n",
      "Epoch: 6788, Train Loss: 0.4256, Test Loss: 2.9839\n",
      "Epoch: 6789, Train Loss: 0.5275, Test Loss: 3.4421\n",
      "Epoch: 6790, Train Loss: 0.3885, Test Loss: 3.9776\n",
      "Epoch: 6791, Train Loss: 0.5211, Test Loss: 3.3327\n",
      "Epoch: 6792, Train Loss: 0.4246, Test Loss: 2.9358\n",
      "Epoch: 6793, Train Loss: 0.4732, Test Loss: 3.2110\n",
      "Epoch: 6794, Train Loss: 0.4184, Test Loss: 3.7361\n",
      "Epoch: 6795, Train Loss: 0.4392, Test Loss: 3.6490\n",
      "Epoch: 6796, Train Loss: 0.4797, Test Loss: 3.0519\n",
      "Epoch: 6797, Train Loss: 0.4413, Test Loss: 2.9584\n",
      "Epoch: 6798, Train Loss: 0.4846, Test Loss: 3.5796\n",
      "Epoch: 6799, Train Loss: 0.4151, Test Loss: 3.7536\n",
      "Epoch: 6800, Train Loss: 0.4295, Test Loss: 3.4587\n",
      "Epoch: 6801, Train Loss: 0.4309, Test Loss: 2.9353\n",
      "Epoch: 6802, Train Loss: 0.4745, Test Loss: 2.9291\n",
      "Epoch: 6803, Train Loss: 0.4711, Test Loss: 3.5612\n",
      "Epoch: 6804, Train Loss: 0.4135, Test Loss: 4.1586\n",
      "Epoch: 6805, Train Loss: 0.5379, Test Loss: 3.2010\n",
      "Epoch: 6806, Train Loss: 0.4150, Test Loss: 2.7634\n",
      "Epoch: 6807, Train Loss: 0.5641, Test Loss: 3.2505\n",
      "Epoch: 6808, Train Loss: 0.4697, Test Loss: 4.2233\n",
      "Epoch: 6809, Train Loss: 0.5392, Test Loss: 3.7878\n",
      "Epoch: 6810, Train Loss: 0.5207, Test Loss: 2.8466\n",
      "Epoch: 6811, Train Loss: 0.4992, Test Loss: 2.7352\n",
      "Epoch: 6812, Train Loss: 0.5000, Test Loss: 3.2147\n",
      "Epoch: 6813, Train Loss: 0.3814, Test Loss: 4.0758\n",
      "Epoch: 6814, Train Loss: 0.4983, Test Loss: 3.8140\n",
      "Epoch: 6815, Train Loss: 0.4381, Test Loss: 3.0231\n",
      "Epoch: 6816, Train Loss: 0.4484, Test Loss: 2.8694\n",
      "Epoch: 6817, Train Loss: 0.4704, Test Loss: 3.2455\n",
      "Epoch: 6818, Train Loss: 0.3860, Test Loss: 3.7183\n",
      "Epoch: 6819, Train Loss: 0.4948, Test Loss: 3.3802\n",
      "Epoch: 6820, Train Loss: 0.4048, Test Loss: 3.1192\n",
      "Epoch: 6821, Train Loss: 0.3956, Test Loss: 3.1365\n",
      "Epoch: 6822, Train Loss: 0.4115, Test Loss: 3.2433\n",
      "Epoch: 6823, Train Loss: 0.4023, Test Loss: 3.8774\n",
      "Epoch: 6824, Train Loss: 0.4906, Test Loss: 3.3779\n",
      "Epoch: 6825, Train Loss: 0.4163, Test Loss: 3.1383\n",
      "Epoch: 6826, Train Loss: 0.3985, Test Loss: 3.1617\n",
      "Epoch: 6827, Train Loss: 0.3962, Test Loss: 3.3555\n",
      "Epoch: 6828, Train Loss: 0.4120, Test Loss: 3.4738\n",
      "Epoch: 6829, Train Loss: 0.4233, Test Loss: 3.4673\n",
      "Epoch: 6830, Train Loss: 0.4475, Test Loss: 3.2842\n",
      "Epoch: 6831, Train Loss: 0.4153, Test Loss: 3.2107\n",
      "Epoch: 6832, Train Loss: 0.3918, Test Loss: 3.3211\n",
      "Epoch: 6833, Train Loss: 0.4123, Test Loss: 3.6616\n",
      "Epoch: 6834, Train Loss: 0.4300, Test Loss: 3.4381\n",
      "Epoch: 6835, Train Loss: 0.4102, Test Loss: 3.0975\n",
      "Epoch: 6836, Train Loss: 0.4566, Test Loss: 3.2036\n",
      "Epoch: 6837, Train Loss: 0.3952, Test Loss: 3.3962\n",
      "Epoch: 6838, Train Loss: 0.4062, Test Loss: 3.2528\n",
      "Epoch: 6839, Train Loss: 0.4159, Test Loss: 3.2858\n",
      "Epoch: 6840, Train Loss: 0.4249, Test Loss: 3.2266\n",
      "Epoch: 6841, Train Loss: 0.4226, Test Loss: 3.2548\n",
      "Epoch: 6842, Train Loss: 0.3824, Test Loss: 3.2521\n",
      "Epoch: 6843, Train Loss: 0.3970, Test Loss: 3.2832\n",
      "Epoch: 6844, Train Loss: 0.4189, Test Loss: 3.1906\n",
      "Epoch: 6845, Train Loss: 0.4128, Test Loss: 3.2360\n",
      "Epoch: 6846, Train Loss: 0.3854, Test Loss: 3.3429\n",
      "Epoch: 6847, Train Loss: 0.4052, Test Loss: 3.3110\n",
      "Epoch: 6848, Train Loss: 0.4392, Test Loss: 3.3082\n",
      "Epoch: 6849, Train Loss: 0.3821, Test Loss: 3.4379\n",
      "Epoch: 6850, Train Loss: 0.4114, Test Loss: 3.6536\n",
      "Epoch: 6851, Train Loss: 0.4880, Test Loss: 3.0739\n",
      "Epoch: 6852, Train Loss: 0.4227, Test Loss: 2.9247\n",
      "Epoch: 6853, Train Loss: 0.4618, Test Loss: 3.3598\n",
      "Epoch: 6854, Train Loss: 0.3983, Test Loss: 3.7066\n",
      "Epoch: 6855, Train Loss: 0.5072, Test Loss: 3.0986\n",
      "Epoch: 6856, Train Loss: 0.4178, Test Loss: 3.0776\n",
      "Epoch: 6857, Train Loss: 0.4823, Test Loss: 3.7031\n",
      "Epoch: 6858, Train Loss: 0.4812, Test Loss: 3.5993\n",
      "Epoch: 6859, Train Loss: 0.4789, Test Loss: 2.8253\n",
      "Epoch: 6860, Train Loss: 0.4555, Test Loss: 2.7420\n",
      "Epoch: 6861, Train Loss: 0.6451, Test Loss: 3.7080\n",
      "Epoch: 6862, Train Loss: 0.4927, Test Loss: 3.8635\n",
      "Epoch: 6863, Train Loss: 0.5191, Test Loss: 3.0050\n",
      "Epoch: 6864, Train Loss: 0.4117, Test Loss: 2.6522\n",
      "Epoch: 6865, Train Loss: 0.5772, Test Loss: 3.0641\n",
      "Epoch: 6866, Train Loss: 0.4204, Test Loss: 3.6842\n",
      "Epoch: 6867, Train Loss: 0.4766, Test Loss: 3.4239\n",
      "Epoch: 6868, Train Loss: 0.4228, Test Loss: 2.8557\n",
      "Epoch: 6869, Train Loss: 0.4447, Test Loss: 2.8210\n",
      "Epoch: 6870, Train Loss: 0.4498, Test Loss: 3.4333\n",
      "Epoch: 6871, Train Loss: 0.4310, Test Loss: 3.5979\n",
      "Epoch: 6872, Train Loss: 0.4420, Test Loss: 3.1760\n",
      "Epoch: 6873, Train Loss: 0.4268, Test Loss: 2.9679\n",
      "Epoch: 6874, Train Loss: 0.4219, Test Loss: 3.0075\n",
      "Epoch: 6875, Train Loss: 0.4202, Test Loss: 3.4500\n",
      "Epoch: 6876, Train Loss: 0.4068, Test Loss: 3.6494\n",
      "Epoch: 6877, Train Loss: 0.4636, Test Loss: 3.0562\n",
      "Epoch: 6878, Train Loss: 0.4033, Test Loss: 2.6805\n",
      "Epoch: 6879, Train Loss: 0.5795, Test Loss: 3.2702\n",
      "Epoch: 6880, Train Loss: 0.3937, Test Loss: 3.8842\n",
      "Epoch: 6881, Train Loss: 0.4896, Test Loss: 3.3832\n",
      "Epoch: 6882, Train Loss: 0.4353, Test Loss: 2.8121\n",
      "Epoch: 6883, Train Loss: 0.4769, Test Loss: 2.8846\n",
      "Epoch: 6884, Train Loss: 0.4408, Test Loss: 3.3102\n",
      "Epoch: 6885, Train Loss: 0.4249, Test Loss: 3.7293\n",
      "Epoch: 6886, Train Loss: 0.4853, Test Loss: 3.2164\n",
      "Epoch: 6887, Train Loss: 0.3679, Test Loss: 2.8349\n",
      "Epoch: 6888, Train Loss: 0.4898, Test Loss: 3.0426\n",
      "Epoch: 6889, Train Loss: 0.4570, Test Loss: 3.7739\n",
      "Epoch: 6890, Train Loss: 0.4400, Test Loss: 3.7433\n",
      "Epoch: 6891, Train Loss: 0.4959, Test Loss: 2.8478\n",
      "Epoch: 6892, Train Loss: 0.4838, Test Loss: 2.8066\n",
      "Epoch: 6893, Train Loss: 0.4958, Test Loss: 3.3704\n",
      "Epoch: 6894, Train Loss: 0.4302, Test Loss: 3.6524\n",
      "Epoch: 6895, Train Loss: 0.5122, Test Loss: 3.0890\n",
      "Epoch: 6896, Train Loss: 0.4318, Test Loss: 2.9058\n",
      "Epoch: 6897, Train Loss: 0.4830, Test Loss: 3.4735\n",
      "Epoch: 6898, Train Loss: 0.4147, Test Loss: 3.8387\n",
      "Epoch: 6899, Train Loss: 0.4648, Test Loss: 3.3890\n",
      "Epoch: 6900, Train Loss: 0.3990, Test Loss: 2.8971\n",
      "Epoch: 6901, Train Loss: 0.4807, Test Loss: 3.1434\n",
      "Epoch: 6902, Train Loss: 0.4166, Test Loss: 3.6257\n",
      "Epoch: 6903, Train Loss: 0.4461, Test Loss: 3.3247\n",
      "Epoch: 6904, Train Loss: 0.3850, Test Loss: 3.1992\n",
      "Epoch: 6905, Train Loss: 0.3971, Test Loss: 3.2271\n",
      "Epoch: 6906, Train Loss: 0.3882, Test Loss: 3.2673\n",
      "Epoch: 6907, Train Loss: 0.3924, Test Loss: 3.1762\n",
      "Epoch: 6908, Train Loss: 0.3934, Test Loss: 3.4040\n",
      "Epoch: 6909, Train Loss: 0.3960, Test Loss: 3.5446\n",
      "Epoch: 6910, Train Loss: 0.4097, Test Loss: 3.2117\n",
      "Epoch: 6911, Train Loss: 0.3856, Test Loss: 2.9433\n",
      "Epoch: 6912, Train Loss: 0.4268, Test Loss: 3.1770\n",
      "Epoch: 6913, Train Loss: 0.3989, Test Loss: 3.5851\n",
      "Epoch: 6914, Train Loss: 0.4026, Test Loss: 3.5577\n",
      "Epoch: 6915, Train Loss: 0.4191, Test Loss: 2.9942\n",
      "Epoch: 6916, Train Loss: 0.4258, Test Loss: 2.8835\n",
      "Epoch: 6917, Train Loss: 0.4606, Test Loss: 3.4575\n",
      "Epoch: 6918, Train Loss: 0.4307, Test Loss: 3.9352\n",
      "Epoch: 6919, Train Loss: 0.4660, Test Loss: 3.3454\n",
      "Epoch: 6920, Train Loss: 0.3988, Test Loss: 2.9240\n",
      "Epoch: 6921, Train Loss: 0.4648, Test Loss: 3.0190\n",
      "Epoch: 6922, Train Loss: 0.4208, Test Loss: 3.6354\n",
      "Epoch: 6923, Train Loss: 0.4094, Test Loss: 3.8294\n",
      "Epoch: 6924, Train Loss: 0.4695, Test Loss: 3.1311\n",
      "Epoch: 6925, Train Loss: 0.3784, Test Loss: 2.9353\n",
      "Epoch: 6926, Train Loss: 0.4632, Test Loss: 3.5177\n",
      "Epoch: 6927, Train Loss: 0.3981, Test Loss: 3.7133\n",
      "Epoch: 6928, Train Loss: 0.4335, Test Loss: 3.2570\n",
      "Epoch: 6929, Train Loss: 0.3831, Test Loss: 3.0988\n",
      "Epoch: 6930, Train Loss: 0.4303, Test Loss: 3.4394\n",
      "Epoch: 6931, Train Loss: 0.4015, Test Loss: 3.7306\n",
      "Epoch: 6932, Train Loss: 0.4206, Test Loss: 3.5315\n",
      "Epoch: 6933, Train Loss: 0.3870, Test Loss: 3.2082\n",
      "Epoch: 6934, Train Loss: 0.4029, Test Loss: 3.3868\n",
      "Epoch: 6935, Train Loss: 0.3807, Test Loss: 3.4659\n",
      "Epoch: 6936, Train Loss: 0.3782, Test Loss: 3.6506\n",
      "Epoch: 6937, Train Loss: 0.3806, Test Loss: 3.5056\n",
      "Epoch: 6938, Train Loss: 0.3821, Test Loss: 3.3192\n",
      "Epoch: 6939, Train Loss: 0.4137, Test Loss: 3.4025\n",
      "Epoch: 6940, Train Loss: 0.3762, Test Loss: 3.4295\n",
      "Epoch: 6941, Train Loss: 0.3833, Test Loss: 3.2152\n",
      "Epoch: 6942, Train Loss: 0.4609, Test Loss: 3.6479\n",
      "Epoch: 6943, Train Loss: 0.4177, Test Loss: 3.6177\n",
      "Epoch: 6944, Train Loss: 0.3963, Test Loss: 3.2243\n",
      "Epoch: 6945, Train Loss: 0.3793, Test Loss: 3.1858\n",
      "Epoch: 6946, Train Loss: 0.4020, Test Loss: 3.3434\n",
      "Epoch: 6947, Train Loss: 0.3786, Test Loss: 3.4879\n",
      "Epoch: 6948, Train Loss: 0.3926, Test Loss: 3.4814\n",
      "Epoch: 6949, Train Loss: 0.4002, Test Loss: 3.2976\n",
      "Epoch: 6950, Train Loss: 0.4011, Test Loss: 3.1819\n",
      "Epoch: 6951, Train Loss: 0.3717, Test Loss: 3.2148\n",
      "Epoch: 6952, Train Loss: 0.3825, Test Loss: 3.3531\n",
      "Epoch: 6953, Train Loss: 0.4084, Test Loss: 3.3641\n",
      "Epoch: 6954, Train Loss: 0.3987, Test Loss: 3.2605\n",
      "Epoch: 6955, Train Loss: 0.3999, Test Loss: 3.3295\n",
      "Epoch: 6956, Train Loss: 0.4400, Test Loss: 3.3594\n",
      "Epoch: 6957, Train Loss: 0.3989, Test Loss: 3.6086\n",
      "Epoch: 6958, Train Loss: 0.4665, Test Loss: 3.2186\n",
      "Epoch: 6959, Train Loss: 0.4338, Test Loss: 3.2690\n",
      "Epoch: 6960, Train Loss: 0.3974, Test Loss: 3.5062\n",
      "Epoch: 6961, Train Loss: 0.4394, Test Loss: 3.3314\n",
      "Epoch: 6962, Train Loss: 0.3943, Test Loss: 3.2810\n",
      "Epoch: 6963, Train Loss: 0.3961, Test Loss: 3.2488\n",
      "Epoch: 6964, Train Loss: 0.4567, Test Loss: 3.5569\n",
      "Epoch: 6965, Train Loss: 0.3983, Test Loss: 3.4426\n",
      "Epoch: 6966, Train Loss: 0.4195, Test Loss: 3.1786\n",
      "Epoch: 6967, Train Loss: 0.4286, Test Loss: 3.3019\n",
      "Epoch: 6968, Train Loss: 0.4084, Test Loss: 3.7360\n",
      "Epoch: 6969, Train Loss: 0.4120, Test Loss: 3.7092\n",
      "Epoch: 6970, Train Loss: 0.4291, Test Loss: 3.1181\n",
      "Epoch: 6971, Train Loss: 0.4307, Test Loss: 3.1965\n",
      "Epoch: 6972, Train Loss: 0.3828, Test Loss: 3.4953\n",
      "Epoch: 6973, Train Loss: 0.4013, Test Loss: 3.7445\n",
      "Epoch: 6974, Train Loss: 0.4716, Test Loss: 3.3420\n",
      "Epoch: 6975, Train Loss: 0.3932, Test Loss: 2.9951\n",
      "Epoch: 6976, Train Loss: 0.4484, Test Loss: 3.2943\n",
      "Epoch: 6977, Train Loss: 0.3748, Test Loss: 3.9758\n",
      "Epoch: 6978, Train Loss: 0.4463, Test Loss: 3.8450\n",
      "Epoch: 6979, Train Loss: 0.4097, Test Loss: 3.2012\n",
      "Epoch: 6980, Train Loss: 0.3938, Test Loss: 2.9721\n",
      "Epoch: 6981, Train Loss: 0.4450, Test Loss: 3.3385\n",
      "Epoch: 6982, Train Loss: 0.3852, Test Loss: 3.7637\n",
      "Epoch: 6983, Train Loss: 0.4267, Test Loss: 3.4865\n",
      "Epoch: 6984, Train Loss: 0.4021, Test Loss: 3.1557\n",
      "Epoch: 6985, Train Loss: 0.4027, Test Loss: 3.0558\n",
      "Epoch: 6986, Train Loss: 0.4025, Test Loss: 3.3454\n",
      "Epoch: 6987, Train Loss: 0.4211, Test Loss: 3.3217\n",
      "Epoch: 6988, Train Loss: 0.3913, Test Loss: 3.2472\n",
      "Epoch: 6989, Train Loss: 0.3881, Test Loss: 3.3290\n",
      "Epoch: 6990, Train Loss: 0.4076, Test Loss: 3.6308\n",
      "Epoch: 6991, Train Loss: 0.4346, Test Loss: 3.5254\n",
      "Epoch: 6992, Train Loss: 0.4133, Test Loss: 3.1957\n",
      "Epoch: 6993, Train Loss: 0.4127, Test Loss: 3.3006\n",
      "Epoch: 6994, Train Loss: 0.3987, Test Loss: 3.6763\n",
      "Epoch: 6995, Train Loss: 0.3966, Test Loss: 3.6222\n",
      "Epoch: 6996, Train Loss: 0.4097, Test Loss: 3.2381\n",
      "Epoch: 6997, Train Loss: 0.4508, Test Loss: 3.4138\n",
      "Epoch: 6998, Train Loss: 0.3951, Test Loss: 3.4763\n",
      "Epoch: 6999, Train Loss: 0.3810, Test Loss: 3.3793\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_graph=data_graph.to(device)\n",
    "data_graph_test=data_graph_test.to(device)\n",
    "model = GraphNetwork(num_features=3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            test_loss = criterion(out, data.y).item()\n",
    "            total_test_loss += test_loss\n",
    "    return total_test_loss / len(test_loader)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(7000):  # Número de épocas\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>USUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.582001</td>\n",
       "      <td>40.032001</td>\n",
       "      <td>20.097000</td>\n",
       "      <td>2.12400</td>\n",
       "      <td>-8.8403</td>\n",
       "      <td>-2.71130</td>\n",
       "      <td>9.487540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.944000</td>\n",
       "      <td>30.215000</td>\n",
       "      <td>44.122002</td>\n",
       "      <td>-1.82430</td>\n",
       "      <td>-2.3479</td>\n",
       "      <td>-0.17169</td>\n",
       "      <td>2.978285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.366001</td>\n",
       "      <td>29.947001</td>\n",
       "      <td>30.886000</td>\n",
       "      <td>0.89116</td>\n",
       "      <td>-1.3280</td>\n",
       "      <td>0.73230</td>\n",
       "      <td>1.758981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.872999</td>\n",
       "      <td>46.994999</td>\n",
       "      <td>49.827000</td>\n",
       "      <td>-3.40900</td>\n",
       "      <td>-6.2240</td>\n",
       "      <td>2.41350</td>\n",
       "      <td>7.495628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.955000</td>\n",
       "      <td>38.188999</td>\n",
       "      <td>36.492001</td>\n",
       "      <td>-0.72033</td>\n",
       "      <td>-2.3768</td>\n",
       "      <td>0.90551</td>\n",
       "      <td>2.643483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>29.174000</td>\n",
       "      <td>48.321999</td>\n",
       "      <td>22.091999</td>\n",
       "      <td>0.36343</td>\n",
       "      <td>-11.1580</td>\n",
       "      <td>-4.61930</td>\n",
       "      <td>12.081845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>35.830002</td>\n",
       "      <td>41.921001</td>\n",
       "      <td>19.884001</td>\n",
       "      <td>-0.75359</td>\n",
       "      <td>-11.5290</td>\n",
       "      <td>-3.88220</td>\n",
       "      <td>12.188405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>24.440001</td>\n",
       "      <td>53.500999</td>\n",
       "      <td>44.631001</td>\n",
       "      <td>-1.92460</td>\n",
       "      <td>-11.8080</td>\n",
       "      <td>4.80600</td>\n",
       "      <td>12.893044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>40.583000</td>\n",
       "      <td>43.110001</td>\n",
       "      <td>19.731001</td>\n",
       "      <td>0.53535</td>\n",
       "      <td>-12.2450</td>\n",
       "      <td>-4.44510</td>\n",
       "      <td>13.037850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>30.790001</td>\n",
       "      <td>32.734001</td>\n",
       "      <td>31.853001</td>\n",
       "      <td>-0.66593</td>\n",
       "      <td>-1.9334</td>\n",
       "      <td>0.33911</td>\n",
       "      <td>2.072798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X          Y          Z        U        V        W       USUM\n",
       "0    46.582001  40.032001  20.097000  2.12400  -8.8403 -2.71130   9.487540\n",
       "1    20.944000  30.215000  44.122002 -1.82430  -2.3479 -0.17169   2.978285\n",
       "2    46.366001  29.947001  30.886000  0.89116  -1.3280  0.73230   1.758981\n",
       "3    17.872999  46.994999  49.827000 -3.40900  -6.2240  2.41350   7.495628\n",
       "4    27.955000  38.188999  36.492001 -0.72033  -2.3768  0.90551   2.643483\n",
       "..         ...        ...        ...      ...      ...      ...        ...\n",
       "694  29.174000  48.321999  22.091999  0.36343 -11.1580 -4.61930  12.081845\n",
       "695  35.830002  41.921001  19.884001 -0.75359 -11.5290 -3.88220  12.188405\n",
       "696  24.440001  53.500999  44.631001 -1.92460 -11.8080  4.80600  12.893044\n",
       "697  40.583000  43.110001  19.731001  0.53535 -12.2450 -4.44510  13.037850\n",
       "698  30.790001  32.734001  31.853001 -0.66593  -1.9334  0.33911   2.072798\n",
       "\n",
       "[699 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_plot=pd.concat([pd.DataFrame(test_init_pos_main.detach().cpu().numpy(),columns=[\"X\",\"Y\",\"Z\"]),pd.DataFrame(test_disp_main.detach().cpu().numpy(),columns=[\"U\",\"V\",\"W\"])],axis=1)\n",
    "aux_plot[\"USUM\"]=np.sqrt(aux_plot[\"U\"]**2+aux_plot[\"V\"]**2+aux_plot[\"W\"]**2)\n",
    "aux_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_pred=pd.DataFrame(model(data_graph_test).detach().cpu().numpy()[:,:3],columns=[\"pred_U\",\"pred_V\",\"pred_W\"])\n",
    "aux_plot[\"pred_X\"]=aux_plot[\"X\"]+aux_pred[\"pred_U\"]\n",
    "aux_plot[\"pred_Y\"]=aux_plot[\"Y\"]+aux_pred[\"pred_V\"]\n",
    "aux_plot[\"pred_Z\"]=aux_plot[\"Z\"]+aux_pred[\"pred_W\"]\n",
    "aux_plot[\"real_X\"]=aux_plot[\"X\"]+aux_plot[\"U\"]\n",
    "aux_plot[\"real_Y\"]=aux_plot[\"Y\"]+aux_plot[\"V\"]\n",
    "aux_plot[\"real_Z\"]=aux_plot[\"Z\"]+aux_plot[\"W\"]\n",
    "aux_plot[\"dif_real_pred_X\"]=aux_plot[\"real_X\"]-aux_plot[\"pred_X\"]\n",
    "aux_plot[\"dif_real_pred_Y\"]=aux_plot[\"real_Y\"]-aux_plot[\"pred_Y\"]\n",
    "aux_plot[\"dif_real_pred_Z\"]=aux_plot[\"real_Z\"]-aux_plot[\"pred_Z\"]\n",
    "aux_plot[\"relativ_dif_real_pred_X\"]=aux_plot[\"dif_real_pred_X\"]/aux_plot[\"U\"]\n",
    "aux_plot[\"relativ_dif_real_pred_Y\"]=aux_plot[\"dif_real_pred_Y\"]/aux_plot[\"V\"]\n",
    "aux_plot[\"relativ_dif_real_pred_Z\"]=aux_plot[\"dif_real_pred_Z\"]/aux_plot[\"W\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>USUM</th>\n",
       "      <th>pred_X</th>\n",
       "      <th>pred_Y</th>\n",
       "      <th>pred_Z</th>\n",
       "      <th>real_X</th>\n",
       "      <th>real_Y</th>\n",
       "      <th>real_Z</th>\n",
       "      <th>dif_real_pred_X</th>\n",
       "      <th>dif_real_pred_Y</th>\n",
       "      <th>dif_real_pred_Z</th>\n",
       "      <th>relativ_dif_real_pred_X</th>\n",
       "      <th>relativ_dif_real_pred_Y</th>\n",
       "      <th>relativ_dif_real_pred_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.516998</td>\n",
       "      <td>45.685001</td>\n",
       "      <td>44.712002</td>\n",
       "      <td>2.10480</td>\n",
       "      <td>-2.8949</td>\n",
       "      <td>0.42117</td>\n",
       "      <td>3.603889</td>\n",
       "      <td>62.280289</td>\n",
       "      <td>41.478485</td>\n",
       "      <td>47.252644</td>\n",
       "      <td>65.621796</td>\n",
       "      <td>42.790100</td>\n",
       "      <td>45.133171</td>\n",
       "      <td>3.341507</td>\n",
       "      <td>1.311615</td>\n",
       "      <td>-2.119473</td>\n",
       "      <td>1.587565</td>\n",
       "      <td>-0.453078</td>\n",
       "      <td>-5.032344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.767000</td>\n",
       "      <td>46.138000</td>\n",
       "      <td>48.994999</td>\n",
       "      <td>-3.29820</td>\n",
       "      <td>-6.0279</td>\n",
       "      <td>2.34430</td>\n",
       "      <td>7.260128</td>\n",
       "      <td>18.417025</td>\n",
       "      <td>39.064743</td>\n",
       "      <td>50.214577</td>\n",
       "      <td>15.468801</td>\n",
       "      <td>40.110100</td>\n",
       "      <td>51.339298</td>\n",
       "      <td>-2.948224</td>\n",
       "      <td>1.045357</td>\n",
       "      <td>1.124722</td>\n",
       "      <td>0.893889</td>\n",
       "      <td>-0.173420</td>\n",
       "      <td>0.479769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.183998</td>\n",
       "      <td>39.841999</td>\n",
       "      <td>51.174999</td>\n",
       "      <td>4.14480</td>\n",
       "      <td>-4.8995</td>\n",
       "      <td>1.66980</td>\n",
       "      <td>6.631191</td>\n",
       "      <td>59.549892</td>\n",
       "      <td>35.236408</td>\n",
       "      <td>53.641766</td>\n",
       "      <td>63.328796</td>\n",
       "      <td>34.942497</td>\n",
       "      <td>52.844799</td>\n",
       "      <td>3.778904</td>\n",
       "      <td>-0.293911</td>\n",
       "      <td>-0.796967</td>\n",
       "      <td>0.911722</td>\n",
       "      <td>0.059988</td>\n",
       "      <td>-0.477283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.930000</td>\n",
       "      <td>52.046001</td>\n",
       "      <td>27.486000</td>\n",
       "      <td>-1.87100</td>\n",
       "      <td>-10.8840</td>\n",
       "      <td>-2.88620</td>\n",
       "      <td>11.414562</td>\n",
       "      <td>48.172474</td>\n",
       "      <td>44.815205</td>\n",
       "      <td>30.073942</td>\n",
       "      <td>45.059002</td>\n",
       "      <td>41.162003</td>\n",
       "      <td>24.599800</td>\n",
       "      <td>-3.113472</td>\n",
       "      <td>-3.653202</td>\n",
       "      <td>-5.474142</td>\n",
       "      <td>1.664068</td>\n",
       "      <td>0.335649</td>\n",
       "      <td>1.896661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.113001</td>\n",
       "      <td>38.709999</td>\n",
       "      <td>56.313000</td>\n",
       "      <td>-2.35070</td>\n",
       "      <td>-6.2051</td>\n",
       "      <td>5.36720</td>\n",
       "      <td>8.534394</td>\n",
       "      <td>30.325914</td>\n",
       "      <td>33.005455</td>\n",
       "      <td>56.057991</td>\n",
       "      <td>27.762300</td>\n",
       "      <td>32.504898</td>\n",
       "      <td>61.680199</td>\n",
       "      <td>-2.563614</td>\n",
       "      <td>-0.500557</td>\n",
       "      <td>5.622208</td>\n",
       "      <td>1.090575</td>\n",
       "      <td>0.080669</td>\n",
       "      <td>1.047512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>42.521999</td>\n",
       "      <td>46.155998</td>\n",
       "      <td>34.637001</td>\n",
       "      <td>0.53492</td>\n",
       "      <td>-11.7180</td>\n",
       "      <td>0.26154</td>\n",
       "      <td>11.733119</td>\n",
       "      <td>42.886932</td>\n",
       "      <td>38.496471</td>\n",
       "      <td>36.840931</td>\n",
       "      <td>43.056919</td>\n",
       "      <td>34.437996</td>\n",
       "      <td>34.898540</td>\n",
       "      <td>0.169987</td>\n",
       "      <td>-4.058475</td>\n",
       "      <td>-1.942390</td>\n",
       "      <td>0.317780</td>\n",
       "      <td>0.346345</td>\n",
       "      <td>-7.426744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>42.058998</td>\n",
       "      <td>47.549999</td>\n",
       "      <td>18.323999</td>\n",
       "      <td>0.34146</td>\n",
       "      <td>-15.7010</td>\n",
       "      <td>-6.12990</td>\n",
       "      <td>16.858639</td>\n",
       "      <td>41.228317</td>\n",
       "      <td>39.208679</td>\n",
       "      <td>21.208759</td>\n",
       "      <td>42.400459</td>\n",
       "      <td>31.848999</td>\n",
       "      <td>12.194099</td>\n",
       "      <td>1.172142</td>\n",
       "      <td>-7.359680</td>\n",
       "      <td>-9.014660</td>\n",
       "      <td>3.432736</td>\n",
       "      <td>0.468740</td>\n",
       "      <td>1.470605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>60.327000</td>\n",
       "      <td>42.020000</td>\n",
       "      <td>52.556000</td>\n",
       "      <td>4.53520</td>\n",
       "      <td>-5.3173</td>\n",
       "      <td>1.61440</td>\n",
       "      <td>7.172727</td>\n",
       "      <td>61.015888</td>\n",
       "      <td>36.241261</td>\n",
       "      <td>55.824726</td>\n",
       "      <td>64.862198</td>\n",
       "      <td>36.702702</td>\n",
       "      <td>54.170399</td>\n",
       "      <td>3.846310</td>\n",
       "      <td>0.461441</td>\n",
       "      <td>-1.654327</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>-0.086781</td>\n",
       "      <td>-1.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>56.569000</td>\n",
       "      <td>33.766998</td>\n",
       "      <td>42.841000</td>\n",
       "      <td>2.01360</td>\n",
       "      <td>-1.8708</td>\n",
       "      <td>0.89799</td>\n",
       "      <td>2.891516</td>\n",
       "      <td>58.007278</td>\n",
       "      <td>25.224068</td>\n",
       "      <td>46.253273</td>\n",
       "      <td>58.582600</td>\n",
       "      <td>31.896198</td>\n",
       "      <td>43.738991</td>\n",
       "      <td>0.575321</td>\n",
       "      <td>6.672131</td>\n",
       "      <td>-2.514282</td>\n",
       "      <td>0.285718</td>\n",
       "      <td>-3.566458</td>\n",
       "      <td>-2.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>24.910999</td>\n",
       "      <td>29.631001</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>-2.07780</td>\n",
       "      <td>-2.0361</td>\n",
       "      <td>1.25260</td>\n",
       "      <td>3.167327</td>\n",
       "      <td>24.399757</td>\n",
       "      <td>20.379864</td>\n",
       "      <td>48.189938</td>\n",
       "      <td>22.833199</td>\n",
       "      <td>27.594900</td>\n",
       "      <td>47.162601</td>\n",
       "      <td>-1.566559</td>\n",
       "      <td>7.215036</td>\n",
       "      <td>-1.027336</td>\n",
       "      <td>0.753951</td>\n",
       "      <td>-3.543557</td>\n",
       "      <td>-0.820163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X          Y          Z        U        V        W       USUM  \\\n",
       "0    63.516998  45.685001  44.712002  2.10480  -2.8949  0.42117   3.603889   \n",
       "1    18.767000  46.138000  48.994999 -3.29820  -6.0279  2.34430   7.260128   \n",
       "2    59.183998  39.841999  51.174999  4.14480  -4.8995  1.66980   6.631191   \n",
       "3    46.930000  52.046001  27.486000 -1.87100 -10.8840 -2.88620  11.414562   \n",
       "4    30.113001  38.709999  56.313000 -2.35070  -6.2051  5.36720   8.534394   \n",
       "..         ...        ...        ...      ...      ...      ...        ...   \n",
       "694  42.521999  46.155998  34.637001  0.53492 -11.7180  0.26154  11.733119   \n",
       "695  42.058998  47.549999  18.323999  0.34146 -15.7010 -6.12990  16.858639   \n",
       "696  60.327000  42.020000  52.556000  4.53520  -5.3173  1.61440   7.172727   \n",
       "697  56.569000  33.766998  42.841000  2.01360  -1.8708  0.89799   2.891516   \n",
       "698  24.910999  29.631001  45.910000 -2.07780  -2.0361  1.25260   3.167327   \n",
       "\n",
       "        pred_X     pred_Y     pred_Z     real_X     real_Y     real_Z  \\\n",
       "0    62.280289  41.478485  47.252644  65.621796  42.790100  45.133171   \n",
       "1    18.417025  39.064743  50.214577  15.468801  40.110100  51.339298   \n",
       "2    59.549892  35.236408  53.641766  63.328796  34.942497  52.844799   \n",
       "3    48.172474  44.815205  30.073942  45.059002  41.162003  24.599800   \n",
       "4    30.325914  33.005455  56.057991  27.762300  32.504898  61.680199   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "694  42.886932  38.496471  36.840931  43.056919  34.437996  34.898540   \n",
       "695  41.228317  39.208679  21.208759  42.400459  31.848999  12.194099   \n",
       "696  61.015888  36.241261  55.824726  64.862198  36.702702  54.170399   \n",
       "697  58.007278  25.224068  46.253273  58.582600  31.896198  43.738991   \n",
       "698  24.399757  20.379864  48.189938  22.833199  27.594900  47.162601   \n",
       "\n",
       "     dif_real_pred_X  dif_real_pred_Y  dif_real_pred_Z  \\\n",
       "0           3.341507         1.311615        -2.119473   \n",
       "1          -2.948224         1.045357         1.124722   \n",
       "2           3.778904        -0.293911        -0.796967   \n",
       "3          -3.113472        -3.653202        -5.474142   \n",
       "4          -2.563614        -0.500557         5.622208   \n",
       "..               ...              ...              ...   \n",
       "694         0.169987        -4.058475        -1.942390   \n",
       "695         1.172142        -7.359680        -9.014660   \n",
       "696         3.846310         0.461441        -1.654327   \n",
       "697         0.575321         6.672131        -2.514282   \n",
       "698        -1.566559         7.215036        -1.027336   \n",
       "\n",
       "     relativ_dif_real_pred_X  relativ_dif_real_pred_Y  relativ_dif_real_pred_Z  \n",
       "0                   1.587565                -0.453078                -5.032344  \n",
       "1                   0.893889                -0.173420                 0.479769  \n",
       "2                   0.911722                 0.059988                -0.477283  \n",
       "3                   1.664068                 0.335649                 1.896661  \n",
       "4                   1.090575                 0.080669                 1.047512  \n",
       "..                       ...                      ...                      ...  \n",
       "694                 0.317780                 0.346345                -7.426744  \n",
       "695                 3.432736                 0.468740                 1.470605  \n",
       "696                 0.848101                -0.086781                -1.024732  \n",
       "697                 0.285718                -3.566458                -2.799900  \n",
       "698                 0.753951                -3.543557                -0.820163  \n",
       "\n",
       "[699 rows x 19 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           9.487540245056152,
           2.9782848358154297,
           1.7589807510375977,
           7.4956278800964355,
           2.6434829235076904,
           8.584027290344238,
           4.70907735824585,
           6.900237083435059,
           11.058979034423828,
           5.985408306121826,
           4.6031999588012695,
           2.080909490585327,
           10.798107147216797,
           5.244495868682861,
           9.740337371826172,
           4.9327850341796875,
           11.603001594543457,
           8.782949447631836,
           7.960291385650635,
           7.510847568511963,
           8.08665657043457,
           9.955036163330078,
           6.629784107208252,
           6.187309741973877,
           2.884850263595581,
           1.66921865940094,
           4.377597332000732,
           1.5905792713165283,
           10.807198524475098,
           8.99752140045166,
           7.812931537628174,
           15.959739685058594,
           14.669604301452637,
           11.756396293640137,
           5.264652252197266,
           3.9190914630889893,
           10.345693588256836,
           2.708669662475586,
           4.8257036209106445,
           9.971203804016113,
           3.359222650527954,
           12.336644172668457,
           9.701464653015137,
           6.950520992279053,
           10.920626640319824,
           3.236285924911499,
           7.6575446128845215,
           2.945477247238159,
           8.881928443908691,
           4.505533695220947,
           18.315589904785156,
           10.452274322509766,
           3.5345311164855957,
           1.1816575527191162,
           11.321113586425781,
           3.5503063201904297,
           8.581148147583008,
           1.694610834121704,
           8.432416915893555,
           10.122684478759766,
           8.510673522949219,
           5.896841049194336,
           4.041261672973633,
           9.501510620117188,
           8.896134376525879,
           6.411411285400391,
           9.769697189331055,
           10.274956703186035,
           6.151399612426758,
           9.085453033447266,
           10.175183296203613,
           1.6786364316940308,
           15.052820205688477,
           8.596508979797363,
           4.472801208496094,
           8.697160720825195,
           8.131217956542969,
           9.372919082641602,
           3.3894076347351074,
           8.83027458190918,
           8.171225547790527,
           6.592477321624756,
           1.691603422164917,
           14.236739158630371,
           6.612325191497803,
           7.737733364105225,
           9.862367630004883,
           3.7501437664031982,
           15.36936092376709,
           7.017298221588135,
           1.8075432777404785,
           7.26289701461792,
           14.141318321228027,
           9.262652397155762,
           6.48207950592041,
           5.478734016418457,
           1.6170605421066284,
           10.518348693847656,
           3.643282175064087,
           6.119032859802246,
           3.1550230979919434,
           2.116529703140259,
           2.958078622817993,
           9.664701461791992,
           4.466794490814209,
           4.170395374298096,
           2.495150566101074,
           5.207603454589844,
           5.460885047912598,
           5.07271146774292,
           10.046731948852539,
           11.825839042663574,
           2.7458629608154297,
           4.667354583740234,
           7.905728816986084,
           8.028797149658203,
           8.759811401367188,
           7.172726631164551,
           1.9932869672775269,
           10.16731071472168,
           4.282119274139404,
           6.977254867553711,
           1.888007640838623,
           6.139432907104492,
           8.547100067138672,
           6.260786056518555,
           13.81306266784668,
           5.554756164550781,
           16.609708786010742,
           3.0888559818267822,
           1.4516386985778809,
           8.349907875061035,
           10.97011661529541,
           5.661365509033203,
           10.41623592376709,
           14.0460844039917,
           14.870316505432129,
           9.984872817993164,
           5.309638500213623,
           12.095102310180664,
           6.812907695770264,
           4.488607406616211,
           10.377259254455566,
           14.206626892089844,
           4.107431888580322,
           6.272810935974121,
           12.341596603393555,
           17.191804885864258,
           9.094109535217285,
           2.64860463142395,
           9.22470474243164,
           14.774089813232422,
           10.597574234008789,
           3.9312517642974854,
           15.973675727844238,
           10.121177673339844,
           9.782426834106445,
           11.874751091003418,
           5.8388166427612305,
           15.923713684082031,
           3.642591714859009,
           4.359543800354004,
           5.382071018218994,
           7.943024158477783,
           14.381734848022461,
           7.264315128326416,
           12.46735954284668,
           6.489171028137207,
           6.994318008422852,
           4.494420051574707,
           5.033657073974609,
           7.612462997436523,
           13.986278533935547,
           13.92394733428955,
           6.057530403137207,
           3.3198113441467285,
           7.787242412567139,
           10.410993576049805,
           10.836237907409668,
           9.740424156188965,
           2.1274704933166504,
           14.640896797180176,
           7.834207534790039,
           3.3410542011260986,
           8.46753978729248,
           7.5794453620910645,
           10.68997859954834,
           7.604531288146973,
           10.803397178649902,
           2.6690874099731445,
           8.865742683410645,
           5.480388641357422,
           11.484112739562988,
           23.456645965576172,
           9.534161567687988,
           6.352453231811523,
           13.225178718566895,
           2.575059413909912,
           2.7643306255340576,
           17.386436462402344,
           2.8740646839141846,
           2.709446668624878,
           5.766144275665283,
           13.33469295501709,
           10.398552894592285,
           9.352166175842285,
           18.93483543395996,
           7.65290641784668,
           10.113685607910156,
           5.798981666564941,
           11.617433547973633,
           1.7906032800674438,
           2.520451068878174,
           7.423653602600098,
           15.775489807128906,
           21.46368408203125,
           9.645406723022461,
           10.409951210021973,
           5.683908462524414,
           9.063125610351562,
           3.6435461044311523,
           1.6902235746383667,
           9.07184886932373,
           17.417734146118164,
           9.298047065734863,
           5.1272077560424805,
           5.7899909019470215,
           2.234848737716675,
           5.189192295074463,
           2.6170003414154053,
           1.2747665643692017,
           11.897953033447266,
           1.6761659383773804,
           10.754142761230469,
           9.218195915222168,
           2.147221565246582,
           9.674327850341797,
           4.210982799530029,
           8.719306945800781,
           8.335956573486328,
           6.527127265930176,
           4.071042537689209,
           12.408970832824707,
           8.905892372131348,
           7.183071613311768,
           5.223270416259766,
           5.486182689666748,
           11.526314735412598,
           7.468102931976318,
           12.343761444091797,
           8.921473503112793,
           4.9865803718566895,
           1.5370582342147827,
           8.635767936706543,
           12.239322662353516,
           1.4590612649917603,
           7.787397384643555,
           12.66048812866211,
           13.056777954101562,
           5.390719890594482,
           7.3830246925354,
           18.701282501220703,
           7.230193138122559,
           8.571084022521973,
           3.7628488540649414,
           3.9137158393859863,
           9.484199523925781,
           8.87949275970459,
           7.388397693634033,
           18.100675582885742,
           4.128393650054932,
           8.536345481872559,
           1.3366531133651733,
           8.413581848144531,
           6.0322465896606445,
           1.349668264389038,
           9.437926292419434,
           12.189102172851562,
           9.853286743164062,
           4.840590953826904,
           8.71859359741211,
           1.9283877611160278,
           7.877467632293701,
           15.848200798034668,
           2.544891595840454,
           9.631067276000977,
           3.769244909286499,
           5.820559024810791,
           7.695474147796631,
           2.086313009262085,
           7.795784950256348,
           7.816500186920166,
           3.836102247238159,
           10.787849426269531,
           6.927855014801025,
           5.494681358337402,
           12.14461612701416,
           10.473138809204102,
           4.933709144592285,
           7.612407684326172,
           12.189887046813965,
           10.371219635009766,
           1.0882500410079956,
           7.89022970199585,
           2.745944023132324,
           16.060590744018555,
           10.285603523254395,
           4.491062164306641,
           10.424837112426758,
           3.845785140991211,
           8.342679023742676,
           12.847496032714844,
           10.776094436645508,
           14.501355171203613,
           9.436773300170898,
           6.2686262130737305,
           11.578232765197754,
           1.9835909605026245,
           9.006593704223633,
           17.534523010253906,
           9.33459186553955,
           3.7161104679107666,
           3.426952362060547,
           8.865633010864258,
           14.199939727783203,
           6.208715915679932,
           4.540375232696533,
           18.442981719970703,
           13.365468978881836,
           15.871304512023926,
           2.845231533050537,
           5.502269268035889,
           6.964817523956299,
           5.46335506439209,
           11.037138938903809,
           4.49680233001709,
           3.6868464946746826,
           10.730064392089844,
           13.399792671203613,
           10.204672813415527,
           10.305628776550293,
           17.151105880737305,
           8.706282615661621,
           12.413788795471191,
           11.184009552001953,
           6.346806049346924,
           3.2229976654052734,
           9.198430061340332,
           1.4063645601272583,
           8.886029243469238,
           10.400947570800781,
           9.938502311706543,
           10.494224548339844,
           7.563658714294434,
           2.759681463241577,
           22.308786392211914,
           11.745682716369629,
           7.059898853302002,
           14.336071014404297,
           4.869500160217285,
           1.4015637636184692,
           6.106007099151611,
           1.0765552520751953,
           11.924989700317383,
           3.334580898284912,
           6.083772659301758,
           12.429152488708496,
           10.296801567077637,
           6.164493560791016,
           4.507903575897217,
           7.951440811157227,
           3.4811770915985107,
           12.008597373962402,
           10.423361778259277,
           15.70008373260498,
           18.586246490478516,
           10.122289657592773,
           17.079345703125,
           4.364841461181641,
           8.994156837463379,
           11.312920570373535,
           9.591304779052734,
           0.6776500344276428,
           3.4091978073120117,
           10.731890678405762,
           6.430738925933838,
           9.584741592407227,
           8.405170440673828,
           11.020166397094727,
           2.487926959991455,
           7.669305324554443,
           3.3983912467956543,
           2.363987445831299,
           1.4799823760986328,
           8.385536193847656,
           8.835596084594727,
           2.3844878673553467,
           2.962568998336792,
           4.288492679595947,
           13.014762878417969,
           5.0341291427612305,
           9.474442481994629,
           8.981672286987305,
           15.40211009979248,
           12.464092254638672,
           3.5961077213287354,
           9.936092376708984,
           2.193946361541748,
           5.372025489807129,
           7.02311897277832,
           5.047826290130615,
           9.24860668182373,
           4.05079984664917,
           11.953597068786621,
           1.3277289867401123,
           5.2341790199279785,
           10.66005802154541,
           7.113900184631348,
           12.760744094848633,
           5.799712657928467,
           2.0211293697357178,
           3.0780508518218994,
           7.611435890197754,
           8.063786506652832,
           4.73379373550415,
           0.9857015609741211,
           4.114099502563477,
           15.253458976745605,
           13.808148384094238,
           5.198212146759033,
           20.46039390563965,
           20.023574829101562,
           8.679780006408691,
           1.7053295373916626,
           7.436163425445557,
           2.942800760269165,
           1.7784754037857056,
           10.974897384643555,
           3.9664785861968994,
           14.195087432861328,
           6.918832302093506,
           4.5354437828063965,
           10.564586639404297,
           13.019279479980469,
           11.563467025756836,
           8.514467239379883,
           12.182350158691406,
           8.545387268066406,
           8.856758117675781,
           6.298455238342285,
           16.167964935302734,
           4.156801223754883,
           8.440314292907715,
           6.481825828552246,
           6.838658809661865,
           10.772255897521973,
           2.51655912399292,
           9.653817176818848,
           12.581450462341309,
           9.83818531036377,
           4.854653358459473,
           10.68476390838623,
           7.2537336349487305,
           5.6536102294921875,
           5.152288436889648,
           2.608083963394165,
           3.213120698928833,
           1.8964744806289673,
           15.592416763305664,
           19.945030212402344,
           15.18327808380127,
           6.070356845855713,
           5.717874050140381,
           4.301895618438721,
           9.10400104522705,
           5.72824239730835,
           13.082653045654297,
           4.492537498474121,
           10.901005744934082,
           3.8489036560058594,
           5.888981342315674,
           1.4922658205032349,
           11.371492385864258,
           2.0515406131744385,
           5.36704158782959,
           13.893566131591797,
           4.280004501342773,
           9.707612037658691,
           4.389944076538086,
           2.267866849899292,
           13.114310264587402,
           5.336579322814941,
           6.693164825439453,
           9.506048202514648,
           7.280970096588135,
           12.935452461242676,
           6.177762031555176,
           5.0348334312438965,
           8.073598861694336,
           16.507692337036133,
           4.764255046844482,
           5.171623706817627,
           10.162890434265137,
           18.992990493774414,
           7.9781646728515625,
           3.7657313346862793,
           9.93892765045166,
           16.973867416381836,
           3.279876708984375,
           2.5335748195648193,
           4.713524341583252,
           2.109898567199707,
           6.037497520446777,
           4.9100728034973145,
           11.94960880279541,
           2.5680043697357178,
           11.827956199645996,
           4.235004425048828,
           13.222310066223145,
           6.200247764587402,
           6.7811503410339355,
           8.415549278259277,
           2.9208571910858154,
           12.383869171142578,
           8.40530014038086,
           19.59270477294922,
           0.8756555318832397,
           8.806013107299805,
           17.797061920166016,
           11.50992202758789,
           1.4681355953216553,
           4.332079887390137,
           5.481050491333008,
           18.191959381103516,
           10.864718437194824,
           3.940065622329712,
           6.848477840423584,
           0.8539532423019409,
           8.27118968963623,
           9.127395629882812,
           2.443709373474121,
           9.159808158874512,
           12.781637191772461,
           13.115983009338379,
           19.10380744934082,
           5.624981880187988,
           8.28538703918457,
           4.26259708404541,
           9.508852005004883,
           15.771617889404297,
           10.845008850097656,
           2.291802167892456,
           14.255663871765137,
           1.9390406608581543,
           3.36799693107605,
           12.295069694519043,
           2.33612060546875,
           1.4821500778198242,
           10.245463371276855,
           6.4733805656433105,
           4.105221748352051,
           6.718235969543457,
           3.6823689937591553,
           6.847806453704834,
           6.835020542144775,
           1.725019931793213,
           2.3754777908325195,
           1.8753407001495361,
           2.4180543422698975,
           3.266650676727295,
           12.116581916809082,
           8.326362609863281,
           9.28061580657959,
           5.627632141113281,
           9.868147850036621,
           1.5868256092071533,
           11.677608489990234,
           4.318599224090576,
           3.562934637069702,
           1.2203072309494019,
           15.291605949401855,
           7.647700786590576,
           9.48463249206543,
           14.699256896972656,
           7.497106552124023,
           20.173139572143555,
           3.6080522537231445,
           10.956258773803711,
           11.788272857666016,
           14.796850204467773,
           8.079368591308594,
           9.268308639526367,
           5.160853862762451,
           12.894466400146484,
           4.9825663566589355,
           6.821156978607178,
           7.756848335266113,
           4.315221786499023,
           6.321982383728027,
           13.6444091796875,
           10.817596435546875,
           6.382436752319336,
           1.2827564477920532,
           0.5504114627838135,
           7.117624282836914,
           3.3656668663024902,
           9.24698257446289,
           12.379745483398438,
           3.350876808166504,
           11.727545738220215,
           2.3854596614837646,
           16.688461303710938,
           7.078394889831543,
           6.331615447998047,
           3.5903618335723877,
           11.992661476135254,
           1.8289375305175781,
           1.6436103582382202,
           8.64022445678711,
           1.4964423179626465,
           5.576413631439209,
           8.837380409240723,
           4.351584434509277,
           7.795935153961182,
           1.1351886987686157,
           10.158668518066406,
           6.275455474853516,
           2.148512840270996,
           14.08239459991455,
           9.723267555236816,
           3.188966989517212,
           3.5621368885040283,
           6.7743730545043945,
           6.965937614440918,
           0.5224103331565857,
           8.540950775146484,
           9.569615364074707,
           3.7238919734954834,
           8.148799896240234,
           11.97314167022705,
           6.444392204284668,
           8.268353462219238,
           6.598717212677002,
           9.280654907226562,
           5.782382011413574,
           18.251056671142578,
           11.22021770477295,
           3.8659093379974365,
           7.0468621253967285,
           9.582733154296875,
           2.3980603218078613,
           9.090533256530762,
           1.3329603672027588,
           3.314542293548584,
           10.964696884155273,
           7.475830078125,
           10.413518905639648,
           7.373569011688232,
           15.791460037231445,
           8.757771492004395,
           5.15372896194458,
           5.502352237701416,
           12.2122220993042,
           6.298299312591553,
           5.222827434539795,
           7.5570759773254395,
           12.626262664794922,
           3.2422313690185547,
           7.677150726318359,
           8.131697654724121,
           8.259493827819824,
           18.13065528869629,
           2.3107433319091797,
           5.744872570037842,
           2.616989850997925,
           12.176941871643066,
           2.071284294128418,
           3.585812568664551,
           15.794623374938965,
           16.296478271484375,
           7.751859188079834,
           6.315246105194092,
           2.2614011764526367,
           10.385115623474121,
           11.832561492919922,
           2.3057522773742676,
           1.565446376800537,
           14.342572212219238,
           6.772126197814941,
           11.255102157592773,
           8.727845191955566,
           1.6182667016983032,
           8.016473770141602,
           2.649289608001709,
           12.0818452835083,
           12.18840503692627,
           12.893043518066406,
           13.037850379943848,
           2.072798490524292
          ],
          "size": 12
         },
         "mode": "markers",
         "name": "Real",
         "scene": "scene",
         "type": "scatter3d",
         "x": [
          48.70600128173828,
          19.119699478149414,
          47.25716018676758,
          14.463998794555664,
          27.234670639038086,
          29.93579864501953,
          27.122100830078125,
          31.895599365234375,
          32.696800231933594,
          24.045101165771484,
          55.97220230102539,
          20.836700439453125,
          21.935400009155273,
          43.28706741333008,
          57.97079849243164,
          61.52050018310547,
          28.018999099731445,
          24.045900344848633,
          58.356502532958984,
          59.24420166015625,
          16.397300720214844,
          29.046199798583984,
          62.00069808959961,
          29.95370101928711,
          32.76666259765625,
          62.478302001953125,
          28.319499969482422,
          52.95907974243164,
          37.11676025390625,
          52.43199920654297,
          57.261600494384766,
          32.78789138793945,
          27.397998809814453,
          40.15375900268555,
          20.291601181030273,
          44.35765838623047,
          50.42300033569336,
          20.2903995513916,
          57.833499908447266,
          47.76679992675781,
          60.47310256958008,
          49.83939743041992,
          49.149200439453125,
          30.300081253051758,
          50.129798889160156,
          50.85820007324219,
          58.97700119018555,
          35.82931137084961,
          53.21099853515625,
          41.406558990478516,
          38.79285430908203,
          45.76628875732422,
          27.743099212646484,
          28.182889938354492,
          51.93558120727539,
          34.36375045776367,
          58.9379997253418,
          64.74140167236328,
          54.32729721069336,
          55.542259216308594,
          25.1825008392334,
          30.432600021362305,
          63.432899475097656,
          30.20250129699707,
          32.103702545166016,
          54.241703033447266,
          29.880300521850586,
          27.16119956970215,
          36.87097930908203,
          57.49530029296875,
          31.83856964111328,
          33.69975280761719,
          44.59291076660156,
          36.40692901611328,
          33.390628814697266,
          32.4214973449707,
          56.260597229003906,
          31.35089874267578,
          36.229671478271484,
          58.195701599121094,
          20.14590072631836,
          24.85849952697754,
          28.51038932800293,
          45.08799743652344,
          56.308929443359375,
          15.792500495910645,
          24.22599983215332,
          30.12965965270996,
          29.207191467285156,
          57.561397552490234,
          45.00402069091797,
          62.640499114990234,
          42.181888580322266,
          45.745399475097656,
          30.77281951904297,
          60.733001708984375,
          50.310218811035156,
          20.425600051879883,
          24.818599700927734,
          14.69379997253418,
          65.65540313720703,
          60.37710189819336,
          52.47800064086914,
          18.777799606323242,
          50.8838996887207,
          62.6614990234375,
          34.248600006103516,
          38.978851318359375,
          17.22920036315918,
          17.61989974975586,
          24.185001373291016,
          25.64859962463379,
          49.63730239868164,
          17.68320083618164,
          14.780599594116211,
          48.13890075683594,
          29.274599075317383,
          64.86219787597656,
          18.48710060119629,
          48.035099029541016,
          62.91570281982422,
          17.682899475097656,
          52.55059814453125,
          45.729801177978516,
          46.778297424316406,
          40.46703338623047,
          46.53350067138672,
          23.679515838623047,
          36.7682991027832,
          36.502410888671875,
          35.17597961425781,
          60.84260177612305,
          46.313899993896484,
          40.1672477722168,
          48.09049987792969,
          42.42601013183594,
          41.5023078918457,
          48.74250030517578,
          16.64229965209961,
          53.93259811401367,
          29.801198959350586,
          36.14991760253906,
          47.566200256347656,
          47.80337905883789,
          25.433029174804688,
          55.972198486328125,
          32.12411880493164,
          38.704559326171875,
          35.059688568115234,
          37.40557861328125,
          46.430198669433594,
          36.4114990234375,
          21.1471004486084,
          53.409202575683594,
          24.460899353027344,
          20.66069984436035,
          55.992000579833984,
          39.95417022705078,
          36.295326232910156,
          23.034698486328125,
          33.42709732055664,
          29.761199951171875,
          18.645000457763672,
          41.39165115356445,
          39.582603454589844,
          64.7770004272461,
          25.205299377441406,
          60.428001403808594,
          14.082599639892578,
          34.17934799194336,
          31.76219940185547,
          55.6973991394043,
          45.31352615356445,
          31.637298583984375,
          16.895999908447266,
          38.58390426635742,
          35.70648956298828,
          20.311199188232422,
          44.41550064086914,
          30.93429946899414,
          30.651599884033203,
          49.36348342895508,
          60.6599006652832,
          25.794719696044922,
          28.48379898071289,
          32.93840026855469,
          32.45854949951172,
          51.53399658203125,
          53.736690521240234,
          25.105899810791016,
          58.26139831542969,
          17.313199996948242,
          28.80415916442871,
          42.29829788208008,
          46,
          57.98299789428711,
          25.208600997924805,
          28.991199493408203,
          25.864500045776367,
          34.565589904785156,
          28.539798736572266,
          51.32659149169922,
          52.290809631347656,
          29.116899490356445,
          25.32229995727539,
          58.07680130004883,
          28.203981399536133,
          35.08979797363281,
          20.495100021362305,
          52.04819869995117,
          28.087799072265625,
          28.896799087524414,
          29.394479751586914,
          17.980499267578125,
          29.338640213012695,
          43.620399475097656,
          49.33820343017578,
          24.18631935119629,
          56.343997955322266,
          27.21310043334961,
          26.725099563598633,
          48.8036994934082,
          35.41773986816406,
          38.02585983276367,
          31.61680030822754,
          17.123699188232422,
          65.65520477294922,
          58.38760757446289,
          31.447599411010742,
          28.7643985748291,
          50.691871643066406,
          26.90089988708496,
          52.12034225463867,
          21.06679916381836,
          38.02973556518555,
          22.986799240112305,
          42.456058502197266,
          42.13840103149414,
          54.63850021362305,
          49.35300064086914,
          18.01650047302246,
          19.06829833984375,
          37.591880798339844,
          31.734432220458984,
          51.37260055541992,
          29.355501174926758,
          51.90370178222656,
          32.736000061035156,
          18.880599975585938,
          27.54520034790039,
          57.42449951171875,
          32.55350112915039,
          27.726280212402344,
          22.326099395751953,
          49.16480255126953,
          35.132659912109375,
          42.10285186767578,
          52.288299560546875,
          47.56203842163086,
          50.721099853515625,
          14.188599586486816,
          34.266910552978516,
          30.238800048828125,
          40.04426956176758,
          41.575950622558594,
          57.02470016479492,
          30.99639892578125,
          51.624698638916016,
          43.76049041748047,
          37.82386779785156,
          37.78352737426758,
          40.469051361083984,
          34.72688674926758,
          61.673099517822266,
          62.70880126953125,
          59.61989974975586,
          26.27899932861328,
          48.13570022583008,
          44.76129913330078,
          65.38020324707031,
          56.567298889160156,
          54.38139724731445,
          53.99469757080078,
          46.385250091552734,
          58.74736022949219,
          37.1739501953125,
          62.390201568603516,
          33.13290023803711,
          60.4213981628418,
          49.28960037231445,
          26.75200080871582,
          47.18431091308594,
          66.23880004882812,
          26.625,
          36.53820037841797,
          18.661800384521484,
          40.660789489746094,
          54.56800079345703,
          23.886899948120117,
          53.27880096435547,
          53.912139892578125,
          55.65890121459961,
          24.373661041259766,
          33.986839294433594,
          26.993398666381836,
          38.190467834472656,
          31.189899444580078,
          27.569700241088867,
          43.582820892333984,
          20.819499969482422,
          33.072601318359375,
          35.75707244873047,
          25.756200790405273,
          39.78961181640625,
          33.340599060058594,
          22.172399520874023,
          28.395099639892578,
          27.904159545898438,
          38.124786376953125,
          36.52117156982422,
          25.100799560546875,
          47.72420120239258,
          57.259498596191406,
          29.51420021057129,
          37.199729919433594,
          42.205169677734375,
          30.45509910583496,
          39.4665412902832,
          51.53278732299805,
          46.021541595458984,
          65.51740264892578,
          55.491302490234375,
          22.587799072265625,
          64.53359985351562,
          29.002729415893555,
          27.352081298828125,
          18.754600524902344,
          36.16246795654297,
          35.764591217041016,
          25.12660026550293,
          56.474700927734375,
          39.59886932373047,
          54.77799987792969,
          24.504798889160156,
          33.71259689331055,
          39.051998138427734,
          37.43172073364258,
          41.860530853271484,
          56.739601135253906,
          25.87980079650879,
          29.853300094604492,
          34.00979995727539,
          38.565773010253906,
          46.323699951171875,
          22.018001556396484,
          38.65441131591797,
          45.5088996887207,
          34.546173095703125,
          23.594099044799805,
          40.3187370300293,
          39.635589599609375,
          19.24530029296875,
          54.74418640136719,
          44.207401275634766,
          50.77539825439453,
          61.63290023803711,
          30.616201400756836,
          46.86109924316406,
          23.273799896240234,
          49.4530029296875,
          54.744102478027344,
          31.70370864868164,
          33.49382781982422,
          19.71940040588379,
          31.941650390625,
          29.687170028686523,
          32.192996978759766,
          41.77079772949219,
          48.81610107421875,
          18.665998458862305,
          49.180999755859375,
          57.34589767456055,
          52.177337646484375,
          18.629188537597656,
          47.971099853515625,
          28.257349014282227,
          53.87092971801758,
          57.943199157714844,
          48.70289993286133,
          43.2735481262207,
          21.206600189208984,
          16.30270004272461,
          47.69356155395508,
          58.599700927734375,
          59.006797790527344,
          43.33940124511719,
          52.59115982055664,
          51.86001968383789,
          65.18720245361328,
          41.8973503112793,
          14.791601181030273,
          24.25979995727539,
          38.76301193237305,
          48.722599029541016,
          33.74129104614258,
          20.514400482177734,
          30.654598236083984,
          50.360050201416016,
          45.13142013549805,
          54.902400970458984,
          66.3362045288086,
          31.57000160217285,
          61.403602600097656,
          52.05270004272461,
          23.261598587036133,
          46.75899887084961,
          23.068201065063477,
          22.48200035095215,
          49.41089630126953,
          46.368099212646484,
          21.583770751953125,
          64.97560119628906,
          49.85369873046875,
          46.80649948120117,
          28.75067138671875,
          23.772680282592773,
          24.35849952697754,
          28.161569595336914,
          29.487699508666992,
          18.354801177978516,
          38.79665756225586,
          37.435272216796875,
          26.788301467895508,
          57.96070098876953,
          58.305450439453125,
          48.71200180053711,
          43.773040771484375,
          24.651599884033203,
          60.314998626708984,
          48.42148971557617,
          13.90999984741211,
          63.35830307006836,
          55.02259826660156,
          28.052200317382812,
          21.36400032043457,
          49.20869827270508,
          26.108800888061523,
          48.61320114135742,
          17.43079948425293,
          48.53960037231445,
          37.315330505371094,
          59.24591827392578,
          53.461700439453125,
          62.059600830078125,
          47.09543991088867,
          25.890338897705078,
          53.66790008544922,
          31.2554988861084,
          45.03620147705078,
          26.534698486328125,
          20.909061431884766,
          29.140600204467773,
          31.210081100463867,
          57.679901123046875,
          54.34479904174805,
          62.91360092163086,
          36.53232192993164,
          40.28049850463867,
          35.398468017578125,
          41.85334014892578,
          52.59703063964844,
          33.3741569519043,
          32.83725357055664,
          61.28929901123047,
          17.30160140991211,
          21.438400268554688,
          33.65279769897461,
          63.0974006652832,
          21.805099487304688,
          31.881851196289062,
          18.973800659179688,
          63.69382095336914,
          37.799232482910156,
          38.13914489746094,
          19.541099548339844,
          41.53145980834961,
          35.01536178588867,
          28.987499237060547,
          66.50540161132812,
          63.589298248291016,
          29.015399932861328,
          33.446598052978516,
          49.02138900756836,
          28.68669891357422,
          43.871498107910156,
          38.26433563232422,
          16.471900939941406,
          65.71479797363281,
          32.70719909667969,
          42.543819427490234,
          43.21134948730469,
          32.13859939575195,
          19.9906005859375,
          43.65224075317383,
          42.351287841796875,
          58.829002380371094,
          44.201698303222656,
          24.52322006225586,
          32.56797790527344,
          54.88397216796875,
          25.348299026489258,
          50.1322021484375,
          18.14579963684082,
          44.47249984741211,
          44.55125045776367,
          58.036502838134766,
          31.038101196289062,
          59.16429901123047,
          32.050201416015625,
          22.337200164794922,
          61.74370193481445,
          25.477001190185547,
          43.69261932373047,
          25.903099060058594,
          44.695701599121094,
          37.682640075683594,
          22.727540969848633,
          47.39030075073242,
          35.45003890991211,
          42.76288986206055,
          28.18885040283203,
          20.09119987487793,
          41.75004959106445,
          26.508909225463867,
          31.127220153808594,
          37.459590911865234,
          58.855899810791016,
          61.358070373535156,
          38.154510498046875,
          21.921998977661133,
          25.703100204467773,
          23.683000564575195,
          24.115100860595703,
          46.71419906616211,
          43.72187042236328,
          61.418800354003906,
          16.272899627685547,
          21.490400314331055,
          44.161109924316406,
          36.69367980957031,
          21.974000930786133,
          31.51129913330078,
          34.35478973388672,
          59.819400787353516,
          20.476900100708008,
          45.692901611328125,
          64.74189758300781,
          43.61897277832031,
          23.154098510742188,
          44.31570053100586,
          64.37409973144531,
          30.196739196777344,
          66.1863021850586,
          15.06869888305664,
          64.03559875488281,
          26.35379981994629,
          64.07340240478516,
          48.09162902832031,
          26.910900115966797,
          57.59850311279297,
          48.9770393371582,
          57.87479782104492,
          58.98509979248047,
          34.532440185546875,
          49.04389953613281,
          21.7656307220459,
          34.203880310058594,
          20.398099899291992,
          18.259601593017578,
          60.968360900878906,
          35.29368209838867,
          43.83251953125,
          46.236602783203125,
          39.125389099121094,
          58.980499267578125,
          27.161386489868164,
          40.84054946899414,
          45.692501068115234,
          48.84270095825195,
          46.24821090698242,
          31.205799102783203,
          38.56068420410156,
          27.627700805664062,
          43.88521957397461,
          59.386497497558594,
          38.009178161621094,
          48.70439910888672,
          52.07482147216797,
          48.56237030029297,
          26.791120529174805,
          36.868167877197266,
          41.01913833618164,
          26.648069381713867,
          63.52299118041992,
          51.33919906616211,
          43.91299819946289,
          54.596099853515625,
          48.24550247192383,
          65.9916000366211,
          36.55315017700195,
          61.87989807128906,
          39.59571075439453,
          47.57500076293945,
          59.641300201416016,
          62.1410026550293,
          26.249698638916016,
          20.937898635864258,
          16.595800399780273,
          59.156002044677734,
          51.55009078979492,
          32.8572998046875,
          16.763099670410156,
          25.10540008544922,
          39.08634948730469,
          27.313230514526367,
          37.5376091003418,
          25.570199966430664,
          46.367618560791016,
          52.84889221191406,
          55.9369010925293,
          52.83180236816406,
          28.600400924682617,
          46.117801666259766,
          28.565799713134766,
          56.121150970458984,
          29.527198791503906,
          48.0994987487793,
          65.19039916992188,
          33.94382858276367,
          34.73134231567383,
          14.324501037597656,
          31.090600967407227,
          20.221601486206055,
          51.775699615478516,
          40.07569122314453,
          37.6598014831543,
          40.611106872558594,
          41.32477951049805,
          26.65669059753418,
          24.460899353027344,
          41.944297790527344,
          23.82990074157715,
          37.53746032714844,
          51.221229553222656,
          24.57859992980957,
          47.94179916381836,
          46.9452018737793,
          60.66320037841797,
          37.46943664550781,
          47.74579620361328,
          65.7843017578125,
          50.59919738769531,
          23.431110382080078,
          64.94419860839844,
          33.77149963378906,
          19.47480010986328,
          35.71350860595703,
          23.388500213623047,
          32.94240188598633,
          33.18000030517578,
          48.080570220947266,
          40.29077911376953,
          23.498600006103516,
          48.18179702758789,
          57.574501037597656,
          54.79801940917969,
          16.1018009185791,
          23.334699630737305,
          34.581199645996094,
          36.263858795166016,
          52.381500244140625,
          63.16790008544922,
          19.92850112915039,
          39.753868103027344,
          26.611101150512695,
          51.914119720458984,
          52.06629943847656,
          41.03521728515625,
          37.5985221862793,
          52.967201232910156,
          33.90699768066406,
          63.05475997924805,
          30.043298721313477,
          51.734901428222656,
          29.537429809570312,
          35.076412200927734,
          22.515399932861328,
          41.118350982666016,
          30.12407112121582
         ],
         "y": [
          31.191701889038086,
          27.86709976196289,
          28.619001388549805,
          40.770999908447266,
          35.812198638916016,
          34.19929885864258,
          29.016700744628906,
          31.227901458740234,
          37.77939987182617,
          37.02880096435547,
          29.35460090637207,
          36.78499984741211,
          40.184898376464844,
          30.65260124206543,
          42.72520065307617,
          26.89389991760254,
          30.584999084472656,
          33.702003479003906,
          41.84510040283203,
          47.164798736572266,
          41.14179992675781,
          35.470699310302734,
          32.946800231933594,
          33.6234016418457,
          26.865129470825195,
          34.39010238647461,
          26.067699432373047,
          31.061969757080078,
          33.922000885009766,
          34.58180236816406,
          32.897098541259766,
          31.413002014160156,
          37.62799835205078,
          29.674999237060547,
          32.227298736572266,
          28.287099838256836,
          31.496601104736328,
          30.05409812927246,
          42.21289825439453,
          27.722702026367188,
          29.118799209594727,
          36.9370002746582,
          35.955902099609375,
          43.36040115356445,
          34.25300216674805,
          27.155101776123047,
          33.78419876098633,
          26.16905975341797,
          34.25749969482422,
          29.69059944152832,
          36.624000549316406,
          39.902000427246094,
          31.81529998779297,
          27.71076011657715,
          45.97100067138672,
          30.661998748779297,
          42.22079849243164,
          41.62030029296875,
          37.31209945678711,
          46.654998779296875,
          36.58159637451172,
          48.00410079956055,
          28.439599990844727,
          26.614700317382812,
          32.983699798583984,
          27.963600158691406,
          28.8125,
          36.025596618652344,
          32.10230255126953,
          46.48350143432617,
          35.5968017578125,
          27.46380043029785,
          42.51100158691406,
          33.203697204589844,
          27.422399520874023,
          33.04209899902344,
          34.10089874267578,
          34.19900131225586,
          27.773500442504883,
          45.14179992675781,
          34.06520080566406,
          29.623498916625977,
          31.58180046081543,
          32.08399963378906,
          48.82659912109375,
          43.22779846191406,
          35.386199951171875,
          33.88119888305664,
          40.1609992980957,
          30.670299530029297,
          27.5981502532959,
          40.60110092163086,
          40.922000885009766,
          34.645896911621094,
          38.222801208496094,
          27.28070068359375,
          31.37809944152832,
          39.17720031738281,
          26.56719970703125,
          42.649200439453125,
          42.05910110473633,
          33.22719955444336,
          28.548200607299805,
          40.993202209472656,
          36.5828971862793,
          41.90070343017578,
          29.253400802612305,
          29.225799560546875,
          30.2992000579834,
          46.64459991455078,
          35.71449661254883,
          43.60879898071289,
          26.18039894104004,
          30.90740203857422,
          36.20109939575195,
          29.99449920654297,
          33.436100006103516,
          36.702701568603516,
          36.734798431396484,
          36.17040252685547,
          38.893402099609375,
          40.86929702758789,
          26.81420135498047,
          24.021398544311523,
          34.467201232910156,
          29.921201705932617,
          33.2609977722168,
          48.75360107421875,
          37.43899917602539,
          29.715700149536133,
          26.127429962158203,
          36.12630081176758,
          28.37900161743164,
          26.95870018005371,
          30.149898529052734,
          37.24699783325195,
          34.766998291015625,
          40.58629608154297,
          33.91460037231445,
          41.15599822998047,
          31.310298919677734,
          26.081298828125,
          28.13559913635254,
          36.150001525878906,
          41.647098541259766,
          27.95469856262207,
          35.26599884033203,
          41.50699996948242,
          33.20719909667969,
          26.356090545654297,
          34.12790298461914,
          38.42499923706055,
          43.98649978637695,
          29.830400466918945,
          39.07099914550781,
          43.345699310302734,
          46.854698181152344,
          34.470001220703125,
          31.249000549316406,
          39.691001892089844,
          25.945100784301758,
          26.838699340820312,
          35.726600646972656,
          30.750600814819336,
          41.099998474121094,
          38.08660125732422,
          40.836997985839844,
          47.35710144042969,
          35.21940231323242,
          30.355300903320312,
          30.74209976196289,
          32.82490158081055,
          36.606998443603516,
          33.10900115966797,
          32.16790008544922,
          29.24679946899414,
          32.46080017089844,
          41.10240173339844,
          34.115501403808594,
          27.861900329589844,
          25.40690040588379,
          40.461997985839844,
          37.35210037231445,
          42.814998626708984,
          31.45760154724121,
          30.175800323486328,
          39.56700134277344,
          32.55590057373047,
          45.43000030517578,
          35.57720184326172,
          43.20859909057617,
          28.37019920349121,
          36.750999450683594,
          33.66200256347656,
          43.51110076904297,
          29.735300064086914,
          38.75300216674805,
          27.625999450683594,
          33.08190155029297,
          41.194000244140625,
          27.169118881225586,
          39.33359909057617,
          40.8390998840332,
          31.91299819946289,
          36.37649917602539,
          44.725303649902344,
          37.88999938964844,
          28.756799697875977,
          45.118900299072266,
          36.73059844970703,
          39.15839767456055,
          25.946300506591797,
          33.02080154418945,
          33.74800109863281,
          36.78700256347656,
          36.00800323486328,
          36.688499450683594,
          43.21099853515625,
          30.291099548339844,
          33.75130081176758,
          25.840999603271484,
          27.09670066833496,
          33.37770080566406,
          34.928001403808594,
          34.17890167236328,
          28.468599319458008,
          31.650901794433594,
          45.771297454833984,
          31.713802337646484,
          26.20309829711914,
          29.953800201416016,
          40.59649658203125,
          26.601421356201172,
          40.17070007324219,
          32.910797119140625,
          34.39469909667969,
          35.79209899902344,
          28.69339942932129,
          35.777099609375,
          35.20370101928711,
          31.94529914855957,
          43.2056999206543,
          34.19599914550781,
          39.06800079345703,
          30.230501174926758,
          46.65260314941406,
          29.538799285888672,
          38.64939880371094,
          39.654197692871094,
          40.452999114990234,
          40.962799072265625,
          29.405000686645508,
          30.363901138305664,
          35.03329849243164,
          43.999000549316406,
          27.07390022277832,
          32.824798583984375,
          41.83000183105469,
          44.36899948120117,
          27.538402557373047,
          36.27689743041992,
          39.62200164794922,
          25.51129913330078,
          32.95459747314453,
          29.816600799560547,
          29.223400115966797,
          29.63739776611328,
          34.09349822998047,
          33.53289794921875,
          40.480003356933594,
          28.532499313354492,
          32.745697021484375,
          26.54391098022461,
          38.30390167236328,
          43.144500732421875,
          30.743440628051758,
          34.86119842529297,
          36.48400115966797,
          35.39299774169922,
          30.524200439453125,
          34.41659927368164,
          27.21722984313965,
          34.91010284423828,
          38.827003479003906,
          50.38359832763672,
          35.0526008605957,
          47.028499603271484,
          29.280399322509766,
          44.763702392578125,
          26.092199325561523,
          30.370399475097656,
          36.438499450683594,
          35.00590133666992,
          37.199100494384766,
          32.3568000793457,
          31.353500366210938,
          36.62860107421875,
          45.22460174560547,
          42.05299758911133,
          30.377199172973633,
          42.395999908447266,
          41.91389846801758,
          34.289859771728516,
          34.030399322509766,
          28.381000518798828,
          31.4739990234375,
          44.962799072265625,
          35.36050033569336,
          37.625,
          25.79949951171875,
          30.407100677490234,
          38.23500061035156,
          37.14659881591797,
          35.64400100708008,
          34.68230056762695,
          46.20530319213867,
          37.86090087890625,
          39.73759841918945,
          32.18539810180664,
          34.14900207519531,
          36.63929748535156,
          29.57849884033203,
          27.750600814819336,
          33.36269760131836,
          34.12199783325195,
          31.686098098754883,
          28.70870018005371,
          33.499000549316406,
          41.336997985839844,
          35.78700256347656,
          36.319698333740234,
          32.80280303955078,
          30.362701416015625,
          29.3002986907959,
          42.2239990234375,
          40.66510009765625,
          30.649702072143555,
          36.78310012817383,
          43.138999938964844,
          36.19110107421875,
          44.8494987487793,
          39.51599884033203,
          34.112300872802734,
          42.61499786376953,
          38.12409973144531,
          31.12289810180664,
          28.292600631713867,
          32.555999755859375,
          35.48371887207031,
          35.21070098876953,
          27.48379898071289,
          35.040496826171875,
          36.38169860839844,
          28.90999984741211,
          45.25189971923828,
          34.79899978637695,
          39.724998474121094,
          33.08489990234375,
          40.55799865722656,
          23.237499237060547,
          25.912050247192383,
          27.988399505615234,
          28.322229385375977,
          36.36479949951172,
          29.45359992980957,
          34.333099365234375,
          29.631000518798828,
          34.803199768066406,
          27.42639923095703,
          30.732202529907227,
          32.291500091552734,
          27.36349105834961,
          34.994998931884766,
          40.92150115966797,
          40.388999938964844,
          36.290000915527344,
          35.93720245361328,
          38.257999420166016,
          31.597200393676758,
          39.81449890136719,
          31.310001373291016,
          44.787200927734375,
          30.2785701751709,
          47.957000732421875,
          28.92300033569336,
          37.05399703979492,
          44.4989013671875,
          35.6880989074707,
          30.09000015258789,
          29.089401245117188,
          30.75790023803711,
          45.310001373291016,
          27.695369720458984,
          28.115468978881836,
          34.521297454833984,
          33.175498962402344,
          43.737998962402344,
          40.977500915527344,
          30.610200881958008,
          29.4060001373291,
          42.14039993286133,
          35.434898376464844,
          32.12529754638672,
          37.42399978637695,
          36.46900177001953,
          28.781099319458008,
          35.891998291015625,
          30.178598403930664,
          32.48029708862305,
          39.24760055541992,
          41.25349807739258,
          34.312198638916016,
          42.742401123046875,
          42.24299621582031,
          26.691160202026367,
          31.442100524902344,
          37.523101806640625,
          29.08839988708496,
          42.0620002746582,
          28.354999542236328,
          39.46439743041992,
          37.25469970703125,
          28.171199798583984,
          31.343299865722656,
          36.377197265625,
          27.057458877563477,
          40.53450012207031,
          38.00600051879883,
          37.573997497558594,
          47.1697998046875,
          35.422996520996094,
          38.15800094604492,
          33.50600051879883,
          33.05220031738281,
          47.51530075073242,
          29.576400756835938,
          27.600200653076172,
          37.9650993347168,
          36.2776985168457,
          43.28700256347656,
          37.60420227050781,
          45.81690216064453,
          40.64390182495117,
          43.183998107910156,
          43.63100051879883,
          37.589599609375,
          38.41600036621094,
          33.577003479003906,
          38.695098876953125,
          29.249799728393555,
          34.042999267578125,
          49.38240051269531,
          32.95249938964844,
          37.91130065917969,
          36.487098693847656,
          44.80699920654297,
          32.39569854736328,
          26.12070083618164,
          29.98000144958496,
          35.6134033203125,
          48.43770217895508,
          39.61000061035156,
          38.117000579833984,
          32.24610137939453,
          30.969701766967773,
          33.144798278808594,
          27.8789005279541,
          27.147201538085938,
          34.95499801635742,
          33.67900085449219,
          42.240997314453125,
          32.775299072265625,
          34.00310134887695,
          47.97599792480469,
          39.86969757080078,
          27.091501235961914,
          29.322999954223633,
          35.538902282714844,
          41.82590103149414,
          27.12129020690918,
          47.090301513671875,
          35.86199951171875,
          30.81500244140625,
          27.98040008544922,
          44.31449890136719,
          35.75899887084961,
          29.07029914855957,
          34.95030212402344,
          40.787696838378906,
          32.38639831542969,
          41.759002685546875,
          30.348398208618164,
          39.17959976196289,
          34.57520294189453,
          24.055599212646484,
          33.61199951171875,
          36.842899322509766,
          42.458797454833984,
          31.763900756835938,
          40.24800109863281,
          31.02779769897461,
          30.595699310302734,
          37.8015022277832,
          35.01300048828125,
          24.26219940185547,
          27.76689910888672,
          34.48830032348633,
          39.24399948120117,
          27.23529052734375,
          40.884098052978516,
          33.9276008605957,
          28.14089012145996,
          37.70240020751953,
          29.62689971923828,
          38.07500076293945,
          26.309661865234375,
          39.368900299072266,
          47.61869812011719,
          32.01499938964844,
          41.881500244140625,
          30.16950225830078,
          34.108097076416016,
          29.753000259399414,
          43.04899978637695,
          24.925003051757812,
          33.97800064086914,
          28.0492000579834,
          33.287200927734375,
          35.55999755859375,
          35.04500198364258,
          26.143199920654297,
          25.998199462890625,
          30.331100463867188,
          37.298004150390625,
          34.305999755859375,
          28.89820098876953,
          37.6255989074707,
          36.51178741455078,
          30.363101959228516,
          34.148399353027344,
          30.756399154663086,
          34.538002014160156,
          40.275001525878906,
          32.220001220703125,
          36.9530029296875,
          37.61810302734375,
          39.78889846801758,
          41.784000396728516,
          35.38370132446289,
          35.83399963378906,
          43.2411994934082,
          25.205198287963867,
          34.68899917602539,
          37.10260009765625,
          26.351699829101562,
          35.23699951171875,
          36.21540069580078,
          26.358739852905273,
          36.98820114135742,
          31.24249839782715,
          29.462799072265625,
          37.99860382080078,
          40.11520004272461,
          42.882999420166016,
          39.74620056152344,
          28.29953956604004,
          33.11669921875,
          29.45349884033203,
          26.7178897857666,
          25.874500274658203,
          45.544002532958984,
          44.52779769897461,
          43.54549789428711,
          32.613101959228516,
          32.57029724121094,
          42.279300689697266,
          34.34700012207031,
          45.00160217285156,
          45.793800354003906,
          31.116580963134766,
          41.957000732421875,
          33.088199615478516,
          34.53820037841797,
          42.43800354003906,
          31.047800064086914,
          37.24500274658203,
          29.78550148010254,
          43.19600296020508,
          38.84770202636719,
          33.68199920654297,
          25.171001434326172,
          33.15570068359375,
          26.26849937438965,
          35.87800216674805,
          44.85230255126953,
          31.64630126953125,
          31.67620086669922,
          43.62779998779297,
          36.825599670410156,
          40.652000427246094,
          36.02629852294922,
          31.522798538208008,
          27.135469436645508,
          42.714088439941406,
          34.19960021972656,
          28.281299591064453,
          35.33110046386719,
          33.63100051879883,
          35.93619918823242,
          39.464500427246094,
          29.406299591064453,
          37.53900146484375,
          25.345701217651367,
          32.595001220703125,
          34.86499786376953,
          42.01789855957031,
          27.794599533081055,
          35.85100173950195,
          44.485198974609375,
          27.027820587158203,
          26.6919002532959,
          40.7516975402832,
          40.630699157714844,
          28.975399017333984,
          33.8838996887207,
          33.899200439453125,
          35.17789840698242,
          27.39807891845703,
          41.48699951171875,
          46.62010192871094,
          27.929800033569336,
          31.663997650146484,
          24.378000259399414,
          37.378700256347656,
          31.5474796295166,
          31.901098251342773,
          34.44919967651367,
          31.980798721313477,
          34.75019836425781,
          35.292999267578125,
          33.71430206298828,
          33.083396911621094,
          47.05009841918945,
          33.51259994506836,
          30.76559829711914,
          36.680999755859375,
          34.27649688720703,
          27.828500747680664,
          39.984500885009766,
          35.0962028503418,
          28.608299255371094,
          39.845703125,
          26.305601119995117,
          46.24690246582031,
          38.31589889526367,
          44.295501708984375,
          27.87030029296875,
          41.67609786987305,
          33.77000045776367,
          40.16830062866211,
          38.10260009765625,
          26.26690101623535,
          42.17799758911133,
          42.34349822998047,
          25.138500213623047,
          41.02470016479492,
          35.672000885009766,
          32.206298828125,
          32.00310134887695,
          32.90500259399414,
          42.56169891357422,
          36.977996826171875,
          27.354400634765625,
          26.611799240112305,
          27.878799438476562,
          43.41899871826172,
          43.47460174560547,
          36.579898834228516,
          36.775001525878906,
          35.013999938964844,
          37.13209915161133,
          29.710100173950195,
          28.074100494384766,
          35.265602111816406,
          44.61199951171875,
          34.23189926147461,
          27.323898315429688,
          42.45099639892578,
          28.011098861694336,
          41.11140060424805,
          32.31380081176758,
          46.1775016784668,
          32.58290100097656,
          30.142200469970703,
          37.163997650146484,
          30.39200210571289,
          41.69300079345703,
          30.865001678466797,
          30.800601959228516
         ],
         "z": [
          17.385700225830078,
          43.95030975341797,
          31.61829948425293,
          52.240501403808594,
          37.39751052856445,
          20.22209930419922,
          23.909761428833008,
          22.91230010986328,
          62.91670227050781,
          45.2244987487793,
          51.05959701538086,
          42.20634078979492,
          58.521400451660156,
          46.7859001159668,
          58.68299865722656,
          50.45396041870117,
          14.809800148010254,
          59.0890998840332,
          49.668296813964844,
          50.088199615478516,
          52.816200256347656,
          61.50389862060547,
          53.53450012207031,
          43.94919967651367,
          48.52689743041992,
          41.09640121459961,
          58.4031982421875,
          38.20833969116211,
          24.50979995727539,
          59.88350296020508,
          57.89849853515625,
          11.475899696350098,
          49.07469940185547,
          15.817900657653809,
          49.07809829711914,
          50.815799713134766,
          15.409000396728516,
          43.74565124511719,
          43.23830032348633,
          15.592498779296875,
          45.19889831542969,
          48.46059799194336,
          63.00410079956055,
          28.31909942626953,
          14.575399398803711,
          26.50037956237793,
          56.448699951171875,
          48.00080108642578,
          61.59669876098633,
          47.78860092163086,
          32.038330078125,
          27.879199981689453,
          42.74150085449219,
          36.04182815551758,
          56.27739715576172,
          35.072879791259766,
          52.882598876953125,
          41.18769836425781,
          49.36870193481445,
          51.78880310058594,
          48.56769943237305,
          32.580101013183594,
          45.753047943115234,
          17.032899856567383,
          63.985198974609375,
          59.31999969482422,
          17.288000106811523,
          15.980500221252441,
          37.79909896850586,
          55.025299072265625,
          40.55019760131836,
          30.254070281982422,
          54.795799255371094,
          37.85879898071289,
          58.938499450683594,
          59.877601623535156,
          57.46860122680664,
          61.60709762573242,
          50.321800231933594,
          50.884403228759766,
          55.96710205078125,
          57.76580047607422,
          29.706439971923828,
          12.157299041748047,
          44.24869918823242,
          52.17009735107422,
          61.07350158691406,
          36.235050201416016,
          54.82350158691406,
          57.1177978515625,
          35.557899475097656,
          53.45330047607422,
          57.916202545166016,
          65.35569763183594,
          31.680978775024414,
          52.702301025390625,
          33.542510986328125,
          59.635101318359375,
          53.68170166015625,
          48.72880172729492,
          44.04524230957031,
          42.15760803222656,
          46.101600646972656,
          56.259498596191406,
          37.89200210571289,
          45.69179916381836,
          36.15549850463867,
          56.8489990234375,
          49.8095588684082,
          45.58039855957031,
          61.105499267578125,
          59.6775016784668,
          27.404451370239258,
          47.65645217895508,
          54.028900146484375,
          19.306798934936523,
          58.6422004699707,
          54.1703987121582,
          42.71245193481445,
          60.97800064086914,
          46.16746139526367,
          49.352699279785156,
          41.43450164794922,
          20.15635871887207,
          41.94879913330078,
          24.325000762939453,
          12.767999649047852,
          41.049800872802734,
          28.852998733520508,
          33.60921859741211,
          26.98710060119629,
          57.51219940185547,
          14.22860050201416,
          61.88479995727539,
          15.827900886535645,
          28.062198638916016,
          49.16550064086914,
          35.880882263183594,
          48.38255310058594,
          52.24729919433594,
          56.08679962158203,
          60.410301208496094,
          14.989700317382812,
          43.35419845581055,
          38.834510803222656,
          57.65060043334961,
          48.310699462890625,
          53.14720153808594,
          24.200199127197266,
          43.83540344238281,
          56.59230041503906,
          24.616300582885742,
          58.148902893066406,
          48.44340133666992,
          48.406097412109375,
          49.795997619628906,
          53.358001708984375,
          29.2549991607666,
          45.88929748535156,
          45.7495002746582,
          23.959199905395508,
          58.62099838256836,
          48.012699127197266,
          62.96710205078125,
          57.909400939941406,
          54.35479736328125,
          56.90370178222656,
          48.872501373291016,
          51.701499938964844,
          45.28300094604492,
          25.513460159301758,
          56.91550064086914,
          35.268550872802734,
          15.074600219726562,
          50.745201110839844,
          40.14039993286133,
          49.79930114746094,
          58.08330154418945,
          48.397098541259766,
          16.960800170898438,
          28.80463981628418,
          52.858402252197266,
          54.920501708984375,
          21.33030128479004,
          19.16520118713379,
          62.50709915161133,
          24.85070037841797,
          56.63560104370117,
          44.64690017700195,
          40.328399658203125,
          51.920501708984375,
          50.59526062011719,
          15.710100173950195,
          41.75539779663086,
          28.146900177001953,
          55.542503356933594,
          48.22920227050781,
          45.38750076293945,
          41.21080017089844,
          52.94540023803711,
          50.79279708862305,
          26.658000946044922,
          19.36359977722168,
          13.027700424194336,
          61.54079818725586,
          57.59409713745117,
          44.72679901123047,
          64.42010498046875,
          55.92850112915039,
          41.51749801635742,
          62.04439926147461,
          30.992238998413086,
          35.54069900512695,
          53.89619827270508,
          46.18619918823242,
          44.38610076904297,
          17.733598709106445,
          42.67290115356445,
          53.145301818847656,
          61.174598693847656,
          56.2135009765625,
          30.186750411987305,
          54.522499084472656,
          31.333900451660156,
          63.58769989013672,
          49.27347183227539,
          50.35830307006836,
          40.52457046508789,
          45.13029861450195,
          28.798189163208008,
          31.027118682861328,
          61.465301513671875,
          39.27040100097656,
          59.826698303222656,
          42.14739990234375,
          41.55060958862305,
          65.65520477294922,
          25.386260986328125,
          55.58869934082031,
          44.73069763183594,
          52.39400100708008,
          43.98040008544922,
          32.818458557128906,
          29.069799423217773,
          19.690000534057617,
          27.263399124145508,
          22.088241577148438,
          62.80799865722656,
          50.017601013183594,
          59.2942008972168,
          51.74089813232422,
          55.28459930419922,
          37.40311813354492,
          55.72560119628906,
          59.13479995727539,
          28.389999389648438,
          38.53219985961914,
          54.768798828125,
          56.880001068115234,
          22.480239868164062,
          52.59020233154297,
          47.11360168457031,
          20.10205078125,
          56.17430114746094,
          29.594051361083984,
          48.637298583984375,
          18.087499618530273,
          62.474998474121094,
          30.894750595092773,
          50.80569839477539,
          53.51250076293945,
          47.85960006713867,
          29.226900100708008,
          57.03689956665039,
          49.629600524902344,
          40.690704345703125,
          58.92570114135742,
          52.039100646972656,
          62.41780090332031,
          47.394649505615234,
          60.02080154418945,
          42.50680160522461,
          53.26609802246094,
          51.9109992980957,
          41.306800842285156,
          66.4884033203125,
          44.74079895019531,
          23.612600326538086,
          52.16729736328125,
          28.45772933959961,
          19.68992042541504,
          33.856353759765625,
          44.51332092285156,
          60.75379943847656,
          41.02280044555664,
          49.920902252197266,
          57.62889862060547,
          57.958702087402344,
          40.81489944458008,
          61.44819641113281,
          46.05059814453125,
          48.73830032348633,
          38.84992980957031,
          27.54170036315918,
          45.666500091552734,
          11.22089958190918,
          36.419700622558594,
          40.656497955322266,
          63.81480026245117,
          49.26000213623047,
          65.38739776611328,
          58.85280227661133,
          61.739402770996094,
          52.0077018737793,
          65.72859954833984,
          42.230499267578125,
          58.47339630126953,
          28.4766902923584,
          66.57540130615234,
          40.5536994934082,
          51.48460006713867,
          45.13159942626953,
          48.74100112915039,
          60.79029846191406,
          35.94970703125,
          43.4656982421875,
          24.966569900512695,
          41.59590148925781,
          43.748199462890625,
          39.766300201416016,
          42.498382568359375,
          49.40800094604492,
          56.85759735107422,
          49.8807258605957,
          38.60530090332031,
          36.7051887512207,
          45.56922912597656,
          62.26930236816406,
          59.478599548339844,
          60.526100158691406,
          51.97570037841797,
          52.05189895629883,
          60.84049987792969,
          56.52280044555664,
          63.013301849365234,
          52.72820281982422,
          26.74298858642578,
          24.377399444580078,
          40.077362060546875,
          53.11249923706055,
          15.84939956665039,
          61.4718017578125,
          62.45589828491211,
          64.322998046875,
          40.08127975463867,
          41.670902252197266,
          30.900400161743164,
          39.46049880981445,
          46.91699981689453,
          20.565519332885742,
          27.808229446411133,
          54.157100677490234,
          38.4729118347168,
          56.74689865112305,
          45.35770034790039,
          51.33219909667969,
          13.723299026489258,
          50.83159637451172,
          57.96260070800781,
          47.100799560546875,
          59.60369873046875,
          55.12229919433594,
          50.641998291015625,
          59.04869842529297,
          54.99089813232422,
          46.907100677490234,
          62.788902282714844,
          33.95310974121094,
          25.676401138305664,
          54.20940017700195,
          14.131199836730957,
          54.87540054321289,
          33.23604965209961,
          43.35559844970703,
          14.642499923706055,
          21.704099655151367,
          42.935302734375,
          57.16899871826172,
          14.42759895324707,
          35.684600830078125,
          58.75170135498047,
          44.152801513671875,
          41.390602111816406,
          39.850399017333984,
          58.77830123901367,
          61.303897857666016,
          23.39837074279785,
          24.965299606323242,
          45.63759994506836,
          12.887199401855469,
          46.63050079345703,
          58.040496826171875,
          24.354000091552734,
          43.23649978637695,
          20.605998992919922,
          46.212738037109375,
          63.45610046386719,
          38.34659957885742,
          29.318300247192383,
          44.50279998779297,
          48.56637954711914,
          65.08319854736328,
          44.77409744262695,
          57.337501525878906,
          40.023799896240234,
          45.54439926147461,
          60.97779846191406,
          59.14080047607422,
          57.68429946899414,
          59.760597229003906,
          41.103797912597656,
          43.46357727050781,
          19.168701171875,
          62.895198822021484,
          37.98270034790039,
          38.681758880615234,
          40.967498779296875,
          44.44820022583008,
          53.317901611328125,
          45.408897399902344,
          46.37120056152344,
          46.30379867553711,
          58.73390197753906,
          41.368309020996094,
          48.18159866333008,
          41.67749786376953,
          30.024539947509766,
          61.510398864746094,
          45.38600158691406,
          54.87569808959961,
          51.137901306152344,
          46.65220260620117,
          55.751399993896484,
          57.5432014465332,
          44.889801025390625,
          38.16960144042969,
          46.331398010253906,
          56.44599914550781,
          55.35850143432617,
          21.563899993896484,
          44.25519943237305,
          43.596702575683594,
          61.7755012512207,
          51.288700103759766,
          29.842060089111328,
          42.46189880371094,
          25.10860824584961,
          16.422698974609375,
          13.075201034545898,
          57.633399963378906,
          42.95330047607422,
          22.050899505615234,
          27.416400909423828,
          50.831398010253906,
          50.76430130004883,
          42.59172058105469,
          46.28750228881836,
          29.319290161132812,
          36.97526931762695,
          44.41219711303711,
          49.25149917602539,
          28.15730094909668,
          32.11772155761719,
          44.977203369140625,
          55.81779861450195,
          55.361698150634766,
          13.115699768066406,
          46.959930419921875,
          57.84729766845703,
          57.922000885009766,
          45.634098052978516,
          40.609619140625,
          17.91590118408203,
          30.229591369628906,
          44.42280197143555,
          21.202999114990234,
          52.201297760009766,
          57.85919952392578,
          46.71731185913086,
          40.92270278930664,
          58.460899353027344,
          52.64609909057617,
          24.897201538085938,
          60.00259780883789,
          18.278701782226562,
          39.748897552490234,
          49.513301849365234,
          48.617000579833984,
          61.055198669433594,
          52.633602142333984,
          29.85439109802246,
          50.09360122680664,
          59.27119827270508,
          46.873600006103516,
          17.180200576782227,
          48.28739929199219,
          56.33440017700195,
          46.116600036621094,
          53.05079650878906,
          38.82651138305664,
          22.334489822387695,
          40.182899475097656,
          48.51900100708008,
          52.20490264892578,
          24.364900588989258,
          46.26728820800781,
          62.426998138427734,
          43.153499603271484,
          15.011399269104004,
          43.52070236206055,
          55.28990173339844,
          18.06599998474121,
          32.016109466552734,
          57.891300201416016,
          17.01689910888672,
          37.79970169067383,
          39.13518142700195,
          63.57400131225586,
          37.183109283447266,
          23.54759979248047,
          32.14241027832031,
          50.05950927734375,
          52.289398193359375,
          46.25729751586914,
          18.703100204467773,
          48.92210006713867,
          50.36899948120117,
          40.67560958862305,
          22.033100128173828,
          59.796600341796875,
          26.83293914794922,
          58.968902587890625,
          52.99089813232422,
          12.727800369262695,
          47.702301025390625,
          48.89580154418945,
          53.76899719238281,
          43.00760269165039,
          65.50090026855469,
          28.301198959350586,
          59.35960006713867,
          27.635499954223633,
          40.92720031738281,
          41.79465866088867,
          46.42115783691406,
          50.2306022644043,
          41.7139778137207,
          27.014209747314453,
          59.412696838378906,
          53.60860061645508,
          45.451171875,
          24.795101165771484,
          44.91586685180664,
          50.229000091552734,
          52.82619857788086,
          40.63269805908203,
          41.178401947021484,
          33.73979187011719,
          48.976898193359375,
          49.469200134277344,
          57.08300018310547,
          47.452999114990234,
          57.91360092163086,
          34.51701736450195,
          17.017601013183594,
          40.1761589050293,
          46.94820022583008,
          42.54969787597656,
          43.473899841308594,
          39.58871841430664,
          56.057899475097656,
          26.745899200439453,
          60.2036018371582,
          57.11389923095703,
          58.359100341796875,
          48.94499969482422,
          39.06230163574219,
          32.03886032104492,
          58.55480194091797,
          13.945500373840332,
          18.552730560302734,
          50.232601165771484,
          59.272300720214844,
          20.214200973510742,
          44.36240005493164,
          49.773399353027344,
          60.03040313720703,
          21.305299758911133,
          24.990699768066406,
          42.805198669433594,
          60.190101623535156,
          46.22249984741211,
          38.34321212768555,
          39.94751739501953,
          48.74169921875,
          45.27729797363281,
          60.80079650878906,
          13.433300018310547,
          43.49930191040039,
          62.75170135498047,
          41.41645812988281,
          28.45870018005371,
          19.365589141845703,
          52.80220031738281,
          45.003150939941406,
          60.65999984741211,
          41.38446044921875,
          41.52122116088867,
          53.852203369140625,
          37.25590133666992,
          61.50360107421875,
          54.994300842285156,
          21.061800003051758,
          64.67729949951172,
          29.327831268310547,
          53.55030059814453,
          46.24610137939453,
          39.0452995300293,
          48.61309814453125,
          55.36220169067383,
          48.70960235595703,
          26.213359832763672,
          19.38641929626465,
          39.849300384521484,
          37.93857192993164,
          64.45339965820312,
          19.097900390625,
          44.15974807739258,
          30.509008407592773,
          34.08770751953125,
          50.617740631103516,
          54.852901458740234,
          44.950401306152344,
          16.45370101928711,
          49.92499923706055,
          32.05860900878906,
          52.2322998046875,
          52.30189895629883,
          18.900800704956055,
          59.96739959716797,
          30.594619750976562,
          45.743202209472656,
          25.76974868774414,
          31.456470489501953,
          59.8557014465332,
          24.8617000579834,
          14.779099464416504,
          51.71979904174805,
          40.33270263671875,
          24.23499870300293,
          48.48032760620117,
          59.93370056152344,
          44.51420211791992,
          51.799400329589844,
          21.707000732421875,
          48.54930114746094,
          28.62019920349121,
          43.92599868774414,
          57.73749923706055,
          52.336997985839844,
          31.73042869567871,
          32.772727966308594,
          44.10749053955078,
          61.104801177978516,
          44.79329299926758,
          51.260398864746094,
          42.35259246826172,
          42.60649871826172,
          34.2861442565918,
          47.92539978027344,
          44.78580093383789,
          53.323299407958984,
          42.251373291015625,
          59.8766975402832,
          58.46159744262695,
          37.1946907043457,
          30.389480590820312,
          57.791099548339844,
          62.90740203857422,
          57.99729919433594,
          64.1052017211914,
          41.634090423583984,
          57.311302185058594,
          41.339500427246094,
          17.472698211669922,
          16.001800537109375,
          49.4370002746582,
          15.285901069641113,
          32.19211196899414
         ]
        },
        {
         "marker": {
          "color": [
           9.487540245056152,
           2.9782848358154297,
           1.7589807510375977,
           7.4956278800964355,
           2.6434829235076904,
           8.584027290344238,
           4.70907735824585,
           6.900237083435059,
           11.058979034423828,
           5.985408306121826,
           4.6031999588012695,
           2.080909490585327,
           10.798107147216797,
           5.244495868682861,
           9.740337371826172,
           4.9327850341796875,
           11.603001594543457,
           8.782949447631836,
           7.960291385650635,
           7.510847568511963,
           8.08665657043457,
           9.955036163330078,
           6.629784107208252,
           6.187309741973877,
           2.884850263595581,
           1.66921865940094,
           4.377597332000732,
           1.5905792713165283,
           10.807198524475098,
           8.99752140045166,
           7.812931537628174,
           15.959739685058594,
           14.669604301452637,
           11.756396293640137,
           5.264652252197266,
           3.9190914630889893,
           10.345693588256836,
           2.708669662475586,
           4.8257036209106445,
           9.971203804016113,
           3.359222650527954,
           12.336644172668457,
           9.701464653015137,
           6.950520992279053,
           10.920626640319824,
           3.236285924911499,
           7.6575446128845215,
           2.945477247238159,
           8.881928443908691,
           4.505533695220947,
           18.315589904785156,
           10.452274322509766,
           3.5345311164855957,
           1.1816575527191162,
           11.321113586425781,
           3.5503063201904297,
           8.581148147583008,
           1.694610834121704,
           8.432416915893555,
           10.122684478759766,
           8.510673522949219,
           5.896841049194336,
           4.041261672973633,
           9.501510620117188,
           8.896134376525879,
           6.411411285400391,
           9.769697189331055,
           10.274956703186035,
           6.151399612426758,
           9.085453033447266,
           10.175183296203613,
           1.6786364316940308,
           15.052820205688477,
           8.596508979797363,
           4.472801208496094,
           8.697160720825195,
           8.131217956542969,
           9.372919082641602,
           3.3894076347351074,
           8.83027458190918,
           8.171225547790527,
           6.592477321624756,
           1.691603422164917,
           14.236739158630371,
           6.612325191497803,
           7.737733364105225,
           9.862367630004883,
           3.7501437664031982,
           15.36936092376709,
           7.017298221588135,
           1.8075432777404785,
           7.26289701461792,
           14.141318321228027,
           9.262652397155762,
           6.48207950592041,
           5.478734016418457,
           1.6170605421066284,
           10.518348693847656,
           3.643282175064087,
           6.119032859802246,
           3.1550230979919434,
           2.116529703140259,
           2.958078622817993,
           9.664701461791992,
           4.466794490814209,
           4.170395374298096,
           2.495150566101074,
           5.207603454589844,
           5.460885047912598,
           5.07271146774292,
           10.046731948852539,
           11.825839042663574,
           2.7458629608154297,
           4.667354583740234,
           7.905728816986084,
           8.028797149658203,
           8.759811401367188,
           7.172726631164551,
           1.9932869672775269,
           10.16731071472168,
           4.282119274139404,
           6.977254867553711,
           1.888007640838623,
           6.139432907104492,
           8.547100067138672,
           6.260786056518555,
           13.81306266784668,
           5.554756164550781,
           16.609708786010742,
           3.0888559818267822,
           1.4516386985778809,
           8.349907875061035,
           10.97011661529541,
           5.661365509033203,
           10.41623592376709,
           14.0460844039917,
           14.870316505432129,
           9.984872817993164,
           5.309638500213623,
           12.095102310180664,
           6.812907695770264,
           4.488607406616211,
           10.377259254455566,
           14.206626892089844,
           4.107431888580322,
           6.272810935974121,
           12.341596603393555,
           17.191804885864258,
           9.094109535217285,
           2.64860463142395,
           9.22470474243164,
           14.774089813232422,
           10.597574234008789,
           3.9312517642974854,
           15.973675727844238,
           10.121177673339844,
           9.782426834106445,
           11.874751091003418,
           5.8388166427612305,
           15.923713684082031,
           3.642591714859009,
           4.359543800354004,
           5.382071018218994,
           7.943024158477783,
           14.381734848022461,
           7.264315128326416,
           12.46735954284668,
           6.489171028137207,
           6.994318008422852,
           4.494420051574707,
           5.033657073974609,
           7.612462997436523,
           13.986278533935547,
           13.92394733428955,
           6.057530403137207,
           3.3198113441467285,
           7.787242412567139,
           10.410993576049805,
           10.836237907409668,
           9.740424156188965,
           2.1274704933166504,
           14.640896797180176,
           7.834207534790039,
           3.3410542011260986,
           8.46753978729248,
           7.5794453620910645,
           10.68997859954834,
           7.604531288146973,
           10.803397178649902,
           2.6690874099731445,
           8.865742683410645,
           5.480388641357422,
           11.484112739562988,
           23.456645965576172,
           9.534161567687988,
           6.352453231811523,
           13.225178718566895,
           2.575059413909912,
           2.7643306255340576,
           17.386436462402344,
           2.8740646839141846,
           2.709446668624878,
           5.766144275665283,
           13.33469295501709,
           10.398552894592285,
           9.352166175842285,
           18.93483543395996,
           7.65290641784668,
           10.113685607910156,
           5.798981666564941,
           11.617433547973633,
           1.7906032800674438,
           2.520451068878174,
           7.423653602600098,
           15.775489807128906,
           21.46368408203125,
           9.645406723022461,
           10.409951210021973,
           5.683908462524414,
           9.063125610351562,
           3.6435461044311523,
           1.6902235746383667,
           9.07184886932373,
           17.417734146118164,
           9.298047065734863,
           5.1272077560424805,
           5.7899909019470215,
           2.234848737716675,
           5.189192295074463,
           2.6170003414154053,
           1.2747665643692017,
           11.897953033447266,
           1.6761659383773804,
           10.754142761230469,
           9.218195915222168,
           2.147221565246582,
           9.674327850341797,
           4.210982799530029,
           8.719306945800781,
           8.335956573486328,
           6.527127265930176,
           4.071042537689209,
           12.408970832824707,
           8.905892372131348,
           7.183071613311768,
           5.223270416259766,
           5.486182689666748,
           11.526314735412598,
           7.468102931976318,
           12.343761444091797,
           8.921473503112793,
           4.9865803718566895,
           1.5370582342147827,
           8.635767936706543,
           12.239322662353516,
           1.4590612649917603,
           7.787397384643555,
           12.66048812866211,
           13.056777954101562,
           5.390719890594482,
           7.3830246925354,
           18.701282501220703,
           7.230193138122559,
           8.571084022521973,
           3.7628488540649414,
           3.9137158393859863,
           9.484199523925781,
           8.87949275970459,
           7.388397693634033,
           18.100675582885742,
           4.128393650054932,
           8.536345481872559,
           1.3366531133651733,
           8.413581848144531,
           6.0322465896606445,
           1.349668264389038,
           9.437926292419434,
           12.189102172851562,
           9.853286743164062,
           4.840590953826904,
           8.71859359741211,
           1.9283877611160278,
           7.877467632293701,
           15.848200798034668,
           2.544891595840454,
           9.631067276000977,
           3.769244909286499,
           5.820559024810791,
           7.695474147796631,
           2.086313009262085,
           7.795784950256348,
           7.816500186920166,
           3.836102247238159,
           10.787849426269531,
           6.927855014801025,
           5.494681358337402,
           12.14461612701416,
           10.473138809204102,
           4.933709144592285,
           7.612407684326172,
           12.189887046813965,
           10.371219635009766,
           1.0882500410079956,
           7.89022970199585,
           2.745944023132324,
           16.060590744018555,
           10.285603523254395,
           4.491062164306641,
           10.424837112426758,
           3.845785140991211,
           8.342679023742676,
           12.847496032714844,
           10.776094436645508,
           14.501355171203613,
           9.436773300170898,
           6.2686262130737305,
           11.578232765197754,
           1.9835909605026245,
           9.006593704223633,
           17.534523010253906,
           9.33459186553955,
           3.7161104679107666,
           3.426952362060547,
           8.865633010864258,
           14.199939727783203,
           6.208715915679932,
           4.540375232696533,
           18.442981719970703,
           13.365468978881836,
           15.871304512023926,
           2.845231533050537,
           5.502269268035889,
           6.964817523956299,
           5.46335506439209,
           11.037138938903809,
           4.49680233001709,
           3.6868464946746826,
           10.730064392089844,
           13.399792671203613,
           10.204672813415527,
           10.305628776550293,
           17.151105880737305,
           8.706282615661621,
           12.413788795471191,
           11.184009552001953,
           6.346806049346924,
           3.2229976654052734,
           9.198430061340332,
           1.4063645601272583,
           8.886029243469238,
           10.400947570800781,
           9.938502311706543,
           10.494224548339844,
           7.563658714294434,
           2.759681463241577,
           22.308786392211914,
           11.745682716369629,
           7.059898853302002,
           14.336071014404297,
           4.869500160217285,
           1.4015637636184692,
           6.106007099151611,
           1.0765552520751953,
           11.924989700317383,
           3.334580898284912,
           6.083772659301758,
           12.429152488708496,
           10.296801567077637,
           6.164493560791016,
           4.507903575897217,
           7.951440811157227,
           3.4811770915985107,
           12.008597373962402,
           10.423361778259277,
           15.70008373260498,
           18.586246490478516,
           10.122289657592773,
           17.079345703125,
           4.364841461181641,
           8.994156837463379,
           11.312920570373535,
           9.591304779052734,
           0.6776500344276428,
           3.4091978073120117,
           10.731890678405762,
           6.430738925933838,
           9.584741592407227,
           8.405170440673828,
           11.020166397094727,
           2.487926959991455,
           7.669305324554443,
           3.3983912467956543,
           2.363987445831299,
           1.4799823760986328,
           8.385536193847656,
           8.835596084594727,
           2.3844878673553467,
           2.962568998336792,
           4.288492679595947,
           13.014762878417969,
           5.0341291427612305,
           9.474442481994629,
           8.981672286987305,
           15.40211009979248,
           12.464092254638672,
           3.5961077213287354,
           9.936092376708984,
           2.193946361541748,
           5.372025489807129,
           7.02311897277832,
           5.047826290130615,
           9.24860668182373,
           4.05079984664917,
           11.953597068786621,
           1.3277289867401123,
           5.2341790199279785,
           10.66005802154541,
           7.113900184631348,
           12.760744094848633,
           5.799712657928467,
           2.0211293697357178,
           3.0780508518218994,
           7.611435890197754,
           8.063786506652832,
           4.73379373550415,
           0.9857015609741211,
           4.114099502563477,
           15.253458976745605,
           13.808148384094238,
           5.198212146759033,
           20.46039390563965,
           20.023574829101562,
           8.679780006408691,
           1.7053295373916626,
           7.436163425445557,
           2.942800760269165,
           1.7784754037857056,
           10.974897384643555,
           3.9664785861968994,
           14.195087432861328,
           6.918832302093506,
           4.5354437828063965,
           10.564586639404297,
           13.019279479980469,
           11.563467025756836,
           8.514467239379883,
           12.182350158691406,
           8.545387268066406,
           8.856758117675781,
           6.298455238342285,
           16.167964935302734,
           4.156801223754883,
           8.440314292907715,
           6.481825828552246,
           6.838658809661865,
           10.772255897521973,
           2.51655912399292,
           9.653817176818848,
           12.581450462341309,
           9.83818531036377,
           4.854653358459473,
           10.68476390838623,
           7.2537336349487305,
           5.6536102294921875,
           5.152288436889648,
           2.608083963394165,
           3.213120698928833,
           1.8964744806289673,
           15.592416763305664,
           19.945030212402344,
           15.18327808380127,
           6.070356845855713,
           5.717874050140381,
           4.301895618438721,
           9.10400104522705,
           5.72824239730835,
           13.082653045654297,
           4.492537498474121,
           10.901005744934082,
           3.8489036560058594,
           5.888981342315674,
           1.4922658205032349,
           11.371492385864258,
           2.0515406131744385,
           5.36704158782959,
           13.893566131591797,
           4.280004501342773,
           9.707612037658691,
           4.389944076538086,
           2.267866849899292,
           13.114310264587402,
           5.336579322814941,
           6.693164825439453,
           9.506048202514648,
           7.280970096588135,
           12.935452461242676,
           6.177762031555176,
           5.0348334312438965,
           8.073598861694336,
           16.507692337036133,
           4.764255046844482,
           5.171623706817627,
           10.162890434265137,
           18.992990493774414,
           7.9781646728515625,
           3.7657313346862793,
           9.93892765045166,
           16.973867416381836,
           3.279876708984375,
           2.5335748195648193,
           4.713524341583252,
           2.109898567199707,
           6.037497520446777,
           4.9100728034973145,
           11.94960880279541,
           2.5680043697357178,
           11.827956199645996,
           4.235004425048828,
           13.222310066223145,
           6.200247764587402,
           6.7811503410339355,
           8.415549278259277,
           2.9208571910858154,
           12.383869171142578,
           8.40530014038086,
           19.59270477294922,
           0.8756555318832397,
           8.806013107299805,
           17.797061920166016,
           11.50992202758789,
           1.4681355953216553,
           4.332079887390137,
           5.481050491333008,
           18.191959381103516,
           10.864718437194824,
           3.940065622329712,
           6.848477840423584,
           0.8539532423019409,
           8.27118968963623,
           9.127395629882812,
           2.443709373474121,
           9.159808158874512,
           12.781637191772461,
           13.115983009338379,
           19.10380744934082,
           5.624981880187988,
           8.28538703918457,
           4.26259708404541,
           9.508852005004883,
           15.771617889404297,
           10.845008850097656,
           2.291802167892456,
           14.255663871765137,
           1.9390406608581543,
           3.36799693107605,
           12.295069694519043,
           2.33612060546875,
           1.4821500778198242,
           10.245463371276855,
           6.4733805656433105,
           4.105221748352051,
           6.718235969543457,
           3.6823689937591553,
           6.847806453704834,
           6.835020542144775,
           1.725019931793213,
           2.3754777908325195,
           1.8753407001495361,
           2.4180543422698975,
           3.266650676727295,
           12.116581916809082,
           8.326362609863281,
           9.28061580657959,
           5.627632141113281,
           9.868147850036621,
           1.5868256092071533,
           11.677608489990234,
           4.318599224090576,
           3.562934637069702,
           1.2203072309494019,
           15.291605949401855,
           7.647700786590576,
           9.48463249206543,
           14.699256896972656,
           7.497106552124023,
           20.173139572143555,
           3.6080522537231445,
           10.956258773803711,
           11.788272857666016,
           14.796850204467773,
           8.079368591308594,
           9.268308639526367,
           5.160853862762451,
           12.894466400146484,
           4.9825663566589355,
           6.821156978607178,
           7.756848335266113,
           4.315221786499023,
           6.321982383728027,
           13.6444091796875,
           10.817596435546875,
           6.382436752319336,
           1.2827564477920532,
           0.5504114627838135,
           7.117624282836914,
           3.3656668663024902,
           9.24698257446289,
           12.379745483398438,
           3.350876808166504,
           11.727545738220215,
           2.3854596614837646,
           16.688461303710938,
           7.078394889831543,
           6.331615447998047,
           3.5903618335723877,
           11.992661476135254,
           1.8289375305175781,
           1.6436103582382202,
           8.64022445678711,
           1.4964423179626465,
           5.576413631439209,
           8.837380409240723,
           4.351584434509277,
           7.795935153961182,
           1.1351886987686157,
           10.158668518066406,
           6.275455474853516,
           2.148512840270996,
           14.08239459991455,
           9.723267555236816,
           3.188966989517212,
           3.5621368885040283,
           6.7743730545043945,
           6.965937614440918,
           0.5224103331565857,
           8.540950775146484,
           9.569615364074707,
           3.7238919734954834,
           8.148799896240234,
           11.97314167022705,
           6.444392204284668,
           8.268353462219238,
           6.598717212677002,
           9.280654907226562,
           5.782382011413574,
           18.251056671142578,
           11.22021770477295,
           3.8659093379974365,
           7.0468621253967285,
           9.582733154296875,
           2.3980603218078613,
           9.090533256530762,
           1.3329603672027588,
           3.314542293548584,
           10.964696884155273,
           7.475830078125,
           10.413518905639648,
           7.373569011688232,
           15.791460037231445,
           8.757771492004395,
           5.15372896194458,
           5.502352237701416,
           12.2122220993042,
           6.298299312591553,
           5.222827434539795,
           7.5570759773254395,
           12.626262664794922,
           3.2422313690185547,
           7.677150726318359,
           8.131697654724121,
           8.259493827819824,
           18.13065528869629,
           2.3107433319091797,
           5.744872570037842,
           2.616989850997925,
           12.176941871643066,
           2.071284294128418,
           3.585812568664551,
           15.794623374938965,
           16.296478271484375,
           7.751859188079834,
           6.315246105194092,
           2.2614011764526367,
           10.385115623474121,
           11.832561492919922,
           2.3057522773742676,
           1.565446376800537,
           14.342572212219238,
           6.772126197814941,
           11.255102157592773,
           8.727845191955566,
           1.6182667016983032,
           8.016473770141602,
           2.649289608001709,
           12.0818452835083,
           12.18840503692627,
           12.893043518066406,
           13.037850379943848,
           2.072798490524292
          ],
          "size": 12
         },
         "mode": "markers",
         "name": "Predicción",
         "scene": "scene2",
         "type": "scatter3d",
         "x": [
          47.740325927734375,
          18.695266723632812,
          46.22615051269531,
          18.728727340698242,
          27.161771774291992,
          31.1745548248291,
          29.463483810424805,
          33.90947723388672,
          35.477230072021484,
          27.0087833404541,
          52.021732330322266,
          21.897594451904297,
          25.540603637695312,
          43.066219329833984,
          54.11506652832031,
          58.54277801513672,
          27.107086181640625,
          26.72027587890625,
          56.510169982910156,
          58.85292053222656,
          20.815723419189453,
          31.16639518737793,
          57.9171028137207,
          30.7300968170166,
          33.484981536865234,
          62.2684211730957,
          29.39858055114746,
          51.20058822631836,
          37.025962829589844,
          49.62834167480469,
          54.00894546508789,
          34.77933120727539,
          27.4887638092041,
          40.18029022216797,
          22.283634185791016,
          43.78071212768555,
          49.07403564453125,
          19.90048599243164,
          56.13128662109375,
          45.27532196044922,
          60.45001220703125,
          49.6722412109375,
          45.46527099609375,
          28.193115234375,
          45.626853942871094,
          47.806148529052734,
          55.70668029785156,
          36.06419372558594,
          50.16949462890625,
          41.648460388183594,
          40.089847564697266,
          46.60951614379883,
          28.14940071105957,
          26.608898162841797,
          53.88240432739258,
          34.00315856933594,
          56.83465576171875,
          63.63882064819336,
          51.91971206665039,
          56.462127685546875,
          28.682937622070312,
          27.486900329589844,
          61.229862213134766,
          32.12623596191406,
          32.757835388183594,
          49.34840774536133,
          31.80544090270996,
          27.366458892822266,
          35.37228775024414,
          54.72309875488281,
          32.24350357055664,
          33.74664306640625,
          44.61298751831055,
          34.95375061035156,
          34.35376739501953,
          33.0271110534668,
          53.41986846923828,
          32.553279876708984,
          36.76289749145508,
          57.59668731689453,
          22.819414138793945,
          28.910993576049805,
          28.605737686157227,
          44.433677673339844,
          56.86893844604492,
          18.69550132751465,
          27.621042251586914,
          30.03633689880371,
          28.474557876586914,
          53.03355407714844,
          43.51573181152344,
          59.05055236816406,
          41.2369499206543,
          44.47755813598633,
          31.023033142089844,
          57.476776123046875,
          49.976226806640625,
          24.82193946838379,
          26.345796585083008,
          16.86994171142578,
          63.02714538574219,
          59.567901611328125,
          50.32782745361328,
          23.555696487426758,
          48.9986457824707,
          60.48114013671875,
          34.16782760620117,
          39.05387878417969,
          19.059202194213867,
          19.22385025024414,
          27.421916961669922,
          26.655271530151367,
          46.20598602294922,
          18.883792877197266,
          19.397262573242188,
          44.79511642456055,
          30.961326599121094,
          60.70082092285156,
          19.209484100341797,
          44.490604400634766,
          59.561431884765625,
          21.131851196289062,
          50.79317092895508,
          43.63166809082031,
          44.92911911010742,
          39.11564636230469,
          45.84498596191406,
          24.89862632751465,
          36.58039093017578,
          36.238826751708984,
          35.08608627319336,
          56.88532638549805,
          43.7359619140625,
          40.546783447265625,
          46.91765213012695,
          43.374202728271484,
          41.806358337402344,
          48.176551818847656,
          19.477567672729492,
          51.60184860229492,
          30.6248722076416,
          35.43744659423828,
          44.7943229675293,
          49.226619720458984,
          26.090576171875,
          50.798404693603516,
          32.74905014038086,
          38.74393844604492,
          35.458255767822266,
          36.96986389160156,
          44.63104248046875,
          36.175052642822266,
          22.89742088317871,
          50.185367584228516,
          25.42003059387207,
          22.24483299255371,
          56.112667083740234,
          39.756229400634766,
          37.17484664916992,
          24.215559005737305,
          35.45707702636719,
          31.414569854736328,
          21.640968322753906,
          39.949867248535156,
          39.0379753112793,
          60.667388916015625,
          26.695171356201172,
          59.74123764038086,
          18.50237274169922,
          35.00148010253906,
          33.79660415649414,
          52.59503173828125,
          46.46189498901367,
          33.77392578125,
          19.180862426757812,
          39.519378662109375,
          37.07938003540039,
          24.238304138183594,
          43.31188201904297,
          32.984432220458984,
          32.13782501220703,
          48.214698791503906,
          56.812435150146484,
          26.849319458007812,
          29.87535285949707,
          33.86063003540039,
          30.40203857421875,
          49.01272201538086,
          55.71480178833008,
          25.733444213867188,
          56.74641036987305,
          21.595243453979492,
          27.718002319335938,
          45.37932586669922,
          48.2073860168457,
          53.50428009033203,
          25.960933685302734,
          29.48668670654297,
          26.26138687133789,
          35.61399841308594,
          29.771663665771484,
          51.085269927978516,
          52.75408935546875,
          30.79729652404785,
          28.38654327392578,
          57.59223937988281,
          26.896360397338867,
          35.9807243347168,
          21.89237403869629,
          49.36037063598633,
          31.51664924621582,
          31.54189109802246,
          29.23475456237793,
          20.72218894958496,
          28.563791275024414,
          45.48086166381836,
          49.04357147216797,
          25.879039764404297,
          52.06364822387695,
          29.477155685424805,
          31.016616821289062,
          46.10210418701172,
          36.44078826904297,
          39.09156036376953,
          32.505245208740234,
          21.2242431640625,
          63.43259048461914,
          57.644779205322266,
          32.47526550292969,
          31.076601028442383,
          50.34757995605469,
          29.76934051513672,
          50.553619384765625,
          25.216516494750977,
          36.848350524902344,
          23.84720802307129,
          42.14496994018555,
          40.73843765258789,
          51.93628692626953,
          46.977516174316406,
          20.368453979492188,
          20.904457092285156,
          38.06202697753906,
          30.702119827270508,
          47.28012466430664,
          26.296472549438477,
          47.828887939453125,
          35.73937225341797,
          22.402851104736328,
          29.447500228881836,
          55.42816925048828,
          33.26141357421875,
          26.28158187866211,
          24.862449645996094,
          47.24446487426758,
          34.932289123535156,
          42.80643844604492,
          49.985931396484375,
          48.693382263183594,
          47.24203109741211,
          18.755407333374023,
          33.01605224609375,
          33.52467727661133,
          38.79592514038086,
          40.82304000854492,
          53.42088317871094,
          32.558441162109375,
          48.84056854248047,
          43.515602111816406,
          37.68642807006836,
          38.17742156982422,
          40.5928955078125,
          34.421287536621094,
          57.41546630859375,
          59.716827392578125,
          60.09252166748047,
          28.709728240966797,
          47.3455696105957,
          41.53923416137695,
          63.45155334472656,
          53.35844421386719,
          52.50202941894531,
          51.37266540527344,
          45.70463180541992,
          57.689598083496094,
          40.03384780883789,
          59.71757888793945,
          35.23008728027344,
          58.537841796875,
          46.20310592651367,
          28.35053062438965,
          45.04664611816406,
          62.9350700378418,
          29.66257095336914,
          35.153751373291016,
          20.214946746826172,
          40.37224197387695,
          53.57362365722656,
          25.214157104492188,
          48.37264633178711,
          54.61448669433594,
          53.55906295776367,
          24.54674530029297,
          34.66938781738281,
          27.82068634033203,
          37.24302673339844,
          27.855409622192383,
          28.009021759033203,
          41.43181610107422,
          21.754579544067383,
          33.885189056396484,
          36.22899627685547,
          29.092267990112305,
          40.2813606262207,
          33.89643859863281,
          24.266992568969727,
          31.628379821777344,
          28.764108657836914,
          36.85661315917969,
          37.948997497558594,
          28.622314453125,
          46.866729736328125,
          54.017982482910156,
          31.206892013549805,
          38.20424270629883,
          41.5794792175293,
          32.783809661865234,
          40.95138168334961,
          53.233314514160156,
          48.075050354003906,
          62.9390869140625,
          51.92453384399414,
          26.85837173461914,
          58.91334915161133,
          27.993040084838867,
          27.54731559753418,
          18.908613204956055,
          38.3494987487793,
          38.39773941040039,
          27.876754760742188,
          56.322113037109375,
          39.224220275878906,
          51.625343322753906,
          25.6801700592041,
          36.26422882080078,
          39.80442428588867,
          36.89277267456055,
          41.41013717651367,
          55.78327560424805,
          29.205785751342773,
          32.36588668823242,
          35.28322982788086,
          40.47052001953125,
          43.89838409423828,
          22.855628967285156,
          37.16976547241211,
          46.658077239990234,
          33.381290435791016,
          24.622203826904297,
          36.64337921142578,
          39.1606330871582,
          23.128664016723633,
          54.966434478759766,
          43.26765823364258,
          49.432674407958984,
          57.989105224609375,
          33.89063262939453,
          45.112300872802734,
          27.436725616455078,
          48.150169372558594,
          51.29934310913086,
          34.10664367675781,
          34.038047790527344,
          23.69274139404297,
          30.8775577545166,
          28.16046142578125,
          33.77607727050781,
          43.728416442871094,
          49.51740646362305,
          23.81365966796875,
          47.42145919799805,
          54.21629333496094,
          52.16357421875,
          19.612850189208984,
          45.351165771484375,
          29.21156120300293,
          55.05631637573242,
          54.874046325683594,
          46.95263671875,
          42.14434814453125,
          25.595911026000977,
          17.57932472229004,
          46.455223083496094,
          57.67399597167969,
          55.5359001159668,
          40.98101806640625,
          53.40694808959961,
          51.87490463256836,
          63.98460006713867,
          41.60851287841797,
          16.158687591552734,
          26.948833465576172,
          38.36555480957031,
          50.647708892822266,
          36.82939910888672,
          20.638486862182617,
          32.52036666870117,
          49.08501434326172,
          44.698463439941406,
          52.96289825439453,
          62.92597961425781,
          32.26142883300781,
          59.49114227294922,
          49.24781036376953,
          26.627418518066406,
          45.87941360473633,
          26.299352645874023,
          26.98630142211914,
          47.0825309753418,
          43.7287483215332,
          22.671905517578125,
          62.07878875732422,
          45.95012283325195,
          43.98188781738281,
          28.772212982177734,
          26.87541961669922,
          25.89003562927246,
          27.487245559692383,
          28.410911560058594,
          19.671066284179688,
          37.43943405151367,
          35.65608215332031,
          29.09699249267578,
          56.965179443359375,
          58.352317810058594,
          47.61422348022461,
          43.071327209472656,
          27.954933166503906,
          57.91923904418945,
          49.34975051879883,
          14.954614639282227,
          60.347415924072266,
          51.969722747802734,
          28.10856819152832,
          23.552772521972656,
          46.80558395385742,
          26.55560874938965,
          46.599544525146484,
          22.584354400634766,
          44.88683319091797,
          38.468685150146484,
          57.91559600830078,
          49.161827087402344,
          57.8592529296875,
          45.71475601196289,
          25.9285888671875,
          51.14692687988281,
          32.88431167602539,
          43.92633819580078,
          29.016319274902344,
          21.952402114868164,
          26.873809814453125,
          30.958206176757812,
          53.56148147583008,
          50.48102569580078,
          62.08250045776367,
          36.68363571166992,
          39.63990020751953,
          36.67380905151367,
          43.25318145751953,
          53.025272369384766,
          34.46199035644531,
          33.28086471557617,
          59.14106369018555,
          22.7135009765625,
          25.32051658630371,
          36.0793571472168,
          60.21941375732422,
          24.46908187866211,
          33.66183853149414,
          20.087099075317383,
          62.932918548583984,
          38.20777130126953,
          37.50765609741211,
          21.30882453918457,
          41.2081298828125,
          35.58002853393555,
          31.010059356689453,
          63.01969528198242,
          63.08045959472656,
          29.820159912109375,
          33.555686950683594,
          48.66447830200195,
          30.66864585876465,
          42.33049774169922,
          39.046226501464844,
          19.969083786010742,
          62.49897766113281,
          33.1891975402832,
          42.384620666503906,
          42.64717102050781,
          32.226951599121094,
          24.49475860595703,
          44.823368072509766,
          41.15149688720703,
          55.32465362548828,
          43.170265197753906,
          25.088043212890625,
          34.71788024902344,
          53.13780975341797,
          26.553529739379883,
          48.443275451660156,
          21.361082077026367,
          44.0528450012207,
          44.936920166015625,
          56.52808380126953,
          34.12022399902344,
          57.252803802490234,
          34.46392822265625,
          23.76599884033203,
          55.534759521484375,
          25.786144256591797,
          42.829349517822266,
          26.34835433959961,
          42.95349884033203,
          38.80685806274414,
          25.690420150756836,
          45.11769485473633,
          35.77680206298828,
          41.875770568847656,
          30.69344139099121,
          21.52709197998047,
          42.03475570678711,
          26.0377254486084,
          33.05081558227539,
          37.92402648925781,
          54.959625244140625,
          61.109188079833984,
          37.49281692504883,
          25.03219223022461,
          26.991455078125,
          26.4622745513916,
          24.52794075012207,
          45.72832107543945,
          45.09111404418945,
          57.32000732421875,
          21.475004196166992,
          23.026649475097656,
          43.46282196044922,
          37.14133834838867,
          24.0927734375,
          32.84660720825195,
          35.537357330322266,
          58.55195999145508,
          20.788375854492188,
          44.53797912597656,
          62.89070129394531,
          43.81689453125,
          25.903274536132812,
          43.85708999633789,
          62.586490631103516,
          30.28236961364746,
          62.89738464355469,
          17.361225128173828,
          60.32917785644531,
          26.560232162475586,
          63.510459899902344,
          47.347145080566406,
          28.200664520263672,
          54.81669998168945,
          50.154083251953125,
          56.633506774902344,
          55.241065979003906,
          33.7199821472168,
          48.34309005737305,
          22.548540115356445,
          34.51805114746094,
          21.650745391845703,
          19.57297134399414,
          61.70152282714844,
          36.50599670410156,
          43.44526290893555,
          43.303550720214844,
          38.85164260864258,
          54.187679290771484,
          26.718740463256836,
          41.52137756347656,
          48.16253662109375,
          46.51503372192383,
          45.79171371459961,
          34.32567596435547,
          39.450279235839844,
          28.807167053222656,
          43.42913055419922,
          57.364776611328125,
          39.046016693115234,
          45.57480239868164,
          53.4927864074707,
          47.798919677734375,
          26.40639877319336,
          39.138519287109375,
          41.163665771484375,
          26.064014434814453,
          63.095821380615234,
          48.925880432128906,
          43.553619384765625,
          51.39132308959961,
          47.0206298828125,
          62.935791015625,
          36.84160232543945,
          60.43511962890625,
          41.05066680908203,
          45.037532806396484,
          55.77700424194336,
          59.79141616821289,
          28.301904678344727,
          21.509410858154297,
          17.135618209838867,
          56.94207000732422,
          50.21737289428711,
          33.73778533935547,
          21.936786651611328,
          26.875320434570312,
          38.954010009765625,
          27.409427642822266,
          38.49971389770508,
          28.55396270751953,
          44.93174743652344,
          53.15205764770508,
          53.561100006103516,
          50.32209396362305,
          30.012386322021484,
          43.72883605957031,
          28.797523498535156,
          54.447265625,
          31.192541122436523,
          47.74525451660156,
          63.93279266357422,
          34.485660552978516,
          35.72408676147461,
          18.247764587402344,
          32.22239685058594,
          21.183134078979492,
          50.10151290893555,
          40.68132400512695,
          37.863800048828125,
          40.7304573059082,
          41.14292907714844,
          28.813447952270508,
          27.514009475708008,
          41.189979553222656,
          25.143888473510742,
          37.2862548828125,
          50.870277404785156,
          27.612443923950195,
          49.69944763183594,
          44.142799377441406,
          57.89283752441406,
          38.88480758666992,
          47.98244094848633,
          62.281917572021484,
          47.5771598815918,
          24.59172821044922,
          61.94050979614258,
          35.770076751708984,
          22.794851303100586,
          36.3260498046875,
          24.694887161254883,
          33.04631805419922,
          34.47850799560547,
          48.45222091674805,
          41.86713409423828,
          26.83828353881836,
          45.19853973388672,
          56.16408157348633,
          54.90816879272461,
          19.58098793029785,
          25.33476448059082,
          34.32304000854492,
          36.74223709106445,
          50.133296966552734,
          57.056758880615234,
          19.910789489746094,
          41.165626525878906,
          26.723976135253906,
          52.07255554199219,
          51.163639068603516,
          40.44697570800781,
          38.41195297241211,
          49.892398834228516,
          34.165374755859375,
          62.5919075012207,
          30.906301498413086,
          50.18995666503906,
          27.83852767944336,
          36.434242248535156,
          23.977069854736328,
          40.984004974365234,
          30.072498321533203
         ],
         "y": [
          34.40317916870117,
          23.0404052734375,
          24.327077865600586,
          43.19955825805664,
          35.52934265136719,
          33.2769660949707,
          23.049291610717773,
          28.9560546875,
          37.95338439941406,
          38.522212982177734,
          25.341398239135742,
          32.552940368652344,
          42.70173645019531,
          27.050901412963867,
          45.822181701660156,
          25.126981735229492,
          36.497825622558594,
          30.278141021728516,
          40.433353424072266,
          44.985477447509766,
          42.76839828491211,
          38.981773376464844,
          36.60196304321289,
          37.514617919921875,
          22.550533294677734,
          26.213945388793945,
          21.555343627929688,
          27.112462997436523,
          42.724464416503906,
          36.78515625,
          29.436222076416016,
          41.14496612548828,
          45.156959533691406,
          34.819236755371094,
          33.60496520996094,
          25.71550178527832,
          33.774993896484375,
          25.634605407714844,
          37.084678649902344,
          35.31196975708008,
          28.32884979248047,
          40.07390213012695,
          38.37556838989258,
          45.75086212158203,
          42.37193298339844,
          24.1053409576416,
          29.04941177368164,
          21.379743576049805,
          35.80327224731445,
          24.50611114501953,
          49.862274169921875,
          41.75259780883789,
          30.66454315185547,
          23.349424362182617,
          53.72273254394531,
          32.7238655090332,
          40.14550018310547,
          27.102745056152344,
          33.284400939941406,
          47.00711441040039,
          41.15216827392578,
          50.70451354980469,
          24.408735275268555,
          29.602787017822266,
          31.151897430419922,
          25.31149673461914,
          30.480518341064453,
          36.682613372802734,
          34.64518737792969,
          50.976993560791016,
          41.728981018066406,
          26.221330642700195,
          56.72565841674805,
          38.470985412597656,
          22.518096923828125,
          31.627849578857422,
          29.872385025024414,
          33.5515251159668,
          21.472782135009766,
          42.936767578125,
          33.63971710205078,
          24.440269470214844,
          24.357622146606445,
          37.60789489746094,
          51.87396240234375,
          45.3259162902832,
          34.92580795288086,
          35.728599548339844,
          52.92416000366211,
          34.330589294433594,
          22.097076416015625,
          38.56401062011719,
          42.74654006958008,
          38.75526428222656,
          37.99318313598633,
          25.86798095703125,
          21.8005313873291,
          43.56629943847656,
          19.478233337402344,
          43.275150299072266,
          38.45001220703125,
          28.562652587890625,
          22.756982803344727,
          43.58766555786133,
          33.65603256225586,
          33.58514404296875,
          30.62122344970703,
          27.592010498046875,
          30.65468978881836,
          42.65634536743164,
          34.914100646972656,
          46.72736740112305,
          19.59231948852539,
          30.119274139404297,
          36.510650634765625,
          33.89225769042969,
          31.41094970703125,
          34.33452224731445,
          31.08469581604004,
          38.15777587890625,
          37.730445861816406,
          44.969852447509766,
          22.111846923828125,
          19.930057525634766,
          34.55765151977539,
          33.724151611328125,
          36.913841247558594,
          43.969669342041016,
          49.45195770263672,
          31.984222412109375,
          23.280712127685547,
          33.18126678466797,
          32.35476303100586,
          27.020610809326172,
          32.767303466796875,
          43.8942756652832,
          39.239437103271484,
          48.01467514038086,
          34.20118713378906,
          45.32371520996094,
          31.571605682373047,
          15.957784652709961,
          33.34324264526367,
          40.379974365234375,
          41.99298858642578,
          25.28549575805664,
          43.80664825439453,
          52.78242111206055,
          39.808990478515625,
          24.77613639831543,
          31.620044708251953,
          49.39612579345703,
          47.282440185546875,
          24.279006958007812,
          45.12501525878906,
          45.527809143066406,
          49.018272399902344,
          41.36590576171875,
          30.622678756713867,
          45.23289489746094,
          22.48801612854004,
          22.14505958557129,
          35.16572952270508,
          31.09245491027832,
          42.22344207763672,
          34.492191314697266,
          46.1363525390625,
          45.07858657836914,
          36.40696716308594,
          30.985889434814453,
          24.299232482910156,
          29.010194778442383,
          43.29461669921875,
          39.931907653808594,
          32.85749053955078,
          19.892433166503906,
          31.50867462158203,
          42.59585189819336,
          37.656776428222656,
          29.70868492126465,
          16.389118194580078,
          49.98805618286133,
          35.119590759277344,
          45.1417350769043,
          30.211891174316406,
          29.488834381103516,
          45.183349609375,
          27.93723487854004,
          51.15354919433594,
          33.74355697631836,
          41.355899810791016,
          28.189271926879883,
          36.5991325378418,
          53.270957946777344,
          45.13267517089844,
          31.113224029541016,
          45.49869155883789,
          20.7038516998291,
          30.43890380859375,
          48.02220916748047,
          27.18120002746582,
          35.762794494628906,
          40.891319274902344,
          43.11372756958008,
          38.06916809082031,
          43.15550994873047,
          51.6113166809082,
          28.391799926757812,
          50.14461135864258,
          30.801633834838867,
          41.94343566894531,
          16.74671173095703,
          33.33769989013672,
          34.40562438964844,
          45.55811309814453,
          47.214962005615234,
          39.84189987182617,
          41.896324157714844,
          28.094247817993164,
          32.261009216308594,
          22.79388427734375,
          19.452369689941406,
          34.216331481933594,
          47.05429458618164,
          32.730777740478516,
          28.01527214050293,
          34.19474411010742,
          40.174259185791016,
          33.45824432373047,
          17.771583557128906,
          18.944067001342773,
          42.71590042114258,
          20.762393951416016,
          42.7060546875,
          39.19123458862305,
          30.997711181640625,
          41.88579177856445,
          28.943435668945312,
          34.886112213134766,
          31.600679397583008,
          32.7078971862793,
          42.21855163574219,
          39.120967864990234,
          41.014808654785156,
          33.080467224121094,
          48.90288162231445,
          30.750104904174805,
          38.72486877441406,
          44.1618537902832,
          43.16098403930664,
          40.286354064941406,
          24.77293586730957,
          27.034143447875977,
          33.6493034362793,
          50.8448371887207,
          25.33660888671875,
          34.72646713256836,
          46.459022521972656,
          44.00472640991211,
          29.491044998168945,
          36.18663787841797,
          56.85930633544922,
          27.23734474182129,
          32.41699981689453,
          26.071596145629883,
          26.414138793945312,
          31.0045166015625,
          37.57402801513672,
          38.7789306640625,
          54.710025787353516,
          24.00674819946289,
          34.573299407958984,
          24.093608856201172,
          37.98036193847656,
          43.49161911010742,
          21.239662170410156,
          29.425514221191406,
          39.90663528442383,
          36.26598358154297,
          29.350265502929688,
          32.760963439941406,
          24.676830291748047,
          34.210792541503906,
          51.299652099609375,
          51.196956634521484,
          34.01875305175781,
          40.369728088378906,
          24.703739166259766,
          44.01714324951172,
          18.914369583129883,
          31.17054557800293,
          38.59738540649414,
          33.23530578613281,
          39.8497200012207,
          35.683982849121094,
          31.21717643737793,
          40.244163513183594,
          53.320858001708984,
          41.24598693847656,
          30.955501556396484,
          48.14308166503906,
          44.666969299316406,
          31.28936767578125,
          37.441917419433594,
          21.81285285949707,
          46.47925567626953,
          52.199310302734375,
          37.10906219482422,
          41.760440826416016,
          22.103120803833008,
          30.51866340637207,
          49.526390075683594,
          39.720794677734375,
          39.40044021606445,
          33.40418243408203,
          41.98805236816406,
          36.212013244628906,
          34.7474365234375,
          29.088851928710938,
          42.550559997558594,
          42.21807861328125,
          24.95737075805664,
          26.678356170654297,
          31.09172821044922,
          39.40727996826172,
          32.66282653808594,
          21.707983016967773,
          42.96546936035156,
          49.868072509765625,
          42.28135681152344,
          32.35520935058594,
          31.61631965637207,
          25.562833786010742,
          27.310657501220703,
          51.16240692138672,
          42.68693923950195,
          27.120990753173828,
          38.26691818237305,
          43.924495697021484,
          38.36060333251953,
          45.28837585449219,
          52.97677993774414,
          35.066097259521484,
          47.99235534667969,
          38.404388427734375,
          28.050466537475586,
          26.480098724365234,
          39.8282470703125,
          33.96208572387695,
          40.423336029052734,
          32.82819366455078,
          33.133644104003906,
          36.76811599731445,
          29.960134506225586,
          42.219364166259766,
          55.15290069580078,
          42.938011169433594,
          36.448184967041016,
          42.80897521972656,
          20.713645935058594,
          20.47041893005371,
          27.977005004882812,
          18.704349517822266,
          38.84922409057617,
          23.885910034179688,
          37.63196563720703,
          33.913658142089844,
          38.0311164855957,
          22.803516387939453,
          27.855932235717773,
          31.441564559936523,
          23.6187686920166,
          42.89665222167969,
          44.20081329345703,
          53.625160217285156,
          48.68457794189453,
          33.9844970703125,
          52.34833908081055,
          29.389015197753906,
          41.66939163208008,
          36.3922233581543,
          48.589111328125,
          20.642749786376953,
          42.63734436035156,
          31.45602035522461,
          35.77336120605469,
          49.218074798583984,
          31.3778133392334,
          33.48912048339844,
          24.379398345947266,
          27.880138397216797,
          44.31894302368164,
          21.945234298706055,
          27.885271072387695,
          30.697458267211914,
          33.559078216552734,
          39.76108932495117,
          36.11934280395508,
          26.56909942626953,
          35.71245574951172,
          26.34745216369629,
          31.09433937072754,
          39.10331344604492,
          44.1232795715332,
          44.0257568359375,
          24.542022705078125,
          36.607200622558594,
          24.929155349731445,
          34.58718490600586,
          33.750648498535156,
          37.05470657348633,
          31.47470474243164,
          34.420143127441406,
          46.512596130371094,
          20.326011657714844,
          29.53790283203125,
          40.70456314086914,
          23.619529724121094,
          48.20161437988281,
          26.857925415039062,
          36.623130798339844,
          34.29020309448242,
          31.547466278076172,
          30.94121551513672,
          38.86981201171875,
          20.463010787963867,
          39.60728073120117,
          46.98457336425781,
          42.41729736328125,
          42.46501922607422,
          53.70452117919922,
          56.52758026123047,
          30.316062927246094,
          30.48467445373535,
          44.72651672363281,
          23.34222412109375,
          20.936748504638672,
          40.32113265991211,
          33.851016998291016,
          46.01039123535156,
          40.31570816040039,
          41.7530403137207,
          47.74361038208008,
          48.419002532958984,
          46.496673583984375,
          40.94441223144531,
          43.24276351928711,
          28.957441329956055,
          41.47795867919922,
          31.176244735717773,
          41.55888748168945,
          51.53995895385742,
          37.420162200927734,
          36.09386444091797,
          37.714176177978516,
          46.04517364501953,
          31.914169311523438,
          30.138565063476562,
          37.345008850097656,
          34.25833511352539,
          44.41727066040039,
          49.974674224853516,
          37.01414489746094,
          30.87115478515625,
          27.11636734008789,
          27.55042266845703,
          24.246477127075195,
          19.45541763305664,
          41.29624938964844,
          44.646183013916016,
          50.23835372924805,
          32.97940444946289,
          35.04751968383789,
          42.969703674316406,
          42.051170349121094,
          24.89580535888672,
          33.65348815917969,
          34.80559539794922,
          43.90343475341797,
          21.316267013549805,
          44.62541961669922,
          28.61111068725586,
          36.01350021362305,
          22.13833236694336,
          44.548492431640625,
          41.341224670410156,
          22.76427459716797,
          33.83543395996094,
          38.26751708984375,
          26.933753967285156,
          47.02928161621094,
          25.510147094726562,
          38.676734924316406,
          31.9180850982666,
          29.5736026763916,
          40.01055908203125,
          37.093448638916016,
          38.297706604003906,
          30.796152114868164,
          55.20870590209961,
          29.74111557006836,
          27.896652221679688,
          42.74920654296875,
          43.034912109375,
          30.46860122680664,
          26.95077133178711,
          34.46979522705078,
          46.440528869628906,
          24.485445022583008,
          36.69040298461914,
          31.612546920776367,
          19.972429275512695,
          37.82844924926758,
          26.25720977783203,
          40.89402770996094,
          22.77350616455078,
          41.94792938232422,
          48.38838195800781,
          38.207908630371094,
          41.497039794921875,
          24.701343536376953,
          40.2993278503418,
          24.17329978942871,
          46.48934555053711,
          31.490604400634766,
          46.409080505371094,
          21.486011505126953,
          35.40352249145508,
          46.666988372802734,
          39.279170989990234,
          18.606178283691406,
          23.583181381225586,
          25.158538818359375,
          47.550941467285156,
          37.320518493652344,
          23.970394134521484,
          37.16621017456055,
          28.840572357177734,
          36.720375061035156,
          33.49028778076172,
          22.15875816345215,
          30.433696746826172,
          46.6643180847168,
          35.85360336303711,
          43.97380065917969,
          35.66102981567383,
          41.243927001953125,
          39.70528030395508,
          40.868831634521484,
          46.5894889831543,
          46.5196418762207,
          18.052040100097656,
          40.569114685058594,
          31.73774528503418,
          22.213760375976562,
          39.50584411621094,
          31.293954849243164,
          26.444169998168945,
          40.722747802734375,
          29.54066276550293,
          24.89181137084961,
          37.279605865478516,
          35.80071258544922,
          43.71678924560547,
          36.37767791748047,
          25.416004180908203,
          27.779754638671875,
          21.552345275878906,
          26.64120101928711,
          25.983579635620117,
          46.651123046875,
          44.114994049072266,
          46.291175842285156,
          34.87496566772461,
          34.73223114013672,
          39.33738327026367,
          42.356651306152344,
          43.81676483154297,
          44.2584114074707,
          21.489662170410156,
          45.97020721435547,
          38.47510528564453,
          34.143531799316406,
          46.06108093261719,
          35.815155029296875,
          57.50050354003906,
          20.924434661865234,
          47.72861099243164,
          46.840782165527344,
          41.712642669677734,
          27.77434730529785,
          33.786983489990234,
          23.880859375,
          40.44022750854492,
          41.97433853149414,
          28.69298553466797,
          31.37722396850586,
          41.337379455566406,
          35.56673812866211,
          45.965576171875,
          36.88811492919922,
          28.935415267944336,
          23.014236450195312,
          27.278743743896484,
          27.910255432128906,
          24.60078239440918,
          36.375003814697266,
          37.672874450683594,
          33.10020065307617,
          32.57539749145508,
          19.379104614257812,
          48.57726287841797,
          22.37176513671875,
          35.01664352416992,
          33.41159439086914,
          44.313106536865234,
          26.65827178955078,
          30.044700622558594,
          44.54189682006836,
          21.50737762451172,
          23.938709259033203,
          42.81230163574219,
          44.03773498535156,
          29.39837646484375,
          25.675695419311523,
          35.75327682495117,
          37.226680755615234,
          21.48863983154297,
          46.72346496582031,
          51.81199264526367,
          22.666959762573242,
          22.621511459350586,
          19.115127563476562,
          41.28409957885742,
          28.962474822998047,
          26.587932586669922,
          37.136940002441406,
          29.099647521972656,
          38.06620788574219,
          39.62549591064453,
          35.55390930175781,
          33.384159088134766,
          45.20222473144531,
          36.90899658203125,
          25.743284225463867,
          50.36772918701172,
          37.83878707885742,
          25.097684860229492,
          45.87983322143555,
          31.110668182373047,
          20.822509765625,
          43.146148681640625,
          22.55450439453125,
          46.580474853515625,
          41.33000183105469,
          45.10321807861328,
          31.304988861083984,
          39.163230895996094,
          39.99203872680664,
          40.83352279663086,
          42.77149200439453,
          25.14144515991211,
          42.497501373291016,
          46.64356994628906,
          23.30201530456543,
          45.732852935791016,
          41.850799560546875,
          31.08845329284668,
          31.034666061401367,
          33.61125564575195,
          44.55282974243164,
          50.87578201293945,
          24.722383499145508,
          24.825950622558594,
          24.66350555419922,
          45.68582534790039,
          37.2476692199707,
          34.742958068847656,
          47.339237213134766,
          45.335697174072266,
          30.840763092041016,
          23.912342071533203,
          25.189319610595703,
          36.56114959716797,
          48.587127685546875,
          35.08022689819336,
          20.2330265045166,
          43.7884407043457,
          27.928964614868164,
          47.155357360839844,
          31.815799713134766,
          39.05080795288086,
          31.95746612548828,
          24.45118522644043,
          39.44483947753906,
          35.333309173583984,
          44.4697380065918,
          37.66122055053711,
          29.094083786010742
         ],
         "z": [
          22.193557739257812,
          48.29719543457031,
          30.625957489013672,
          52.103328704833984,
          38.50615310668945,
          28.001981735229492,
          27.84869956970215,
          29.220741271972656,
          58.96340560913086,
          44.45167541503906,
          51.468685150146484,
          42.54340744018555,
          57.490360260009766,
          47.26652526855469,
          54.05018997192383,
          49.29990005493164,
          19.254249572753906,
          57.53425979614258,
          49.165794372558594,
          50.4881706237793,
          52.584556579589844,
          57.532020568847656,
          52.157684326171875,
          42.83113479614258,
          47.00095748901367,
          45.30309295654297,
          59.81711196899414,
          41.48102569580078,
          30.18817901611328,
          56.5439453125,
          59.30122375488281,
          20.549379348754883,
          45.621864318847656,
          24.788070678710938,
          48.4952507019043,
          47.884822845458984,
          20.65286636352539,
          47.08831024169922,
          39.42476272583008,
          18.4217586517334,
          48.083221435546875,
          47.556053161621094,
          60.594486236572266,
          32.907779693603516,
          18.316003799438477,
          29.41912269592285,
          59.16590118408203,
          46.54472732543945,
          58.568504333496094,
          48.185035705566406,
          36.735355377197266,
          32.667755126953125,
          40.53523254394531,
          33.95199203491211,
          52.624813079833984,
          34.94943618774414,
          52.466182708740234,
          43.24457931518555,
          47.20848083496094,
          49.80440139770508,
          46.262630462646484,
          32.53449249267578,
          48.480308532714844,
          22.33917236328125,
          59.52994155883789,
          57.8682975769043,
          24.079315185546875,
          25.46428680419922,
          38.47138214111328,
          53.7990608215332,
          40.1561393737793,
          31.34992790222168,
          49.5574951171875,
          38.275917053222656,
          56.760467529296875,
          58.2047119140625,
          59.313411712646484,
          58.65693664550781,
          49.76531219482422,
          51.101234436035156,
          56.098175048828125,
          58.35624694824219,
          32.77638626098633,
          20.011463165283203,
          42.44136047363281,
          50.721527099609375,
          57.64860534667969,
          37.238502502441406,
          47.843589782714844,
          54.00732421875,
          37.49699783325195,
          54.081756591796875,
          52.862239837646484,
          59.41511917114258,
          36.502994537353516,
          50.93190002441406,
          34.34970474243164,
          58.61671447753906,
          57.94004821777344,
          47.880523681640625,
          43.49683380126953,
          45.079654693603516,
          48.27446365356445,
          55.199241638183594,
          39.23337173461914,
          51.18063735961914,
          35.15815734863281,
          54.429988861083984,
          51.23176193237305,
          47.610111236572266,
          57.36034393310547,
          56.23877716064453,
          31.751041412353516,
          49.7415771484375,
          55.117210388183594,
          24.319005966186523,
          57.46921920776367,
          54.253387451171875,
          42.875160217285156,
          59.047977447509766,
          46.119117736816406,
          48.49564743041992,
          42.98763656616211,
          21.783266067504883,
          42.131141662597656,
          27.721223831176758,
          21.08197021484375,
          43.725730895996094,
          33.30112075805664,
          33.13115692138672,
          28.098690032958984,
          59.34099197387695,
          19.190397262573242,
          59.09112548828125,
          21.290863037109375,
          33.61766052246094,
          42.817230224609375,
          36.34144592285156,
          49.56328201293945,
          48.46647644042969,
          53.972774505615234,
          61.457977294921875,
          18.94736671447754,
          44.631614685058594,
          39.73225021362305,
          56.69929885864258,
          47.14879608154297,
          50.841880798339844,
          30.019149780273438,
          42.35295104980469,
          52.667598724365234,
          29.839557647705078,
          56.1619758605957,
          49.929386138916016,
          47.74675369262695,
          48.1157341003418,
          50.47563552856445,
          31.519533157348633,
          45.489566802978516,
          45.03913879394531,
          24.699214935302734,
          59.26620101928711,
          49.852535247802734,
          61.00217819213867,
          53.07612609863281,
          55.32948684692383,
          51.04473876953125,
          49.411521911621094,
          52.6759147644043,
          44.3145751953125,
          32.239376068115234,
          57.31119918823242,
          38.092018127441406,
          23.688243865966797,
          51.76841354370117,
          41.685123443603516,
          48.17512512207031,
          58.06892395019531,
          46.101715087890625,
          24.113811492919922,
          27.705135345458984,
          47.848052978515625,
          55.764644622802734,
          23.6416015625,
          25.723140716552734,
          60.560977935791016,
          31.04852867126465,
          51.631832122802734,
          43.92878723144531,
          40.5086555480957,
          51.73210525512695,
          49.495758056640625,
          27.55916404724121,
          41.31407928466797,
          32.036216735839844,
          53.47500991821289,
          45.18617248535156,
          45.44444274902344,
          39.93655014038086,
          56.01960754394531,
          50.02223587036133,
          28.789203643798828,
          26.000228881835938,
          20.066648483276367,
          57.251468658447266,
          57.87909698486328,
          44.189937591552734,
          62.99058532714844,
          53.102256774902344,
          43.19142150878906,
          59.319122314453125,
          31.262310028076172,
          36.46009826660156,
          54.34529113769531,
          44.066200256347656,
          41.53694534301758,
          21.762239456176758,
          45.23097229003906,
          52.16254806518555,
          58.98711013793945,
          55.1859016418457,
          33.99330139160156,
          51.322444915771484,
          36.24159622192383,
          59.1140022277832,
          48.460758209228516,
          51.20326614379883,
          37.330265045166016,
          44.2296257019043,
          28.657915115356445,
          32.30308151245117,
          59.03960418701172,
          41.41686248779297,
          59.601165771484375,
          40.729515075683594,
          41.29212951660156,
          59.68666076660156,
          28.145036697387695,
          56.08163833618164,
          45.14868927001953,
          52.989173889160156,
          42.879215240478516,
          35.12479019165039,
          35.08455276489258,
          24.071277618408203,
          30.338970184326172,
          25.646018981933594,
          59.33249282836914,
          48.83964920043945,
          56.35067367553711,
          50.56779479980469,
          54.49729919433594,
          35.65947723388672,
          55.34709930419922,
          54.103790283203125,
          29.827739715576172,
          37.917057037353516,
          50.154510498046875,
          58.60918045043945,
          25.191287994384766,
          54.13370895385742,
          47.155677795410156,
          25.61989402770996,
          52.48930740356445,
          32.77423095703125,
          49.3744010925293,
          25.189315795898438,
          57.91245651245117,
          32.24742126464844,
          48.732810974121094,
          52.30168533325195,
          46.13508224487305,
          30.273740768432617,
          56.93735885620117,
          47.68544387817383,
          46.32638931274414,
          58.1732292175293,
          49.6789665222168,
          61.170806884765625,
          49.34938430786133,
          60.549766540527344,
          42.99208068847656,
          53.63998794555664,
          46.532745361328125,
          39.86209487915039,
          64.3516845703125,
          46.96322250366211,
          29.999391555786133,
          51.63439178466797,
          32.50130081176758,
          22.756826400756836,
          36.14794921875,
          49.35200881958008,
          58.59805679321289,
          40.56991958618164,
          50.91039276123047,
          49.040714263916016,
          53.45930480957031,
          41.916046142578125,
          61.85067367553711,
          45.53545379638672,
          46.8137321472168,
          39.18753433227539,
          33.17449188232422,
          45.933441162109375,
          17.593097686767578,
          35.01071548461914,
          40.86245346069336,
          60.07341003417969,
          51.210784912109375,
          63.03651809692383,
          51.45808029174805,
          58.89654541015625,
          44.90623092651367,
          60.51915740966797,
          45.21754837036133,
          57.616844177246094,
          26.67453956604004,
          56.6206169128418,
          41.678592681884766,
          48.31040573120117,
          47.39183807373047,
          48.959197998046875,
          59.25123977661133,
          38.48843002319336,
          43.1740837097168,
          30.47085952758789,
          41.91838073730469,
          43.59067153930664,
          42.098838806152344,
          47.956539154052734,
          48.201324462890625,
          57.98101043701172,
          55.490142822265625,
          37.21451187133789,
          37.39351272583008,
          49.03601837158203,
          58.88178634643555,
          48.4722900390625,
          55.70391082763672,
          50.14988708496094,
          48.24186325073242,
          58.97999572753906,
          50.80073928833008,
          58.94840621948242,
          50.81010055541992,
          28.648542404174805,
          28.605117797851562,
          41.977596282958984,
          49.67588806152344,
          20.9411563873291,
          53.70164108276367,
          59.530494689941406,
          60.01853561401367,
          41.523006439208984,
          41.5800666809082,
          35.17058563232422,
          39.74538040161133,
          46.620811462402344,
          25.527912139892578,
          28.612123489379883,
          54.01695251464844,
          40.920780181884766,
          48.552764892578125,
          48.48415756225586,
          50.006683349609375,
          19.13212776184082,
          47.93800354003906,
          58.69995880126953,
          48.21934509277344,
          57.27815628051758,
          54.735084533691406,
          48.85979461669922,
          58.12385559082031,
          48.238590240478516,
          44.908382415771484,
          54.706451416015625,
          36.83709716796875,
          28.122060775756836,
          53.69363784790039,
          19.34170913696289,
          53.79423141479492,
          34.58008575439453,
          46.18111038208008,
          19.84146499633789,
          28.100799560546875,
          43.112674713134766,
          59.71636199951172,
          19.836034774780273,
          37.738914489746094,
          59.37274169921875,
          42.744834899902344,
          43.759910583496094,
          40.701229095458984,
          61.219810485839844,
          59.77193069458008,
          27.607295989990234,
          28.082170486450195,
          48.33074951171875,
          22.518951416015625,
          47.70029830932617,
          57.37044143676758,
          29.83972930908203,
          44.088104248046875,
          26.64124298095703,
          49.59599304199219,
          55.904273986816406,
          42.28621292114258,
          31.14148712158203,
          42.64944839477539,
          45.33501052856445,
          60.29544448852539,
          50.171634674072266,
          52.60420227050781,
          42.69701385498047,
          46.33070373535156,
          58.507286071777344,
          60.633209228515625,
          52.716529846191406,
          57.97148513793945,
          41.765933990478516,
          48.45681381225586,
          23.54427719116211,
          58.526615142822266,
          38.90258026123047,
          41.35301208496094,
          41.657100677490234,
          43.58125305175781,
          53.043922424316406,
          47.87181854248047,
          45.280357360839844,
          46.953311920166016,
          57.63371276855469,
          43.78396987915039,
          49.03239440917969,
          45.66324234008789,
          31.457304000854492,
          59.728755950927734,
          46.558319091796875,
          55.776084899902344,
          49.386592864990234,
          47.914337158203125,
          50.65068817138672,
          53.670204162597656,
          42.287967681884766,
          39.077239990234375,
          44.26091003417969,
          51.528175354003906,
          54.450504302978516,
          26.174299240112305,
          42.89739990234375,
          41.30209732055664,
          57.60813522338867,
          51.5541877746582,
          32.67342758178711,
          43.58646011352539,
          26.83953857421875,
          21.14995002746582,
          21.000350952148438,
          55.8044548034668,
          45.242862701416016,
          27.147357940673828,
          33.53456497192383,
          49.86683654785156,
          50.43528747558594,
          45.98847579956055,
          45.367713928222656,
          31.14635467529297,
          39.58058166503906,
          42.62396240234375,
          47.27132797241211,
          33.316192626953125,
          36.16640853881836,
          46.27303695678711,
          55.25719451904297,
          55.44285583496094,
          19.70174789428711,
          48.423519134521484,
          56.62761688232422,
          58.036624908447266,
          47.31916046142578,
          44.824180603027344,
          25.43572235107422,
          31.999042510986328,
          42.69581985473633,
          27.05593490600586,
          52.29572677612305,
          55.94910430908203,
          46.01551818847656,
          44.6189079284668,
          54.45970153808594,
          52.1705207824707,
          28.383438110351562,
          58.711952209472656,
          19.391584396362305,
          40.14068603515625,
          51.546390533447266,
          45.01201248168945,
          58.9002685546875,
          47.899940490722656,
          32.62913131713867,
          49.04000473022461,
          57.91682434082031,
          43.37637710571289,
          18.646583557128906,
          48.66438674926758,
          50.226375579833984,
          46.40803527832031,
          52.17924118041992,
          40.2056770324707,
          24.846826553344727,
          44.49470520019531,
          50.19939041137695,
          49.4491081237793,
          29.931556701660156,
          44.76060104370117,
          59.142818450927734,
          40.645751953125,
          23.24624252319336,
          43.81684494018555,
          58.768714904785156,
          23.247560501098633,
          35.05949401855469,
          53.40648651123047,
          18.563583374023438,
          39.550636291503906,
          41.97401428222656,
          58.14215850830078,
          38.90829849243164,
          27.936580657958984,
          32.73231887817383,
          51.492942810058594,
          50.75414276123047,
          46.49946594238281,
          26.667373657226562,
          48.66427230834961,
          49.3213996887207,
          43.742759704589844,
          26.552875518798828,
          57.30879211425781,
          30.544620513916016,
          57.523590087890625,
          49.61832809448242,
          20.226701736450195,
          43.471580505371094,
          48.73737335205078,
          53.967918395996094,
          43.57455062866211,
          59.52709197998047,
          33.040321350097656,
          56.878387451171875,
          26.18425750732422,
          41.64270782470703,
          43.64838409423828,
          49.61424255371094,
          47.678916931152344,
          46.236637115478516,
          27.262109756469727,
          55.30523681640625,
          50.3726692199707,
          48.42056655883789,
          30.062305450439453,
          44.3022575378418,
          49.11747360229492,
          54.507022857666016,
          39.19841766357422,
          44.87690734863281,
          33.417293548583984,
          48.74423599243164,
          49.49099349975586,
          57.87743377685547,
          47.562469482421875,
          53.56418991088867,
          36.325687408447266,
          22.18682098388672,
          41.09339141845703,
          46.191795349121094,
          42.708824157714844,
          41.899078369140625,
          45.82938766479492,
          57.14760971069336,
          29.47769546508789,
          58.68000411987305,
          52.8796272277832,
          55.35072708129883,
          48.138267517089844,
          40.819122314453125,
          34.43967819213867,
          52.377906799316406,
          21.3181095123291,
          24.445894241333008,
          48.021392822265625,
          59.70037841796875,
          25.97493553161621,
          40.17702102661133,
          48.93891525268555,
          56.290740966796875,
          27.84210968017578,
          28.578075408935547,
          44.44764709472656,
          57.33849334716797,
          46.62805938720703,
          36.59596633911133,
          41.7935791015625,
          40.631103515625,
          45.705406188964844,
          58.770843505859375,
          20.355695724487305,
          48.93181610107422,
          58.478519439697266,
          48.134422302246094,
          34.36695861816406,
          21.846099853515625,
          51.055824279785156,
          46.62751770019531,
          57.950374603271484,
          42.55789566040039,
          42.43770217895508,
          53.17061996459961,
          39.31789779663086,
          58.03092575073242,
          54.57706069946289,
          23.002601623535156,
          62.95686340332031,
          32.6954345703125,
          50.076473236083984,
          44.83853530883789,
          41.23314666748047,
          46.71403503417969,
          53.944061279296875,
          50.03533172607422,
          31.481285095214844,
          21.609386444091797,
          39.96580123901367,
          40.19727325439453,
          65.04598236083984,
          23.115966796875,
          46.1931037902832,
          35.25409698486328,
          36.5867805480957,
          51.452880859375,
          52.54319381713867,
          46.536376953125,
          20.61266326904297,
          49.72128677368164,
          35.749454498291016,
          48.62664031982422,
          49.6783447265625,
          23.21011734008789,
          58.06127166748047,
          33.36042022705078,
          44.329864501953125,
          26.73306655883789,
          32.475460052490234,
          57.87436294555664,
          28.309978485107422,
          19.34662437438965,
          51.187110900878906,
          41.58477783203125,
          28.655044555664062,
          47.894691467285156,
          58.98509216308594,
          43.9849739074707,
          50.13564682006836,
          22.861215591430664,
          47.37464904785156,
          32.64488983154297,
          42.5754508972168,
          55.929901123046875,
          49.71677017211914,
          34.438514709472656,
          37.128013610839844,
          44.99360275268555,
          60.23851013183594,
          43.22860336303711,
          49.0293083190918,
          47.13213348388672,
          42.655033111572266,
          36.35331344604492,
          43.46053695678711,
          44.7803955078125,
          57.57671356201172,
          44.963096618652344,
          56.666358947753906,
          54.96890640258789,
          40.03911590576172,
          31.40220832824707,
          53.46612548828125,
          60.976600646972656,
          52.66375732421875,
          60.56418991088867,
          39.453739166259766,
          55.77119064331055,
          45.42621994018555,
          27.985782623291016,
          22.976333618164062,
          48.27013397216797,
          24.493104934692383,
          33.39896011352539
         ]
        },
        {
         "marker": {
          "color": [
           9.487540245056152,
           7.4956278800964355,
           5.985408306121826,
           10.798107147216797,
           9.740337371826172,
           11.603001594543457,
           8.08665657043457,
           9.955036163330078,
           6.629784107208252,
           6.187309741973877,
           10.807198524475098,
           8.99752140045166,
           15.959739685058594,
           14.669604301452637,
           11.756396293640137,
           5.264652252197266,
           10.345693588256836,
           9.971203804016113,
           12.336644172668457,
           9.701464653015137,
           6.950520992279053,
           10.920626640319824,
           8.881928443908691,
           18.315589904785156,
           10.452274322509766,
           11.321113586425781,
           3.5503063201904297,
           8.510673522949219,
           5.896841049194336,
           9.501510620117188,
           9.769697189331055,
           6.151399612426758,
           9.085453033447266,
           10.175183296203613,
           15.052820205688477,
           8.596508979797363,
           14.236739158630371,
           6.612325191497803,
           7.737733364105225,
           3.7501437664031982,
           15.36936092376709,
           7.017298221588135,
           14.141318321228027,
           9.262652397155762,
           10.518348693847656,
           6.119032859802246,
           9.664701461791992,
           2.495150566101074,
           11.825839042663574,
           8.028797149658203,
           10.16731071472168,
           6.977254867553711,
           6.260786056518555,
           13.81306266784668,
           16.609708786010742,
           3.0888559818267822,
           10.97011661529541,
           10.41623592376709,
           14.0460844039917,
           14.870316505432129,
           9.984872817993164,
           12.095102310180664,
           10.377259254455566,
           14.206626892089844,
           12.341596603393555,
           17.191804885864258,
           9.094109535217285,
           14.774089813232422,
           10.597574234008789,
           15.973675727844238,
           10.121177673339844,
           9.782426834106445,
           11.874751091003418,
           15.923713684082031,
           12.46735954284668,
           6.994318008422852,
           4.494420051574707,
           13.986278533935547,
           13.92394733428955,
           6.057530403137207,
           10.410993576049805,
           10.836237907409668,
           9.740424156188965,
           14.640896797180176,
           3.3410542011260986,
           10.68997859954834,
           10.803397178649902,
           23.456645965576172,
           9.534161567687988,
           6.352453231811523,
           13.225178718566895,
           17.386436462402344,
           13.33469295501709,
           10.398552894592285,
           18.93483543395996,
           10.113685607910156,
           11.617433547973633,
           2.520451068878174,
           7.423653602600098,
           15.775489807128906,
           21.46368408203125,
           9.645406723022461,
           9.07184886932373,
           17.417734146118164,
           5.7899909019470215,
           5.189192295074463,
           11.897953033447266,
           10.754142761230469,
           9.218195915222168,
           9.674327850341797,
           6.527127265930176,
           12.408970832824707,
           8.905892372131348,
           7.183071613311768,
           5.223270416259766,
           5.486182689666748,
           7.468102931976318,
           12.343761444091797,
           12.239322662353516,
           7.787397384643555,
           12.66048812866211,
           5.390719890594482,
           18.701282501220703,
           7.230193138122559,
           9.484199523925781,
           8.87949275970459,
           7.388397693634033,
           18.100675582885742,
           8.536345481872559,
           12.189102172851562,
           9.853286743164062,
           15.848200798034668,
           2.544891595840454,
           7.795784950256348,
           7.816500186920166,
           10.787849426269531,
           6.927855014801025,
           12.14461612701416,
           10.473138809204102,
           7.612407684326172,
           12.189887046813965,
           10.371219635009766,
           7.89022970199585,
           16.060590744018555,
           10.285603523254395,
           4.491062164306641,
           10.424837112426758,
           12.847496032714844,
           10.776094436645508,
           14.501355171203613,
           17.534523010253906,
           9.33459186553955,
           14.199939727783203,
           6.208715915679932,
           18.442981719970703,
           13.365468978881836,
           15.871304512023926,
           11.037138938903809,
           4.49680233001709,
           10.730064392089844,
           10.204672813415527,
           17.151105880737305,
           8.706282615661621,
           12.413788795471191,
           9.198430061340332,
           8.886029243469238,
           10.400947570800781,
           7.563658714294434,
           22.308786392211914,
           11.745682716369629,
           7.059898853302002,
           14.336071014404297,
           11.924989700317383,
           6.083772659301758,
           12.429152488708496,
           10.296801567077637,
           12.008597373962402,
           10.423361778259277,
           15.70008373260498,
           18.586246490478516,
           17.079345703125,
           8.994156837463379,
           11.312920570373535,
           9.591304779052734,
           10.731890678405762,
           9.584741592407227,
           11.020166397094727,
           13.014762878417969,
           8.981672286987305,
           15.40211009979248,
           12.464092254638672,
           5.372025489807129,
           11.953597068786621,
           10.66005802154541,
           12.760744094848633,
           7.611435890197754,
           4.73379373550415,
           15.253458976745605,
           13.808148384094238,
           20.46039390563965,
           20.023574829101562,
           10.974897384643555,
           14.195087432861328,
           6.918832302093506,
           10.564586639404297,
           13.019279479980469,
           11.563467025756836,
           8.514467239379883,
           12.182350158691406,
           8.856758117675781,
           6.298455238342285,
           16.167964935302734,
           4.156801223754883,
           8.440314292907715,
           6.838658809661865,
           10.772255897521973,
           9.653817176818848,
           12.581450462341309,
           10.68476390838623,
           15.592416763305664,
           19.945030212402344,
           15.18327808380127,
           5.717874050140381,
           9.10400104522705,
           13.082653045654297,
           10.901005744934082,
           11.371492385864258,
           13.893566131591797,
           13.114310264587402,
           7.280970096588135,
           12.935452461242676,
           16.507692337036133,
           10.162890434265137,
           18.992990493774414,
           7.9781646728515625,
           16.973867416381836,
           11.94960880279541,
           11.827956199645996,
           4.235004425048828,
           13.222310066223145,
           8.415549278259277,
           12.383869171142578,
           8.40530014038086,
           19.59270477294922,
           8.806013107299805,
           17.797061920166016,
           11.50992202758789,
           18.191959381103516,
           10.864718437194824,
           8.27118968963623,
           12.781637191772461,
           13.115983009338379,
           19.10380744934082,
           8.28538703918457,
           9.508852005004883,
           15.771617889404297,
           10.845008850097656,
           14.255663871765137,
           12.295069694519043,
           10.245463371276855,
           6.847806453704834,
           12.116581916809082,
           9.28061580657959,
           5.627632141113281,
           9.868147850036621,
           11.677608489990234,
           15.291605949401855,
           7.647700786590576,
           14.699256896972656,
           7.497106552124023,
           20.173139572143555,
           10.956258773803711,
           11.788272857666016,
           14.796850204467773,
           8.079368591308594,
           12.894466400146484,
           13.6444091796875,
           10.817596435546875,
           9.24698257446289,
           12.379745483398438,
           16.688461303710938,
           6.331615447998047,
           11.992661476135254,
           8.837380409240723,
           4.351584434509277,
           10.158668518066406,
           6.275455474853516,
           14.08239459991455,
           9.723267555236816,
           6.965937614440918,
           9.569615364074707,
           8.148799896240234,
           11.97314167022705,
           6.444392204284668,
           9.280654907226562,
           18.251056671142578,
           11.22021770477295,
           7.0468621253967285,
           9.090533256530762,
           3.314542293548584,
           10.964696884155273,
           7.475830078125,
           10.413518905639648,
           15.791460037231445,
           5.15372896194458,
           6.298299312591553,
           7.5570759773254395,
           12.626262664794922,
           8.131697654724121,
           8.259493827819824,
           18.13065528869629,
           12.176941871643066,
           15.794623374938965,
           16.296478271484375,
           10.385115623474121,
           11.832561492919922,
           2.3057522773742676,
           14.342572212219238,
           11.255102157592773,
           12.0818452835083,
           12.18840503692627,
           12.893043518066406,
           13.037850379943848
          ],
          "size": 12
         },
         "mode": "markers",
         "name": "Predicción con Diferencia > 0.1",
         "scene": "scene3",
         "type": "scatter3d",
         "x": [
          47.740325927734375,
          18.728727340698242,
          27.0087833404541,
          25.540603637695312,
          54.11506652832031,
          27.107086181640625,
          20.815723419189453,
          31.16639518737793,
          57.9171028137207,
          30.7300968170166,
          37.025962829589844,
          49.62834167480469,
          34.77933120727539,
          27.4887638092041,
          40.18029022216797,
          22.283634185791016,
          49.07403564453125,
          45.27532196044922,
          49.6722412109375,
          45.46527099609375,
          28.193115234375,
          45.626853942871094,
          50.16949462890625,
          40.089847564697266,
          46.60951614379883,
          53.88240432739258,
          34.00315856933594,
          28.682937622070312,
          27.486900329589844,
          32.12623596191406,
          31.80544090270996,
          35.37228775024414,
          54.72309875488281,
          32.24350357055664,
          44.61298751831055,
          34.95375061035156,
          44.433677673339844,
          56.86893844604492,
          18.69550132751465,
          30.03633689880371,
          28.474557876586914,
          53.03355407714844,
          41.2369499206543,
          44.47755813598633,
          24.82193946838379,
          16.86994171142578,
          23.555696487426758,
          34.16782760620117,
          26.655271530151367,
          44.79511642456055,
          44.490604400634766,
          21.131851196289062,
          39.11564636230469,
          45.84498596191406,
          36.58039093017578,
          36.238826751708984,
          43.7359619140625,
          46.91765213012695,
          43.374202728271484,
          41.806358337402344,
          48.176551818847656,
          51.60184860229492,
          44.7943229675293,
          49.226619720458984,
          32.74905014038086,
          38.74393844604492,
          35.458255767822266,
          36.175052642822266,
          22.89742088317871,
          25.42003059387207,
          22.24483299255371,
          56.112667083740234,
          39.756229400634766,
          24.215559005737305,
          26.695171356201172,
          18.50237274169922,
          35.00148010253906,
          46.46189498901367,
          33.77392578125,
          19.180862426757812,
          24.238304138183594,
          43.31188201904297,
          32.984432220458984,
          48.214698791503906,
          26.849319458007812,
          30.40203857421875,
          55.71480178833008,
          45.37932586669922,
          48.2073860168457,
          53.50428009033203,
          25.960933685302734,
          35.61399841308594,
          30.79729652404785,
          28.38654327392578,
          26.896360397338867,
          21.89237403869629,
          31.51664924621582,
          29.23475456237793,
          20.72218894958496,
          28.563791275024414,
          45.48086166381836,
          49.04357147216797,
          36.44078826904297,
          39.09156036376953,
          63.43259048461914,
          32.47526550292969,
          29.76934051513672,
          25.216516494750977,
          36.848350524902344,
          42.14496994018555,
          20.368453979492188,
          38.06202697753906,
          30.702119827270508,
          47.28012466430664,
          26.296472549438477,
          47.828887939453125,
          22.402851104736328,
          29.447500228881836,
          47.24446487426758,
          42.80643844604492,
          49.985931396484375,
          47.24203109741211,
          33.01605224609375,
          33.52467727661133,
          32.558441162109375,
          48.84056854248047,
          43.515602111816406,
          37.68642807006836,
          40.5928955078125,
          47.3455696105957,
          41.53923416137695,
          45.70463180541992,
          57.689598083496094,
          28.35053062438965,
          45.04664611816406,
          29.66257095336914,
          35.153751373291016,
          40.37224197387695,
          53.57362365722656,
          48.37264633178711,
          54.61448669433594,
          53.55906295776367,
          34.66938781738281,
          37.24302673339844,
          27.855409622192383,
          28.009021759033203,
          41.43181610107422,
          36.22899627685547,
          29.092267990112305,
          40.2813606262207,
          37.948997497558594,
          28.622314453125,
          38.20424270629883,
          41.5794792175293,
          40.95138168334961,
          53.233314514160156,
          48.075050354003906,
          27.993040084838867,
          27.54731559753418,
          38.3494987487793,
          27.876754760742188,
          39.224220275878906,
          51.625343322753906,
          25.6801700592041,
          41.41013717651367,
          29.205785751342773,
          32.36588668823242,
          43.89838409423828,
          37.16976547241211,
          46.658077239990234,
          33.381290435791016,
          24.622203826904297,
          43.26765823364258,
          57.989105224609375,
          33.89063262939453,
          45.112300872802734,
          34.038047790527344,
          23.69274139404297,
          30.8775577545166,
          28.16046142578125,
          43.728416442871094,
          23.81365966796875,
          47.42145919799805,
          54.21629333496094,
          45.351165771484375,
          55.05631637573242,
          46.95263671875,
          41.60851287841797,
          38.36555480957031,
          50.647708892822266,
          36.82939910888672,
          44.698463439941406,
          49.24781036376953,
          26.299352645874023,
          47.0825309753418,
          45.95012283325195,
          28.772212982177734,
          27.487245559692383,
          28.410911560058594,
          37.43943405151367,
          35.65608215332031,
          27.954933166503906,
          49.34975051879883,
          14.954614639282227,
          51.969722747802734,
          28.10856819152832,
          23.552772521972656,
          46.80558395385742,
          26.55560874938965,
          22.584354400634766,
          44.88683319091797,
          38.468685150146484,
          57.91559600830078,
          49.161827087402344,
          45.71475601196289,
          25.9285888671875,
          32.88431167602539,
          43.92633819580078,
          26.873809814453125,
          36.67380905151367,
          43.25318145751953,
          53.025272369384766,
          33.28086471557617,
          22.7135009765625,
          36.0793571472168,
          24.46908187866211,
          38.20777130126953,
          41.2081298828125,
          29.820159912109375,
          42.33049774169922,
          39.046226501464844,
          42.384620666503906,
          24.49475860595703,
          44.823368072509766,
          41.15149688720703,
          25.088043212890625,
          44.936920166015625,
          34.12022399902344,
          57.252803802490234,
          34.46392822265625,
          25.786144256591797,
          26.34835433959961,
          42.95349884033203,
          38.80685806274414,
          45.11769485473633,
          35.77680206298828,
          41.875770568847656,
          26.0377254486084,
          33.05081558227539,
          37.49281692504883,
          24.52794075012207,
          45.72832107543945,
          45.09111404418945,
          21.475004196166992,
          43.46282196044922,
          37.14133834838867,
          24.0927734375,
          35.537357330322266,
          44.53797912597656,
          25.903274536132812,
          17.361225128173828,
          50.154083251953125,
          55.241065979003906,
          33.7199821472168,
          48.34309005737305,
          34.51805114746094,
          36.50599670410156,
          43.44526290893555,
          38.85164260864258,
          54.187679290771484,
          26.718740463256836,
          48.16253662109375,
          46.51503372192383,
          45.79171371459961,
          34.32567596435547,
          43.42913055419922,
          26.40639877319336,
          39.138519287109375,
          51.39132308959961,
          47.0206298828125,
          41.05066680908203,
          55.77700424194336,
          28.301904678344727,
          21.936786651611328,
          26.875320434570312,
          38.49971389770508,
          28.55396270751953,
          53.15205764770508,
          53.561100006103516,
          28.797523498535156,
          47.74525451660156,
          34.485660552978516,
          35.72408676147461,
          18.247764587402344,
          50.10151290893555,
          37.863800048828125,
          40.7304573059082,
          28.813447952270508,
          25.143888473510742,
          50.870277404785156,
          27.612443923950195,
          49.69944763183594,
          44.142799377441406,
          38.88480758666992,
          62.281917572021484,
          61.94050979614258,
          22.794851303100586,
          36.3260498046875,
          34.47850799560547,
          48.45222091674805,
          41.86713409423828,
          54.90816879272461,
          34.32304000854492,
          36.74223709106445,
          41.165626525878906,
          26.723976135253906,
          52.07255554199219,
          40.44697570800781,
          49.892398834228516,
          27.83852767944336,
          36.434242248535156,
          23.977069854736328,
          40.984004974365234
         ],
         "y": [
          34.40317916870117,
          43.19955825805664,
          38.522212982177734,
          42.70173645019531,
          45.822181701660156,
          36.497825622558594,
          42.76839828491211,
          38.981773376464844,
          36.60196304321289,
          37.514617919921875,
          42.724464416503906,
          36.78515625,
          41.14496612548828,
          45.156959533691406,
          34.819236755371094,
          33.60496520996094,
          33.774993896484375,
          35.31196975708008,
          40.07390213012695,
          38.37556838989258,
          45.75086212158203,
          42.37193298339844,
          35.80327224731445,
          49.862274169921875,
          41.75259780883789,
          53.72273254394531,
          32.7238655090332,
          41.15216827392578,
          50.70451354980469,
          29.602787017822266,
          30.480518341064453,
          34.64518737792969,
          50.976993560791016,
          41.728981018066406,
          56.72565841674805,
          38.470985412597656,
          37.60789489746094,
          51.87396240234375,
          45.3259162902832,
          35.728599548339844,
          52.92416000366211,
          34.330589294433594,
          42.74654006958008,
          38.75526428222656,
          43.56629943847656,
          43.275150299072266,
          43.58766555786133,
          30.62122344970703,
          46.72736740112305,
          33.89225769042969,
          38.15777587890625,
          44.969852447509766,
          33.724151611328125,
          36.913841247558594,
          49.45195770263672,
          31.984222412109375,
          32.35476303100586,
          32.767303466796875,
          43.8942756652832,
          39.239437103271484,
          48.01467514038086,
          45.32371520996094,
          33.34324264526367,
          40.379974365234375,
          43.80664825439453,
          52.78242111206055,
          39.808990478515625,
          49.39612579345703,
          47.282440185546875,
          45.12501525878906,
          45.527809143066406,
          49.018272399902344,
          41.36590576171875,
          45.23289489746094,
          46.1363525390625,
          36.40696716308594,
          30.985889434814453,
          43.29461669921875,
          39.931907653808594,
          32.85749053955078,
          42.59585189819336,
          37.656776428222656,
          29.70868492126465,
          49.98805618286133,
          45.1417350769043,
          45.183349609375,
          51.15354919433594,
          53.270957946777344,
          45.13267517089844,
          31.113224029541016,
          45.49869155883789,
          48.02220916748047,
          43.11372756958008,
          38.06916809082031,
          51.6113166809082,
          50.14461135864258,
          41.94343566894531,
          33.33769989013672,
          34.40562438964844,
          45.55811309814453,
          47.214962005615234,
          39.84189987182617,
          34.216331481933594,
          47.05429458618164,
          34.19474411010742,
          33.45824432373047,
          42.71590042114258,
          42.7060546875,
          39.19123458862305,
          41.88579177856445,
          32.7078971862793,
          39.120967864990234,
          41.014808654785156,
          33.080467224121094,
          48.90288162231445,
          30.750104904174805,
          44.1618537902832,
          43.16098403930664,
          50.8448371887207,
          34.72646713256836,
          46.459022521972656,
          29.491044998168945,
          56.85930633544922,
          27.23734474182129,
          31.0045166015625,
          37.57402801513672,
          38.7789306640625,
          54.710025787353516,
          34.573299407958984,
          39.90663528442383,
          36.26598358154297,
          51.299652099609375,
          51.196956634521484,
          31.17054557800293,
          38.59738540649414,
          39.8497200012207,
          35.683982849121094,
          40.244163513183594,
          53.320858001708984,
          30.955501556396484,
          48.14308166503906,
          44.666969299316406,
          37.441917419433594,
          46.47925567626953,
          52.199310302734375,
          37.10906219482422,
          41.760440826416016,
          49.526390075683594,
          39.720794677734375,
          39.40044021606445,
          42.550559997558594,
          42.21807861328125,
          39.40727996826172,
          32.66282653808594,
          42.96546936035156,
          49.868072509765625,
          42.28135681152344,
          51.16240692138672,
          42.68693923950195,
          38.26691818237305,
          38.36060333251953,
          52.97677993774414,
          35.066097259521484,
          47.99235534667969,
          39.8282470703125,
          40.423336029052734,
          32.82819366455078,
          29.960134506225586,
          55.15290069580078,
          42.938011169433594,
          36.448184967041016,
          42.80897521972656,
          38.84922409057617,
          37.63196563720703,
          33.913658142089844,
          38.0311164855957,
          42.89665222167969,
          44.20081329345703,
          53.625160217285156,
          48.68457794189453,
          52.34833908081055,
          41.66939163208008,
          36.3922233581543,
          48.589111328125,
          31.45602035522461,
          49.218074798583984,
          33.48912048339844,
          35.71245574951172,
          39.10331344604492,
          44.1232795715332,
          44.0257568359375,
          34.58718490600586,
          46.512596130371094,
          40.70456314086914,
          48.20161437988281,
          31.547466278076172,
          38.86981201171875,
          46.98457336425781,
          42.41729736328125,
          53.70452117919922,
          56.52758026123047,
          40.32113265991211,
          46.01039123535156,
          40.31570816040039,
          47.74361038208008,
          48.419002532958984,
          46.496673583984375,
          40.94441223144531,
          43.24276351928711,
          41.47795867919922,
          31.176244735717773,
          41.55888748168945,
          51.53995895385742,
          37.420162200927734,
          37.714176177978516,
          46.04517364501953,
          30.138565063476562,
          37.345008850097656,
          49.974674224853516,
          41.29624938964844,
          44.646183013916016,
          50.23835372924805,
          35.04751968383789,
          42.051170349121094,
          33.65348815917969,
          43.90343475341797,
          36.01350021362305,
          41.341224670410156,
          47.02928161621094,
          29.5736026763916,
          40.01055908203125,
          55.20870590209961,
          42.74920654296875,
          43.034912109375,
          30.46860122680664,
          46.440528869628906,
          40.89402770996094,
          41.94792938232422,
          48.38838195800781,
          38.207908630371094,
          40.2993278503418,
          46.48934555053711,
          31.490604400634766,
          46.409080505371094,
          35.40352249145508,
          46.666988372802734,
          39.279170989990234,
          47.550941467285156,
          37.320518493652344,
          36.720375061035156,
          46.6643180847168,
          35.85360336303711,
          43.97380065917969,
          41.243927001953125,
          40.868831634521484,
          46.5894889831543,
          46.5196418762207,
          40.569114685058594,
          39.50584411621094,
          40.722747802734375,
          43.71678924560547,
          46.651123046875,
          46.291175842285156,
          34.87496566772461,
          34.73223114013672,
          42.356651306152344,
          45.97020721435547,
          38.47510528564453,
          46.06108093261719,
          35.815155029296875,
          57.50050354003906,
          47.72861099243164,
          46.840782165527344,
          41.712642669677734,
          27.77434730529785,
          40.44022750854492,
          45.965576171875,
          36.88811492919922,
          36.375003814697266,
          37.672874450683594,
          48.57726287841797,
          35.01664352416992,
          44.313106536865234,
          42.81230163574219,
          44.03773498535156,
          35.75327682495117,
          37.226680755615234,
          46.72346496582031,
          51.81199264526367,
          41.28409957885742,
          37.136940002441406,
          38.06620788574219,
          39.62549591064453,
          35.55390930175781,
          36.90899658203125,
          50.36772918701172,
          37.83878707885742,
          45.87983322143555,
          43.146148681640625,
          46.580474853515625,
          41.33000183105469,
          45.10321807861328,
          31.304988861083984,
          39.99203872680664,
          42.77149200439453,
          46.64356994628906,
          45.732852935791016,
          41.850799560546875,
          33.61125564575195,
          44.55282974243164,
          50.87578201293945,
          45.68582534790039,
          47.339237213134766,
          45.335697174072266,
          36.56114959716797,
          48.587127685546875,
          35.08022689819336,
          43.7884407043457,
          47.155357360839844,
          39.44483947753906,
          35.333309173583984,
          44.4697380065918,
          37.66122055053711
         ],
         "z": [
          22.193557739257812,
          52.103328704833984,
          44.45167541503906,
          57.490360260009766,
          54.05018997192383,
          19.254249572753906,
          52.584556579589844,
          57.532020568847656,
          52.157684326171875,
          42.83113479614258,
          30.18817901611328,
          56.5439453125,
          20.549379348754883,
          45.621864318847656,
          24.788070678710938,
          48.4952507019043,
          20.65286636352539,
          18.4217586517334,
          47.556053161621094,
          60.594486236572266,
          32.907779693603516,
          18.316003799438477,
          58.568504333496094,
          36.735355377197266,
          32.667755126953125,
          52.624813079833984,
          34.94943618774414,
          46.262630462646484,
          32.53449249267578,
          22.33917236328125,
          24.079315185546875,
          38.47138214111328,
          53.7990608215332,
          40.1561393737793,
          49.5574951171875,
          38.275917053222656,
          20.011463165283203,
          42.44136047363281,
          50.721527099609375,
          37.238502502441406,
          47.843589782714844,
          54.00732421875,
          52.862239837646484,
          59.41511917114258,
          58.61671447753906,
          47.880523681640625,
          55.199241638183594,
          35.15815734863281,
          56.23877716064453,
          24.319005966186523,
          59.047977447509766,
          48.49564743041992,
          27.721223831176758,
          21.08197021484375,
          33.30112075805664,
          33.13115692138672,
          19.190397262573242,
          21.290863037109375,
          33.61766052246094,
          42.817230224609375,
          36.34144592285156,
          48.46647644042969,
          18.94736671447754,
          44.631614685058594,
          47.14879608154297,
          50.841880798339844,
          30.019149780273438,
          29.839557647705078,
          56.1619758605957,
          47.74675369262695,
          48.1157341003418,
          50.47563552856445,
          31.519533157348633,
          45.03913879394531,
          51.04473876953125,
          52.6759147644043,
          44.3145751953125,
          38.092018127441406,
          23.688243865966797,
          51.76841354370117,
          58.06892395019531,
          46.101715087890625,
          24.113811492919922,
          47.848052978515625,
          23.6416015625,
          31.04852867126465,
          43.92878723144531,
          41.31407928466797,
          32.036216735839844,
          53.47500991821289,
          45.18617248535156,
          56.01960754394531,
          20.066648483276367,
          57.251468658447266,
          44.189937591552734,
          53.102256774902344,
          59.319122314453125,
          36.46009826660156,
          54.34529113769531,
          44.066200256347656,
          41.53694534301758,
          21.762239456176758,
          51.322444915771484,
          36.24159622192383,
          51.20326614379883,
          44.2296257019043,
          59.03960418701172,
          59.601165771484375,
          40.729515075683594,
          59.68666076660156,
          52.989173889160156,
          35.12479019165039,
          35.08455276489258,
          24.071277618408203,
          30.338970184326172,
          25.646018981933594,
          48.83964920043945,
          56.35067367553711,
          54.103790283203125,
          37.917057037353516,
          50.154510498046875,
          25.191287994384766,
          47.155677795410156,
          25.61989402770996,
          25.189315795898438,
          57.91245651245117,
          32.24742126464844,
          48.732810974121094,
          46.13508224487305,
          49.6789665222168,
          61.170806884765625,
          46.532745361328125,
          39.86209487915039,
          22.756826400756836,
          36.14794921875,
          58.59805679321289,
          40.56991958618164,
          49.040714263916016,
          53.45930480957031,
          61.85067367553711,
          45.53545379638672,
          46.8137321472168,
          33.17449188232422,
          17.593097686767578,
          35.01071548461914,
          40.86245346069336,
          60.07341003417969,
          51.45808029174805,
          58.89654541015625,
          44.90623092651367,
          41.678592681884766,
          48.31040573120117,
          38.48843002319336,
          43.1740837097168,
          41.91838073730469,
          43.59067153930664,
          42.098838806152344,
          37.21451187133789,
          37.39351272583008,
          58.88178634643555,
          55.70391082763672,
          48.24186325073242,
          58.97999572753906,
          50.80073928833008,
          28.605117797851562,
          49.67588806152344,
          20.9411563873291,
          60.01853561401367,
          41.5800666809082,
          35.17058563232422,
          39.74538040161133,
          46.620811462402344,
          48.552764892578125,
          50.006683349609375,
          19.13212776184082,
          47.93800354003906,
          48.85979461669922,
          58.12385559082031,
          48.238590240478516,
          44.908382415771484,
          36.83709716796875,
          53.69363784790039,
          19.34170913696289,
          53.79423141479492,
          19.84146499633789,
          43.112674713134766,
          19.836034774780273,
          22.518951416015625,
          29.83972930908203,
          44.088104248046875,
          26.64124298095703,
          31.14148712158203,
          52.60420227050781,
          58.507286071777344,
          52.716529846191406,
          23.54427719116211,
          38.90258026123047,
          43.58125305175781,
          53.043922424316406,
          45.280357360839844,
          46.953311920166016,
          59.728755950927734,
          55.776084899902344,
          49.386592864990234,
          50.65068817138672,
          53.670204162597656,
          42.287967681884766,
          39.077239990234375,
          44.26091003417969,
          54.450504302978516,
          26.174299240112305,
          42.89739990234375,
          41.30209732055664,
          57.60813522338867,
          32.67342758178711,
          43.58646011352539,
          21.14995002746582,
          21.000350952148438,
          27.147357940673828,
          39.58058166503906,
          42.62396240234375,
          47.27132797241211,
          36.16640853881836,
          55.25719451904297,
          19.70174789428711,
          56.62761688232422,
          25.43572235107422,
          27.05593490600586,
          54.45970153808594,
          19.391584396362305,
          40.14068603515625,
          47.899940490722656,
          57.91682434082031,
          43.37637710571289,
          18.646583557128906,
          46.40803527832031,
          29.931556701660156,
          59.142818450927734,
          40.645751953125,
          23.24624252319336,
          23.247560501098633,
          53.40648651123047,
          18.563583374023438,
          39.550636291503906,
          58.14215850830078,
          38.90829849243164,
          27.936580657958984,
          46.49946594238281,
          26.667373657226562,
          26.552875518798828,
          49.61832809448242,
          20.226701736450195,
          43.471580505371094,
          53.967918395996094,
          59.52709197998047,
          33.040321350097656,
          56.878387451171875,
          41.64270782470703,
          47.678916931152344,
          55.30523681640625,
          49.11747360229492,
          57.87743377685547,
          53.56418991088867,
          36.325687408447266,
          22.18682098388672,
          46.191795349121094,
          57.14760971069336,
          29.47769546508789,
          52.8796272277832,
          55.35072708129883,
          48.138267517089844,
          34.43967819213867,
          52.377906799316406,
          21.3181095123291,
          24.445894241333008,
          25.97493553161621,
          44.44764709472656,
          57.33849334716797,
          58.770843505859375,
          20.355695724487305,
          34.36695861816406,
          51.055824279785156,
          57.950374603271484,
          54.57706069946289,
          23.002601623535156,
          50.076473236083984,
          44.83853530883789,
          46.71403503417969,
          53.944061279296875,
          39.96580123901367,
          23.115966796875,
          35.25409698486328,
          36.5867805480957,
          51.452880859375,
          20.61266326904297,
          35.749454498291016,
          48.62664031982422,
          23.21011734008789,
          44.329864501953125,
          32.475460052490234,
          57.87436294555664,
          28.309978485107422,
          19.34662437438965,
          41.58477783203125,
          47.894691467285156,
          50.13564682006836,
          47.37464904785156,
          32.64488983154297,
          49.71677017211914,
          34.438514709472656,
          37.128013610839844,
          49.0293083190918,
          36.35331344604492,
          43.46053695678711,
          56.666358947753906,
          54.96890640258789,
          40.03911590576172,
          53.46612548828125,
          52.66375732421875,
          27.985782623291016,
          22.976333618164062,
          48.27013397216797,
          24.493104934692383
         ]
        }
       ],
       "layout": {
        "height": 600,
        "scene": {
         "domain": {
          "x": [
           0,
           0.2888888888888889
          ],
          "y": [
           0,
           1
          ]
         }
        },
        "scene2": {
         "domain": {
          "x": [
           0.35555555555555557,
           0.6444444444444445
          ],
          "y": [
           0,
           1
          ]
         }
        },
        "scene3": {
         "domain": {
          "x": [
           0.7111111111111111,
           1
          ],
          "y": [
           0,
           1
          ]
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Comparación de Gráficos 3D"
        },
        "width": 1800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear una figura con subplots\n",
    "fig = make_subplots(rows=1, cols=3, \n",
    "                    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "\n",
    "# Añadir el primer gráfico tridimensional\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=aux_plot['real_X'],\n",
    "    y=aux_plot['real_Y'],\n",
    "    z=aux_plot['real_Z'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color=aux_plot['USUM']),\n",
    "    name='Real'\n",
    "), row=1, col=1)\n",
    "\n",
    "# Añadir el segundo gráfico tridimensional\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=aux_plot['pred_X'],\n",
    "    y=aux_plot['pred_Y'],\n",
    "    z=aux_plot['pred_Z'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color=aux_plot['USUM']),\n",
    "    name='Predicción'\n",
    "), row=1, col=2)\n",
    "\n",
    "# Añadir el tercer gráfico tridimensional\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=aux_plot.loc[aux_plot[\"relativ_dif_real_pred_Y\"] > 0.1, 'pred_X'],\n",
    "    y=aux_plot.loc[aux_plot[\"relativ_dif_real_pred_Y\"] > 0.1, 'pred_Y'],\n",
    "    z=aux_plot.loc[aux_plot[\"relativ_dif_real_pred_Y\"] > 0.1, 'pred_Z'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color=aux_plot.loc[aux_plot[\"relativ_dif_real_pred_Y\"] > 0.1, 'USUM']),\n",
    "    name='Predicción con Diferencia > 0.1'\n",
    "), row=1, col=3)\n",
    "\n",
    "# Actualizar el layout de la figura\n",
    "fig.update_layout(height=600, width=1800, title_text=\"Comparación de Gráficos 3D\")\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(aux_plot,x=\"real_X\",y=\"real_Y\",z=\"real_Z\",color=\"USUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(aux_plot,x=\"pred_X\",y=\"pred_Y\",z=\"pred_Z\",color=\"USUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(aux_plot.loc[aux_plot[\"relativ_dif_real_pred_Y\"]>0.1],x=\"pred_X\",y=\"pred_Y\",z=\"pred_Z\",color=\"USUM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y ahora con tema PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal  \n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self,layers,init_w=\"xavier\"):\n",
    "        super().__init__() \n",
    "        self.layers=layers\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linears = nn.ModuleList([GCNConv(layers[i], layers[i+1]) for i in range(len(layers) - 1)])\n",
    "    \n",
    "        if init_w==\"xavier\":\n",
    "            #Xavier Normal Initialization\n",
    "            for i in range(len(layers)-1):\n",
    "                nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "                \n",
    "                # set biases to zero\n",
    "                nn.init.zeros_(self.linears[i].bias.data)\n",
    "        # elif init_w==\"gorot\":\n",
    "        #         nn.init. (self.linears[i].weight.data, gain=1.0)\n",
    "                \n",
    "        #         # set biases to zero\n",
    "        #         nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "\n",
    "    def forward(self,data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "              \n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)             \n",
    "\n",
    "        #xn = normalize(x, Pos_min, Pos_max)\n",
    "\n",
    "        # convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "\n",
    "        # inpunt and hidden layers forward computation\n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = self.linears[i](a,edge_index)          \n",
    "            a = self.activation(z)\n",
    "\n",
    "        # output layer forward computation            \n",
    "        b = self.linears[-1](a)\n",
    "        \n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(DNN):\n",
    "    '''\n",
    "        Esta clase hace el registro de todos los parámetros, los cálculos de derivadas y de losses\n",
    "        Y aplica las restricciones que hagan falta\n",
    "    '''\n",
    "    def __init__(self, layers,init_values=None,device=None,separate_data_losses=True,loss_weights={\"Data\":1,\"PDE\":1,\"BC\":1}):\n",
    "        super().__init__(layers)\n",
    "        \n",
    "        if device is None:\n",
    "            self.device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device=device\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "        #History of losses\n",
    "        self.loss_history = {\"Data\": [],\n",
    "                             \"PDE\": [],\n",
    "                             \"BC\": [],\n",
    "                             \"Total\":[]}\n",
    "         #Parameters trials\n",
    "        self.params_history = { \"E\": [] ,\n",
    "                                \"alpha\": []}\n",
    "\n",
    "        #inicialización de parametros\n",
    "        self.nu = init_values[0]\n",
    "        self.alpha = nn.Parameter(torch.tensor(init_values[2],dtype=torch.float32).to(self.device))\n",
    "        self.E_ref = init_values[1]\n",
    "        self.E = torch.tensor((1+self.alpha)*self.E_ref,dtype=torch.float32).to(self.device)\n",
    "        self.separate_data_losses=separate_data_losses\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        self.w_data=loss_weights[\"Data\"]\n",
    "        self.w_PDE=loss_weights[\"PDE\"]\n",
    "        self.w_BC=loss_weights[\"BC\"]\n",
    "        self.iter_n=0\n",
    "\n",
    "    def compute_XYZ(self, positions):\n",
    "        # clone the input data and add AD\n",
    "        pos = positions.clone().to(self.device)\n",
    "        X = pos[:,0].reshape(-1,1)\n",
    "        Y = pos[:,1].reshape(-1,1)\n",
    "        Z = pos[:,2].reshape(-1,1)\n",
    "        return X, Y, Z\n",
    "\n",
    "    def compute_displacements(self,X, Y, Z):\n",
    "        XYZ = torch.cat((X,Y,Z), dim=1).to( self.device )\n",
    "        # Compute the output of the DNN\n",
    "        U = self(XYZ)\n",
    "        # Separating vector of directional displacements\n",
    "        u = U[:,0].reshape(-1,1)\n",
    "        v = U[:,1].reshape(-1,1)\n",
    "        w = U[:,2].reshape(-1,1)\n",
    "        return u, v, w\n",
    "\n",
    "    # Los cálculos de las funciones de pérdida de hacen 1 vez, y luego ya vemos que hacemos\n",
    "    # con esos datos, si usar pesos, si usar otras cosas...\n",
    "\n",
    "    def loss_PDE(self, collocation_points, save = True):\n",
    "        # estos son los puntos en los que imponemos las leyes físicas  \n",
    "        # puede ser la ecuación de equilibrio, pero también pueden ser otras relaciones\n",
    "        # que conozcamos entre inputs y outputs, como por ejemplo, relaciones termodinámicas etc..\n",
    "        # si el output fuera posicion y tensión, entonces podriamos imponer la ley de hooke sobre este\n",
    "        # input.\n",
    "        X, Y, Z = self.compute_XYZ(collocation_points)\n",
    "        u, v, w = self.compute_displacements(X,Y,Z)\n",
    "\n",
    "\n",
    "        # y ahora tenemos que obtener las derivadas necesarias para aplicar la eq de equilibrio\n",
    "        epsilon = self.compute_strain(X,Y,Z,u, v, w)\n",
    "        sigma = self.compute_stress(epsilon)\n",
    "        div_sigma = self.divergence(sigma,X, Y, Z)\n",
    "\n",
    "        value_loss_PDE=self.loss_function(div_sigma,torch.zeros_like(div_sigma).to(self.device))\n",
    "        \n",
    "        if save:\n",
    "            self.loss_history[\"PDE\"].append(value_loss_PDE.item())#.to('cpu').detach().numpy())\n",
    "        \n",
    "        return value_loss_PDE\n",
    "    \n",
    "    def loss_BC(self,pos_reales,sigmas_reales,save=True):\n",
    "        #aqui tenemos las de Dirichlet y las de Neumann\n",
    "        # en mi caso que no hay contacto ni nada, pero si tengo nodos fijos!! las de Dirichlet tbn\n",
    "\n",
    "        #en esta implementación voy a imponer las sigmas solo aqui, en una futura, podría meter también\n",
    "        #las de Dirichlet para poder darles mas peso\n",
    "\n",
    "        #calculamos sobre las posiciones de las sigmas que tenemos, las dadas por el modelo\n",
    "        # predict U\n",
    "        X,Y,Z=self.compute_XYZ(pos_reales)\n",
    "        u, v, w=self.compute_displacements(X,Y,Z)\n",
    "        \n",
    "        epsilon = self.compute_strain(X,Y,Z,u, v, w)\n",
    "        sigma = self.compute_stress(epsilon) \n",
    "        #este sigma tiene primera dimension batchsize, y segunda dimension 6\n",
    "        #que corresponde con: s11,s22,s33,s23,s13,s12\n",
    "        #le imponemos que sean iguales de modo que aplicamos la loss y ya\n",
    "        value_loss_BC=self.loss_function(sigma,sigmas_reales)\n",
    "\n",
    "        if save:\n",
    "            self.loss_history[\"BC\"].append(value_loss_BC.item())#.to('cpu').detach().numpy())\n",
    "\n",
    "        return value_loss_BC\n",
    "\n",
    "    #def loss_DBC(fix_nodes)\n",
    "    #   pass\n",
    "\n",
    "    def loss_data(self, pos_reales,desp_reales,save=True):\n",
    "        #pos_reales=pos_reales.to(device)\n",
    "        u_predict=self(pos_reales)\n",
    "        if self.separate_data_losses:\n",
    "            sepatared_loss=torch.nn.MSELoss(reduction=\"none\")\n",
    "            #aux=self.loss_function(u_predict,desp_reales)\n",
    "            #esto nos devuelve la diferencia cuadrática de cada elemento, para evaluarlos por\n",
    "            #separado, vamos a hacer la media en columnas\n",
    "            # x_mse,y_mse,z_mse=torch.mean(aux,axis=0)\n",
    "            # x_mse,y_mse,z_mse=torch.sqrt(x_mse),torch.sqrt(y_mse),torch.sqrt(z_mse)\n",
    "            value_loss_data=torch.mean(torch.sqrt(torch.mean(sepatared_loss(u_predict,desp_reales),axis=0)))\n",
    "        else:\n",
    "            value_loss_data=self.loss_function(u_predict,desp_reales)\n",
    "\n",
    "        if save:\n",
    "            self.loss_history[\"Data\"].append(value_loss_data.item())#.to('cpu').detach().numpy())\n",
    "        \n",
    "        return value_loss_data\n",
    "    \n",
    "    def loss(self, pos_data,desp_data,pos_colloc,pos_BC,sigmas_BC,save=True):\n",
    "        #esto hace que se calculen todas las losses de la pinn\n",
    "        #además, si tenemos un parámetro E, este se actualizará, pe si estamos actualizando alpha en lugar\n",
    "        #de E, despues de la recalculación tendremos que actualizar E. \n",
    "\n",
    "        #actualizamos E antes de calcular nada\n",
    "\n",
    "        self.E=(1+self.alpha)*self.E_ref\n",
    "\n",
    "        value_loss_PCE=self.loss_PDE(pos_colloc,save=save)\n",
    "        value_loss_BC=self.loss_BC(pos_BC,sigmas_BC,save=save)\n",
    "        value_loss_data=self.loss_data(pos_data,desp_data,save=save)  \n",
    "        value_loss= self.w_data*value_loss_data + self.w_PDE*value_loss_PCE+ self.w_BC*value_loss_BC\n",
    "\n",
    "\n",
    "        if save: \n",
    "            #self.params_history[\"nu\"].append(self.nu)#self.nu.to('cpu').detach().numpy())\n",
    "            self.params_history[\"E\"].append(self.E.to('cpu').detach().numpy())\n",
    "            self.params_history[\"alpha\"].append(self.alpha.to('cpu').detach().numpy())\n",
    "            self.loss_history[\"Total\"].append(value_loss.item())#.to('cpu').detach().numpy())\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    def compute_gradU(self, X, Y, Z, U, V, W):\n",
    "\n",
    "        # Compute the gradient of U\n",
    "        Ux,Uy,Uz = autograd.grad(U, (X,Y,Z), torch.ones([X.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)\n",
    "        # Uy = autograd.grad(U, Y, torch.ones([Y.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "        # Uz = autograd.grad(U, Z, torch.ones([Z.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # Compute the gradient of V\n",
    "        Vx,Vy,Vz = autograd.grad(V, (X,Y,Z), torch.ones([X.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)\n",
    "        # Vy = autograd.grad(V, Y, torch.ones([Y.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "        # Vz = autograd.grad(V, Z, torch.ones([Z.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        # Compute the gradient of W\n",
    "        Wx,Wy,Wz = autograd.grad(W, (X,Y,Z), torch.ones([X.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)\n",
    "        # Wy = autograd.grad(W, Y, torch.ones([Y.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "        # Wz = autograd.grad(W, Z, torch.ones([Z.shape[0], 1]).to(self.device),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        grad_u = torch.cat((Ux , Uy, Uz), dim=1).to(torch.float32)\n",
    "        grad_v = torch.cat((Vx , Vy, Vz), dim=1).to(torch.float32)\n",
    "        grad_w = torch.cat((Wx , Wy, Wz), dim=1).to(torch.float32)\n",
    "\n",
    "        gradU = torch.cat((grad_u, grad_v, grad_w), dim=1).to(torch.float32).reshape(-1,3,3)\n",
    "\n",
    "        return gradU\n",
    "\n",
    "    def compute_strain(self, X,Y,Z,u, v, w):\n",
    "        # Compute strain components using autograd\n",
    "        nabla_U = self.compute_gradU(X,Y,Z,u, v, w).squeeze()\n",
    "        strain = 0.5 * (nabla_U + nabla_U.swapaxes(1,2))\n",
    "        return strain\n",
    "\n",
    "\n",
    "    def compute_stress(self,strain):\n",
    "        strain_flat=strain[:,(0,1,2,1,0,0),(0,1,2,2,2,1)]*torch.tensor([1,1,1,2,2,2],dtype=torch.float32).to(self.device)\n",
    "        #strain_flat=strain[:,(0,1,2,0,0,1),(0,1,2,1,2,2)] # e00,e11,e22,e01,e02,e12\n",
    "        self.C=(self.E/((1+self.nu)*(1-2*self.nu)))*torch.tensor(\n",
    "                [[1-self.nu,self.nu,self.nu,0,0,0],\n",
    "                [self.nu,1-self.nu,self.nu,0,0,0],\n",
    "                [self.nu,self.nu,1-self.nu,0,0,0],\n",
    "                [0,0,0,(1-2*self.nu)/2,0,0],\n",
    "                [0,0,0,0,(1-2*self.nu)/2,0],\n",
    "                [0,0,0,0,0,(1-2*self.nu)/2]]).float().to(self.device)\n",
    "\n",
    "        return torch.matmul(self.C,strain_flat.T.float()).T.squeeze() #s11,s22,s33,s23,s13,s12\n",
    "\n",
    "    def divergence(self,sigma, X, Y, Z):\n",
    "        div_T = torch.zeros(sigma.shape[0],3)\n",
    "        div_T[:,0]=autograd.grad(sigma[:,0],X,grad_outputs=torch.ones_like(sigma[:,0]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,5],Y,grad_outputs=torch.ones_like(sigma[:,5]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,4],Z,grad_outputs=torch.ones_like(sigma[:,4]).to(self.device),retain_graph=True)[0].squeeze()\n",
    "        div_T[:,1]=autograd.grad(sigma[:,5],X,grad_outputs=torch.ones_like(sigma[:,5]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,1],Y,grad_outputs=torch.ones_like(sigma[:,1]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,3],Z,grad_outputs=torch.ones_like(sigma[:,3]).to(self.device),retain_graph=True)[0].squeeze()\n",
    "        div_T[:,2]=autograd.grad(sigma[:,4],X,grad_outputs=torch.ones_like(sigma[:,4]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,3],Y,grad_outputs=torch.ones_like(sigma[:,3]).to(self.device),retain_graph=True)[0].squeeze()+autograd.grad(sigma[:,2],Z,grad_outputs=torch.ones_like(sigma[:,2]).to(self.device),retain_graph=True)[0].squeeze()\n",
    "\n",
    "        return div_T\n",
    "    \n",
    "\n",
    "    def step_closure(self,opt,train_init_pos_main2,train_disp_main2,return_colloc_points2,position_selected_stresses2,return_stress2):\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss = self.loss(train_init_pos_main2,train_disp_main2,return_colloc_points2,position_selected_stresses2,return_stress2)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter_n+=1\n",
    "        # Print material parameters and loss evolition\n",
    "        print(f'LBFGS iter: {self.iter_n}, Loss: {loss.item()}')\n",
    "            \n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tenemos que cargar correctamente los datos para usarlos\n",
    "data=Data(\"MULTIPLE_E_VALUES_NEW\",10,E=\"0.005\")\n",
    "data.load_data()\n",
    "train_init_pos_main,train_disp_main,test_init_pos_main,test_disp_main,position_selected_stresses,return_stress,return_colloc_points=data.prepare_pytorch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
