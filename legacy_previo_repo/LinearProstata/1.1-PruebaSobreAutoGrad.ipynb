{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import functional\n",
    "import torch \n",
    "import numpy as np \n",
    "import math \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(input):\n",
    "    x,y,z=input\n",
    "    return (x**2 * y**2 * x**2 ,x**2 +y**2 * z**2 +y , x*z*y**2+y*z*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4., grad_fn=<MulBackward0>),\n",
       " tensor(39., grad_fn=<AddBackward0>),\n",
       " tensor(18., grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "b=u(*a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ux(input):\n",
    "    x,y,z=input\n",
    "    return x**2 * y**2 * z**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176., grad_fn=<TraceBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.trace(functional.hessian(ux,torch.tensor([3.0,2.0,2.0],requires_grad=True),create_graph=True))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(c).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59., grad_fn=<TraceBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#la divergencia es la traza del jacobiano de modo que \n",
    "input_tensor=torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "div=torch.trace(torch.stack(functional.jacobian(u,input_tensor,create_graph=True),axis=0))\n",
    "div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([56., 39., 24.]),)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(div, input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuaciones del equilibrio  \n",
    "$$\\nabla^2\\bold{u} + \\frac{1}{1-2\\nu}\\nabla(\\nabla \\cdot \\bold{u}) = 0$$  \n",
    "\n",
    "$$\\nabla^2\\bold{u} = (\\nabla^2 u_x , \\nabla^2 u_y,\\nabla^2 u_z)$$\n",
    "\n",
    "$$\\nabla(\\nabla \\cdot \\bold{u})=\\nabla(\\frac{\\partial}{\\partial x}u_x+\\frac{\\partial}{\\partial y}u_y + \\frac{\\partial}{\\partial z}u_z) = \\left( \\frac{\\partial}{\\partial x}(\\frac{\\partial}{\\partial x}u_x+\\frac{\\partial}{\\partial y}u_y + \\frac{\\partial}{\\partial z}u_z) , \\frac{\\partial}{\\partial y}(\\frac{\\partial}{\\partial x}u_x+\\frac{\\partial}{\\partial y}u_y + \\frac{\\partial}{\\partial z}u_z), \\frac{\\partial}{\\partial z}(\\frac{\\partial}{\\partial x}u_x+\\frac{\\partial}{\\partial y}u_y + \\frac{\\partial}{\\partial z}u_z) \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el hessiano y le jacobiano para hacer todo esto mejor con pytorch autograd.   \n",
    "\n",
    "Entonces, sea $\\bold{u}(x,y,z)$ la función que va ha imitar nuestra red neuronal. Es una función vectorial $\\bold{u}(x,y,z)=(u_x,u_y,u_z)$.  \n",
    "Sea $J\\bold{u}$ el jacobiano de esta función con respecto a las entradas $(x,y,z)$. Y sea el hessiano de cada uno de los componentes $Hess \\bold{u}_i$ con $i=x,y,z$   \n",
    "El jacobiano por un lado, representa las derivadas de todas las salidas con respecto a todas las entradas.  \n",
    "Y el Hessiano representa las derivadas segundas de un componente de la salida, con respecto a todas las combinaciones de las entradads.    \n",
    "\n",
    "\n",
    "La forma de relacionar esto con lo anterior es como sigue:     \n",
    "$$\\nabla^2\\bold{u} = (\\nabla^2 u_x , \\nabla^2 u_y,\\nabla^2 u_z)= ( \\text{tr}(\\it{Hess \\ u_x}),\\text{tr}(\\it{Hess \\ u_y}),\\text{tr}(\\it{Hess \\ u_z}))$$   \n",
    "\n",
    "$$\\nabla(\\nabla \\cdot \\bold{u})=\\nabla(\\frac{\\partial}{\\partial x}u_x+\\frac{\\partial}{\\partial y}u_y + \\frac{\\partial}{\\partial z}u_z) = \\nabla ( \\text{tr}(J \\ \\bold{u}) ) $$\n",
    "\n",
    "En resumen, las ecuaciones del equilibrio quedan (son 3 ):  \n",
    "\n",
    "$$( \\text{tr}(\\it{Hess \\ u_x}),\\text{tr}(\\it{Hess \\ u_y}),\\text{tr}(\\it{Hess \\ u_z})) + \\nabla ( \\text{tr}(J \\ \\bold{u}) ) = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
